[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Computing using R and Python",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#content-overload",
    "href": "index.html#content-overload",
    "title": "Statistical Computing using R and Python",
    "section": "Content Overload!",
    "text": "Content Overload!\nThis book is designed to demonstrate introductory statistical programming concepts and techniques. It is intended as a substitute for hours and hours of video lectures - watching someone code and talk about code is not usually the best way to learn how to code. It’s far better to learn how to code by … coding.\nI hope that you will work through this book week by week over the semester. I have included comics, snark, gifs, YouTube videos, extra resources, and more: my goal is to make this a collection of the best information I can find on statistical programming.\nIn most cases, this book includes way more information than you need. Everyone comes into this class with a different level of computing experience, so I’ve attempted to make this book comprehensive. Unfortunately, that means some people will be bored and some will be overwhelmed. Use this book in the way that works best for you - skip over the stuff you know already, ignore the stuff that seems too complex until you understand the basics. Come back to the scary stuff later and see if it makes more sense to you."
  },
  {
    "objectID": "index.html#book-format-guide",
    "href": "index.html#book-format-guide",
    "title": "Statistical Computing using R and Python",
    "section": "Book Format Guide",
    "text": "Book Format Guide\nI’ve made an effort to use some specific formatting and enable certain features that make this book a useful tool for this class.\nButtons/Links\nThe book contains a number of features which should help you navigate, use, improve, and respond to the textbook.\n\n\nTextbook features, menus, and interactive options\n\n\nSpecial Sections\nSome instructions depend on your operating system. Where it’s shorter, I will use tabs to provide you with OS specific instructions. Here are the icons I will use:\n\n\n Windows\n Mac\n Linux\n\n\n\nWindows-specific instructions\n\n\nMac specific instructions\n\n\nLinux specific instructions. I will usually try to make this generic, but if it’s gui based, my instructions will usually be for KDE.\n\n\n\n\n\n\n\n\n\nWarnings\n\n\n\nThese sections contain things you may want to look out for: common errors, mistakes, and unfortunate situations that may arise when programming.\n\n\n\n\n\n\n\n\nDemonstrations\n\n\n\nThese sections demonstrate how the code being discussed is used (in a simple way).\n\n\n\n\n\n\n\n\nExamples\n\n\n\nThese sections contain illustrations of the concepts discussed in the chapter. Don’t skip them, even though they may be long!\n\n\n\n\n\n\n\n\nTry it out\n\n\n\nThese sections contain activities you should do to reinforce the things you’ve just read. You will be much more successful if you read the material, review the example, and then try to write your own code. Most of the time, these sections will have a specific format:\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe problem will be in the first tab for you to start with\n\n\nA solution will be provided in R, potentially with an explanation.\n\n\nA solution will be provided in Python as well.\n\n\n\nIn some cases, the problem will be more open-ended and may not adhere to this format, but most try it out sections in this book will have solutions provided. I highly recommend that you attempt to solve the problem yourself before you look at the solutions - this is the best way to learn. Passively reading code does not result in information retention.\n\n\n\n\n\n\n\n\nEssential Reading\n\n\n\nThese sections may direct you to additional reading material that is essential for understanding the topic. For instance, I will sometimes link to other online textbooks rather than try to rehash the content myself when someone else has done it better.\n\n\n\n\n\n\n\n\nLearn More\n\n\n\nThese sections will direct you to additional resources that may be helpful to consult as you learn about a topic. You do not have to use these sections unless you are 1) bored, or 2) hopelessly lost. They’re provided to help but are not expected reading (Unlike the essential reading sections in red).\n\n\n\n\n\n\n\n\nNotes\n\n\n\nThese generic sections contain information I may want to call attention to, but that isn’t necessarily urgent or a common error trap.\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nThese sections are intended to apply to more advanced courses. If you are taking an introductory course, feel free to skip that content for now.\n\n\nExpandable Sections\nThese are expandable sections, with additional information when you click on the line\nThis additional information may be information that is helpful but not essential, or it may be that an example just takes a LOT of space and I want to make sure you can skim the book without having to scroll through a ton of output.\n\n\n\n\n\n\nAnother type of expandable note\n\n\n\n\n\nAnswers or punchlines may be hidden in this type of expandable section as well."
  },
  {
    "objectID": "index.html#analytics",
    "href": "index.html#analytics",
    "title": "Statistical Computing using R and Python",
    "section": "Analytics",
    "text": "Analytics\nI have enabled Google Analytics on this site for the purposes of measuring this work’s impact and use both in my own classes and elsewhere. I’m not using the individual tracking/ad-targeting settings (to the best of my knowledge) - my only purpose in using Google Analytics is to assess how often this site is used, and where its’ users are located at a rough (state/regional) level.\nIf you are using this site and aren’t affiliated with the University of Nebraska Lincoln, or have found it useful, please let me know by making a comment in Giscus (below) or sending me an email! These affirmations help me make a case that spending time on this resource is actually a good investment."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical Computing using R and Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe cover of this book is an amalgam of different images by the lovely @allison_horst, which are released under the cc-by 4.0 license. I have modified them to remove most of the R package references and arrange them to represent the topics covered in this book.\nLaptop icon used in the tab/logo created by Good Ware - Flaticon\nThroughout this book, I have borrowed liberally from other online tutorials, published books, and blog posts. I have tried to ensure that I link to the source material throughout the book and provide appropriate credit to anyone whose examples I have used, modified, or repurposed. Special thanks to the tutorials provided by Posit/RStudio and the tidyverse project."
  },
  {
    "objectID": "part-tools/00-tools-intro.html",
    "href": "part-tools/00-tools-intro.html",
    "title": "Part I: Tools",
    "section": "",
    "text": "This part of the textbook provides an overview of the different tools we will be using: R, python, quarto, markdown, pandoc, consoles, and so on. It can be a bit confusing at first, especially if you’re not familiar with how your computer works, where files are stored, and different ways to tell your computer what to do.\nChapter 1 gives you some important background material about how a computer functions.\nChapter 2 tells you exactly what software you need to install for the rest of this textbook.\nChapter 3 discusses the different ways we can talk to R and python, and the pros and cons of each."
  },
  {
    "objectID": "part-tools/01-computer-basics.html#fa-bullseye-objectives",
    "href": "part-tools/01-computer-basics.html#fa-bullseye-objectives",
    "title": "1  Computer Basics",
    "section": "1.1  Objectives",
    "text": "1.1  Objectives\n\nKnow the meaning of computer hardware and operating system terms such as hard drive, memory, CPU, OS/operating system, file system, directory, and system paths\nUnderstand the basics of how the above concepts relate to each other and contribute to how a computer works\nUnderstand the file system mental model for computers"
  },
  {
    "objectID": "part-tools/01-computer-basics.html#hardware",
    "href": "part-tools/01-computer-basics.html#hardware",
    "title": "1  Computer Basics",
    "section": "1.2 Hardware",
    "text": "1.2 Hardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). Most of the other details aren’t necessary (for now).\n\n\nChapter 1 of Python for Everybody - Computer hardware architecture"
  },
  {
    "objectID": "part-tools/01-computer-basics.html#operating-systems",
    "href": "part-tools/01-computer-basics.html#operating-systems",
    "title": "1  Computer Basics",
    "section": "1.3 Operating Systems",
    "text": "1.3 Operating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time."
  },
  {
    "objectID": "part-tools/01-computer-basics.html#file-systems",
    "href": "part-tools/01-computer-basics.html#file-systems",
    "title": "1  Computer Basics",
    "section": "1.4 File Systems",
    "text": "1.4 File Systems\n\nFor this class, it will probably be important to distinguish between local file storage (C:/ drive , /user/your-name/ , or /home/your-name/ ) and network/virtual file systems, such as OneDrive and iCloud. Over time, it has become harder to ensure that you are working on a local machine, but working “in the cloud” can cause odd errors when programming and in particular when working with version control systems1.\nYou want to save your files in this class to your physical hard drive. This will save you a lot of troubleshooting time.\n\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore. This article [1] is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\n\nThat’s not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders).\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your “base folder” is known as your working directory - the place your program thinks of as home."
  },
  {
    "objectID": "part-tools/01-computer-basics.html#system-paths",
    "href": "part-tools/01-computer-basics.html#system-paths",
    "title": "1  Computer Basics",
    "section": "1.5 System Paths",
    "text": "1.5 System Paths\nWhen you install software, it is saved in a specific location on your computer, like C:/Program Files/ on , /Applications/ on , or /usr/local/bin/ on . For the most part, you don’t need to keep track of where programs are installed, because the install process (usually) automatically creates icons on your desktop or in your start menu, and you find your programs there.\nUnfortunately, that isn’t sufficient when you’re programming, because you may need to know where a program is in order to reference that program – for instance, if you need to pop open a browser window as part of your program, you’re (most likely) going to have to tell your computer where that browser executable file lives.\nTo simplify this process, operating systems have what’s known as a “system path” or “user path” - a list of folders containing important places to look for executable and other important files. You may, at some point, have to edit your system path to add a new folder to it, making the executable files within that folder more easily available.\n\n\n\n\n\n\nHow To Modify System Paths\n\n\n\n\n\nHow to set system paths (general)\nOperating-system specific instructions cobbled together from a variety of different sources:\n\n On Windows\n On Mac\n On Linux\n\n\n\n\nIf you run across an error that says something along the lines of\n\ncould not locate xxx.exe\nThe system cannot find the path specified\nCommand Not Found\n\nyou might start thinking about whether your system path is set correctly for what you’re trying to do.\nIf you want to locate where an executable is found (in this example, we’ll use git), you can run where git on windows, or which git on OSX/Linux.\nSome programs, like RStudio, have places where you can set the locations of common dependencies. If you go to Tools > Global Options > Git/SVN, you can set the path to git."
  },
  {
    "objectID": "part-tools/01-computer-basics.html#part-tools-01-refs",
    "href": "part-tools/01-computer-basics.html#part-tools-01-refs",
    "title": "1  Computer Basics",
    "section": "1.6 References",
    "text": "1.6 References\n\n\n\n\n[1] D. Robitzski, “Gen z kids apparently don’t understand how file systems work. Futurism,” Sep. 24, 2021. [Online]. Available: https://futurism.com/the-byte/gen-z-kids-file-systems. [Accessed: Jan. 09, 2023]"
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#fa-bullseye-objectives",
    "href": "part-tools/02-setting-up-computer.html#fa-bullseye-objectives",
    "title": "2  Setting Up Your Computer",
    "section": "\n2.1  Objectives",
    "text": "2.1  Objectives\n\nSet up RStudio, R, Quarto, and python\nBe able to run demo code in R and python"
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#installation-process",
    "href": "part-tools/02-setting-up-computer.html#installation-process",
    "title": "2  Setting Up Your Computer",
    "section": "\n2.2 Installation Process",
    "text": "2.2 Installation Process\nIn this section, I will provide you with links to set up various programs on your own machine. If you have trouble with these instructions or encounter an error, post on the class message board or contact me for help.\n\n\nDownload and run the R installer for your operating system from CRAN:\n\n\n Windows: https://cran.rstudio.com/bin/windows/base/\n\n\n Mac: https://cran.rstudio.com/bin/macosx/\n\n\n Linux: https://cran.rstudio.com/bin/linux/ (pick your distribution)\n\nIf you are on  Windows, you should also install the Rtools4 package; this will ensure you get fewer warnings later when installing packages.\nMore detailed instructions for  Windows are available here\n\n\nDownload and install the latest version of python 3\n\n Windows: check the box that asks if you want to add Python to the system path. This will save you a lot of time and frustration.\nIf you’re interested in python, you should install Jupyter using the instructions here (I would just do pip3 install jupyterlab)\nWe will not use jupyter much in this book - I prefer quarto - but the python community has decided to distribute code primarily in jupyter notebooks, so having it on your machine may be useful so that you can run other people’s code.\nAdditional instructions for installing Python 3 from Python for Everybody if you have trouble.\n\n\nDownload and install the latest version of RStudio for your operating system. RStudio is a integrated development environment (IDE) for R - it contains a set of tools designed to make writing R code easier.\nDownload and install the latest version of Quarto for your operating system. Quarto is a command-line tool released by RStudio that allows Rstudio to work with python and other R specific tools in a unified way.\n\nThe following steps may be necessary depending on which class you’re in. If you want to be safe, go ahead and complete these steps as well.\n\nInstall git using the instructions here. Consult the troubleshooting guide if you have issues. If that fails, then seek help in office hours.\n\nInstall LaTeX and rmarkdown:\n\nLaunch R, and type the following commands into the console:\n\n\n\n\ninstall.packages(c(\"tinytex\", \"knitr\", \"rmarkdown\", \"quarto\"))\nlibrary(tinytex)\ninstall_tinytex()\n\n\n\n\n\n\n\nYour turn\n\n\n\nOpen RStudio on your computer and explore a bit.\n\nCan you find the R console? Type in 2+2 to make sure the result is 4.\nRun the following code in the R console:\n\ninstall.packages(\n  c(\"tidyverse\", \"rmarkdown\", \"knitr\", \"quarto\")\n)\n\nCan you find the text editor?\n\nCreate a new quarto document (File -> New File -> Quarto Document).\nPaste in the contents of this document.\nCompile the document (Ctrl/Cmd + Shift + K) and use the Viewer pane to see the result.\nIf this all worked, you have RStudio, Quarto, R, and Python set up correctly on your machine."
  },
  {
    "objectID": "part-tools/03-scripts-notebooks.html#fa-bullseye-objectives",
    "href": "part-tools/03-scripts-notebooks.html#fa-bullseye-objectives",
    "title": "3  Scripts and Notebooks",
    "section": "\n3.1  Objectives",
    "text": "3.1  Objectives\n\nUnderstand the different modes that can be used to interact with a programming language\nIdentify which mode and language is being used given a screenshot or other contextual information\nSelect the appropriate mode (interactive, script, notebook) for a given task given considerations such as target audience, human intervention, and need to repeat the analysis."
  },
  {
    "objectID": "part-tools/03-scripts-notebooks.html#a-short-history-of-talking-to-computers",
    "href": "part-tools/03-scripts-notebooks.html#a-short-history-of-talking-to-computers",
    "title": "3  Scripts and Notebooks",
    "section": "\n3.2 A Short History of Talking to Computers",
    "text": "3.2 A Short History of Talking to Computers\nThe fundamental goal of this chapter is to learn how to talk to R and Python. In the very beginning, people told computers what to do using punch cards [1]. This required that you have every step of your program and data planned out in advance - you’d submit your punch cards to the computer, and then come back 24-72 hours later to find out you’d gotten two cards out of order. Dropping a tray of punch cards was … problematic.\nThankfully, we’re mostly free of the days where being a bit clumsy could erase a semester of hard work. As things grew more evolved and we got actual monitors and (eventually) graphical interfaces, we started using interactive terminals (interactive mode) to boss computers around.\n\n\n\n\n\n\nYour Turn - Interactive Mode\n\n\n\n\n\nR\nPython\n\n\n\nOpen RStudio and navigate to the Console tab. You can issue commands directly to R by typing something in at the > prompt.\nTry typing in 2+2 and hit enter.\n\n\nOpen RStudio and navigate to the Terminal tab. This is your computer’s ‘terminal’ - where you tell the computer what to do.\nFirst, we have to tell it what language we’d like to work in - by default, it’s going to work in  Batch (Windows),  Zsh (Mac), or  Bash (Linux). Luckily, we can avoid these and tell the computer we want to work in python by typing in python3 or python (depending on how your computer is set up). This will launch an interactive python session (ipython).\nYou should get a prompt that looks like this: >>>\nType in 2+2 and hit enter.\n\n\n\n\n\nInteractive mode is useful for quick, one-off analyses, but if you need to repeat an analysis (or remember what you did), interactive mode is just awful. Once you close the program, the commands (and results) are gone. This is particularly inconvenient when you need to run the same task multiple times. For example, each day I may want to pull the weather forecast and observed weather values from the national weather service using the same commands. I don’t want to manually re-type them each day!\nTo somewhat address this issue, most computing languages allow you to provide a sequence of commands in a text file known as a script. Scripts are typically meant to run on their own - they may perform computations, format data and save it, scrape data from the web… the possibilities are endless, but they are typically meant to run without the person running the script having to read all of the commands.\n\n\n\n\n\n\nYour Turn - Terminal Mode\n\n\n\n\nDownload scripts.zip and unzip the file.\nOpen a system terminal in the directory where you unzipped the files.\nFollow the directions below exactly to ensure that you have the terminal open in the correct location.\n\n\n\n Windows\n Mac\n Linux\n\n\n\nOpen the folder. Type cmd into the location bar at the top of the window and hit enter. The command prompt will open in the desired location.\n\n\nOpen a finder window and navigate to the folder you want to use. If you don’t have a path bar at the bottom of the finder window, choose View > Show Path Bar. Control-click the folder in the path bar and choose Open in Terminal.\n\n\nOpen the folder in your file browser. Select the path to the folder in the path bar and copy it to the clipboard. Launch a terminal and type cd, and then paste the copied path. Hit enter. (There may be more efficient ways to do this, but these instructions work for most window managers).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor more information about how to use system terminals, see Section 27.1.\n\n\n\nNow, let’s try out script mode in R and Python!\n\n\n\nR\nPython\n\n\n\nThis assumes that the R binary has been added to your system path. If these instructions don’t work, please ask for help or visit office hours.\nIn the terminal, type Rscript words.R dickens-oliver-twist.txt\nYou should get some output that looks like this:\nuser@computer:~/scripts$ Rscript words.R dickens-oliver-twist.txt \ntext\n the  and        to   of    a  his   in   he  was \n8854 4902 4558 3767 3763 3569 2272 2224 1931 1684\n\n\nThis assumes that the python binary has been added to your system path. If these instructions don’t work, please ask for help or visit office hours.\nIn the terminal, type python3 words.py and hit Enter. You will be prompted for the file name. Enter dickens-oliver-twist.txt and hit Enter again.\nYou should get some output that looks like this:\nuser@computer:~/scripts$ python3 words.py \nEnter file:dickens-oliver-twist.txt\nthe 8854\n\n\n\n\n\nScripts, and compiled programs generated from scripts, are responsible for much of what you interact with on a computer or cell phone day-to-day. When the goal is to process a file or complete a task in exactly the same way each time, a script is the right choice for the job.\nHowever, when working with data, we sometimes prefer to combine scripts with interactive mode - that is, we use a script file to keep track of which commands we run, but we run the script interactively. About 60% of my day-to-day computing is done using R or python scripts that are run interactively.\n\n\n\n\n\n\nYour Turn - Script Mode\n\n\n\nIf you haven’t already, download scripts.zip and unzip the file.\nOpen RStudio and use RStudio to complete the following tasks.\n\n\nR\nPython\n\n\n\n\nUse RStudio to open the words-noinput.R file in the scripts folder you downloaded and unzipped.\nWhat do you notice about the appearance of the file? Is there an icon in the tab to tell you what type of file it is? Are some words in the file highlighted?\nCopy the path to the scripts folder.\nOS Specific Instructions:  Windows,  Mac,  Linux\nIn the R Console, type in setwd(\"<paste path here>\"), where you paste your file path from step 3 between the quotes. Hit enter.\nIn the words-noinput.R file, hit the “source” button in the top right. Do you get the same output that you got from running the file as a script from the terminal? Why do you think that is?\nClick on the last line of the file and hit Run (or Ctrl/Cmd + Enter). Do you get the output now?\nClick on the first line of the file and hit Run (or Ctrl/Cmd + Enter). This runs a single line of the file. Use this to run each line of the file in turn. What could you learn from doing this?\n\n\n\n\nUse RStudio or your preferred python editor to open the words-noinput.py file in the scripts folder you downloaded and unzipped.\nWhat do you notice about the appearance of the file? Is there an icon in the tab to tell you what type of file it is? Are some words in the file highlighted?\nCopy the path to the scripts folder.\nOS Specific Instructions:  Windows,  Mac,  Linux\nIn the R Console, type in setwd(\"<paste path here>\"), where you paste your file path from step 3 between the quotes. Hit enter.\nIn the words-noinput.py file, hit the “source” button in the top right. Do you get the same output that you got from running the file as a script from the terminal? What changes?\nClick on the first line of the file and hit Run (or Ctrl/Cmd + Enter). This runs a single line of the file. Use this to run each line of the file in turn. What do you learn from doing this?\n\n\n\n\n\n\nUsing scripts interactively allows us to see what is happening in the script step-by-step, and to examine the results during the program’s evaluation. This can be beneficial when applying a script to a new dataset, because it allows us to change things on the fly while still keeping the same basic order of operations."
  },
  {
    "objectID": "part-tools/03-scripts-notebooks.html#writing-code-for-people",
    "href": "part-tools/03-scripts-notebooks.html#writing-code-for-people",
    "title": "3  Scripts and Notebooks",
    "section": "\n3.3 Writing Code for People",
    "text": "3.3 Writing Code for People\nOne problem with scripts and interactive modes of using programming languages is that we’re spending most of our time writing code for computers to read – which doesn’t necessarily imply that our code is easy for humans to read.\nThere are two solutions to this problem, and I encourage you to make liberal use of both of them (together).\n\n3.3.1 Code Comments\nA comment is a part of computer code which is intended only for people to read. It is not evaluated or run by the computing language.\nTo “comment out” a single line of code in R or python, put a # (pound sign/hashtag) just before the part of the code you do not want to be evaluated.\n\n\n\n\n\n\nAdding Comments to Code\n\n\n\n\n\nR\nPython\n\n\n\n\n2 + 2 + 3\n\n[1] 7\n\n2 + 2 # + 3\n\n[1] 4\n\n# This line is entirely commented out\n\n\n\n\n2 + 2 + 3\n\n7\n\n2 + 2 # + 3\n# This line is entirely commented out\n\n4\n\n\n\n\n\n\n\nMany computing languages, such as Java, C/C++, and JavaScript have mechanisms to comment out an entire paragraph. Neither R nor Python has so-called “block comments” - instead, you can use keyboard shortcuts in RStudio to comment out an entire chunk of code (or text) using Ctrl/Cmd-Shift-C.\n\n3.3.2 Literate Programming - Notebooks and more!\nWhile code comments add human-readable text to code, scripts with comments are still primarily formatted for the computer’s convenience. However, most of the time spent on any given document is spent by people, not by computers. We often write parallel documents - user manuals, academic papers, tutorials, etc. which explain the purpose of our code and how to use it, but this can get clumsy over time, and requires updating multiple documents (sometimes in multiple places), which often leads to the documentation getting out-of-sync from the code.\nTo solve this problem, Donald Knuth invented the concept of literate programming: interspersing text and code in the same document using structured text to indicate which lines are code and which lines are intended for human consumption.\nThis textbook is written using a literate format - quarto markdown - which allows me to include code chunks in R, python, and other languages, alongside the text, pictures, and other formatting necessary to create a textbook.\n\n3.3.2.1 Quarto\nOne type of literate programming document is a quarto markdown document.\nWe will use quarto markdown documents for most of the components of this class because they allow you to answer assignment questions, write reports with figures and tables generated from data, and provide code all in the same file.\nWhile literate documents aren’t ideal for jobs where a computer is doing things unobserved (such as pulling data from a web page every hour), they are extremely useful in situations where it is desireable to have both code and an explanation of what the code is doing and what the results of that code are in the same document.\n\n\n\n\n\n\nYour turn: Quarto Markdown\n\n\n\nIn RStudio, create a new quarto markdown document: File > New File > Quarto Document. Give your document a title and an author, and select HTML as the output.\nCopy the following text into your document and hit the “Render” button at the top of the file.\nThis defines an R code chunk. The results will be included in the compiled HTML file.\n\n```{r}\n2 + 2 \n```\n\nThis defines a python code chunk. The results will be included in the compiled HTML file.\n\n```{python}\n2 + 2\n```\n\n# This is a header\n\n## This is a subheader\n\nI can add paragraphs of text, as well as other structured text such as lists:\n\n1. First thing\n2. Second thing\n  - nested list\n  - nested list item 2\n3. Third thing\n\nI can even include images and [links](https://www.oldest.org/entertainment/memes/)\n\n![Goodwin's law is almost as old as the internet itself.](https://www.oldest.org/wp-content/uploads/2017/10/Godwins-Law.jpg)\n\n\nMarkdown is a format designed to be readable and to allow document creators to focus on content rather than style.\n\nA Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. – John Gruber\n\nYou can read more about pandoc markdown (and quarto markdown, which is a specific type of pandoc markdown) here [2].\nMarkdown documents are compiled into their final form (usually, HTML, PDF, Docx) in multiple stages:\n\nAll code chunks are run and the results are saved and inserted into the markdown document.\nRmd/qmd -> md\nThe markdown document is converted into its final format using pandoc, a program that is designed to ensure you can generate almost any document format. This may involve conversion to an intermediate file (e.g. .tex files for PDF documents).\n\nAn error in your code will likely cause a failure at stage 1 of the process. An error in the formatting of your document, or missing pictures, and miscellaneous other problems may cause errors in stage 2.\n\n\n\n\n\n\nHistory\n\n\n\nQuarto markdown is the newest version of a long history of literate document writing in R. A previous version, Rmarkdown, had to be compiled using R; quarto can be compiled using R or python or the terminal directly.\nPrior to Rmarkdown, the R community used knitr and Sweave to integrate R code with LaTeX documents (another type of markup document that has a steep learning curve and is harder to read).\n\n\n\n3.3.2.2 Jupyter\nWhere quarto comes primarily out of the R community and those who are agnostic whether R or Python is preferable for data science related computing, Jupyter is essentially an equivalent notebook technology that comes from the python side of the world.\nQuarto supports using the jupyter engine for chunk compilation, but jupyter notebooks have some (rather technical) features that make them less desirable for an introductory computing class [3].\n\n\n\n\n\n\nLearn More about Notebooks\n\n\n\nThere are some excellent opinions surrounding the use of notebooks in data analysis:\n\n\nWhy I Don’t Like Notebooks” by Joel Grus at JupyterCon 2018\n\nThe First Notebook War by Yihui Xie (response to Joel’s talk).\n\nYihui Xie is the person responsible for knitr and Rmarkdown and was involved in the development of quarto."
  },
  {
    "objectID": "part-tools/03-scripts-notebooks.html#part-tools-03-refs",
    "href": "part-tools/03-scripts-notebooks.html#part-tools-03-refs",
    "title": "3  Scripts and Notebooks",
    "section": "\n3.4 References",
    "text": "3.4 References\n\n\n\n\n[1] \n\n“Punched card input/output.” Jan. 08, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Punched_card_input/output&oldid=1132250858\n\n\n\n[2] \nPosit PBC, “Quarto - markdown basics,” 2023. [Online]. Available: https://quarto.org/docs/authoring/markdown-basics.html. [Accessed: Jan. 09, 2023]\n\n\n[3] \nY. Xie, “The first notebook war,” Sep. 10, 2018. [Online]. Available: https://yihui.org/en/2018/09/notebook-war/. [Accessed: Jan. 09, 2023]"
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#fa-bullseye-objectives",
    "href": "part-tools/04-Rstudio-interface.html#fa-bullseye-objectives",
    "title": "4  RStudio’s Interface",
    "section": "4.1  Objectives",
    "text": "4.1  Objectives\n\nLocate different panes of RStudio\nUse cues such as buttons and icons to identify what type of file is open and what language is being interpreted"
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#overview",
    "href": "part-tools/04-Rstudio-interface.html#overview",
    "title": "4  RStudio’s Interface",
    "section": "4.2 Overview",
    "text": "4.2 Overview\n An RStudio window is by default divided into 4 panes, each of which may contain several tabs. You can reconfigure the locations of these tabs based on your preferences by selecting the toolbar button with 4 squares (just left of the Addins dropdown menu).\nIn the default configuration, - The top left is the editor pane, where you will write code and other content. - The bottom left is the console pane, which contains your R/python interactive consoles as well as a system terminal and location for checking the status of background jobs. - The top right contains the environment and history tabs (among others) - The top left contains the files and help tabs (among others)\nYou do not need to know what all of these tabs do right now. For the moment, it’s enough to get a sense of the basics - where to write code (top left), where to look for results (bottom left), where to get help (bottom right), and where to monitor what R/python are doing (top right)."
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#the-editorfile-pane-top-left",
    "href": "part-tools/04-Rstudio-interface.html#the-editorfile-pane-top-left",
    "title": "4  RStudio’s Interface",
    "section": "4.3 The Editor/File Pane (Top Left)",
    "text": "4.3 The Editor/File Pane (Top Left)\nThe buttons and layout within this pane change based on the type of file you have open.\n\nR scriptPython scriptQuarto markdownText file\n\n\n\n\n\nThe logo on the script file indicates the file type. When an R file is open, there are Run and Source buttons on the top which allow you to run selected lines of code (Run) or source (run) the entire file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the script file indicates the file type. When a python file is open, there are Run and Source buttons on the top which allow you to run selected lines of code (Run) or source (run) the entire file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the script file indicates the file type. When a quarto markdown file is open, there is a render button at the top which allows you to compile the file to see its “pretty”, non-markup form. In the same toolbar, there are buttons to add a code chunk as well as to run a selcted line of code or chunk of code. You can toggle between source (shown) and visual mode to see a more word-like rendering of the quarto markdown file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the text file indicates the file type. When a text file (or other unknown file extension) is open, there are very few buttons in the editor window. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is."
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#the-console-pane-bottom-left",
    "href": "part-tools/04-Rstudio-interface.html#the-console-pane-bottom-left",
    "title": "4  RStudio’s Interface",
    "section": "4.4 The Console Pane (Bottom Left)",
    "text": "4.4 The Console Pane (Bottom Left)\nLet’s compare what the console pane looks like when we run a line of R code compared to a line of python code. The differences will help you figure out whether you need to exit out of Python to run R code and may help you debug some errors.\n\nPythonR\n\n\n\n\n\nWhen running python code from a script file, the console will show you that you are running in python by the logo at the top of the console pane. You will initially see lines indicating that you’re running R, and then you’ll see the lines highlighted in red which show R running the code in python – this is what converts the console from R to python. The command you ran will appear after >>>, and the results will appear immediately below. A >>> waits for a new command - to get back to R, you will need to type exit (as instructed by the red text). In the environment pane, you cna see another indicator that you’re viewing the python environment, with an object named ‘r’ that will allow you to move data back and forth between the two languages if you want to do so.\n\n\n\n\n\n\n\nWhen running R code from a script file, the console will show you that you are running in R by the logo at the top of the console pane. You will initially see lines indicating that you’re running R (they’re missing here because this isn’t the first command I ran in this session). The command you ran will appear after >, and the results will appear immediately below, with boxed numbers in front of each sequential line. A > waits for a new command . In the environment pane, you may see a new value pop up named .Last.value - this is part of user settings and you can stop it from appearing if you want to."
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#the-top-right-pane",
    "href": "part-tools/04-Rstudio-interface.html#the-top-right-pane",
    "title": "4  RStudio’s Interface",
    "section": "4.5 The Top Right Pane",
    "text": "4.5 The Top Right Pane\nThis pane contains a set of tabs that change based on your project and what you have enabled. If you’re using git with an Rstudio project, then this tab will show your git repository. If you’re working with an Rstudio project that has multiple files, such as a book or a website, then the pane will also have a Build tab that will build all of your project files.\nFor now, though, let’s assume you’re not in an Rstudio project and you just want to know what the heck an Environment pane (or any of the other tabs in here by default) is. We’re going to focus on two of the tabs that are the most relevant to you right now: Environment, and History.\n\n4.5.1 Environment tab\nThe Environment tab shows you any objects which are defined in memory in whatever language you’re currently using (as long as it’s R or python). You’ll see headers like “Data”, “Values”, and “Functions” within this table, and two columns - the name of the thing, and the value of the thing (if it’s a complicated object, you’ll see what type of object it is and possibly how long it is).\n\n\n\nThe environment tab shows you all of the objects in memory that the language you’re working in knows about.\n\n\nIf you’re working in both R and python, you can toggle which language’s environment you’re looking at using the language drop down button on the far left side.\n\n\n4.5.2 History tab\nAnother useful tab in this pane is the History tab, which shows you a running list of every command you’ve ever run. While I strongly encourage you to write your code in a text file in the editor pane, sometimes you deleted a line of code accidentally and want to get it back… and the history tab has you covered (unless you’ve cleared the history out).\n\n\n\nThe history tab shows you a list of all commands you’ve run and allows you to send them to the console or to source (the text editor)."
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#the-bottom-right-pane",
    "href": "part-tools/04-Rstudio-interface.html#the-bottom-right-pane",
    "title": "4  RStudio’s Interface",
    "section": "4.6 The Bottom Right Pane",
    "text": "4.6 The Bottom Right Pane\nThis pane also contains a mishmash of tabs that have various uses. Here, we’ll focus on 3: Files, Packages, and Help. But first, to quickly summarize the remaining tabs, the Plots tab shows any plots you’ve generated (which we haven’t done yet), and the Viewer/Presentation tabs show you compiled documents (markdown), interactive graphics, and presentations.\n\n4.6.1 Files tab\n\n\n\nThe files tab shows you the files in your current working directory (by default), though you can navigate through it and find other files as necessary. If you want to return to your working directory, there’s a button for that in the “More” menu. One of the most important pieces of information in this pane is your path - you can construct the file path by using ~/ for home, and then for each folder, adding a slash between. The path to the folder we’re looking at here is thus ~/Projects/Class/stat-computing-r-python/.\n\n\n\n\n4.6.2 Packages tab\nThe packages tab isn’t quite relevant yet, but it will be soon. R and python both work off of packages - extensions to the default language that make it easier to accomplish certain tasks, like reading data from Excel files or drawing pretty charts. This tab shows all of the R packages you have installed on your machine, and which ones are currently loaded.\n\n\n\nYou can get important information from the packages tab, like what packages are loaded, easy access to documentation for each package, and what version of the package is installed.\n\n\nUnfortunately, the packages tab doesn’t cover python packages yet.\n\n\n4.6.3 Help tab\nThe help tab is a wonderful way to get help with how to use an R or python function.\n\n\n\nThe help tab makes it easy to get access to function documentation within Rstudio, so you don’t have to switch windows.\n\n\nBy default, you can search for an R function name in the search window, and documentation for matching functions will appear in the main part of the pane. To get help with python functions, you need to (in the python console) use ?<function name, so I would type in at the >>> prompt ?print to get the equivalent python help file."
  },
  {
    "objectID": "part-tools/05-git-and-github.html#fa-bullseye-objectives",
    "href": "part-tools/05-git-and-github.html#fa-bullseye-objectives",
    "title": "5  Version Control with Git",
    "section": "\n5.1  Objectives",
    "text": "5.1  Objectives\n\nInstall git\nCreate a github account\nUnderstand why version control is useful and what problems it can solve\nUnderstand the distinction between git and github, and what each is used for\nUse version control to track changes to a document (git add, commit, push, pull)"
  },
  {
    "objectID": "part-tools/05-git-and-github.html#installation",
    "href": "part-tools/05-git-and-github.html#installation",
    "title": "5  Version Control with Git",
    "section": "\n5.2 Installation",
    "text": "5.2 Installation\n\nInstall git using the instructions here.\n\nConsult the troubleshooting guide if you have issues.\nIf 1-2 fail, seek help in office hours.\n\n\n\n\n\n\n\n Mac Warning\n\n\n\nWith each version upgrade, you may find that git breaks. To fix it, you will have to reinstall Mac command line tools. Once you do this, git will start working again. See [2] for more information.\n\n\n\n5.2.1 Optional: Install a git client\nInstructions\nI don’t personally use a git client other than RStudio, but you may prefer to have a client that allows you to use a point-and-click interface. It’s up to you."
  },
  {
    "objectID": "part-tools/05-git-and-github.html#what-is-version-control",
    "href": "part-tools/05-git-and-github.html#what-is-version-control",
    "title": "5  Version Control with Git",
    "section": "\n5.3 What is Version Control ?",
    "text": "5.3 What is Version Control ?\n\n\n\n\n\n\nNote\n\n\n\nMost of this section is either heavily inspired by Happy Git and Github for the UseR [1] or directly links to that book.\n\n\n\n\n\nGit is a version control system - a structured way for tracking changes to files over the course of a project that may also make it easy to have multiple people working on the same files at the same time.\n\n\nVersion control is the answer to this file naming problem. Image Source “Piled Higher and Deeper” by Jorge Cham www.phdcomics.com\n\n\nGit manages a collection of files in a structured way - rather like “track changes” in Microsoft Word or version history in Dropbox, but much more powerful.\nIf you are working alone, you will benefit from adopting version control because it will remove the need to add _final.R to the end of your file names. However, most of us work in collaboration with other people (or will have to work with others eventually), so one of the goals of this program is to teach you how to use git because it is a useful tool that will make you a better collaborator.\nIn data science programming, we use git for a similar, but slightly different purpose. We use it to keep track of changes not only to code files, but to data files, figures, reports, and other essential bits of information.\nGit itself is nice enough, but where git really becomes amazing is when you combine it with GitHub - an online service that makes it easy to use git across many computers, share information with collaborators, publish to the web, and more. Git is great, but GitHub is … essential. In this class, we’ll be using both git and github, and your homework will be managed with GitHub Classroom.\n\n5.3.1 Git Basics\n\n\nIf that doesn’t fix it, git.txt contains the phone number of a friend of mine who understands git. Just wait through a few minutes of ‘It’s really pretty simple, just think of branches as…’ and eventually you’ll learn the commands that will fix everything. Image by Randall Munroe (XKCD) CC-A-NC-2.5.\n\n\nGit tracks changes to each file that it is told to monitor, and as the files change, you provide short labels describing what the changes were and why they exist (called “commits”). The log of these changes (along with the file history) is called your git commit history.\nWhen writing papers, this means you can cut material out freely, so long as the paper is being tracked by git - you can always go back and get that paragraph you cut out if you need to. You also don’t have to rename files - you can confidently save over your old files, so long as you remember to commit frequently.\n\n\n\n\n\n\nEssential Reading: Git\n\n\n\nThe git material in this chapter is just going to link directly to the book “Happy Git with R” by Jenny Bryan. It’s amazing, amusing, and generally well written. I’m not going to try to do better.\nGo read Chapter 1, if you haven’t already.\n\n\nNow that you have a general idea of how git works and why we might use it, let’s talk a bit about GitHub.\n\n5.3.2 GitHub: Git on the Web\n\n\n\n\n\n\nSet up a GitHub Account Now\n\n\n\nInstructions for setting up a GitHub account.\nBe sure you remember your signup email, username, and password - you will need them later.\n\n\nGit is a program that runs on your machine and keeps track of changes to files that you tell it to monitor. GitHub is a website that hosts people’s git repositories. You can use git without GitHub, but you can’t use GitHub without git.\n\n\n\n\n\n\nGit and Github: Slightly crude (but memorable) analogy\n\n\n\n\n\nGit is to GitHub what Porn is to PornHub. Specifically, GitHub hosts git repositories publicly, while PornHub hosts porn publicly. But it would be silly to equate porn and PornHub, and it’s similarly silly to think of GitHub as the only place you can use git repositories.\n\n\n\nIf you want, you can hook Git up to GitHub, and make a copy of your local git repository that lives in the cloud. Then, if you configure things correctly, your local repository will talk to GitHub without too much trouble. Using Github with Git allows you to easily make a cloud backup of your important code, so that even if your computer suddenly catches on fire, all of your important code files exist somewhere else.\nRemember: any data you don’t have in 3 different places is data you don’t care about.1"
  },
  {
    "objectID": "part-tools/05-git-and-github.html#using-version-control-with-rstudio",
    "href": "part-tools/05-git-and-github.html#using-version-control-with-rstudio",
    "title": "5  Version Control with Git",
    "section": "\n5.4 Using Version Control (with RStudio)",
    "text": "5.4 Using Version Control (with RStudio)\nThe first skill you need to actually practice in this class is using version control. By using version control from the very beginning, you will learn better habits for programming, but you’ll also get access to a platform for collaboration, hosting your work online, keeping track of features and necessary changes, and more.\n\n\n\n\n\n\n\n\n\n\n\nSo, what does your typical git/GitHub workflow look like? I’ll go through this in (roughly) chronological order. This is based off of a relatively high-level understanding of git - I do not have any idea how it works under the hood, but I’m pretty comfortable with the clone/push/pull/commit/add workflows, and I’ve used a few of the more complicated features (branches, pull requests) on occasion.\n\n5.4.1 Introduce yourself to git and set up SSH authentication\nYou need to tell git what your name and email address are, because every “commit” you make will be signed. This needs to be done once on each computer you’re using.\nFollow the instructions here, or run the lines below:\n\n\n\n\n\n\nNote\n\n\n\nThe lines of code below use interactive prompts. Click the copy button in the upper right corner of the box below, and then paste the whole thing into the R console. You will see a line that says “Your full name:” - type your name into the console. Similarly, the next line will ask you for an email address.)\n\n\n\n\nuser_name <- readline(prompt = \"Your full name: \")\nuser_email <- readline(prompt = \"The address associated w your github account: \")\n\ninstall.packages(\"usethis\")\nlibrary(usethis)\n\nuse_git_config(user.name = user_name, user.email = user_email, scope = \"user\")\n\n# Tell git to ignore all files that are OS-dependent and don't have useful data.\ngit_vaccinate() \n\n# Create a ssh key if one doesn't already exist\nif (!file.exists(git2r::ssh_path(\"id_rsa.pub\"))) {\n  # Create an ssh key (with no password - less secure, but simpler)\n  system(\"ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -q -N ''\") \n  # Find the ssh-agent that will keep track of the password\n  system(\"eval $(ssh-agent -s)\")\n  # Add the key\n  system(\"ssh-add ~/.ssh/id_rsa\")\n} \n\nThen, in RStudio, go to Tools > Global Options > Git/SVN. View your public key, and copy it to the clipboard.\nThen, proceed to github. Make sure you’re signed into GitHub. Click on your profile pic in upper right corner and go Settings, then SSH and GPG keys. Click “New SSH key”. Paste your public key in the “Key” box. Give it an informative title. For example, you might use 2022-laptop to record the year and computer. Click “Add SSH key”.\n\n5.4.2 Create a Repository\nRepositories are single-project containers. You may have code, documentation, data, TODO lists, and more associated with a project. If you combine a git repository with an RStudio project, you get a very powerful combination that will make your life much easier, allowing you to focus on writing code instead of figuring out where all of your files are for each different project you start.\nTo create a repository, you can start with your local computer first, or you can start with the online repository first.\n\n\n\n\n\n\nImportant\n\n\n\nBoth methods are relatively simple, but the options you choose depend on which method you’re using, so be careful not to get them confused.\n\n\n\n5.4.2.1 Local repository first\nLet’s suppose you already have a folder on your machine named hello-world-1 (you may want to create this folder now). You’ve created a starter document, say, a text file named README with “hello world” written in it.\nIf you want, you can use the following R code to set this up:\n\ndir <- \"./hello-world-1\"\nif (!dir.exists(dir)) {\n  dir.create(dir)\n}\nfile <- file.path(dir, \"README\")\nif (!file.exists(file)) {\n  writeLines(\"hello world\", con = file)\n}\n\nTo create a local git repository, we can go to the terminal (in Mac/Linux) or the git bash shell (in Windows), navigate to our repository folder (not shown, will be different on each computer), and type in\ngit init\nAlternately, if you prefer a GUI (graphical user interface) approach, that will work too:\n\nOpen Rstudio\nProject (upper right corner) -> New Project -> Existing Directory. Navigate to the directory.\n(In your new project) Tools -> Project options -> Git/SVN -> select git from the dropdown, initialize new repository. RStudio will need to restart.\nNavigate to your new Git tab on the top right.\n\n\n\n\n\nThe next step is to add our file to the repository.\nUsing the command line, you can type in git add README (this tells git to track the file) and then commit your changes (enter them into the record) using git commit -m \"Add readme file\".\nUsing the GUI, you navigate to the git pane, check the box next to the README file, click the Commit button, write a message (“Add readme file”), and click the commit button.\n\n\n\n\nThe final step is to create a corresponding repository on GitHub. Navigate to your GitHub profile and make sure you’re logged in. Create a new repository using the “New” button. Name your repository whatever you want, fill in the description if you want (this can help you later, if you forget what exactly a certain repo was for), and DO NOT add a README, license file, or anything else (if you do, you will have a bad time).\nYou’ll be taken to your empty repository, and git will provide you the lines to paste into your git shell (or terminal) – you can access this within RStudio, as shown below. Paste those lines in, and you’ll be good to go.\n\n\n\n\n\n5.4.2.2 GitHub repository first\nIn the GitHub-first method, you’ll create a repository in GitHub and then clone it to your local machine (clone = create an exact copy locally).\nGUI method:\n\nLog into GitHub and create a new repository\nInitialize your repository with a README\nCopy the repository location by clicking on the “Code” button on the repo homepage\nOpen RStudio -> Project -> New Project -> From version control. Paste your repository URL into the box. Hit enter.\nMake a change to the README file\nClick commit, then push your changes\nCheck that the remote repository (Github) updated\n\n\n\n\n\nCommand line method:\n\nLog into GitHub and create a new repository\nInitialize your repository with a README\nCopy the repository location by clicking on the “Code” button on the repo homepage\nNavigate to the location you want your repository to live on your machine.\nClone the repository by using the git shell or terminal: git clone <your repo url here>. In my case, this looks like git clone git@github.com:stat850-unl/hello-world-2.git\n\nMake a change to your README file and save the change\nCommit your changes: git commit -a -m \"change readme\" (-a = all, that is, any changed file git is already tracking).\nPush your changes to the remote (GitHub) repository and check that the repo has updated: git push\n\n\n\n\n\n\n\n5.4.3 Adding files\ngit add tells git that you want it to track a particular file.\n\n\ngit add diagram: add tells git to add the file to the index of files git monitors.\n\n\nYou don’t need to understand exactly what git is doing on the backend, but it is important to know that the actual contents of the file aren’t logged by git add - you have to commit your changes for the contents to change. git add deals solely with the index of files that git “knows about”, and what it thinks belongs in each commit.\nIf you use the RStudio GUI for your git interface, you generally won’t have to do much with git add; it’s (sort-of, kind-of) equivalent to clicking the check box.\n\n5.4.3.1 What files should I add to git?\nGit is built for tracking text files. It will (begrudgingly) deal with small binary files (e.g. images, PDFs) without complaining too much, but it is NOT meant for storing large files, and GitHub will not allow you to push anything that has a file larger than 100MB2. Larger files can be handled with git-lfs (large file storage), but storing large files online is not something you can get for free.\nIn general, you should only add a file to git if you created it by hand. If you compiled the result, that should not be in the git repository under normal conditions (there are exceptions to this rule – this book is hosted on GitHub, which means I’ve pushed the compiled book to the GitHub repository).\nYou should also be cautious about adding files like .Rprog, .directory, .DS_Store, etc. These files are used by your operating system or by RStudio, and pushing them may cause problems for your collaborators (if you’re collaborating). Tracking changes to these files also doesn’t really do much good.\nI highly recommend that you make a point to only add and commit files which you consciously want to track.\n\n5.4.4 Staging your changes\nIn RStudio, when you check a box next to the file name in the git tab, you are effectively adding the file (if it is not already added) AND staging all of the changes you’ve made to the file. In practice, git add will both add and stage all of the changes to any given file, but it is also useful in some cases to stage only certain lines from a file.\nMore formally, staging is saying “I’d like these changes to be added to the current version, I think”. Before you commit your changes, you have to first stage them. You can think of this like going to the grocery store: you have items in your cart, but you can put them back at any point before checkout. Staging changes is like adding items to your cart; committing those changes is like checking out.\nIndividually staging lines of a file is most useful in situations where you’ve made changes which should be part of multiple commits. To stage individual lines of a file, you can use git add -i at the command line, or you can attempt to use RStudio’s “stage selection” interface. Both will work, though git can’t always separate changes quite as finely as you might want (and as a result, RStudio’s interface sometimes seems unresponsive, even though the underlying issue is with what git can do).\n\n5.4.5 Committing your changes\nA git commit is the equivalent of a log entry - it tells git to record the state of the file, along with a message about what that state means. On the back end, git will save a copy of the file in its current state to its cache.\n\n\nHere, we commit the red line as a change to our file.\n\n\nIn general, you want your commit message to be relatively short, but also informative. The best way to do this is to commit small blocks of changes. Work to commit every time you’ve accomplished a small task. This will do two things:\n\nYou’ll have small, bite-sized changes that are briefly described to serve as a record of what you’ve done (and what still needs doing)\nWhen you mess up (or end up in a merge conflict) you will have a much easier time pinpointing the spot where things went bad, what code was there before, and (because you have nice, descriptive commit messages) how the error occurred.\n\n5.4.6 Pushing and Pulling\nWhen you’re working alone, you generally won’t need to worry about having to update your local copy of the repository (unless you’re using multiple machines). However, statistics is collaborative, and one of the most powerful parts of git is that you can use it to keep track of changes when multiple people are working on the same document.\n\nIf you are working collaboratively and you and your collaborator are working on the same file, git will be able to resolve the change you make SO LONG AS YOU’RE NOT EDITING THE SAME LINE. Git works based on lines of text - it detects when there is a change in any line of a text document.\nFor this reason, I find it makes my life easier to put each sentence on a separate line, so that I can tweak things with fewer merge conflicts. Merge conflicts aren’t a huge deal, but they slow the workflow down, and are best avoided where possible.\n\nPulling describes the process of updating your local copy of the repository (the copy on your computer) with the files that are “in the cloud” (on GitHub). git pull (or using the Pull button in RStudio) will perform this update for you. If you are working with collaborators in real time, it is good practice to pull, commit, and push often, because this vastly reduces the merge conflict potential (and the scope of any conflicts that do pop up).\nPushing describes the process of updating the copy of the repository on another machine (e.g. on GitHub) so that it has the most recent changes you’ve made to your machine.\n\n\n\n\n\ngit push copies the version of the project on your computer to GitHub\n\n\n\n\n\n\ngit pull copies the version of the project on GitHub to your computer\n\n\n\n\nFigure 5.1: Git push and git pull are used to sync your computer with the remote repository (usually hosted on GitHub)\n\n\nIn general, your workflow will be\n\nClone the project or create a new repository\nMake some changes\nStage the changes with git add\nCommit the changes with git commit\nPull any changes from the remote repository\nResolve any merge conflicts\nPush the changes (and merged files) with git push\n\nIf you’re working alone, steps 5 and 6 are not likely to be necessary, but it is good practice to just pull before you push anyways."
  },
  {
    "objectID": "part-tools/05-git-and-github.html#references",
    "href": "part-tools/05-git-and-github.html#references",
    "title": "5  Version Control with Git",
    "section": "\n5.5 References",
    "text": "5.5 References\n\n\n\n\n[1] \nJ. Bryan, J. Hester, and {The Stat 545 TAs}, Happy git and GitHub for the useR. 2021 [Online]. Available: https://happygitwithr.com/. [Accessed: May 09, 2022]\n\n\n[2] \ndustbuster, “Answer to \"git is not working after macOS update (xcrun: Error: Invalid active developer path (/library/developer/CommandLineTools)\". Stack overflow,” Sep. 26, 2018. [Online]. Available: https://stackoverflow.com/a/52522566/2859168. [Accessed: Jan. 13, 2023]"
  },
  {
    "objectID": "part-gen-prog/00-gen-prog.html",
    "href": "part-gen-prog/00-gen-prog.html",
    "title": "Part II: General Programming",
    "section": "",
    "text": "In this portion of the textbook, we’ll talk about the basics of programming in a general sense (that is, we’re not yet focusing on programming with data).\nBefore we start in on the hard stuff, we’ll quickly go through what programming is and what the vocabulary of a programming language looks like in Chapter 6.\nChapter 7 will discuss the basics: variable types, how to assign variables, and how to convert between simple variable types.\nChapter 8 will discuss how to use built-in and package functions to make R and python more powerful. After this section, you should be able to use R or Python as a calculator.\nChapter 9 will discuss the use of vectors and matrices in R and Python. Along the way, you’ll get a quick refresher in mathematical logic - the use of And, Or, and Not.\nIf you’ve had linear algebra, Chapter 10 will tell you how to use R and python to perform matrix calculations. If you haven’t had linear algebra yet, skip this section and move on to Chapter 11.\nChapter 11 will discuss control structures - ways to change the flow of a program based on variable values and operating condition. This will include discussions of if-statements and different types of loops.\nChapter 12 will discuss writing your own functions.\nOnce we’ve covered these topics, we should be ready to focus on programming with, for, and on data."
  },
  {
    "objectID": "part-gen-prog/00-intro.html#what-is-programming",
    "href": "part-gen-prog/00-intro.html#what-is-programming",
    "title": "6  Introduction to Programming",
    "section": "\n6.1 What is Programming?",
    "text": "6.1 What is Programming?\n\nProgramming today is a race between software engineers striving to build bigger and better idiot-proof programs, and the universe trying to produce bigger and better idiots. So far, the universe is winning. - Rick Cook\n\nProgramming is the art of solving a problem by developing a sequence of steps that make up a solution, and then very carefully communicating those steps to the computer. To program, you need to know how to\n\nbreak a problem down into smaller, easily solvable problems\nsolve the small problems\ncommunicate the solution to a computer using a programming language\n\nIn this book, we’ll be using both R and Python, and we’ll be using these languages to solve problems that are related to working with data. At first, we’ll start with smaller, simpler problems that don’t involve data, but by the end, you will hopefully be able to solve some statistical problems using one or both languages.\nIt will be hard at first - you have to learn the vocabulary in both languages in order to be able to put commands into logical “sentences”. The problem solving skills are the same for all programming languages, though, and while those are harder to learn, they’ll last you a lifetime.\nJust as you wouldn’t expect to learn French or Mandarin fluently after taking a single class, you cannot expect to be fluent in R or python once you’ve worked through this book. Fluency takes years of work and practice, and lots of mistakes along the way. You cannot learn a language (programming or otherwise) if you’re worried about making mistakes. Take a minute and put those concerns away somewhere, take a deep breath, and remember the Magic School Bus Motto:\n\n\nFor those who don’t know, the Magic School Bus is a PBS series that aired in the 1990s and was brought back by Netflix in 2017. It taught kids about different principles of science and the natural world."
  },
  {
    "objectID": "part-gen-prog/00-intro.html#programming-vocabulary-hello-world",
    "href": "part-gen-prog/00-intro.html#programming-vocabulary-hello-world",
    "title": "6  Introduction to Programming",
    "section": "\n6.2 Programming Vocabulary: Hello World",
    "text": "6.2 Programming Vocabulary: Hello World\nI particularly like the way that Python for Everybody [1] explains vocabulary:\n\nUnlike human languages, the Python vocabulary is actually pretty small. We call this “vocabulary” the “reserved words”. These are words that have very special meaning to Python. When Python sees these words in a Python program, they have one and only one meaning to Python. Later as you write programs you will make up your own words that have meaning to you called variables. You will have great latitude in choosing your names for your variables, but you cannot use any of Python’s reserved words as a name for a variable.\n\n\nWhen we train a dog, we use special words like “sit”, “stay”, and “fetch”. When you talk to a dog and don’t use any of the reserved words, they just look at you with a quizzical look on their face until you say a reserved word. For example, if you say, “I wish more people would walk to improve their overall health”, what most dogs likely hear is, “blah blah blah walk blah blah blah blah.” That is because “walk” is a reserved word in dog language. Many might suggest that the language between humans and cats has no reserved words.\n\n\nThe reserved words in the language where humans talk to Python include the following:\n\nand       del       global      not       with\nas        elif      if          or        yield\nassert    else      import      pass\nbreak     except    in          raise\nclass     finally   is          return\ncontinue  for       lambda      try\ndef       from      nonlocal    while\n\nThat is it, and unlike a dog, Python is already completely trained. When you say ‘try’, Python will try every time you say it without fail.\n\n\nWe will learn these reserved words and how they are used in good time, but for now we will focus on the Python equivalent of “speak” (in human-to-dog language). The nice thing about telling Python to speak is that we can even tell it what to say by giving it a message in quotes:\n\n\nprint('Hello world!')\n## Hello world!\n\n\nAnd we have even written our first syntactically correct Python sentence. Our sentence starts with the function print followed by a string of text of our choosing enclosed in single quotes. The strings in the print statements are enclosed in quotes. Single quotes and double quotes do the same thing; most people use single quotes except in cases like this where a single quote (which is also an apostrophe) appears in the string.\n\nR has a slightly smaller set of reserved words:\nif          else     repeat      while\nfor         in       next        break\nTRUE        FALSE    NULL        Inf\nNA_integer_ NA_real_ NA_complex_ NA_character_\nNaN         NA       function    ...\nIn R, the “Hello World” program looks exactly the same as it does in python.\n\nprint('Hello world!')\n## [1] \"Hello world!\"\n\nIn many situations, R and python will be similar because both languages are based on C. R has a more complicated history [2], because it is also similar to Lisp, but both languages are still very similar to C and run C or C++ code in the background."
  },
  {
    "objectID": "part-gen-prog/00-intro.html#getting-help",
    "href": "part-gen-prog/00-intro.html#getting-help",
    "title": "6  Introduction to Programming",
    "section": "\n6.3 Getting help",
    "text": "6.3 Getting help\nIn both R and python, you can access help with a ? - the order is just slightly different.\nSuppose we want to get help on a for loop in either language.\nIn R, we can run this line of code to get help on for loops.\n\n?`for`\n\nBecause for is a reserved word in R, we have to use backticks (the key above the TAB key) to surround the word for so that R knows we’re talking about the function itself. Most other function help can be accessed using ?function_name. The backtick trick also works for functions that don’t start with letters, like +.\nIn python, we use for? to access the same information.\n\nfor? # help printed in the terminal\n?for # help printed in the help pane\n\n(You will have to run this in interactive mode for it to work in either language)\nw3schools has an excellent python help page that may be useful as well. Searching for help using google also works well, particularly if you know what sites are likely to be helpful, like w3schools and stackoverflow. A similar set of pages exists for R help on basic functions\n\n\n\n\n\n\nLearn More\n\n\n\nA nice explanation of the difference between an interpreter and a compiler. Both Python and R are interpreted languages that are compiled from lower-level languages like C."
  },
  {
    "objectID": "part-gen-prog/00-intro.html#sec-gen-prog-refs",
    "href": "part-gen-prog/00-intro.html#sec-gen-prog-refs",
    "title": "6  Introduction to Programming",
    "section": "\n6.4 References",
    "text": "6.4 References\n\n\n\n\n[1] \nD. C. R. Severance, Python for Everybody: Exploring Data in Python 3. Ann Arbor, MI: CreateSpace Independent Publishing Platform, 2016 [Online]. Available: https://www.py4e.com/html3/\n\n\n\n[2] \nR. Ihaka, “R : Past and future history,” 1998 [Online]. Available: https://www.stat.auckland.ac.nz/~ihaka/downloads/Interface98.pdf"
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#fa-bullseye-objectives",
    "href": "part-gen-prog/01-basic-var-types.html#fa-bullseye-objectives",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.1  Objectives",
    "text": "7.1  Objectives\n\nKnow the basic data types and what their restrictions are\nKnow how to test to see if a variable is a given data type\nUnderstand the basics of implicit and explicit type conversion\nWrite code that assigns values to variables"
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#basic-definitions",
    "href": "part-gen-prog/01-basic-var-types.html#basic-definitions",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.2 Basic Definitions",
    "text": "7.2 Basic Definitions\nFor a general overview, [1] is an excellent introduction to data types:\n\n\n\n\nLet’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn most programming languages (including R and python), there are some very basic data types:\n\nlogical or boolean - FALSE/TRUE or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\n\ndouble or float or numeric- decimal numbers.\n\n\nfloat is short for floating-point value.\n\ndouble is a floating-point value with more precision (“double precision”).1\n\nR uses the name numeric to indicate a decimal value, regardless of precision.\n\n\ncharacter or string - holds text, usually enclosed in quotes.\n\n\n\n\n\n\n\nCapitalization matters!\n\n\n\nIn R, boolean values are TRUE and FALSE, but in Python they are True and False. Capitalization matters a LOT.\nOther things matter too: if we try to write a million, we would write it 1000000 instead of 1,000,000 (in both languages). Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data."
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#variables",
    "href": "part-gen-prog/01-basic-var-types.html#variables",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.3 Variables",
    "text": "7.3 Variables\nProgramming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\n\n7.3.1 Assignment\nWe assign variables values using the syntax object_name <- value (R) or object_name = value (python). You can read this as “object name gets value” in your head.\nIn R, <- is used for assigning a value to a variable. So x <- \"R is awesome\" is read “x gets ‘R is awesome’” or “x is assigned the value ‘R is awesome’”. Technically, you can also use = to assign things to variables in R, but most style guides consider this to be poor programming practice, so seriously consider defaulting to <-.\nIn Python, = is used for assigning a value to a variable. This tends to be much easier to say out loud, but lacks any indication of directionality.\n\n\n\n\n\n\nDemo: Assignment\n\n\n\n\n\nR\nPython\n\n\n\n\nmessage <- \"So long and thanks for all the fish\"\nyear <- 2025\nthe_answer <- 42L\nearth_demolished <- FALSE\n\n\n\n\nmessage = \"So long and thanks for all the fish\"\nyear = 2025\nthe_answer = 42\nearth_demolished = False\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in R, we assign variables values using the <- operator, where in Python, we assign variables values using the = operator. Technically, = will work for assignment in both languages, but <- is more common than = in R by convention.\n\n\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\n\n7.3.2 Naming Variables\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n– Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, _, and . in R. In Python, object names must start with a letter and can consist of letters, numbers, and _ (that is, . is not a valid character in a Python variable name). While it is technically fine to use uppercase variable names in Python, it’s recommended that you use lowercase names for variables (you’ll see why later).\nWhat happens if we try to create a variable name that isn’t valid?\nIn both languages, starting a variable name with a number will get you an error message that lets you know that something isn’t right - “unexpected symbol” in R and “invalid syntax” in python.\n\n\n\n\n\n\nInvalid Names\n\n\n\n\n\nR\nPython\n\n\n\n\n1st_thing <- \"check your variable names!\"\n## Error: <text>:1:2: unexpected symbol\n## 1: 1st_thing\n##      ^\n\n\n\n\n1st_thing <- \"check your variable names!\"\n\nNote: Run the above chunk in your python window - the book won’t compile if I set it to evaluate 😥. It generates an error of SyntaxError: invalid syntax (<string>, line 1)\n\nsecond.thing <- \"this isn't valid\"\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'second' is not defined\n\nIn python, trying to have a . in a variable name gets a more interesting error: “ is not defined”. This is because in python, some objects have components and methods that can be accessed with .. We’ll get into this more later, but there is a good reason for python’s restriction about not using . in variable names.\n\n\n\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\n\n\n\n\n\nLearn More\n\n\n\nWhy is naming things hard? - Blog post by Neil Kakkar\n\n\nThere are a few different conventions for naming things that may be useful:\n\n\nsome_people_use_snake_case, where words are separated by underscores\n\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\n\nsome.people.use.periods (in R, obviously this doesn’t work in python)\nA few people mix conventions with variables_thatLookLike.this and they are almost universally hated 👿\n\nAs long as you pick ONE naming convention and don’t mix-and-match, you’ll be fine. It will be easier to remember what you named your variables (or at least guess) and you’ll have fewer moments where you have to go scrolling through your script file looking for a variable you named."
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#types",
    "href": "part-gen-prog/01-basic-var-types.html#types",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.4 Types",
    "text": "7.4 Types\n\n\n\n\n\n\nTesting Types\n\n\n\nYou can use different functions to test whether a variable has a specific type.\n\n\nR\nPython\n\n\n\n\nis.logical(FALSE)\nis.integer(2L) # by default, R treats all numbers as numeric/decimal values. \n          # The L indicates that we're talking about an integer. \nis.integer(2)\nis.numeric(2)\nis.character(\"Hello, programmer!\")\nis.function(print)\n## [1] TRUE\n## [1] TRUE\n## [1] FALSE\n## [1] TRUE\n## [1] TRUE\n## [1] TRUE\n\nIn R, you use is.xxx functions, where xxx is the name of the type in question.\n\n\n\nisinstance(False, bool)\nisinstance(2, int)\nisinstance(2, (int, float)) # Test for one of multiple types\nisinstance(3.1415, float)\nisinstance(\"This is python code\", str)\n## True\n## True\n## True\n## True\n## True\n\nIn python, test for types using the isinstance function with an argument containing one or more data types in a tuple ((int, float) is an example of a tuple - a static set of multiple values).\nIf we want to test for whether something is callable (can be used like a function), we have to get slightly more complicated:\n\ncallable(print)\n## True\n\nThis is glossing over some much more technical information about differences between functions and classes (that we haven’t covered) [2].\n\n\n\n\n\n\n\n\n\n\n\nExample: Assignment and Testing Types\n\n\n\n\n\nCharacter\nLogical\nInteger\nDouble\nNumeric\n\n\n\n\nx <- \"R is awesome\"\ntypeof(x)\n## [1] \"character\"\nis.character(x)\n## [1] TRUE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] FALSE\n\n\nx = \"python is awesome\"\ntype(x)\n## <class 'str'>\nisinstance(x, str)\n## True\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## False\nisinstance(x, float)\n## False\n\n\n\n\nx <- FALSE\ntypeof(x)\n## [1] \"logical\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] TRUE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] FALSE\n\nIn R, is possible to use the shorthand F and T, but be careful with this, because F and T are not reserved, and other information can be stored within them. See this discussion for pros and cons of using F and T as variables vs. shorthand for true and false. 2\n\nx = False\ntype(x)\n## <class 'bool'>\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## True\nisinstance(x, int)\n## True\nisinstance(x, float)\n## False\n\nNote that in python, boolean variables are also integers. If your goal is to test whether something is a T/F value, you may want to e.g. test whether its value is one of 0 or 1, rather than testing whether it is a boolean variable directly, since integers can also function directly as bools in Python.\n\n\n\nx <- 2\ntypeof(x)\n## [1] \"double\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] TRUE\n\nWait, 2 is an integer, right?\n2 is an integer, but in R, values are assumed to be doubles unless specified. So if we want R to treat 2 as an integer, we need to specify that it is an integer specifically.\n\nx <- 2L # The L immediately after the 2 indicates that it is an integer.\ntypeof(x)\n## [1] \"integer\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] TRUE\nis.double(x)\n## [1] FALSE\nis.numeric(x)\n## [1] TRUE\n\n\nx = 2\ntype(x)\n## <class 'int'>\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## True\nisinstance(x, float)\n## False\n\n\n\n\nx <- 2.45\ntypeof(x)\n## [1] \"double\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] TRUE\nis.numeric(x)\n## [1] TRUE\n\n\nx = 2.45\ntype(x)\n## <class 'float'>\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## False\nisinstance(x, float)\n## True\n\n\n\nA fifth common “type”3, numeric is really the union of two types: integer and double, and you may come across it when using str() or mode(), which are similar to typeof() but do not quite do the same thing.\nThe numeric category exists because when doing math, we can add an integer and a double, but adding an integer and a string is … trickier. Testing for numeric variables guarantees that we’ll be able to do math with those variables. is.numeric() and as.numeric() work as you would expect them to work.\nThe general case of this property of a language is called implicit type conversion - that is, R will implicitly (behind the scenes) convert your integer to a double and then add the other double, so that the result is unambiguously a double."
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#sec-type-conversions",
    "href": "part-gen-prog/01-basic-var-types.html#sec-type-conversions",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.5 Type Conversions",
    "text": "7.5 Type Conversions\nProgramming languages will generally work hard to seamlessly convert variables to different types. This is called implicit type casting - the computer implicitly changes the variable type to avoid a conflict.\n\n\n\n\n\n\nImplicit Type Conversion\n\n\n\n\n\nR\nPython\n\n\n\n\nTRUE + 2\n## [1] 3\n\n2L + 3.1415\n## [1] 5.1415\n\n\"abcd\" + 3\n## Error in \"abcd\" + 3: non-numeric argument to binary operator\n\n\n\n\nTrue + 2\n## 3\nint(2) + 3.1415\n## 5.141500000000001\n\"abcd\" + 3\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: can only concatenate str (not \"int\") to str\n\n\n\n\nThis conversion doesn’t always work - there’s no clear way to make “abcd” into a number we could use in addition. So instead, R or python will issue an error. This error pops up frequently when something went wrong with data import and all of a sudden you just tried to take the mean of a set of string/character variables. Whoops.\n\n\nWhen you want to, you can also use as.xxx() to make the type conversion explicit. So, the analogue of the code above, with explicit conversions would be:\n\n\n\n\n\n\nExplicit Type Conversion\n\n\n\n\n\nR\nPython\n\n\n\n\nas.double(TRUE) + 2\n## [1] 3\n\nas.double(2L) + 3.1415\n## [1] 5.1415\n\nas.numeric(\"abcd\") + 3\n## [1] NA\n\n\n\n\nint(True) + 2\n## 3\nfloat(2) + 3.1415\n## 5.141500000000001\nfloat(\"abcd\") + 3\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: could not convert string to float: 'abcd'\nimport pandas as pd # Load pandas library\npd.to_numeric(\"abcd\", errors = 'coerce') + 3\n## nan\n\n\n\n\nWhen we make our intent explicit (convert “abcd” to a numeric variable) we get an NA - a missing value - in R. In Python, we get a more descriptive error by default, but we can use the pandas library (which adds some statistical functionality) to get a similar result to the result we get in R.\nThere’s still no easy way to figure out where “abcd” is on a number line, but our math will still have a result - NA + 3 is NA."
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#what-type-is-it",
    "href": "part-gen-prog/01-basic-var-types.html#what-type-is-it",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.6 What Type is it?",
    "text": "7.6 What Type is it?\nIf you don’t know what type a value is, both R and python have functions to help you with that.\n\n\n\n\n\n\nDetermining Variable Types\n\n\n\n\n\nR\nPython\n\n\n\nIf you are unsure what the type of a variable is, use the typeof() function to find out.\n\nw <- \"a string\"\nx <- 3L\ny <- 3.1415\nz <- FALSE\n\ntypeof(w)\n## [1] \"character\"\ntypeof(x)\n## [1] \"integer\"\ntypeof(y)\n## [1] \"double\"\ntypeof(z)\n## [1] \"logical\"\n\n\n\nIf you are unsure what the type of a variable is, use the type() function to find out.\n\nw = \"a string\"\nx = 3\ny = 3.1415\nz = False\n\ntype(w)\n## <class 'str'>\ntype(x)\n## <class 'int'>\ntype(y)\n## <class 'float'>\ntype(z)\n## <class 'bool'>\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: Variables and Types\n\n\n\n\n\nR\nPython\nR Solution\nPython Solution\n\n\n\n\nCreate variables string, integer, decimal, and logical, with types that match the relevant variable names.\n\n\nstring <- \ninteger <- \ndecimal <- \nlogical <- \n\n\nCan you get rid of the error that occurs when this chunk is run?\n\n\nlogical + decimal\ninteger + decimal\nstring + integer\n\n\nWhat happens when you add string to string? logical to logical?\n\n\n\n\nCreate variables string, integer, decimal, and logical, with types that match the relevant variable names.\n\n\nstring = \ninteger = \ndecimal = \nlogical = \n\n\nCan you get rid of the error that occurs when this chunk is run?\n\n\nlogical + decimal\ninteger + decimal\nstring + integer\n\n\nWhat happens when you add string to string? logical to logical?\n\n\n\n\nstring <- \"hi, I'm a string\"\ninteger <- 4L\ndecimal <- 5.412\nlogical <- TRUE\n\nlogical + decimal\n## [1] 6.412\ninteger + decimal\n## [1] 9.412\nas.numeric(string) + integer\n## [1] NA\n\n\"abcd\" + \"efgh\"\n## Error in \"abcd\" + \"efgh\": non-numeric argument to binary operator\nTRUE + TRUE\n## [1] 2\n\nIn R, adding a string to a string creates an error (“non-numeric argument to binary operator”). Adding a logical to a logical, e.g. TRUE + TRUE, results in 2, which is a numeric value.\nTo concatenate strings in R (like the default behavior in python), we would use the paste0 function: paste0(\"abcd\", \"efgh\"), which returns abcdefgh.\n\n\n\nimport pandas as pd\n\nstring = \"hi, I'm a string\"\ninteger = 4\ndecimal = 5.412\nlogical = True\n\nlogical + decimal\n## 6.412\ninteger + decimal\n## 9.411999999999999\npd.to_numeric(string, errors='coerce') + integer\n## nan\n\"abcd\" + \"efgh\"\n## 'abcdefgh'\nTrue + True\n## 2\n\nIn Python, when a string is added to another string, the two strings are concatenated. This differs from the result in R, which is a “non-numeric argument to binary operator” error."
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#sec-basic-var-types-refs",
    "href": "part-gen-prog/01-basic-var-types.html#sec-basic-var-types-refs",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.7 References",
    "text": "7.7 References\n\n\n\n\n[1] \n\nWhy TRUE + TRUE = 2: Data Types. (Feb. 03, 2020) [Online]. Available: https://www.youtube.com/watch?v=6otW6OXjR8c. [Accessed: May 18, 2022]\n\n\n[2] \nRyan, “Answer to \"how do i detect whether a variable is a function?\". Stack overflow,” Mar. 09, 2009. [Online]. Available: https://stackoverflow.com/a/624948/2859168. [Accessed: Jan. 10, 2023]"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#fa-bullseye-objectives",
    "href": "part-gen-prog/02-prog-functions.html#fa-bullseye-objectives",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.1  Objectives",
    "text": "8.1  Objectives\n\nUnderstand how functions are used in R and python\nUnderstand how to install packages in R and python\nUnderstand how to load packages in R and python\nUse pipes to restructure code so that it is more readable"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#mathematical-operators",
    "href": "part-gen-prog/02-prog-functions.html#mathematical-operators",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.2 Mathematical Operators",
    "text": "8.2 Mathematical Operators\nLet’s first start with a special class of functions that you’re probably familiar with from your math classes - mathematical operators.\nHere are a few of the most important ones:\n\n\nOperation\nR symbol\nPython symbol\n\n\n\nAddition\n+\n+\n\n\nSubtraction\n-\n-\n\n\nMultiplication\n*\n*\n\n\nDivision\n/\n/\n\n\nInteger Division\n%/%\n//\n\n\nModular Division\n%%\n%\n\n\nExponentiation\n^\n**\n\n\n\nThese operands are all for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated (and different between R and python).\n\n\n\n\n\n\nExample: Integer and Modular Division\n\n\n\nInteger division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\nLet’s demonstrate with the problem 14/3, which evaluates to 4.6666667 when division is used, but has integer part 4 and remainder 2.\n\n\nR\nPython\n\n\n\n14 %/% 3 in R would be 4, and 14 %% 3 in R would be 2.\n\n14 %/% 3\n## [1] 4\n14 %% 3\n## [1] 2\n\n\n\n\n14 // 3\n## 4\n14 % 3\n## 2"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#order-of-operations",
    "href": "part-gen-prog/02-prog-functions.html#order-of-operations",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.3 Order of Operations",
    "text": "8.3 Order of Operations\nBoth R and Python operate under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\n\nR\nPython\n\n\n\n\n(1+1)^(5-2) \n## [1] 8\n1 + 2^3 * 4 \n## [1] 33\n3*1^3 \n## [1] 3\n\n\n\n\n(1+1)**(5-2)\n## 8\n1 + 2**3*4\n## 33\n3*1**3\n## 3"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#simple-string-operations",
    "href": "part-gen-prog/02-prog-functions.html#simple-string-operations",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.4 Simple String Operations",
    "text": "8.4 Simple String Operations\nPython has some additional operators that work on strings. In R, you will have to use functions to perform these operations, as R does not have string operators.\n\n\n\n\n\n\nDemo\n\n\n\n\n\nPython\nR\n\n\n\nIn Python, + will concatenate (stick together) two strings. Multiplying a string by an integer will repeat the string the specified number of times.\n\n\"first \" + \"second\"\n## 'first second'\n\"hello \" * 3\n## 'hello hello hello '\n\n\n\nIn R, to concatenate things, we need to use functions: paste or paste0:\n\npaste(\"first\", \"second\", sep = \" \")\n## [1] \"first second\"\npaste(\"first\", \"second\", collapse = \" \")\n## [1] \"first second\"\npaste(c(\"first\", \"second\"), sep = \" \") # sep only works w/ 2 objects passed in\n## [1] \"first\"  \"second\"\npaste(c(\"first\", \"second\"), collapse = \" \") # collapse works on vectors\n## [1] \"first second\"\n\npaste(c(\"a\", \"b\", \"c\", \"d\"), \n      c(\"first\", \"second\", \"third\", \"fourth\"), \n      sep = \"-\", collapse = \" \")\n## [1] \"a-first b-second c-third d-fourth\"\n# sep is used to collapse parameters, then collapse is used to collapse vectors\n\npaste0(c(\"a\", \"b\", \"c\"))\n## [1] \"a\" \"b\" \"c\"\npaste0(\"a\", \"b\", \"c\") # equivalent to paste(..., sep = \"\")\n## [1] \"abc\"\n\nYou don’t need to understand the details of this code at this point in the class, but it is useful to know how to combine strings in both languages."
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#using-functions",
    "href": "part-gen-prog/02-prog-functions.html#using-functions",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.5 Using Functions",
    "text": "8.5 Using Functions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, mathematical operators (like those above) are a special type of functions – but we aren’t going to get into that now.\nWe’re also not going to talk about how to create our own functions just yet. Instead, I’m going to show you how to use functions.\n\n\n\n\n\n\nCheat Sheets!\n\n\n\nIt may be helpful at this point to print out the R reference card1 and the Python reference card2 . These cheat sheets contain useful functions for a variety of tasks in each language .\n\n\nMethods are a special type of function that operate on a specific variable type. In Python, methods are applied using the syntax variable.method_name(). So, you can get the length of a string variable my_string using my_string.length().\nR has methods too, but they are invoked differently. In R, you would get the length of a string variable using length(my_string).\nRight now, it is not really necessary to know too much more about functions than this: you can invoke a function by passing in arguments, and the function will do a task and return the value.\n\n\n\n\n\n\nYour Turn\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nTry out some of the functions mentioned on the R and Python cheatsheets.\nCan you figure out how to define a list or vector of numbers? If so, can you use a function to calculate the maximum value?\nCan you find the R functions that will allow you to repeat a string variable multiple times or concatenate two strings? Can you do this task in Python?\n\n\n\n# Define a vector of numbers\nx <- c(1, 2, 3, 4, 5)\n\n# Calculate the maximum\nmax(x)\n## [1] 5\n\n# function to repeat a variable multiple times\nrep(\"test\", 3)\n## [1] \"test\" \"test\" \"test\"\n# Concatenate strings, using \"ing... \" as the separator\npaste(rep(\"test\", 3), collapse = \"ing... \")\n## [1] \"testing... testing... test\"\n\n\n\n\n# Define a list of numbers\nx = [1, 2, 3, 4, 5]\n\n# Calculate the maximum\nmax(x)\n\n# Repeat a string multiple times\n## 5\nx = (\"test\", )*3 # String multiplication \n                 # have to use a tuple () to get separate items\n# Then use 'yyy'.join(x) to paste items of x together with yyy as separators\n'ing... '.join(x)\n## 'testing... testing... test'"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#overpowerd-calculators",
    "href": "part-gen-prog/02-prog-functions.html#overpowerd-calculators",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.6 Overpowerd Calculators",
    "text": "8.6 Overpowerd Calculators\nNow that you’re familiar with how to use functions, if not how to define them, you are capable of using R or python as a very fancy calculator. Obviously, both languages can do many more interesting things, which we’ll get to, but let’s see if we can make R and Python do some very basic stuff that hopefully isn’t too foreign to you.\n\n\n\n\n\n\nExample: Triangle Side Length\n\n\n\n\n\nA right triangle with sides a, b, and hypotenuse c labeled.\n\n\nConsider this triangle. I’ve measured the sides in an image editor and determined that \\(a = 212\\) pixels, \\(b = 345\\) pixels, and \\(c = 406\\) pixels. I suspect, however, that my measurements aren’t quite right - for one thing, I tried to measure in the center of the line, but it wasn’t easy on the diagonal.\nLet’s assume that my measurements for \\(a\\) and \\(b\\) are accurate and calculate how far off my estimate was for side \\(c\\).\n\n\nR\nPython\n\n\n\n\n# Define variables for the 3 sides of the triangle\na <- 212\nb <- 345\nc_meas <- 406\nc_actual <- sqrt(a^2 + b^2)\n\n# Calculate difference between measured and actual\n# relative to actual \n# and make it a percentage\npct_error <- (c_meas - c_actual)/c_actual * 100\npct_error\n## [1] 0.2640307\n\n\n\n\n# To get the sqrt function, we have to import the math package\nimport math\n\n# Define variables for the 3 sides of the triangle\na = 212\nb = 345\nc_meas = 406\nc_actual = math.sqrt(a**2 + b**2)\n\n# Calculate difference between measured and actual\n# relative to actual \n# and make it a percentage\npct_error = (c_meas - c_actual)/c_actual * 100\npct_error\n## 0.264030681414134\n\n\n\n\nInteresting, I wasn’t as inaccurate as I thought!\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nOf course, if you remember trigonometry, we don’t have to work with right triangles. Let’s see if we can use trigonometric functions to do the same task with an oblique triangle.\n\n\nProblem\nR solution\nPython solution\n\n\n\nJust in case you’ve forgotten your Trig, the Law of Cosines says that \\[c^2 = a^2 + b^2 - 2 a b \\cos(C),\\] where \\(C\\) is the angle between sides \\(a\\) and \\(b\\).\n\n\nAn oblique triangle with sides labeled a, b, and c, and angles labeled as A, B, C with capital letter opposite the lowercase side.\n\n\nI measure side \\(a = 291\\) pixels, side \\(b = 414\\) pixels, and the angle between \\(a\\) and \\(b\\) to be \\(67.6^\\circ\\). What will I likely get for the length of side \\(c\\) in pixels?\nRemember to check whether R and python compute trig functions using radians or degrees! As a reminder, \\(\\pi\\) radians = \\(180^\\circ\\).\n\n\n\n# Define variables for the 3 sides of the triangle\na <- 291\nb <- 414\nc_angle <- 67.6\nc_actual <- sqrt(a^2 + b^2 - 2*a*b*cos(c_angle/180*pi))\nc_actual\n## [1] 405.2886\n\nI measured the length of side \\(c\\) as 407 pixels.\n\n\n\n# To get the sqrt and cos functions, we have to import the math package\nimport math\n\n# Define variables for the 3 sides of the triangle\na = 291\nb = 414\nc_angle = 67.6\nc_actual = math.sqrt(a**2 + b**2 - 2*a*b*math.cos(c_angle/180*math.pi))\nc_actual\n## 405.28860699402117\n\nI measured the length of side \\(c\\) as 407 pixels.\n\n\n\n\n\nCongratulations, if you used a TI-83 in high school to do this sort of stuff, you’re now just about as proficient with R and python as you were with that!"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#libraries",
    "href": "part-gen-prog/02-prog-functions.html#libraries",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.7 Libraries",
    "text": "8.7 Libraries\nBoth R and python have a very robust system for extending the language with user-written packages. These packages will give you access to features that aren’t present in the base language, including new statistical methods, all sorts of plotting and visualization libraries, ways of interacting with data that are way more convenient than the default base language methods, and more.\nThere are tons of considerations to think about when using a new library, like how well it’s maintained, how many dependencies it has, and whether the developers of the package prioritize backwards-compatibility. For the moment, we’re going to ignore most of those considerations in favor of learning how to install packages and how to use functions from packages in our code.\n\n8.7.1 Package management\nBefore we talk about how to install packages, though, we need to step back and think a little bit about the pros and cons of different ways of managing packages, if only because the most common R and python setups use very different approaches.\nImagine that you’re an accomplished programmer, and you are juggling multiple different projects. Each project uses some of the same packages, but some different packages as well. You open up a project that you haven’t run in a year, and you find out that one of the packages you’ve updated more recently breaks a bunch of code you wrote a year ago, because the functions in the package have been renamed.\nWhat could prevent this from happening?\nOne way to solve this problem is to store the packages used in each project inside the project directory, in what we might call a project environment. This will keep each project isolated from the others, so that if you update a package in one project, it doesn’t affect any other project.\nHowever, this approach results in a lot of duplication: for one thing, you have copies of each package hanging around in every folder on your computer. That’s not storage efficient, but it does keep your code from breaking as frequently.\nTypically, Python programmers prefer the first approach (project-specific virtual environments), and R programmers default to the second approach (installing packages at the user or system level).\nThis is one of the things that can make starting to learn python so difficult - it can be hard to make sure you’re using the right environment. It doesn’t help that there are several different environment management systems in python - virtualenv, pipenv, and conda are the main options.\n\n\n\n\n\n\nPackage Management in Python with RStudio\n\n\n\nMany of the instructions here modified from [1].\nconda and virtualenv are both virtual environment management systems.\nConda is sometimes preferred for scientific computing because it handles the complex dependencies that arise from large packages like numpy and scipi and pandas a bit better than pip does alone.\nThis guide assumes you have conda set up already. By default, Chapter 2 just installs python at the system level. If you want to use anaconda or miniconda you should go read the documentation for those installers and follow those steps first. Alternately, you can install and load the reticulate R package and then run install_miniconda() - this will install miniconda somewhere that RStudio can find it, but it may make using miniconda outside of RStudio difficult.\n\n\nSystem package installation\nvenv Terminal setup\nvenv RStudio setup\nconda Terminal setup\nconda RStudio setup\n\n\n\nYou can absolutely install all python packages at the user/system level using pip. This has the previously mentioned disadvantages, but has the major advantage of being very simple.\nI highly recommend that you pick one of these options and use that consistently, rather than trying the advantages and disadvantages of each option in different projects. Here is a webcomic to serve as a cautionary tale if you do not heed this warning.\n\n\nPython Environment, by Randall Munroe of [xkcd](https://xkcd.com/1987/). CC-By-NC-2.5The Python environmental protection agency wants to seal it in a cement chamber, with pictorial messages to future civilizations warning them about the danger of using sudo to install random Python packages.\n\n\n\n\nIn your system terminal, navigate to your project directory. Items within < > are intended to be replaced with values specific to your situation.\n\ncd <project-directory>\npip3 install virtualenv # install virtual environments\n\n# Create a virtual environment\nvirtualenv <env-name>\n\n# Activate your virtual environment\nsource <env-name>/bin/activate\n\n# Install packages\npip install <pkg1> <pkg2> <pkg3>\n\nThen, in RStudio, you will want to run the following lines in the R terminal:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"<env-name>/bin/python\") \n\nYou can make this step permanent by modifying the .Rprofile file in your project directory and adding the Sys.setenv() line to that file.\nRestart your R session before you start trying to work in python.\n\n\nOpen your RStudio project. In your R terminal, run the following lines:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\nvirtualenv_create(envname = \"<env-name>\",\n                  packages = c(\"<pkg1>\", \"<pkg2>\", \"<pkg3>\"))\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"<env-name>/bin/python\") \n\n# Activate your virtual environment\nuse_virtualenv(\"<env-name>\")\n\n# Check that the correct python instance is being used\npy_config()\n\n# Check that packages are installed in your virtual env\ngrep(pattern = \"<pkg1>|<pkg2>|<pkg3>\",\n     x = as.character(py_list_packages(envname = \"<env-name>\")$package))\n\nRestart your R session before you start trying to work in python.\n\n\nThese steps constructed from [2].\n\ncd <project-directory>\n# Create conda environment and install specific python version and packages \nconda create --prefix ./<env-name> python=<python-version> <pkg1> <pkg2> <pkg3> \n\n# Activate your virtual environment\nconda activate ./<env-name>\n\nThen, in RStudio, you will want to run the following lines in the R terminal:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"./<env-name>/bin/python\") \n\nYou can make this step permanent by modifying the .Rprofile file in your project directory and adding the Sys.setenv() line to that file.\nRestart your R session before you start trying to work in python.\n\n\nOpen your RStudio project. In your R terminal, run the following lines:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\nconda_create(envname = \"<env-name>\",\n             packages = c(\"<pkg1>\", \"<pkg2>\", \"<pkg3>\"))\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"<env-name>/bin/python\") \n\n# Activate your virtual environment\nuse_condaenv(\"<env-name>\")\n\n# Check that the correct python instance is being used\npy_config()\n\n# Check that packages are installed in your virtual env\ngrep(pattern = \"<pkg1>|<pkg2>|<pkg3>\",\n     x = as.character(py_list_packages(envname = \"<env-name>\")$package))\n\nRestart your R session before you start trying to work in python.\n\n\n\n\n\n\n\n\n\n\n\nPackage Management in R\n\n\n\nPackage management in R is a bit simpler than package management in python.\n\n\nSystem package installation\nrenv\n\n\n\nThis is by far the most common option for R users and it requires just about no setup on your part. Just install packages with install.packages() if you’re installing packages that are on CRAN, or remotes::install_github() if you’re installing packages from github, and you’re good to go, unless something goes wrong.\n\n\nrenv is an attempt to bring virtual environments to R. It’s pretty useful for complex or long-term projects, but it can be pain for very simple and uncomplicated projects.\nIf you want to use renv, you can do that by following these steps:\n\ninstall.packages(\"renv\")\n\nlibrary(renv)\n\n# Activate renv for a project\nrenv::activate()\n\n# this will install from github or CRAN\nrenv::install(c(\"pkg1\", \"pkg2\", \"githubuser/pkg3\")) \n\nI use renv to manage dependencies in this book, but I don’t generally use it for most of my other projects because the overhead is a bit of a pain.\n\n\n\n\n\n\n8.7.2 Package repositories\nBoth R and Python have package systems, though generally, R is a bit more straightforward to deal with than python, because python has more package systems and I don’t fully understand them all, where all R packages seem to go through the same basic installation process and are just hosted in different places.\n\n\n\nFormally Published\nInformally Published/Beta\n\n\n\nR\n\nCRAN, Bioconductor\n\ngithub and other version control. See the remotes package documentation for all of the options.\n\n\nPython\nPyPi\ngithub and other version control systems\n\n\n\n8.7.3 Installing packages\nOnce you’ve configured your package management in R and python, installation is (usually) relatively simple.\n\n\n\n\n\n\nPackage installation\n\n\n\n\n\nR\nPython\n\n\n\n\n# CRAN packages\ninstall.packages(\"<pkg1>\")\n\n# Github packages\nremotes::install_github(\"username/reponame\")\n\n\n\nIn python, you will typically want to install packages using a system terminal.\n\n# If you're using virtualenv\npip install <pkg1>\n\n# If you're using conda, try this first\nconda install <pkg1>\n# If that fails, try pip\n\nIf you’re working in a quarto markdown chunk you can use a special trick to install a package:\n\nimport sys\n# For pip installation\n!{sys.executable} -m pip install <pkg1>\n\n# For conda installation\n!{sys.executable} -m conda install <pkg1>\n\nLoading the sys package ensures that you’re using the version of python that your file will be compiled with to install the package. Once you’ve installed the package on your machine, you can comment these lines out so that they don’t run every time - this makes it a bit easier if you try to run old code on a new machine, as you can just uncomment those lines.\n\n\n\n\n\n\n8.7.4 Loading Packages\nOnce you have the package installed, you need to load the package into memory so that you can use the functions and data contained within. Again, R and python differ slightly in how programmers conventionally handle this process.\n\nR: Load all of the package’s functions, overwriting already loaded functions if necessary\nPython: Load all of the package’s functions, contained within an object that is either the package name or a shortened alias.\n\nNow, both R and python can load packages in either way, so this isn’t an either/or thing - it’s about knowing what the conventions of the language are, and then deciding whether or not it is appropriate to follow those conventions in your project.\n\n\n\n\n\n\nImport the whole package and all functions\n\n\n\nTo demonstrate this approach, let’s create a simple plot with a plotting library (ggplot2 in R, plotnine in Python).\n\n\nR\nPython\n\n\n\nAll of the other packages in this plot are present by default in any new R environment.\n\nlibrary(ggplot2)\n\n# This code lists all the functions available to be called\npkgs <- search()\npkgs <- pkgs[grep(\"package:\",pkgs)]\n# get all the functions in each package that is loaded\nall_fns <- lapply(pkgs, function(x) as.character(lsf.str(x)))\n# create a data frame\npkg_fns <- data.frame(pkg = rep(pkgs, sapply(all_fns, length)), \n                      fn = unlist(all_fns))\npkg_fns$pkg <- gsub(\"package:\", \"\", pkg_fns$pkg)\n\n\nggplot(pkg_fns, aes(x = pkg, y = after_stat(count), fill = pkg)) + \n  geom_bar() + theme(legend.position = \"none\") + \n  ylab(\"# Functions\") + xlab(\"Package\")\n\n\n\n\n\n\n\nfrom plotnine import *\n\n# I have no clue how to get all callable objects in python \n# classes and methods for those classes make this a lot harder... ugh\n\npkg_fns = r.pkg_fns # This is just the same data from R\n\n(\n  ggplot(pkg_fns, aes(x = \"pkg\", fill = \"pkg\")) + \n  geom_bar(aes(y = after_stat(\"count\"))) + \n  theme(legend_position = \"none\") + \n  ylab(\"# Functions\") + xlab(\"Package\")\n)\n## <ggplot: (8749313477599)>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse functions from the package without loading everything\n\n\n\n\n\nR\nPython\n\n\n\n\n# This code lists all the functions available to be called\npkgs <- search()\npkgs <- pkgs[grep(\"package:\",pkgs)]\n# get all the functions in each package that is loaded\nall_fns <- lapply(pkgs, function(x) as.character(lsf.str(x)))\n# create a data frame\npkg_fns <- data.frame(pkg = rep(pkgs, sapply(all_fns, length)), \n                      fn = unlist(all_fns))\npkg_fns$pkg <- gsub(\"package:\", \"\", pkg_fns$pkg)\n\nggplot2::ggplot(pkg_fns, ggplot2::aes(x = pkg, fill = pkg)) + \n  ggplot2::geom_bar(y = ggplot2::after_stat(count)) + \n  ggplot2::theme(legend.position = \"none\") + \n  ggplot2::xlab(\"Package\") + ggplot2::ylab(\"# Functions\")\n## Error in ggplot2::after_stat(count): object 'count' not found\n\n\n\n\nimport plotnine as p9\npkg_fns = r.pkg_fns\n\n(\n  p9.ggplot(pkg_fns, p9.aes(x = \"pkg\", fill = \"pkg\")) + \n  p9.geom_bar(y = p9.after_stat(\"count\")) + \n  p9.theme(legend_position = \"none\") + \n  p9.xlab(\"Package\") + p9.ylab(\"# Functions\")\n)\n## <ggplot: (8749331920530)>\n\n\n\n\n\n\n\n\n\nIn python, you can use import package as nickname, or you can just use import package and reference the package name directly. There are some packages which have typical aliases, and it’s best to use those so that you can look things up and not get too confused.\n\nCommon Python package aliases\n\n\n\n\n\n\nPackage\nCommon Alias\nExplanation\n\n\n\npandas\npd\nshorter\n\n\nnumpy\nnp\nshorter\n\n\nseaborn\nsns\nThis is a reference to Samuel Norman Seaborn, played by Rob Lowe, in the TV show The West Wing\n\n\nplotnine\np9\n\n\n\nBeautifulSoup (bs4)\nbs\nBeautifulSoup is a reference to Alice in Wonderland. The package name in PyPi is actually bs4."
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#pipes",
    "href": "part-gen-prog/02-prog-functions.html#pipes",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.8 Pipes",
    "text": "8.8 Pipes\nPipes are useful items for moving things from one place to another. In programming, and in particular, in data programming, pipes are operators that let us move data around. In R, we have two primary pipes that are similar (you may see both used if you google for code online). Any R version after 4.1 has a built-in pipe, |>; the tidyverse libraries use a pipe from the magrittr package, %>%.\nFor right now, it’s ok to think of the two pipes as essentially the same (but you can read about the differences [3]).\nFundamentally, a pipe allows you to take a function b() and apply it to x, like b(x), but write it as x |> b() or x %>% b(). This is particularly useful in cases where there are multiple sequential analysis steps, because where in regular notation you have to read the functions from the inside out to understand the sequential steps, with pipes, you have a clear step-by-step list of the order of operations.\nIn Python, there is a pipe function in the Pandas library that works using .pipe(function) notation [4]. From what I’ve seen reading code online, however, pipes are less commonly used in Python code than they are in R code. That’s ok - languages have different conventions, and it is usually best to adopt the convention of the language you’re working in so that your code can be read, run, and maintained by others more easily.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR solution\nPython solution\n\n\n\nGenerate 100 draws from a standard normal distribution and calculate the mean.\nIn R, simulate from a normal distribution with rnorm. In python, use np.random.normal - you’ll have to import numpy as np first.\nUse 3 approaches: 1. Store the data in a variable, then calculate the mean of the variable 2. Calculate the mean of the data by nesting the two functions (e.g. mean(generate_normal(100)) in pseudocode) 3. Calculate the mean of the data using the pipe (e.g. generate_normal(100) |> mean())\nConsider: What are the advantages and disadvantages of each approach? Would your answer change if there were more steps/functions required to get to the right answer?\n\n\n\ndata <- rnorm(100)\nmean(data)\n## [1] -0.1112366\n\nmean(rnorm(100))\n## [1] -0.02118166\n\nlibrary(magrittr) # load the pipe %>%\n\nrnorm(100) %>%\n  mean()\n## [1] -0.04973444\n\nrnorm(100) |> mean()\n## [1] -0.1859403\n\n\n\nIn python, task 3 isn’t really possible, because of the way Python function chaining works, but task 2 is basically the equivalent.\n\nimport numpy as np\nimport pandas as pd\n\nnums = pd.Series(np.random.normal(size = 100))\nnums.mean()\n## 0.15955073405762848\nnp.random.normal(size=100).mean()\n## 0.2244761864608238\n\nThe conclusion here is that it’s far easier to not use the pipe in python because the .function notation that python uses mimics the step-by-step approach of pipes in R even without using the actual pipe function. When you use data frames instead of Series, you might start using the pipe, but only in some circumstances - with user-defined functions, instead of methods. Methods are functions that are attached to a data type (technically, a class) and only work if they are defined for that class - for instance, .mean() is defined for both Pandas series and numpy arrays.\n\n\n\n\n\n\n\n\n\n\n[1] \nG. Makarov, “Use python in rstudio. RPubs,” May 02, 2022. [Online]. Available: https://rpubs.com/georgy_makarov/897844. [Accessed: Jan. 23, 2023]\n\n\n[2] \nD. Blackwood, “How to use python in r with reticulate and conda. Save the data,” Nov. 04, 2021. [Online]. Available: https://medium.com/save-the-data/how-to-use-python-in-r-with-reticulate-and-conda-36685534f06a. [Accessed: Jan. 23, 2023]\n\n\n[3] \nS. Machlis, “Use the new r pipe built into r 4.1. InfoWorld,” Jun. 10, 2021. [Online]. Available: https://www.infoworld.com/article/3621369/use-the-new-r-pipe-built-into-r-41.html. [Accessed: Jan. 13, 2023]\n\n\n[4] \nshadowtalker, “Answer to \"functional pipes in python like %>% from r’s magrittr\". Stack overflow,” Jun. 24, 2015. [Online]. Available: https://stackoverflow.com/a/31037901/2859168. [Accessed: Jan. 13, 2023]"
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#fa-bullseye-objectives",
    "href": "part-gen-prog/03-data-struct.html#fa-bullseye-objectives",
    "title": "9  Data Structures",
    "section": "\n9.1  Objectives",
    "text": "9.1  Objectives\n\nUnderstand the differences between lists, vectors, data frames, matrices, and arrays in R and python\nBe able to use location-based indexing in R or python to pull out subsets of a complex data object"
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#data-structures-overview",
    "href": "part-gen-prog/03-data-struct.html#data-structures-overview",
    "title": "9  Data Structures",
    "section": "\n9.2 Data Structures Overview",
    "text": "9.2 Data Structures Overview\nIn Chapter 7, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complex arrangements of information, but they are still (usually) created using the same data types we have previously discussed.\n\n\n\nHomogeneous\nHeterogeneous\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThose of you who have taken programming classes that were more computer science focused will realize that I am leaving out a lot of information about lower-level structures like pointers. I’m making a deliberate choice to gloss over most of those details in this chapter, because it’s already hard enough to learn 2 languages worth of data structures at a time. In addition, R doesn’t have pointers No Pointers in R, [1], so leaving out this material in python streamlines teaching both two languages, at the cost of overly simplifying some python concepts. If you want to read more about the Python concepts I’m leaving out, check out [2]."
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#lists",
    "href": "part-gen-prog/03-data-struct.html#lists",
    "title": "9  Data Structures",
    "section": "\n9.3 Lists",
    "text": "9.3 Lists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\nA lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\n\n\nR\nPython\n\n\n\n\nx <- list(\"a\", 3, FALSE)\nx\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] 3\n## \n## [[3]]\n## [1] FALSE\n\n\n\n\nx = [\"a\", 3, False]\nx\n## ['a', 3, False]\n\n\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. We call that process indexing.\n\n9.3.1 Indexing\nEvery element in a list has an index (a location, indicated by an integer position)1.\n\n\nR concept\nR code\nPython concept\nPython code\n\n\n\nIn R, we count from 1.\n\n\nAn R-indexed lego list, counting from 1 to 5\n\n\n\n\n\nx <- list(\"a\", 3, FALSE)\n\nx[1] # This returns a list\n## [[1]]\n## [1] \"a\"\nx[1:2] # This returns multiple elements in the list\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] 3\n\nx[[1]] # This returns the item\n## [1] \"a\"\nx[[1:2]] # This doesn't work - you can only use [[]] with a single index\n## Error in x[[1:2]]: subscript out of bounds\n\nIn R, list indexing with [] will return a list with the specified elements.\nTo actually retrieve the item in the list, use [[]]. The only downside to [[]] is that you can only access one thing at a time.\n\n\nIn Python, we count from 0.\n\n\nA python-indexed lego list, counting from 0 to 4\n\n\n\n\n\nx = [\"a\", 3, False]\n\nx[0]\n## 'a'\nx[1]\n## 3\nx[0:2]\n## ['a', 3]\n\nIn Python, we can use single brackets to get an object or a list back out, but we have to know how slices work. Essentially, in Python, 0:2 indicates that we want objects 0 and 1, but want to stop at 2 (not including 2). If you use a slice, Python will return a list; if you use a single index, python just returns the value in that location in the list.\n\n\n\nWe’ll talk more about indexing as it relates to vectors, but indexing is a general concept that applies to just about any multi-value object."
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#vectors",
    "href": "part-gen-prog/03-data-struct.html#vectors",
    "title": "9  Data Structures",
    "section": "\n9.4 Vectors",
    "text": "9.4 Vectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want:\n\n\nvectors of different data types\n\n\n\n9.4.1 Indexing by Location\nEach element in a vector has an index - an integer telling you what the item’s position within the vector is. I’m going to demonstrate indices with the string vector\n\n\n\n\n\n\nR\nPython\n\n\n\n1-indexed language\n0-indexed language\n\n\nCount elements as 1, 2, 3, 4, …, N\nCount elements as 0, 1, 2, 3, , …, N-1\n\n\n\n\n\n\n\n\n\nR\nPython Vectors\nPython Series (Pandas)\n\n\n\nIn R, we create vectors with the c() function, which stands for “concatenate” - basically, we stick a bunch of objects into a row.\n\ndigits_pi <- c(3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5)\n\n# Access individual entries\ndigits_pi[1]\n## [1] 3\ndigits_pi[2]\n## [1] 1\ndigits_pi[3]\n## [1] 4\n\n# R is 1-indexed - a list of 11 things goes from 1 to 11\ndigits_pi[0]\n## numeric(0)\ndigits_pi[11]\n## [1] 5\n\n# Print out the vector\ndigits_pi\n##  [1] 3 1 4 1 5 9 2 6 5 3 5\n\n\n\nIn python, we create vectors using the array function in the numpy module. To add a python module, we use the syntax import <name> as <nickname>. Many modules have conventional (and very short) nicknames - for numpy, we will use np as the nickname. Any functions we reference in the numpy module will then be called using np.fun_name() so that python knows where to find them.2\n\nimport numpy as np\ndigits_list = [3,1,4,1,5,9,2,6,5,3,5]\ndigits_pi = np.array(digits_list)\n\n# Access individual entries\ndigits_pi[0]\n## 3\ndigits_pi[1]\n## 1\ndigits_pi[2]\n\n# Python is 0 indexed - a list of 11 things goes from 0 to 10\n## 4\ndigits_pi[0]\n## 3\ndigits_pi[11] \n\n# multiplication works on the whole vector at once\n## Error in py_call_impl(callable, dots$args, dots$keywords): IndexError: index 11 is out of bounds for axis 0 with size 11\ndigits_pi * 2\n\n# Print out the vector\n## array([ 6,  2,  8,  2, 10, 18,  4, 12, 10,  6, 10])\nprint(digits_pi)\n## [3 1 4 1 5 9 2 6 5 3 5]\n\n\n\nPython has multiple things that look like vectors, including the pandas library’s Series structure. A Series is a one-dimensional array-like object containing a sequence of values and an associated array of labels (called its index).\n\nimport pandas as pd\ndigits_pi = pd.Series([3,1,4,1,5,9,2,6,5,3,5])\n\n# Access individual entries\ndigits_pi[0]\n## 3\ndigits_pi[1]\n## 1\ndigits_pi[2]\n\n# Python is 0 indexed - a list of 11 things goes from 0 to 10\n## 4\ndigits_pi[0]\n## 3\ndigits_pi[11] \n\n# logical indexing works here too\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: 11\ndigits_pi[digits_pi > 3]\n# simple multiplication works in a vectorized manner\n# that is, the whole vector is multiplied at once\n## 2     4\n## 4     5\n## 5     9\n## 7     6\n## 8     5\n## 10    5\n## dtype: int64\ndigits_pi * 2\n\n# Print out the series\n## 0      6\n## 1      2\n## 2      8\n## 3      2\n## 4     10\n## 5     18\n## 6      4\n## 7     12\n## 8     10\n## 9      6\n## 10    10\n## dtype: int64\nprint(digits_pi)\n## 0     3\n## 1     1\n## 2     4\n## 3     1\n## 4     5\n## 5     9\n## 6     2\n## 7     6\n## 8     5\n## 9     3\n## 10    5\n## dtype: int64\n\nThe Series object has a list of labels in the first printed column, and a list of values in the second. If we want, we can specify the labels manually to use as e.g. plot labels later:\n\nimport pandas as pd\nweekdays = pd.Series(['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'], index = ['S', 'M', 'T', 'W', 'R', 'F', 'Sat'])\n\n# access individual objs\nweekdays[0]\n## 'Sunday'\nweekdays[1]\n## 'Monday'\nweekdays['S']\n## 'Sunday'\nweekdays['Sat']\n\n# access the index\n## 'Saturday'\nweekdays.index\n## Index(['S', 'M', 'T', 'W', 'R', 'F', 'Sat'], dtype='object')\nweekdays.index[6] = 'Z' # you can't assign things to the index to change it\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: Index does not support mutable operations\nweekdays\n## S         Sunday\n## M         Monday\n## T        Tuesday\n## W      Wednesday\n## R       Thursday\n## F         Friday\n## Sat     Saturday\n## dtype: object\n\n\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific things as well:\n\n\n\nR\nPython\n\n\n\n\nfavorite_cats <- c(\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\")\n\nfavorite_cats\n## [1] \"Grumpy\"   \"Garfield\" \"Jorts\"    \"Jean\"\n\nfavorite_cats[2] <- \"Nyan Cat\"\n\nfavorite_cats\n## [1] \"Grumpy\"   \"Nyan Cat\" \"Jorts\"    \"Jean\"\n\n\n\n\nfavorite_cats = [\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\"]\n\nfavorite_cats\n## ['Grumpy', 'Garfield', 'Jorts', 'Jean']\nfavorite_cats[1] = \"Nyan Cat\"\n\nfavorite_cats\n## ['Grumpy', 'Nyan Cat', 'Jorts', 'Jean']\n\n\n\n\nIf you’re curious about any of these cats, see the footnotes3.\n\n\n9.4.2 Indexing with Logical Vectors\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that goes along with a vector of another type to use as a logical index.\n\n\nlego vectors - a pink/purple hued set of 1x3 bricks representing the data and a corresponding set of 1x1 grey and black bricks representing the logical index vector of the same length\n\n\nIf we let the black lego represent “True” and the grey lego represent “False”, we can use the logical vector to pull out all values in the main vector.\n\n\n\n\n\n\nBlack = True, Grey = False\nGrey = True, Black = False\n\n\n\n\n\n\nNote that for logical indexing to work properly, the logical index must be the same length as the vector we’re indexing. This constraint will return when we talk about data frames, but for now just keep in mind that logical indexing doesn’t make sense when this constraint isn’t true.\n\n\nIndexing with logical vectors in R\nIndexing with logical vectors in python\n\n\n\n\n# Define a character vector\nweekdays <- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\nweekend <- c(\"Sunday\", \"Saturday\")\n\n# Create logical vectors\nrelax_days <- c(1, 0, 0, 0, 0, 0, 1) # doing this the manual way\nrelax_days <- weekdays %in% weekend # This creates a logical vector \n                                    # with less manual construction\nrelax_days\n## [1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nschool_days <- !relax_days # FALSE if weekend, TRUE if not\nschool_days\n## [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n\n# Using logical vectors to index the character vector\nweekdays[school_days] # print out all school days\n## [1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"\n\n\n\n\nimport numpy as np;\n\nanimals = np.array([\"Cat\", \"Dog\", \"Snake\", \"Lizard\", \"Tarantula\", \"Hamster\", \"Gerbil\", \"Otter\"])\n\n# Define a logical vector\ngood_pets = np.array([True, True, False, False, False, True, True, False])\nbad_pets = np.invert(good_pets) # Invert the logical vector \n                                # so True -> False and False -> True\n\nanimals[good_pets]\n## array(['Cat', 'Dog', 'Hamster', 'Gerbil'], dtype='<U9')\nanimals[bad_pets]\n## array(['Snake', 'Lizard', 'Tarantula', 'Otter'], dtype='<U9')\nanimals[~good_pets] # equivalent to using bad_pets\n## array(['Snake', 'Lizard', 'Tarantula', 'Otter'], dtype='<U9')\n\n\n\n\n\n9.4.3 Reviewing Types\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently-typed things?\n\n\nR\nPython\n\n\n\n\nc(2L, FALSE, 3.1415, \"animal\") # all converted to strings\n## [1] \"2\"      \"FALSE\"  \"3.1415\" \"animal\"\n\nc(2L, FALSE, 3.1415) # converted to numerics\n## [1] 2.0000 0.0000 3.1415\n\nc(2L, FALSE) # converted to integers\n## [1] 2 0\n\n\n\n\nimport numpy as np\n\nnp.array([2, False, 3.1415, \"animal\"]) # all converted to strings\n## array(['2', 'False', '3.1415', 'animal'], dtype='<U32')\nnp.array([2, False, 3.1415]) # converted to floats\n## array([2.    , 0.    , 3.1415])\nnp.array([2, False]) # converted to integers\n## array([2, 0])\n\n\n\n\nAs a reminder, this is an example of implicit type conversion - R and python decide what type to use for you, going with the type that doesn’t lose data but takes up as little space as possible.\nTry it Out!\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nCreate a vector of the integers from one to 30. Use logical indexing to pick out only the numbers which are multiples of 3.\n\n\n\nx <- 1:30\nx [ x %% 3 == 0]\n##  [1]  3  6  9 12 15 18 21 24 27 30\n\n\n\n\nimport numpy as np\nx = np.array(range(1, 31)) # because python is 0 indexed\nx[ x % 3 == 0]\n## array([ 3,  6,  9, 12, 15, 18, 21, 24, 27, 30])\n\n\n\n\n\n\nChallenge\nGeneral Solution\nR Solution\nPython Solution\n\n\n\nExtra challenge: Pick out numbers which are multiples of 2 or 3, but not multiples of 6!\n\n\nThis operation is xor, a.k.a. exclusive or. That is, X or Y, but not X AND Y.\nWe can write xor as (X OR Y) & !(X AND Y) – or we can use a predefined function: xor() in R, ^ in python.\n\n\n\nx <- 1:30\n\nx2 <- x %% 2 == 0 # multiples of 2\nx3 <- x %% 3 == 0 # multiples of 3\nx2xor3 <- xor(x2, x3)\nx2xor3_2 <- (x2 | x3) & !(x2 & x3)\nx[x2xor3]\n##  [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\nx[x2xor3_2]\n##  [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\n\n\n\n\nimport numpy as np\nx = np.array(range(1, 31))\n\nx2 = x % 2 == 0 # multiples of 2\nx3 = x % 3 == 0 # multiples of 3\nx2xor3 = x2 ^ x3\n\nx[x2xor3]\n## array([ 2,  3,  4,  8,  9, 10, 14, 15, 16, 20, 21, 22, 26, 27, 28])"
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#matrices",
    "href": "part-gen-prog/03-data-struct.html#matrices",
    "title": "9  Data Structures",
    "section": "\n9.5 Matrices",
    "text": "9.5 Matrices\nA matrix is the next step after a vector - it’s a set of values arranged in a two-dimensional, rectangular format.\n\n\nMatrix (Lego)\nR\nPython\n\n\n\n\n\nlego depiction of a 3-row, 4-column matrix of 2x2 red-colored blocks\n\n\n\n\n\n# Minimal matrix in R: take a vector, \n# tell R how many rows you want\nmatrix(1:12, nrow = 3)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    4    7   10\n## [2,]    2    5    8   11\n## [3,]    3    6    9   12\n\nmatrix(1:12, ncol = 3) # or columns\n##      [,1] [,2] [,3]\n## [1,]    1    5    9\n## [2,]    2    6   10\n## [3,]    3    7   11\n## [4,]    4    8   12\n\n# by default, R will fill in column-by-column\n# the byrow parameter tells R to go row-by-row\nmatrix(1:12, nrow = 3, byrow = T)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n\n# We can also easily create square matrices \n# with a specific diagonal (this is useful for modeling)\ndiag(rep(1, times = 4))\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    0    0    0\n## [2,]    0    1    0    0\n## [3,]    0    0    1    0\n## [4,]    0    0    0    1\n\n\n\nIn python, matrices are just a special case of a class called ndarray - n-dimensional arrays.\n\nimport numpy as np\n# Minimal ndarray in python by typing in the values in a structured format\nnp.array([[0,  1,  2],\n          [3,  4,  5],\n          [6,  7,  8],\n          [9, 10, 11]])\n# This syntax creates a list of the rows we want in our matrix\n\n# Matrix in python using a data vector and size parameters\n## array([[ 0,  1,  2],\n##        [ 3,  4,  5],\n##        [ 6,  7,  8],\n##        [ 9, 10, 11]])\nnp.reshape(range(0,12), (3,4))\n## array([[ 0,  1,  2,  3],\n##        [ 4,  5,  6,  7],\n##        [ 8,  9, 10, 11]])\nnp.reshape(range(0,12), (4,3))\n## array([[ 0,  1,  2],\n##        [ 3,  4,  5],\n##        [ 6,  7,  8],\n##        [ 9, 10, 11]])\nnp.reshape(range(0,12), (3,4), order = 'F')\n## array([[ 0,  3,  6,  9],\n##        [ 1,  4,  7, 10],\n##        [ 2,  5,  8, 11]])\n\nIn python, we create 2-dimensional arrays (aka matrices) either by creating a list of rows to join together or by reshaping a 1-dimensional array. The trick with reshaping the 1-dimensional array is the order argument: ‘F’ stands for “Fortran-like” and ‘C’ stands for “C-like”… so to go by column, you use ‘F’ and to go by row, you use ‘C’. Totally intuitive, right?\n\n\n\nMost of the problems we’re going to work on will not require much in the way of matrix or array operations. For now, you need the following:\n\nKnow that matrices exist and what they are (2-dimensional arrays of numbers)\nUnderstand how they are indexed (because it is extremely similar to data frames that we’ll work with in the next chapter)\nBe aware that there are lots of functions that depend on matrix operations at their core (including linear regression)\n\nFor more on matrix operations and matrix calculations, see Chapter 10.\n\n9.5.1 Indexing in Matrices\nBoth R and python use [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix in R, we would use [3,1] to get to the third row and first column entry; in python, we would use [2,0] (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\n\nR\nPython\n\n\n\n\nmy_mat <- matrix(1:12, nrow = 3, byrow = T)\n\nmy_mat[3,1] <- 500\n\nmy_mat\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]  500   10   11   12\n\n\n\nRemember that zero-indexing!\n\nimport numpy as np\n\nmy_mat = np.reshape(range(1, 13), (3,4))\n\nmy_mat[2,0] = 500\n\nmy_mat\n## array([[  1,   2,   3,   4],\n##        [  5,   6,   7,   8],\n##        [500,  10,  11,  12]])\n\n\n\n\n\n9.5.2 Matrix Operations\nThere are a number of matrix operations that we need to know for basic programming purposes:\n\nscalar multiplication \\[c*\\textbf{X} = c * \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] = \\left[\\begin{array}{cc} c*x_{1,1} & c*x_{1, 2}\\\\c*x_{2,1} & c*x_{2,2}\\end{array}\\right]\\]\n\ntranspose - flip the matrix across the left top -> right bottom diagonal. \\[t(\\textbf{X}) = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right]^T = \\left[\\begin{array}{cc} x_{1,1} & x_{2,1}\\\\x_{1,2} & x_{2,2}\\end{array}\\right]\\]\n\nmatrix multiplication (dot product) - If you haven’t had this in Linear Algebra, here’s a preview. See [3] for a better explanation \\[\\textbf{X}*\\textbf{Y} = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] * \\left[\\begin{array}{cc} y_{1,1} \\\\y_{2,1} \\end{array}\\right] = \\left[\\begin{array}{c}x_{1,1}*y_{1,1} + x_{1,2}*y_{2,1} \\\\x_{2, 1}*y_{1,1} + x_{2,2}*y_{2,1}\\end{array}\\right]\\] Note that matrix multiplication depends on having matrices of compatible dimensions. If you have two matrices of dimension \\((a \\times b)\\) and \\((c \\times d)\\), then \\(b\\) must be equal to \\(c\\) for the multiplication to work, and your result will be \\((a \\times d)\\).\n\n\n\nR\nPython\n\n\n\n\nx <- matrix(c(1, 2, 3, 4), nrow = 2, byrow = T)\ny <- matrix(c(5, 6), nrow = 2)\n\n# Scalar multiplication\nx * 3\n##      [,1] [,2]\n## [1,]    3    6\n## [2,]    9   12\n3 * x\n##      [,1] [,2]\n## [1,]    3    6\n## [2,]    9   12\n\n# Transpose\nt(x)\n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\nt(y)\n##      [,1] [,2]\n## [1,]    5    6\n\n# matrix multiplication (dot product)\nx %*% y\n##      [,1]\n## [1,]   17\n## [2,]   39\n\n\n\n\nimport numpy as np\nx = np.array([[1,2],[3,4]])\ny = np.array([[5],[6]])\n\n# scalar multiplication\nx*3\n## array([[ 3,  6],\n##        [ 9, 12]])\n3*x\n\n# transpose\n## array([[ 3,  6],\n##        [ 9, 12]])\nx.T # shorthand\n## array([[1, 3],\n##        [2, 4]])\nx.transpose() # Long form\n\n# Matrix multiplication (dot product)\n## array([[1, 3],\n##        [2, 4]])\nnp.dot(x, y)\n## array([[17],\n##        [39]])"
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#arrays",
    "href": "part-gen-prog/03-data-struct.html#arrays",
    "title": "9  Data Structures",
    "section": "\n9.6 Arrays",
    "text": "9.6 Arrays\nArrays are a generalized n-dimensional version of a vector: all elements have the same type, and they are indexed using square brackets in both R and python: [dim1, dim2, dim3, ...]\nI don’t think you will need to create 3+ dimensional arrays in this class, but if you want to try it out, here is some code.\n\n\nR\nPython\n\n\n\n\narray(1:8, dim = c(2,2,2))\n## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    5    7\n## [2,]    6    8\n\nNote that displaying this requires 2 slices, since it’s hard to display 3D information in a 2D terminal arrangement.\n\n\n\nimport numpy as np\n\nnp.array([[[1,2],[3,4]],[[5,6], [7,8]]])\n## array([[[1, 2],\n##         [3, 4]],\n## \n##        [[5, 6],\n##         [7, 8]]])"
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#data-frames",
    "href": "part-gen-prog/03-data-struct.html#data-frames",
    "title": "9  Data Structures",
    "section": "\n9.7 Data Frames",
    "text": "9.7 Data Frames\nIn the previous sections, we talked about homogeneous structures: arrangements of data, like vectors and matrices, where every entry in the larger structure has the same type. In the rest of this chapter, we’ll be talking about the root of most data science analysis projects: the data frame.\nLike an excel spreadsheet, data frames are arrangements of data in columns and rows.\nThis format has two main restrictions:\n\nEvery entry in each column must have the same data type\nEvery column must have the same number of rows\n\n\n\nA lego data frame of 4 columns and 12 rows. Each column is a separate color hue (data type), with slight variations in the hue of each individual bricks.\n\n\nThe picture above shows a data frame of 4 columns, each with a different data type (brick size/hue). The data frame has 12 rows. This picture may look similar to one that we used to show logical indexing in the last chapter, and that is not a coincidence. You can get everything from a data frame that you would get from a collection of 4 separate vectors… but there are advantages to keeping things in a data frame instead.\n\n\n\n\n\n\nMotivating Data Frames: Working with Multiple Vectors\n\n\n\nConsider for a moment https://worldpopulationreview.com/states, which lists the population of each state. You can find this dataset in CSV form here.\nIn the previous sections, we learned how to make different vectors in R, numpy, and pandas. Let’s see what happens when we work with the data above as a set of vectors/Series compared to what happens when we work with data frames.\n\n\nPython\nR\n\n\n\n(I’m going to cheat and read this in using pandas functions we haven’t learned yet to demonstrate why this stuff matters.)\n\nimport pandas as pd\n\ndata = pd.read_html(\"https://worldpopulationreview.com/states\")[0]\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: No tables found\nlist(data.columns) # get names\n\n# Create a few population series\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2022 = pd.Series(data['2022 Population'].values, index = data['State'].values)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2021 = pd.Series(data['2021 Population'].values, index = data['State'].values)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2010 = pd.Series(data['2010 Census'].values, index = data['State'].values)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\n\nSuppose that we want to sort each population vector by the population in that year.\n\nimport pandas as pd\ndata = pd.read_html(\"https://worldpopulationreview.com/states\")[0]\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: No tables found\npopulation2022 = pd.Series(data['2022 Population'].values, index = data['State'].values).sort_values()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2021 = pd.Series(data['2021 Population'].values, index = data['State'].values).sort_values()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2010 = pd.Series(data['2010 Census'].values, index = data['State'].values).sort_values()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2022.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'population2022' is not defined\npopulation2021.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'population2021' is not defined\npopulation2010.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'population2010' is not defined\n\nThe only problem is that by doing this, we’ve now lost the ordering that matched across all 3 vectors. Pandas Series are great for this, because they use labels that allow us to reconstitute which value corresponds to which label, but in R or even in numpy arrays, vectors don’t inherently come with labels. In these situations, sorting by one value can actually destroy the connection between two vectors!\n\n\n\ndf <- read.csv(\"https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/population2022.csv\")\n\n# Use vectors instead of the data frame\nstate <- df$State\npop2022 <- df$Pop\npop2021 <- df$Pop2021\npop2010 <- df$Pop2010\n\n# Create a vector to index population in 2022 in order\norder2022 <- order(pop2022)\n\n# To keep variables together, we have to do things like this:\nhead(state[order2022])\n## [1] \"Wyoming\"              \"Vermont\"              \"District of Columbia\"\n## [4] \"Alaska\"               \"North Dakota\"         \"South Dakota\"\nhead(pop2022[order2022])\n## [1] 582233 622882 718355 720763 774008 902542\n\n# It makes more sense just to reorder the whole data frame:\nhead(df[order2022,])\n##    rank                State    Pop  Growth Pop2021 Pop2010 growthSince2010\n## 52   52              Wyoming 582233  0.0020  581075  564487          0.0314\n## 51   51              Vermont 622882 -0.0006  623251  625879         -0.0048\n## 50   50 District of Columbia 718355  0.0059  714153  605226          0.1869\n## 49   49               Alaska 720763 -0.0050  724357  713910          0.0096\n## 48   48         North Dakota 774008  0.0052  770026  674715          0.1472\n## 47   47         South Dakota 902542  0.0066  896581  816166          0.1058\n##    Percent    density\n## 52  0.0017     5.9967\n## 51  0.0019    67.5797\n## 50  0.0021 11776.3115\n## 49  0.0021     1.2631\n## 48  0.0023    11.2173\n## 47  0.0027    11.9052\n\n\n\n\n\n\nThe primary advantage to data frames is that rows of data are kept together. Since we often think of a row of data as a single observation in a sample, this is an extremely important feature that makes data frames a huge improvement on a collection of vectors of the same length: it’s much harder for observations in a single row to get shuffled around and mismatched!\n\n9.7.1 Data Frame Basics\nIn R, data frames are built in as type data.frame, though there are packages that provide other implementations of data frames that have additional features, such as the tibble package used in many other common packages. We will cover functions from both base R and the tibble package in this chapter.\nIn Python, we will use the pandas library, which is conventionally abbreviated pd. So before you use any data frames in python, you will need to add the following line to your code: import pandas as pd.\n\n\n\n\n\n\nExamining Data Frames\n\n\n\n\n\nR\nPython\n\n\n\nWhen you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The head(n) command shows the first \\(n\\) rows of a data frame (enough to see what’s there, not enough to overflow your screen).\n\ndata(mtcars) # Load the data -- included in base R\nhead(mtcars) # Look at the first 6 rows\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nstr(mtcars) # Examine the structure of the object\n## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\nYou can change column values or add new columns easily using assignment. The summary() function can be used on specific columns to perform summary operations (a 5-number summary useful for making e.g. boxplots is provided by default).\n\nmtcars$gpm <- 1/mtcars$mpg # gpm is sometimes used to assess efficiency\n\nsummary(mtcars$gpm)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n## 0.02950 0.04386 0.05208 0.05423 0.06483 0.09615\nsummary(mtcars$mpg)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   10.40   15.43   19.20   20.09   22.80   33.90\n\nOften, it is useful to know the dimensions of a data frame. The number of rows can be obtained by using nrow(df) and similarly, the columns can be obtained using ncol(df) (or, get both with dim()). There is also an easy way to get a summary of each column in the data frame, using summary().\n\nsummary(mtcars)\n##       mpg             cyl             disp             hp       \n##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n##       drat             wt             qsec             vs        \n##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n##        am              gear            carb            gpm         \n##  Min.   :0.0000   Min.   :3.000   Min.   :1.000   Min.   :0.02950  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:0.04386  \n##  Median :0.0000   Median :4.000   Median :2.000   Median :0.05208  \n##  Mean   :0.4062   Mean   :3.688   Mean   :2.812   Mean   :0.05423  \n##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:0.06483  \n##  Max.   :1.0000   Max.   :5.000   Max.   :8.000   Max.   :0.09615\ndim(mtcars)\n## [1] 32 12\nnrow(mtcars)\n## [1] 32\nncol(mtcars)\n## [1] 12\n\nMissing variables in an R data frame are indicated with NA.\n\n\nWhen you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The df.head(n) command shows the first \\(n\\) rows of a data frame (enough to see what’s there, not enough to overflow your screen).\n\nmtcars = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\")\n\nmtcars.head(5)\n##           Unnamed: 0   mpg  cyl   disp   hp  ...   qsec  vs  am  gear  carb\n## 0          Mazda RX4  21.0    6  160.0  110  ...  16.46   0   1     4     4\n## 1      Mazda RX4 Wag  21.0    6  160.0  110  ...  17.02   0   1     4     4\n## 2         Datsun 710  22.8    4  108.0   93  ...  18.61   1   1     4     1\n## 3     Hornet 4 Drive  21.4    6  258.0  110  ...  19.44   1   0     3     1\n## 4  Hornet Sportabout  18.7    8  360.0  175  ...  17.02   0   0     3     2\n## \n## [5 rows x 12 columns]\nmtcars.info()\n## <class 'pandas.core.frame.DataFrame'>\n## RangeIndex: 32 entries, 0 to 31\n## Data columns (total 12 columns):\n##  #   Column      Non-Null Count  Dtype  \n## ---  ------      --------------  -----  \n##  0   Unnamed: 0  32 non-null     object \n##  1   mpg         32 non-null     float64\n##  2   cyl         32 non-null     int64  \n##  3   disp        32 non-null     float64\n##  4   hp          32 non-null     int64  \n##  5   drat        32 non-null     float64\n##  6   wt          32 non-null     float64\n##  7   qsec        32 non-null     float64\n##  8   vs          32 non-null     int64  \n##  9   am          32 non-null     int64  \n##  10  gear        32 non-null     int64  \n##  11  carb        32 non-null     int64  \n## dtypes: float64(5), int64(6), object(1)\n## memory usage: 3.1+ KB\n\nYou can change column values or add new columns easily using assignment. It’s also easy to access specific columns to perform summary operations. You can access a column named xyz using df.xyz or using df[\"xyz\"]. To create a new column, you must use df[\"xyz\"].\n\nmtcars[\"gpm\"] = 1/mtcars.mpg # gpm is sometimes used to assess efficiency\n\nmtcars.gpm.describe()\n## count    32.000000\n## mean      0.054227\n## std       0.016424\n## min       0.029499\n## 25%       0.043860\n## 50%       0.052083\n## 75%       0.064834\n## max       0.096154\n## Name: gpm, dtype: float64\nmtcars.mpg.describe()\n## count    32.000000\n## mean     20.090625\n## std       6.026948\n## min      10.400000\n## 25%      15.425000\n## 50%      19.200000\n## 75%      22.800000\n## max      33.900000\n## Name: mpg, dtype: float64\n\nOften, it is useful to know the dimensions of a data frame. The dimensions of a data frame (rows x columns) can be accessed using df.shape. There is also an easy way to get a summary of each column in the data frame, using df.describe().\n\nmtcars.describe()\n##              mpg        cyl        disp  ...       gear     carb        gpm\n## count  32.000000  32.000000   32.000000  ...  32.000000  32.0000  32.000000\n## mean   20.090625   6.187500  230.721875  ...   3.687500   2.8125   0.054227\n## std     6.026948   1.785922  123.938694  ...   0.737804   1.6152   0.016424\n## min    10.400000   4.000000   71.100000  ...   3.000000   1.0000   0.029499\n## 25%    15.425000   4.000000  120.825000  ...   3.000000   2.0000   0.043860\n## 50%    19.200000   6.000000  196.300000  ...   4.000000   2.0000   0.052083\n## 75%    22.800000   8.000000  326.000000  ...   4.000000   4.0000   0.064834\n## max    33.900000   8.000000  472.000000  ...   5.000000   8.0000   0.096154\n## \n## [8 rows x 12 columns]\nmtcars.shape\n## (32, 13)\n\nMissing variables in a pandas data frame are indicated with nan or NULL.\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nSetup\nProblem\nR Solution\nPython Solution\n\n\n\nThe dataset state.x77 contains information on US state statistics in the 1970s. By default, it is a matrix, but we can easily convert it to a data frame, as shown below.\n\ndata(state)\nstate_facts <- data.frame(state.x77)\nstate_facts <- cbind(state = row.names(state_facts), state_facts, stringsAsFactors = F) \n# State names were stored as row labels\n# Store them in a variable instead, and add it to the data frame\n\nrow.names(state_facts) <- NULL # get rid of row names\n\nhead(state_facts)\n##        state Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## 1    Alabama       3615   3624        2.1    69.05   15.1    41.3    20  50708\n## 2     Alaska        365   6315        1.5    69.31   11.3    66.7   152 566432\n## 3    Arizona       2212   4530        1.8    70.55    7.8    58.1    15 113417\n## 4   Arkansas       2110   3378        1.9    70.66   10.1    39.9    65  51945\n## 5 California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## 6   Colorado       2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n# Write data out so that we can read it in using Python\nwrite.csv(state_facts, file = \"data/state_facts.csv\", row.names = F)\n## Error in file(file, ifelse(append, \"a\", \"w\")): cannot open the connection\n\nWe can write out the built in R data and read it in using pd.read_csv, which creates a DataFrame in pandas.\n\nimport pandas as pd\n\nstate_facts = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/state_facts.csv\")\n\n\n\n\nHow many rows and columns does it have? Can you find different ways to get that information?\nThe Illiteracy column contains the percent of the population of each state that is illiterate. Calculate the number of people in each state who are illiterate, and store that in a new column called TotalNumIlliterate. Note: Population contains the population in thousands.\nCalculate the average population density of each state (population per square mile) and store it in a new column PopDensity. Using the R reference card, can you find functions that you can combine to get the state with the minimum population density?\n\n\n\n\n# 3 ways to get rows and columns\nstr(state_facts)\n## 'data.frame':    50 obs. of  9 variables:\n##  $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Population: num  3615 365 2212 2110 21198 ...\n##  $ Income    : num  3624 6315 4530 3378 5114 ...\n##  $ Illiteracy: num  2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ...\n##  $ Life.Exp  : num  69 69.3 70.5 70.7 71.7 ...\n##  $ Murder    : num  15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ...\n##  $ HS.Grad   : num  41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ...\n##  $ Frost     : num  20 152 15 65 20 166 139 103 11 60 ...\n##  $ Area      : num  50708 566432 113417 51945 156361 ...\ndim(state_facts)\n## [1] 50  9\nnrow(state_facts)\n## [1] 50\nncol(state_facts)\n## [1] 9\n\n# Illiteracy\nstate_facts$TotalNumIlliterate <- state_facts$Population * 1e3 * (state_facts$Illiteracy/100) \n\n# Population Density\nstate_facts$PopDensity <- state_facts$Population * 1e3/state_facts$Area \n# in people per square mile\n\n# minimum population\nstate_facts$state[which.min(state_facts$PopDensity)]\n## [1] \"Alaska\"\n\n\n\n\n# Ways to get rows and columns\nstate_facts.shape\n## (50, 9)\nstate_facts.index.size # rows\n## 50\nstate_facts.columns.size # columns\n## 9\nstate_facts.info() # columns + rows + missing counts + data types\n\n# Illiteracy\n## <class 'pandas.core.frame.DataFrame'>\n## RangeIndex: 50 entries, 0 to 49\n## Data columns (total 9 columns):\n##  #   Column      Non-Null Count  Dtype  \n## ---  ------      --------------  -----  \n##  0   state       50 non-null     object \n##  1   Population  50 non-null     int64  \n##  2   Income      50 non-null     int64  \n##  3   Illiteracy  50 non-null     float64\n##  4   Life.Exp    50 non-null     float64\n##  5   Murder      50 non-null     float64\n##  6   HS.Grad     50 non-null     float64\n##  7   Frost       50 non-null     int64  \n##  8   Area        50 non-null     int64  \n## dtypes: float64(4), int64(4), object(1)\n## memory usage: 3.6+ KB\nstate_facts[\"TotalNumIlliterate\"] = state_facts[\"Population\"] * 1e3 * state_facts[\"Illiteracy\"]/100\n\n# Population Density\nstate_facts[\"PopDensity\"] = state_facts[\"Population\"] * 1e3/state_facts[\"Area\"] \n# in people per square mile\n\n# minimum population\nmin_dens = state_facts[\"PopDensity\"].min()\n# Get location of minimum population\nloc_min_dens = state_facts.PopDensity.isin([min_dens])\n# Pull out matching state\nstate_facts.state[loc_min_dens]\n## 1    Alaska\n## Name: state, dtype: object\n\n\n\n\n\n\n\n9.7.2 Creating Data Frames\nIt is possible to create data frames from scratch by building them out of simpler components, such as lists of vectors or dicts of Series. This tends to be useful for small data sets, but it is more common to read data in from e.g. CSV files, which I’ve used several times already but haven’t yet shown you how to do (see Chapter 15 for the full how-to).\n\n\n\n\n\n\nData Frames from Scratch\n\n\n\n\n\nR\nPython\n\n\n\n\nmath_and_lsd <- data.frame(\n  lsd_conc = c(1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41),\n  test_score = c(78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97))\nmath_and_lsd\n##   lsd_conc test_score\n## 1     1.17      78.93\n## 2     2.97      58.20\n## 3     3.26      67.47\n## 4     4.69      37.47\n## 5     5.83      45.65\n## 6     6.00      32.92\n## 7     6.41      29.97\n\n# add a column - character vector\nmath_and_lsd$subjective <- c(\"finally coming back\", \"getting better\", \"it's totally better\", \"really tripping out\", \"is it over?\", \"whoa, man\", \"I can taste color, but I can't do math\")\n\nmath_and_lsd\n##   lsd_conc test_score                             subjective\n## 1     1.17      78.93                    finally coming back\n## 2     2.97      58.20                         getting better\n## 3     3.26      67.47                    it's totally better\n## 4     4.69      37.47                    really tripping out\n## 5     5.83      45.65                            is it over?\n## 6     6.00      32.92                              whoa, man\n## 7     6.41      29.97 I can taste color, but I can't do math\n\n\n\n\nmath_and_lsd = pd.DataFrame({\n  \"lsd_conc\": [1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41],\n  \"test_score\": [78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97]})\nmath_and_lsd\n\n# add a column - character vector\n##    lsd_conc  test_score\n## 0      1.17       78.93\n## 1      2.97       58.20\n## 2      3.26       67.47\n## 3      4.69       37.47\n## 4      5.83       45.65\n## 5      6.00       32.92\n## 6      6.41       29.97\nmath_and_lsd[\"subjective\"] = [\"finally coming back\", \"getting better\", \"it's totally better\", \"really tripping out\", \"is it over?\", \"whoa, man\", \"I can taste color, but I can't do math\"]\n\nmath_and_lsd\n##    lsd_conc  test_score                              subjective\n## 0      1.17       78.93                     finally coming back\n## 1      2.97       58.20                          getting better\n## 2      3.26       67.47                     it's totally better\n## 3      4.69       37.47                     really tripping out\n## 4      5.83       45.65                             is it over?\n## 5      6.00       32.92                               whoa, man\n## 6      6.41       29.97  I can taste color, but I can't do math\n\n\n\n\n\n\nWhile it’s not so hard to create data frames from scratch for small data sets, it’s very tedious if you have a lot of data (or if you can’t type accurately). An easier way to create a data frame (rather than typing the whole thing in) is to read in data from somewhere else - a file, a table on a webpage, etc. We’re not going to go into the finer points of this (you’ll get into that in Chapter 15), but it is useful to know how to read neatly formatted data.\nOne source of (relatively neat) data is the TidyTuesday github repository4\n\n\n\n\n\n\nReading in Data\n\n\n\n\n\nBase R\nreadR package\nPandas\n\n\n\nIn Base R, we can read the data in using the read.csv function\n\nairmen <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv')\nhead(airmen)\n##                    name last_name    first_name      graduation_date\n## 1   Adams, John H., Jr.     Adams  John H., Jr. 1945-04-15T00:00:00Z\n## 2           Adams, Paul     Adams          Paul 1943-04-29T00:00:00Z\n## 3 Adkins, Rutherford H.    Adkins Rutherford H. 1944-10-16T00:00:00Z\n## 4    Adkins, Winston A.    Adkins    Winston A. 1944-02-08T00:00:00Z\n## 5 Alexander, Halbert L. Alexander    Halbert L. 1944-11-20T00:00:00Z\n## 6  Alexander, Harvey R. Alexander     Harvey R. 1944-04-15T00:00:00Z\n##   rank_at_graduation     class graduated_from    pilot_type\n## 1             2nd Lt   SE-45-B           TAAF Single engine\n## 2             2nd Lt   SE-43-D           TAAF Single engine\n## 3             2nd Lt SE-44-I-1           TAAF Single engine\n## 4             2nd Lt   TE-44-B           TAAF   Twin engine\n## 5             2nd Lt   SE-44-I           TAAF Single engine\n## 6             2nd Lt   TE-44-D           TAAF   Twin engine\n##   military_hometown_of_record state aerial_victory_credits\n## 1                 Kansas City    KS                   <NA>\n## 2                  Greenville    SC                   <NA>\n## 3                  Alexandria    VA                   <NA>\n## 4                     Chicago    IL                   <NA>\n## 5                  Georgetown    IL                   <NA>\n## 6                  Georgetown    IL                   <NA>\n##   number_of_aerial_victory_credits reported_lost reported_lost_date\n## 1                                0          <NA>               <NA>\n## 2                                0          <NA>               <NA>\n## 3                                0          <NA>               <NA>\n## 4                                0          <NA>               <NA>\n## 5                                0          <NA>               <NA>\n## 6                                0          <NA>               <NA>\n##   reported_lost_location                                   web_profile\n## 1                   <NA>     https://cafriseabove.org/john-h-adams-jr/\n## 2                   <NA>          https://cafriseabove.org/paul-adams/\n## 3                   <NA> https://cafriseabove.org/rutherford-h-adkins/\n## 4                   <NA>                                          <NA>\n## 5                   <NA> https://cafriseabove.org/halbert-l-alexander/\n## 6                   <NA>  https://cafriseabove.org/harvey-r-alexander/\n\n\n\nIf we want instead to create a tibble, we can use the readr package’s read_csv function, which is a bit more robust and has a few additional features.\n\nlibrary(readr)\nairmen <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv')\nhead(airmen)\n## # A tibble: 6 × 16\n##   name         last_…¹ first…² graduation_date     rank_…³ class gradu…⁴ pilot…⁵\n##   <chr>        <chr>   <chr>   <dttm>              <chr>   <chr> <chr>   <chr>  \n## 1 Adams, John… Adams   John H… 1945-04-15 00:00:00 2nd Lt  SE-4… TAAF    Single…\n## 2 Adams, Paul  Adams   Paul    1943-04-29 00:00:00 2nd Lt  SE-4… TAAF    Single…\n## 3 Adkins, Rut… Adkins  Ruther… 1944-10-16 00:00:00 2nd Lt  SE-4… TAAF    Single…\n## 4 Adkins, Win… Adkins  Winsto… 1944-02-08 00:00:00 2nd Lt  TE-4… TAAF    Twin e…\n## 5 Alexander, … Alexan… Halber… 1944-11-20 00:00:00 2nd Lt  SE-4… TAAF    Single…\n## 6 Alexander, … Alexan… Harvey… 1944-04-15 00:00:00 2nd Lt  TE-4… TAAF    Twin e…\n## # … with 8 more variables: military_hometown_of_record <chr>, state <chr>,\n## #   aerial_victory_credits <chr>, number_of_aerial_victory_credits <dbl>,\n## #   reported_lost <chr>, reported_lost_date <dttm>,\n## #   reported_lost_location <chr>, web_profile <chr>, and abbreviated variable\n## #   names ¹​last_name, ²​first_name, ³​rank_at_graduation, ⁴​graduated_from,\n## #   ⁵​pilot_type\n\n\n\nIn pandas, we can read the csv using pd.read_csv\n\nimport pandas as pd\n\nairmen = pd.read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv\")\nairmen.head()\n##                     name  ...                                    web_profile\n## 0    Adams, John H., Jr.  ...      https://cafriseabove.org/john-h-adams-jr/\n## 1            Adams, Paul  ...           https://cafriseabove.org/paul-adams/\n## 2  Adkins, Rutherford H.  ...  https://cafriseabove.org/rutherford-h-adkins/\n## 3     Adkins, Winston A.  ...                                            NaN\n## 4  Alexander, Halbert L.  ...  https://cafriseabove.org/halbert-l-alexander/\n## \n## [5 rows x 16 columns]\n\n\n\n\n\n\n\n9.7.3 Working with Data Frames\nOften, we want to know what a data frame contains. R and pandas both have easy summary methods for data frames.\n\n\n\n\n\n\nData Frame Summaries\n\n\n\nNotice that the type of summary depends on the data type.\n\n\nR\nPython\n\n\n\n\nsummary(airmen)\n##      name            last_name          first_name       \n##  Length:1006        Length:1006        Length:1006       \n##  Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character  \n##                                                          \n##                                                          \n##                                                          \n##                                                          \n##  graduation_date                   rank_at_graduation    class          \n##  Min.   :1942-03-06 00:00:00.000   Length:1006        Length:1006       \n##  1st Qu.:1943-10-22 00:00:00.000   Class :character   Class :character  \n##  Median :1944-05-23 00:00:00.000   Mode  :character   Mode  :character  \n##  Mean   :1944-07-02 13:18:52.462                                        \n##  3rd Qu.:1945-04-15 00:00:00.000                                        \n##  Max.   :1948-10-12 00:00:00.000                                        \n##  NA's   :11                                                             \n##  graduated_from      pilot_type        military_hometown_of_record\n##  Length:1006        Length:1006        Length:1006                \n##  Class :character   Class :character   Class :character           \n##  Mode  :character   Mode  :character   Mode  :character           \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##     state           aerial_victory_credits number_of_aerial_victory_credits\n##  Length:1006        Length:1006            Min.   :0.0000                  \n##  Class :character   Class :character       1st Qu.:0.0000                  \n##  Mode  :character   Mode  :character       Median :0.0000                  \n##                                            Mean   :0.1118                  \n##                                            3rd Qu.:0.0000                  \n##                                            Max.   :4.0000                  \n##                                                                            \n##  reported_lost      reported_lost_date   reported_lost_location\n##  Length:1006        Min.   :1943-07-02   Length:1006           \n##  Class :character   1st Qu.:1943-07-02   Class :character      \n##  Mode  :character   Median :1943-07-02   Mode  :character      \n##                     Mean   :1943-07-02                         \n##                     3rd Qu.:1943-07-02                         \n##                     Max.   :1943-07-02                         \n##                     NA's   :1004                               \n##  web_profile       \n##  Length:1006       \n##  Class :character  \n##  Mode  :character  \n##                    \n##                    \n##                    \n## \n\nlibrary(skimr) # Fancier summaries\nskim(airmen)\n\n\nData summary\n\n\nName\nairmen\n\n\nNumber of rows\n1006\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nnumeric\n1\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nname\n0\n1.00\n9\n28\n0\n1003\n0\n\n\nlast_name\n0\n1.00\n3\n12\n0\n617\n0\n\n\nfirst_name\n0\n1.00\n3\n17\n0\n804\n0\n\n\nrank_at_graduation\n5\n1.00\n3\n14\n0\n7\n0\n\n\nclass\n20\n0.98\n3\n9\n0\n72\n0\n\n\ngraduated_from\n0\n1.00\n4\n23\n0\n4\n0\n\n\npilot_type\n0\n1.00\n11\n13\n0\n5\n0\n\n\nmilitary_hometown_of_record\n9\n0.99\n3\n19\n0\n366\n0\n\n\nstate\n11\n0.99\n2\n5\n0\n48\n0\n\n\naerial_victory_credits\n934\n0.07\n31\n137\n0\n50\n0\n\n\nreported_lost\n1004\n0.00\n1\n1\n0\n1\n0\n\n\nreported_lost_location\n1004\n0.00\n23\n23\n0\n1\n0\n\n\nweb_profile\n813\n0.19\n34\n95\n0\n190\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nnumber_of_aerial_victory_credits\n0\n1\n0.11\n0.46\n0\n0\n0\n0\n4\n▇▁▁▁▁\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\ngraduation_date\n11\n0.99\n1942-03-06\n1948-10-12\n1944-05-23\n52\n\n\nreported_lost_date\n1004\n0.00\n1943-07-02\n1943-07-02\n1943-07-02\n1\n\n\n\n\n\n\n\n\n# All variables - strings are summarized with NaNs\nairmen.describe(include = 'all')\n\n# Only summarize numeric variables\n##                       name  ...                                        web_profile\n## count                 1006  ...                                                193\n## unique                1003  ...                                                190\n## top     Brothers, James E.  ...  https://cafriseabove.org/captain-graham-smith-...\n## freq                     2  ...                                                  2\n## mean                   NaN  ...                                                NaN\n## std                    NaN  ...                                                NaN\n## min                    NaN  ...                                                NaN\n## 25%                    NaN  ...                                                NaN\n## 50%                    NaN  ...                                                NaN\n## 75%                    NaN  ...                                                NaN\n## max                    NaN  ...                                                NaN\n## \n## [11 rows x 16 columns]\nairmen.describe(include = [np.number])\n\n# Only summarize string variables (objects)\n##        number_of_aerial_victory_credits\n## count                       1006.000000\n## mean                           0.111829\n## std                            0.457844\n## min                            0.000000\n## 25%                            0.000000\n## 50%                            0.000000\n## 75%                            0.000000\n## max                            4.000000\nairmen.describe(include = ['O'])\n\n# Get counts of how many NAs in each column\n##                       name  ...                                        web_profile\n## count                 1006  ...                                                193\n## unique                1003  ...                                                190\n## top     Brothers, James E.  ...  https://cafriseabove.org/captain-graham-smith-...\n## freq                     2  ...                                                  2\n## \n## [4 rows x 15 columns]\nairmen.info(show_counts=True)\n## <class 'pandas.core.frame.DataFrame'>\n## RangeIndex: 1006 entries, 0 to 1005\n## Data columns (total 16 columns):\n##  #   Column                            Non-Null Count  Dtype  \n## ---  ------                            --------------  -----  \n##  0   name                              1006 non-null   object \n##  1   last_name                         1006 non-null   object \n##  2   first_name                        1006 non-null   object \n##  3   graduation_date                   995 non-null    object \n##  4   rank_at_graduation                999 non-null    object \n##  5   class                             986 non-null    object \n##  6   graduated_from                    1006 non-null   object \n##  7   pilot_type                        1006 non-null   object \n##  8   military_hometown_of_record       997 non-null    object \n##  9   state                             995 non-null    object \n##  10  aerial_victory_credits            72 non-null     object \n##  11  number_of_aerial_victory_credits  1006 non-null   float64\n##  12  reported_lost                     2 non-null      object \n##  13  reported_lost_date                2 non-null      object \n##  14  reported_lost_location            2 non-null      object \n##  15  web_profile                       193 non-null    object \n## dtypes: float64(1), object(15)\n## memory usage: 125.9+ KB\n\nIn pandas, you will typically want to separate out .describe() calls for numeric and non-numeric columns. Another handy function in pandas is .info(), which you can use to show the number of non-NA values. This is particularly useful in sparse datasets where there may be a LOT of missing values and you may want to find out which columns have useful information for more than just a few rows."
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#sec-data-struct-refs",
    "href": "part-gen-prog/03-data-struct.html#sec-data-struct-refs",
    "title": "9  Data Structures",
    "section": "\n9.8 References",
    "text": "9.8 References\n\n\n\n\n[1] \nN. Matloff, The art of r programming: A tour of statistical software design. No Starch Press, 2011 [Online]. Available: https://books.google.com/books?id=o2aLBAAAQBAJ\n\n\n\n[2] \nM. Fripp, “Answer to \"python pandas dataframe, is it pass-by-value or pass-by-reference\". Stack overflow,” Aug. 12, 2016. [Online]. Available: https://stackoverflow.com/a/38925257/2859168. [Accessed: Jan. 10, 2023]\n\n\n[3] \nMathIsFun.com, “How to multiply matrices. Math is fun,” 2021. [Online]. Available: https://www.mathsisfun.com/algebra/matrix-multiplying.html. [Accessed: Jan. 10, 2023]"
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#fa-bullseye-objectives",
    "href": "part-gen-prog/035-matrix-calcs.html#fa-bullseye-objectives",
    "title": "10  Matrix Calculations",
    "section": "\n10.1  Objectives",
    "text": "10.1  Objectives\n\nUnderstand how to do matrix algebra in relevant programming languages\n\nWhile R, SAS, and Python are all extremely powerful statistical programming languages, the core of most programming languages is the ability to do basic calculations and matrix arithmetic. As almost every dataset is stored as a matrix-like structure (data sets and data frames both allow for multiple types, which isn’t quite compatible with more canonical matrices), it is useful to know how to do matrix-level calculations in whatever language you are planning to use to work with data.\nIn this section, we will essentially be using our programming language as overgrown calculators."
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#matrix-operations",
    "href": "part-gen-prog/035-matrix-calcs.html#matrix-operations",
    "title": "10  Matrix Calculations",
    "section": "\n10.2 Matrix Operations",
    "text": "10.2 Matrix Operations\n\n\nTable 10.1: Table of common mathematical and matrix operations in R, SAS, and Python [1].\n\n\n\n\n\n\n\nOperation\nR\nSAS\nPython\n\n\n\nAddition\n+\n+\n+\n\n\nSubtraction\n-\n-\n-\n\n\nElementwise Multiplication\n*\n#\n*\n\n\nDivision\n/\n/\n/\n\n\nModulo (Remainder)\n%%\nMOD\n%\n\n\nInteger Division\n%/%\nFLOOR(x\\y)\n//\n\n\nElementwise Exponentiation\n^\n##\n**\n\n\nMatrix/Vector Multiplication\n%*%\n*\nnp.dot()\n\n\nMatrix Exponentiation\n^\n**\nnp.exp()\n\n\nMatrix Transpose\nt(A)\n\nA`\nnp.transpose(A)\n\n\nMatrix Determinant\ndet(A)\ndet(A)\nnp.linalg.det(A)\n\n\nMatrix Diagonal\ndiag(A)\ndiag(A)\nnp.linalg.diag(A)\n\n\nMatrix Inverse\nsolve(A)\nsolve(A, diag({...}))\nnp.linalg.inv(A)\n\n\n\n\n\n\n\n\n\n\nBasic Mathematical Operators\n\n\n\n\n\nR\nPython\nSAS\n\n\n\n\nx <- 1:10\ny <- seq(3, 30, by = 3)\n\nx + y\n##  [1]  4  8 12 16 20 24 28 32 36 40\nx - y\n##  [1]  -2  -4  -6  -8 -10 -12 -14 -16 -18 -20\nx * y\n##  [1]   3  12  27  48  75 108 147 192 243 300\nx / y\n##  [1] 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333\n##  [8] 0.3333333 0.3333333 0.3333333\nx^2\n##  [1]   1   4   9  16  25  36  49  64  81 100\nt(x) %*% y\n##      [,1]\n## [1,] 1155\n\n\n\n\nimport numpy as np\n\nx = np.array(range(1, 11))\ny = np.array(range(3, 33, 3)) # python indexes are not inclusive\n\nx + y\n## array([ 4,  8, 12, 16, 20, 24, 28, 32, 36, 40])\nx - y\n## array([ -2,  -4,  -6,  -8, -10, -12, -14, -16, -18, -20])\nx * y\n## array([  3,  12,  27,  48,  75, 108, 147, 192, 243, 300])\nx / y\n## array([0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n##        0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333])\nx ** 2\n## array([  1,   4,   9,  16,  25,  36,  49,  64,  81, 100])\nnp.dot(x.T, y)\n## 1155\n\n\n\nBy default, SAS creates row vectors with do(a, b, by = c) syntax. The transpose operator (a single backtick) can be used to transform A into A`.\nproc iml; \n  x = do(1, 10, 1);\n  y = do(3, 30, 3);\n\n  z = x + y;\n  z2 = x - y;\n  z3 = x # y;\n  z4 = x/y;\n  z5 = x##2;\n  z6 = x` * y;\n  print z, z2, z3, z4, z5, z6;\nquit;\n\n\n\n\n\n\n\n\n\n\n\nMatrix Operations\n\n\n\nOther matrix operations, such as determinants and extraction of the matrix diagonal, are similarly easy:\n\n\nR\nPython\nSAS\n\n\n\n\nmat <- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T)\nmat\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    6    4    5\n## [3,]    7    8    9\nt(mat) # transpose\n##      [,1] [,2] [,3]\n## [1,]    1    6    7\n## [2,]    2    4    8\n## [3,]    3    5    9\ndet(mat) # get the determinant\n## [1] 18\ndiag(mat) # get the diagonal\n## [1] 1 4 9\ndiag(diag(mat)) # get a square matrix with off-diag 0s\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    4    0\n## [3,]    0    0    9\ndiag(1:3) # diag() also will create a diagonal matrix if given a vector\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    2    0\n## [3,]    0    0    3\n\n\n\n\nimport numpy as np\nmat = np.array([[1, 2, 3],[6, 4, 5],[7, 8, 9]], dtype = int, order ='C')\n\nmat\n## array([[1, 2, 3],\n##        [6, 4, 5],\n##        [7, 8, 9]])\nmat.T\n## array([[1, 6, 7],\n##        [2, 4, 8],\n##        [3, 5, 9]])\nnp.linalg.det(mat) # numerical precision...\n## 18.000000000000004\nnp.diag(mat)\n## array([1, 4, 9])\nnp.diag(np.diag(mat))\n## array([[1, 0, 0],\n##        [0, 4, 0],\n##        [0, 0, 9]])\nnp.diag(range(1, 4))\n## array([[1, 0, 0],\n##        [0, 2, 0],\n##        [0, 0, 3]])\n\n\n\nproc iml;\n  mat = {1 2 3, 6 4 5, 7 8 9}; \n  tmat = mat`; /* transpose */\n  determinant = det(mat); /* get the determinant */\n  diagonal_vector = vecdiag(mat); /* get the diagonal as a vector */\n  diagonal_mat = diag(mat); /* get the diagonal as a square matrix */\n                            /* with 0 on off-diagonal entries */\n  \n  dm = diag({1 2 3}); /* make a square matrix with vector as the diagonal */\n  \n  print tmat, determinant, diagonal_vector, diagonal_mat, dm;\nquit;\n\n\n\n\n\n\n\n\n\n\n\nMatrix Inverse\n\n\n\nThe other important matrix-related function is the inverse. In R, A^-1 will get you the elementwise reciprocal of the matrix. Not exactly what we’d like to see… Instead, in R and SAS, we use the solve() function. The inverse is defined as the matrix B such that AB = I where I is the identity matrix (1’s on diagonal, 0’s off-diagonal). So if we solve(A) (in R) or solve(A, diag(n)) in SAS (where n is a vector of 1s the size of A), we will get the inverse matrix. In Python, we use the np.linalg.inv() function to invert a matrix, which may be a bit more linguistically familiar.\n\n\nR\nPython\nSAS\n\n\n\n\nmat <- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T)\n\nminv <- solve(mat) # get the inverse\n\nminv\n##            [,1]       [,2]       [,3]\n## [1,] -0.2222222  0.3333333 -0.1111111\n## [2,] -1.0555556 -0.6666667  0.7222222\n## [3,]  1.1111111  0.3333333 -0.4444444\nmat %*% minv \n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    1    0\n## [3,]    0    0    1\n\n\n\n\nimport numpy as np\nmat = np.array([[1, 2, 3],[6, 4, 5],[7, 8, 9]], dtype = int, order ='C')\n\nminv = np.linalg.inv(mat)\nminv\n## array([[-0.22222222,  0.33333333, -0.11111111],\n##        [-1.05555556, -0.66666667,  0.72222222],\n##        [ 1.11111111,  0.33333333, -0.44444444]])\nnp.dot(mat, minv)\n## array([[1., 0., 0.],\n##        [0., 1., 0.],\n##        [0., 0., 1.]])\nnp.round(np.dot(mat, minv), 2)\n## array([[1., 0., 0.],\n##        [0., 1., 0.],\n##        [0., 0., 1.]])\n\n\n\nDocumentation\n    proc iml;\n      mat = {1 2 3, 6 4 5, 7 8 9};\n\n      mat_inv = solve(mat, diag({1 1 1})); /* get the inverse */\n      mat_inv2 = inv(mat); /* less efficient and less accurate */\n      print mat_inv, mat_inv2;\n\n      id = mat * mat_inv;\n      id2 = mat * mat_inv2;\n      print id, id2; \n    quit;"
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#references",
    "href": "part-gen-prog/035-matrix-calcs.html#references",
    "title": "10  Matrix Calculations",
    "section": "\n10.3 References",
    "text": "10.3 References\n\n\n\n\n[1] \nQuartz25, Jesdisciple, H. Röst, D. Ross, L. D’Oliveiro, and BLibrestez55, Python Programming. Wikibooks, 2016 [Online]. Available: https://en.wikibooks.org/wiki/Python_Programming. [Accessed: May 28, 2022]"
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#fa-bullseye-objectives",
    "href": "part-gen-prog/04-control-struct.html#fa-bullseye-objectives",
    "title": "11  Control Structures",
    "section": "\n11.1  Objectives",
    "text": "11.1  Objectives\n\nUnderstand how to use conditional statements\nUnderstand how conditional statements are evaluated by a program\nUse program flow diagrams to break a problem into parts and evaluate how a program will execute\nUnderstand how to use loops\nSelect the appropriate type of loop for a problem"
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#mindset",
    "href": "part-gen-prog/04-control-struct.html#mindset",
    "title": "11  Control Structures",
    "section": "\n11.2 Mindset",
    "text": "11.2 Mindset\nBefore we start on the types of control structures, let’s get in the right mindset. We’re all used to “if-then” logic, and use it in everyday conversation, but computers require another level of specificity when you’re trying to provide instructions.\nCheck out this video of the classic “make a peanut butter sandwich instructions challenge”:\n\n\n\n\nHere’s another example:\n\n\n‘If you’re done being pedantic, we should get dinner.’ ‘You did it again!’ ‘No, I didn’t.’ Image from Randal Munroe, xkcd.com, available under a CC-By 2.5 license.\n\n\nThe key takeaways from these bits of media are that you should read this section with a focus on exact precision - state exactly what you mean, and the computer will do what you say. If you instead expect the computer to get what you mean, you’re going to have a bad time."
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#conditional-statements",
    "href": "part-gen-prog/04-control-struct.html#conditional-statements",
    "title": "11  Control Structures",
    "section": "\n11.3 Conditional Statements",
    "text": "11.3 Conditional Statements\nConditional statements determine if code is evaluated.\nThey look like this:\nif (condition)\n  then\n    (thing to do)\n  else\n    (other thing to do)\nThe else (other thing to do) part may be omitted.\nWhen this statement is read by the computer, the computer checks to see if condition is true or false. If the condition is true, then (thing to do) is also run. If the condition is false, then (other thing to do) is run instead.\nLet’s try this out:\n\n\nR\nPython\n\n\n\n\nx <- 3\ny <- 1\n\nif (x > 2) { \n  y <- 8\n} else {\n  y <- 4\n}\n\nprint(paste(\"x =\", x, \"; y =\", y))\n## [1] \"x = 3 ; y = 8\"\n\nIn R, the logical condition after if must be in parentheses. It is common to then enclose the statement to be run if the condition is true in {} so that it is clear what code matches the if statement. You can technically put the condition on the line after the if (x > 2) line, and everything will still work, but then it gets hard to figure out what to do with the else statement - it technically would also go on the same line, and that gets hard to read.\n\nx <- 3\ny <- 1\n\nif (x > 2) y <- 8 else y <- 4\n\nprint(paste(\"x =\", x, \"; y =\", y))\n## [1] \"x = 3 ; y = 8\"\n\nSo while the 2nd version of the code technically works, the first version with the brackets is much easier to read and understand. Please try to emulate the first version!\n\n\n\nx = 3\ny = 1\n\nif x > 2:\n  y = 8\nelse:\n  y = 4\n\nprint(\"x =\", x, \"; y =\", y)\n## x = 3 ; y = 8\n\nIn python, all code grouping is accomplished with spaces instead of with brackets. So in python, we write our if statement as if x > 2: with the colon indicating that what follows is the code to evaluate. The next line is indented with 2 spaces to show that the code on those lines belongs to that if statement. Then, we use the else: statement to provide an alternative set of code to run if the logical condition in the if statement is false. Again, we indent the code under the else statement to show where it “belongs”.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPython will throw errors if you mess up the spacing. This is one thing that is very annoying about Python… but it’s a consequence of trying to make the code more readable.\n\n\n\n11.3.1 Representing Conditional Statements as Diagrams\nA common way to represent conditional logic is to draw a flow chart diagram.\nIn a flow chart, conditional statements are represented as diamonds, and other code is represented as a rectangle. Yes/no or True/False branches are labeled. Typically, after a conditional statement, the program flow returns to a single point.\n\n\nProgram flow diagram outline of a simple if/else statement\n\n\n\n11.3.2 Chaining Conditional Statements: Else-If\nIn many cases, it can be helpful to have a long chain of conditional statements describing a sequence of alternative statements.\n\n\n\n\n\n\nExample - Conditional Evaluation\n\n\n\nSuppose I want to determine what categorical age bracket someone falls into based on their numerical age. All of the bins are mutually exclusive - you can’t be in the 25-40 bracket and the 41-55 bracket.\n\n\nProgram Flow Map\nR\nPython\n\n\n\n\n\nProgram flow map for a series of mutually exclusive categories. If our goal is to take a numeric age variable and create a categorical set of age brackets, such as <18, 18-25, 26-40, 41-55, 56-65, and >65, we can do this with a series of if-else statements chained together. Only one of the bracket assignments is evaluated, so it is important to place the most restrictive condition first.\n\n\nThe important thing to realize when examining this program flow map is that if age <= 18 is true, then none of the other conditional statements even get evaluated. That is, once a statement is true, none of the other statements matter. Because of this, it is important to place the most restrictive statement first.\n\n\nProgram flow map for a series of mutually exclusive categories, emphasizing that only some statements are evaluated. When age = 40, only (age <= 18), (age <= 25), and (age <= 40) are evaluated conditionally. Of the assignment statements, only bracket = ‘26-40’ is evaluated when age = 40.\n\n\nIf for some reason you wrote your conditional statements in the wrong order, the wrong label would get assigned:\n\n\nProgram flow map for a series of mutually exclusive categories, with category labels in the wrong order - <40 is evaluated first, and so <= 25 and <= 18 will never be evaluated and the wrong label will be assigned for anything in those categories.\n\n\nIn code, we would write this statement using else-if (or elif) statements.\n\n\n\nage <- 40 # change this as you will to see how the code works\n\nif (age < 18) {\n  bracket <- \"<18\"\n} else if (age <= 25) {\n  bracket <- \"18-25\"\n} else if (age <= 40) {\n  bracket <- \"26-40\"\n} else if (age <= 55) {\n  bracket <- \"41-55\" \n} else if (age <= 65) {\n  bracket <- \"56-65\"\n} else {\n  bracket <- \">65\"\n}\n\nbracket\n## [1] \"26-40\"\n\n\n\nPython uses elif as a shorthand for else if statements. As always, indentation/white space in python matters. If you put an extra blank line between two elif statements, then the interpreter will complain. If you don’t indent properly, the interpreter will complain.\n\nage = 40 # change this to see how the code works\n\nif age < 18:\n  bracket = \"<18\"\nelif age <= 25:\n  bracket = \"18-25\"\nelif age <= 40:\n  bracket = \"26-40\"\nelif age <= 55:\n  bracket = \"41-55\"\nelif age <= 65:\n  bracket = \"56-65\"\nelse:\n  bracket = \">65\"\n  \nbracket\n## '26-40'\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out - Chained If/Else Statements\n\n\n\n\n\nProblem\nFlow Map\nR Solution\nPython Solution\n\n\n\nThe US Tax code has brackets, such that the first $10,275 of your income is taxed at 10%, anything between $10,275 and $41,775 is taxed at 12%, and so on.\nHere is the table of tax brackets for single filers in 2022:\n\n\nrate\nIncome\n\n\n\n10%\n$0 to $10,275\n\n\n12%\n$10,275 to $41,775\n\n\n22%\n$41,775 to $89,075\n\n\n24%\n$89,075 to $170,050\n\n\n32%\n$170,050 to $215,950\n\n\n35%\n$215,950 to $539,900\n\n\n37%\n$539,900 or more\n\n\n\nNote: For the purposes of this problem, we’re ignoring the personal exemption and the standard deduction, so we’re already simplifying the tax code.\nWrite a set of if statements that assess someone’s income and determine what their overall tax rate is.\nHint: You may want to keep track of how much of the income has already been taxed in a variable and what the total tax accumulation is in another variable.\n\n\n\n\n\n\n\n\nThe control flow diagram for the tax brackets\n\n\nControl flow diagrams can be extremely helpful when figuring out how programs work (and where gaps in your logic are when you’re debugging). It can be very helpful to map out your program flow as you’re untangling a problem.\n\n\n\n# Start with total income\nincome <- 200000\n\n# x will hold income that hasn't been taxed yet\nx <- income\n# y will hold taxes paid\ny <- 0\n\nif (x <= 10275) {\n  y <- x*.1 # tax paid\n  x <- 0 # All money has been taxed\n} else {\n  y <- y + 10275 * .1\n  x <- x - 10275 # Money remaining that hasn't been taxed\n}\n\nif (x <= (41775 - 10275)) {\n  y <- y + x * .12\n  x <- 0\n} else {\n  y <- y + (41775 - 10275) * .12\n  x <- x - (41775 - 10275) \n}\n\nif (x <= (89075 - 41775)) {\n  y <- y + x * .22\n  x <- 0\n} else {\n  y <- y + (89075 - 41775) * .22\n  x <- x - (89075 - 41775)\n}\n\nif (x <= (170050 - 89075)) {\n  y <- y + x * .24\n  x <- 0\n} else {\n  y <- y + (170050 - 89075) * .24\n  x <- x - (170050 - 89075)\n}\n\nif (x <= (215950 - 170050)) {\n  y <- y + x * .32\n  x <- 0\n} else {\n  y <- y + (215950 - 170050) * .32\n  x <- x - (215950 - 170050)\n}\n\nif (x <= (539900 - 215950)) {\n  y <- y + x * .35\n  x <- 0\n} else {\n  y <- y + (539900 - 215950) * .35\n  x <- x - (539900 - 215950)\n}\n\nif (x > 0) {\n  y <- y + x * .37\n}\n\n\nprint(paste(\"Total Tax Rate on $\", income, \" in income = \", round(y/income, 4)*100, \"%\"))\n## [1] \"Total Tax Rate on $ 2e+05  in income =  22.12 %\"\n\n\n\n\n# Start with total income\nincome = 200000\n\n# untaxed will hold income that hasn't been taxed yet\nuntaxed = income\n# taxed will hold taxes paid\ntaxes = 0\n\nif untaxed <= 10275:\n  taxes = untaxed*.1 # tax paid\n  untaxed = 0 # All money has been taxed\nelse:\n  taxes = taxes + 10275 * .1\n  untaxed = untaxed - 10275 # money remaining that hasn't been taxed\n\nif untaxed <= (41775 - 10275):\n  taxes = taxes + untaxed * .12\n  untaxed = 0\nelse:\n  taxes = taxes + (41775 - 10275) * .12\n  untaxed = untaxed - (41775 - 10275) \n\n\nif untaxed <= (89075 - 41775):\n  taxes = taxes + untaxed * .22\n  untaxed = 0\nelse: \n  taxes = taxes + (89075 - 41775) * .22\n  untaxed = untaxed - (89075 - 41775)\n\nif untaxed <= (170050 - 89075):\n  taxes = taxes + untaxed * .24\n  untaxed = 0\nelse: \n  taxes = taxes + (170050 - 89075) * .24\n  untaxed = untaxed - (170050 - 89075)\n\nif untaxed <= (215950 - 170050):\n  taxes = taxes + untaxed * .32\n  untaxed = 0\nelse:\n  taxes = taxes + (215950 - 170050) * .32\n  untaxed = untaxed - (215950 - 170050)\n\nif untaxed <= (539900 - 215950):\n  taxes = taxes + untaxed * .35\n  untaxed = 0\nelse: \n  taxes = taxes + (539900 - 215950) * .35\n  untaxed = untaxed - (539900 - 215950)\n\n\nif untaxed > 0:\n  taxes = taxes + untaxed * .37\n\n\n\nprint(\"Total Tauntaxed Rate on $\", income, \" in income = \", round(taxes/income, 4)*100, \"%\")\n## Total Tauntaxed Rate on $ 200000  in income =  22.12 %\n\nWe will find a better way to represent this calculation once we discuss loops - we can store each bracket’s start and end point in a vector and loop through them. Any time you find yourself copy-pasting code and changing values, you should consider using a loop (or eventually a function) instead."
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#loops",
    "href": "part-gen-prog/04-control-struct.html#loops",
    "title": "11  Control Structures",
    "section": "\n11.4 Loops",
    "text": "11.4 Loops\n\nOften, we write programs which update a variable in a way that the new value of the variable depends on the old value:\nx = x + 1\nThis means that we add one to the current value of x.\nBefore we write a statement like this, we have to initialize the value of x because otherwise, we don’t know what value to add one to.\nx = 0\nx = x + 1\nWe sometimes use the word increment to talk about adding one to the value of x; decrement means subtracting one from the value of x.\nA particularly powerful tool for making these types of repetitive changes in programming is the loop, which executes statements a certain number of times. Loops can be written in several different ways, but all loops allow for executing a block of code a variable number of times.\n\n11.4.1 While Loops\nIn the previous section, we discussed conditional statements, where a block of code is only executed if a logical statement is true. The simplest type of loop is the while loop, which executes a block of code until a statement is no longer true.\n\n\n\n\n\n\nExample - While Loops\n\n\n\n\n\nFlow Map\nR\nPython\n\n\n\n\n\nFlow map showing while-loop pseudocode (while x <= N) { # code that changes x in some way} and the program flow map expansion where we check if x > N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then change x and start over.\n\n\n\n\n\nx <- 0\n\nwhile (x < 10) { \n  # Everything in here is executed \n  # during each iteration of the loop\n  print(x)\n  x <- x + 1\n}\n## [1] 0\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n\n\n\n\nx = 0\n\nwhile x < 10:\n  print(x)\n  x = x + 1\n## 0\n## 1\n## 2\n## 3\n## 4\n## 5\n## 6\n## 7\n## 8\n## 9\n\n\n\n\n\n\n\n\n\n\n\n\nTry it Out - While Loops\n\n\n\n\n\nProblem\nMath Notation\nR Solution\nPython solution\n\n\n\nWrite a while loop that verifies that \\[\\lim_{N \\rightarrow \\infty} \\prod_{k=1}^N \\left(1 + \\frac{1}{k^2}\\right) = \\frac{e^\\pi - e^{-\\pi}}{2\\pi}.\\]\nTerminate your loop when you get within 0.0001 of \\(\\frac{e^\\pi - e^{-\\pi}}{2\\pi}\\). At what value of \\(k\\) is this point reached?\n\n\nBreaking down math notation for code:\n\nIf you are unfamiliar with the notation \\(\\prod_{k=1}^N f(k)\\), this is the product of \\(f(k)\\) for \\(k = 1, 2, ..., N\\), \\[f(1)\\cdot f(2)\\cdot ... \\cdot f(N)\\]\nTo evaluate a limit, we just keep increasing \\(N\\) until we get arbitrarily close to the right hand side of the equation.\n\nIn this problem, we can just keep increasing \\(k\\) and keep track of the cumulative product. So we define k=1, prod = 1, and ans before the loop starts. Then, we loop over k, multiplying prod by \\((1 + 1/k^2)\\) and then incrementing \\(k\\) by one each time. At each iteration, we test whether prod is close enough to ans to stop the loop.\n\n\nIn R, you will use pi and exp() - these are available by default without any additional libraries or packages.\n\nk <- 1\nprod <- 1\nans <- (exp(pi) - exp(-pi))/(2*pi)\ndelta <- 0.0001\n\nwhile (abs(prod - ans) >= 0.0001) {\n  prod <- prod * (1 + 1/k^2)\n  k <- k + 1\n}\n\nk\n## [1] 36761\nprod\n## [1] 3.675978\nans\n## [1] 3.676078\n\n\n\nNote that in python, you will have to import the math library to get the values of pi and the exp function. You can refer to these as math.pi and math.exp() respectively.\n\nimport math\n\nk = 1\nprod = 1\nans = (math.exp(math.pi) - math.exp(-math.pi))/(2*math.pi)\ndelta = 0.0001\n\nwhile abs(prod - ans) >= 0.0001:\n  prod = prod * (1 + k**-2)\n  k = k + 1\n  if k > 500000:\n    break\n\n\nprint(\"At \", k, \" iterations, the product is \", prod, \"compared to the limit \", ans,\".\")\n## At  36761  iterations, the product is  3.675977910975878 compared to the limit  3.676077910374978 .\n\n\n\n\n\n\n\n\n\n\n\n\nWarning: Avoid Infinite Loops\n\n\n\nIt is very easy to create an infinite loop when you are working with while loops. Infinite loops never exit, because the condition is always true. If in the while loop example we decrement x instead of incrementing x, the loop will run forever.\nYou want to try very hard to avoid ever creating an infinite loop - it can cause your session to crash.\nOne common way to avoid infinite loops is to create a second variable that just counts how many times the loop has run. If that variable gets over a certain threshold, you exit the loop.\n\n\nR\nPython\n\n\n\nThis while loop runs until either x < 10 or n > 50 - so it will run an indeterminate number of times and depends on the random values added to x. Since this process (a ‘random walk’) could theoretically continue forever, we add the n>50 check to the loop so that we don’t tie up the computer for eternity.\n\nx <- 0\nn <- 0 # count the number of times the loop runs\n\nwhile (x < 10) { \n  print(x)\n  x <- x + rnorm(1) # add a random normal (0, 1) draw each time\n  n <- n + 1\n  if (n > 50) \n    break # this stops the loop if n > 50\n}\n## [1] 0\n## [1] -1.741162\n## [1] -2.713118\n## [1] -3.128355\n## [1] -2.797743\n## [1] -3.085553\n## [1] -4.249505\n## [1] -3.613085\n## [1] -4.273522\n## [1] -6.170019\n## [1] -7.319188\n## [1] -7.040531\n## [1] -6.533678\n## [1] -6.687705\n## [1] -9.365987\n## [1] -9.503921\n## [1] -8.463873\n## [1] -8.822558\n## [1] -9.154577\n## [1] -8.725692\n## [1] -8.273073\n## [1] -8.783948\n## [1] -9.265829\n## [1] -9.52336\n## [1] -9.902511\n## [1] -10.6319\n## [1] -10.95456\n## [1] -9.669384\n## [1] -8.163363\n## [1] -6.54415\n## [1] -7.328121\n## [1] -6.712128\n## [1] -8.511542\n## [1] -9.717282\n## [1] -9.47317\n## [1] -7.929371\n## [1] -6.88268\n## [1] -7.100445\n## [1] -7.223095\n## [1] -6.217357\n## [1] -6.128911\n## [1] -5.552426\n## [1] -3.521243\n## [1] -2.832453\n## [1] -1.864777\n## [1] -3.442974\n## [1] -2.388184\n## [1] -1.225569\n## [1] -0.9583126\n## [1] -2.296629\n## [1] -1.70195\n\n\n\n\nimport numpy as np; # for the random normal draw\n\nx = 0\nn = 0 # count the number of times the loop runs\n\nwhile x < 10:\n  print(x)\n  x = x + np.random.normal(0, 1, 1) # add a random normal (0, 1) draw each time\n  n = n + 1\n  if n > 50:\n    break # this stops the loop if n > 50\n## 0\n## [0.60414286]\n## [0.76292735]\n## [0.31610562]\n## [2.52835708]\n## [1.80812798]\n## [0.01249116]\n## [-1.04704306]\n## [0.81739898]\n## [2.63324838]\n## [2.52378621]\n## [4.14108508]\n## [3.36789804]\n## [6.1690095]\n## [4.97717438]\n## [4.45480789]\n## [4.22268124]\n## [2.74268572]\n## [1.96308924]\n## [-0.53979336]\n## [-1.18250676]\n## [-1.91382713]\n## [-3.09552535]\n## [-5.04437455]\n## [-4.43468457]\n## [-4.11523001]\n## [-5.6735387]\n## [-7.18403756]\n## [-8.04035779]\n## [-9.25499092]\n## [-8.51689461]\n## [-7.48524]\n## [-7.24414609]\n## [-7.41381719]\n## [-7.37391402]\n## [-7.72715143]\n## [-7.46526566]\n## [-6.24995808]\n## [-6.16648467]\n## [-7.14395027]\n## [-7.54928932]\n## [-7.70627649]\n## [-8.17273617]\n## [-8.91842819]\n## [-9.06208149]\n## [-8.99403954]\n## [-8.00821398]\n## [-8.36301317]\n## [-7.48480326]\n## [-7.55209805]\n## [-7.63057492]\n\n\n\n\nIn both of the examples above, there are more efficient ways to write a random walk, but we will get to that later. The important thing here is that we want to make sure that our loops don’t run for all eternity.\n\n\n\n11.4.2 For Loops\nAnother common type of loop is a for loop. In a for loop, we run the block of code, iterating through a series of values (commonly, one to N, but not always). Generally speaking, for loops are known as definite loops because the code inside a for loop is executed a specific number of times. While loops are known as indefinite loops because the code within a while loop is evaluated until the condition is falsified, which is not always a known number of times.\n\n\nA visual demonstration of for loops iterating through a vector of monsters to dress them up for a parade. Image by Allison Horst.\n\n\n\n\n\n\n\n\nExample - For Loop Syntax\n\n\n\n\n\nFlow Map\nR\nPython\n\n\n\n\n\nFlow map showing for-loop pseudocode (for j in 1 to N) { # code} and the program flow map expansion where j starts at 1 and we check if j > N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then increment j and start over.\n\n\n\n\n\nfor (i in 1:5 ) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\n\n\n\nfor i in range(5):\n  print(i)\n## 0\n## 1\n## 2\n## 3\n## 4\n\nBy default range(5) goes from 0 to 5, the upper bound. When i = 5 the loop exits. This is because range(5) creates a vector [0, 1, 2, 3, 4].\n\n\n\n\n\nFor loops are often run from 1 to N (or 0 to N-1 in python) but in essence, a for loop is very commonly used to do a task for every value of a vector.\n\n11.4.2.1 Example - For Loops\n\n\nR\nPython\n\n\n\nFor instance, in R, there is a built-in variable called month.name. Type month.name into your R console to see what it looks like. If we want to iterate along the values of month.name, we can:\n\nfor (i in month.name)\n  print(i)\n## [1] \"January\"\n## [1] \"February\"\n## [1] \"March\"\n## [1] \"April\"\n## [1] \"May\"\n## [1] \"June\"\n## [1] \"July\"\n## [1] \"August\"\n## [1] \"September\"\n## [1] \"October\"\n## [1] \"November\"\n## [1] \"December\"\n\nWe can even pick out the first 3 letters of each month name and store them into a vector called abbr3\n\n# Create new vector of the correct length\nabbr3 <- rep(\"\", length(month.name))\n\n# We have to iterate along the index (1 to length) instead of the name \n# in this case because we want to store the result in a corresponding\n# row of a new vector\nfor (i in 1:length(month.name))\n  abbr3[i] <- substr(month.name[i], 1, 3)\n\n# We can combine the two vectors into a data frame \n# so that each row corresponds to a month and there are two columns:\n# full month name, and abbreviation\ndata.frame(full_name = month.name, abbrev = abbr3)\n##    full_name abbrev\n## 1    January    Jan\n## 2   February    Feb\n## 3      March    Mar\n## 4      April    Apr\n## 5        May    May\n## 6       June    Jun\n## 7       July    Jul\n## 8     August    Aug\n## 9  September    Sep\n## 10   October    Oct\n## 11  November    Nov\n## 12  December    Dec\n\n\n\nIn python, we have to define our vector or list to start out with, but that’s easy enough:\n\nimport calendar\n# Create a list with month names. For some reason, by default there's a \"\" as \n# the first entry, so we'll get rid of that\nmonth_name = list(calendar.month_name)[1:13]\n\nfor i in month_name:\n  print(i)\n## January\n## February\n## March\n## April\n## May\n## June\n## July\n## August\n## September\n## October\n## November\n## December\n\nWe can even pick out the first 3 letters of each month name and store them into a vector called abbr3.\nPython handles lists best when you use pythonic expressions. The linked post has an excellent explanation of why enumerate works best here.\n\n# Create new vector of the correct length\nabbr3 = [\"\"] * len(month_name)\n\n# We have to iterate along the index because we want to \n# store the result in a corresponding row of a new vector\n# Python allows us to iterate along both the index i and the value val\n# at the same time, which is convenient.\nfor i, val in enumerate(month_name):\n  abbr3[i] = val[0:3:] # Strings have indexes by character, so this gets \n                       # characters 0, 1, and 2.\n  \nabbr3\n## ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']"
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#other-control-structures",
    "href": "part-gen-prog/04-control-struct.html#other-control-structures",
    "title": "11  Control Structures",
    "section": "\n11.5 Other Control Structures",
    "text": "11.5 Other Control Structures\n\n11.5.1 Conditional Statements\ncase statements, e.g. case_when in tidyverse\n\n11.5.2 Loops\n\n11.5.2.1 Controlling Loops\nWhile I do not often use break, next, and continue statements, they do exist in both languages and can be useful for controlling the flow of program execution. I have moved the section on this to Section 27.3 for the sake of brevity and to reduce the amount of new material those without programming experience are being exposed to in this section.\n\n11.5.2.2 Other Types of Loops\nThere are other types of loops in most languages, such as the do-while loop, which runs the code first and then evaluates the logical condition to determine whether the loop will be run again.\n\n\n\n\n\n\nExample: do-while loops\n\n\n\n\n\nR\nPython\n\n\n\nIn R, do-while loops are most naturally implemented using a very primitive type of iteration: a repeat statement.\n\nrepeat {\n  # statements go here\n  if (condition)\n    break # this exits the repeat statement\n}\n\n\n\nIn python, do-while loops are most naturally implemented using a while loop with condition TRUE:\n\nwhile TRUE:\n  # statements go here\n  if condition:\n    break\n\n\n\n\n\n\nAn additional means of running code an indeterminate number of times is the use of recursion, which we cannot cover until we learn about functions. I have added an additional section, Section 27.4, to cover this topic, but it is not essential to being able to complete most basic data programming tasks. Recursion is useful when working with structures such as trees (including phylogenetic trees) and nested lists."
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#sec-control-struct-refs",
    "href": "part-gen-prog/04-control-struct.html#sec-control-struct-refs",
    "title": "11  Control Structures",
    "section": "\n11.6 References",
    "text": "11.6 References"
  },
  {
    "objectID": "part-gen-prog/05-functions.html#fa-bullseye-objectives",
    "href": "part-gen-prog/05-functions.html#fa-bullseye-objectives",
    "title": "12  Writing Functions",
    "section": "\n12.1  Objectives",
    "text": "12.1  Objectives\n\nIdentify the parts of a function from provided source code\nPredict what the function will return when provided with input values and source code\nGiven a task, lay out the steps necessary to complete the task in pseudocode\nWrite a function which uses necessary input values to complete a task"
  },
  {
    "objectID": "part-gen-prog/05-functions.html#when-to-write-a-function",
    "href": "part-gen-prog/05-functions.html#when-to-write-a-function",
    "title": "12  Writing Functions",
    "section": "\n12.2 When to write a function?",
    "text": "12.2 When to write a function?\nIf you’ve written the same code (with a few minor changes, like variable names) more than twice, you should probably write a function instead. There are a few benefits to this rule:\n\nYour code stays neater (and shorter), so it is easier to read, understand, and maintain.\nIf you need to fix the code because of errors, you only have to do it in one place.\nYou can re-use code in other files by keeping functions you need regularly in a file (or if you’re really awesome, in your own package!)\nIf you name your functions well, your code becomes easier to understand thanks to grouping a set of actions under a descriptive function name.\n\n\n\n\n\n\n\nLearn more about functions\n\n\n\nThere is some extensive material on this subject in R for Data Science [1] on functions. If you want to really understand how functions work in R, that is a good place to go.\n\n\n\n\n\n\n\n\nExample: Turning Code into Functions\n\n\n\nThis example is modified from R for Data Science [2, Ch. 19].\nWhat does this code do? Does it work as intended?\n\n\nR\nPython\n\n\n\n\ndf <- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b <- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c <- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d <- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n  'a': np.random.randn(10), \n  'b': np.random.randn(10), \n  'c': np.random.randn(10), \n  'd': np.random.randn(10)})\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\ndf.b = (df.b - min(df.b))/(max(df.b) - min(df.a))\ndf.c = (df.c - min(df.c))/(max(df.c) - min(df.c))\ndf.d = (df.d - min(df.d))/(max(df.d) - min(df.d))\n\n\n\n\nThe code rescales a set of variables to have a range from 0 to 1. But, because of the copy-pasting, the code’s author made a mistake and forgot to change an a to b.\nWriting a function to rescale a variable would prevent this type of copy-paste error.\nTo write a function, we first analyze the code to determine how many inputs it has:\n\n\nR\nPython\n\n\n\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\nThis code has only one input: df$a.\n\n\n\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\n\nThis code has only one input: df.a\n\n\n\nTo convert the code into a function, we start by rewriting it using general names:\n\n\nR\nPython\n\n\n\nIn this case, it might help to replace df$a with x.\n\nx <- df$a \n\n(x - min(x, na.rm = TRUE)) / \n  (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n##  [1] 0.3266877 0.8454891 0.3755147 0.6827532 1.0000000 0.5599201 0.6903440\n##  [8] 0.6067227 0.0000000 0.3585888\n\n\n\nIn this case, it might help to replace df.a with x.\n\nx = df.a\n\n(x - min(x))/(max(x) - min(x))\n## 0    1.000000\n## 1    0.561338\n## 2    0.431346\n## 3    0.000000\n## 4    0.563970\n## 5    0.489798\n## 6    0.967892\n## 7    0.571396\n## 8    0.568369\n## 9    0.989954\n## Name: a, dtype: float64\n\n\n\n\nThen, we make it a bit easier to read, removing duplicate computations if possible (for instance, computing min two times).\n\n\nR\nPython\n\n\n\nIn R, we can use the range function, which computes the maximum and minimum at the same time and returns the result as c(min, max)\n\nrng <- range(x, na.rm = T)\n\n(x - rng[1])/(rng[2] - rng[1])\n##  [1] 0.3266877 0.8454891 0.3755147 0.6827532 1.0000000 0.5599201 0.6903440\n##  [8] 0.6067227 0.0000000 0.3585888\n\n\n\nIn python, range is the equivalent of seq() in R, so we are better off just using min and max.\n\nx = df.a\n\n\nxmin, xmax = [x.min(), x.max()]\n(x - xmin)/(xmax - xmin)\n## 0    1.000000\n## 1    0.561338\n## 2    0.431346\n## 3    0.000000\n## 4    0.563970\n## 5    0.489798\n## 6    0.967892\n## 7    0.571396\n## 8    0.568369\n## 9    0.989954\n## Name: a, dtype: float64\n\n\n\n\nFinally, we turn this code into a function:\n\n\nR\nPython\n\n\n\n\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])\n}\n\nrescale01(df$a)\n##  [1] 0.3266877 0.8454891 0.3755147 0.6827532 1.0000000 0.5599201 0.6903440\n##  [8] 0.6067227 0.0000000 0.3585888\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df$a, df$b, df$c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, between { and } (this is true in R, in python, there are different conventions, but the same principle applies)\nThe function returns the last value computed: in this case, (x - rng[1])/(rng[2]-rng[1]). You can make this explicit by adding a return() statement around that calculation.\n\n\n\n\ndef rescale01(x):\n  xmin, xmax = [x.min(), x.max()]\n  return (x - xmin)/(xmax - xmin)\n\nrescale01(df.a)\n## 0    1.000000\n## 1    0.561338\n## 2    0.431346\n## 3    0.000000\n## 4    0.563970\n## 5    0.489798\n## 6    0.967892\n## 7    0.571396\n## 8    0.568369\n## 9    0.989954\n## Name: a, dtype: float64\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df.a, df.b, df.c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, indented relative to the line with def: function_name():. At the end of the function, you should have a blank line with no spaces or tabs.\nThe function returns the value it is told to return: in this case, (x - xmin)/(xmax - xmin). In Python, you must return a value if you want the function to perform a computation. 1\n\n\n\n\nThe process for creating a function is important: first, you figure out how to do the thing you want to do. Then, you simplify the code as much as possible. Only at the end of that process do you create an actual function."
  },
  {
    "objectID": "part-gen-prog/05-functions.html#syntax",
    "href": "part-gen-prog/05-functions.html#syntax",
    "title": "12  Writing Functions",
    "section": "\n12.3 Syntax",
    "text": "12.3 Syntax\n\n\nR and python syntax for defining functions. Portions of the command that indicate the function name, function scope, and return statement are highlighted in each block.\n\n\nIn R, functions are defined as other variables, using <-, but we specify the arguments a function takes by using the function() statement. The contents of the function are contained within { and }. If the function returns a value, a return() statement can be used; alternately, if there is no return statement, the last computation in the function will be returned.\nIn python, functions are defined using the def command, with the function name, parentheses, and the function arguments to follow. The first line of the function definition ends with a :, and all subsequent lines of the function are indented (this is how python knows where the end of the function is). A python function return statement is return <value>, with no parentheses needed.\nNote that in python, the return statement is not optional. It is not uncommon to have python functions that don’t return anything; in R, this is a bit less common, for reasons we won’t get into here."
  },
  {
    "objectID": "part-gen-prog/05-functions.html#arguments-and-parameters",
    "href": "part-gen-prog/05-functions.html#arguments-and-parameters",
    "title": "12  Writing Functions",
    "section": "\n12.4 Arguments and Parameters",
    "text": "12.4 Arguments and Parameters\nAn argument is the name for the object you pass into a function.\nA parameter is the name for the object once it is inside the function (or the name of the thing as defined in the function).\n\n\n\n\n\n\nExample: Parts of a Function\n\n\n\nLet’s examine the difference between arguments and parameters by writing a function that takes a dog’s name and returns “ is a good pup!”.\n\n\nR\nPython\n\n\n\n\ndog <- \"Eddie\"\n\ngoodpup <- function(name) {\n  paste(name, \"is a good pup!\")\n}\n\ngoodpup(dog)\n## [1] \"Eddie is a good pup!\"\n\n\n\n\ndog = \"Eddie\"\n\ndef goodpup(name):\n  return name + \" is a good pup!\"\n\ngoodpup(dog)\n## 'Eddie is a good pup!'\n\n\n\n\nIn this example function, when we call goodpup(dog), dog is the argument. name is the parameter.\nWhat is happening inside the computer’s memory as goodpup runs?\n\n\nA sketch of the execution of the program goodpup, showing that name is only defined within the local environment that is created while goodpup is running. We can never access name in our global environment.\n\n\n\n\nThis is why the distinction between arguments and parameters matters. Parameters are only accessible while inside of the function - and in that local environment, we need to call the object by the parameter name, not the name we use outside the function (the argument name).\nWe can even call a function with an argument that isn’t defined outside of the function call: goodpup(\"Tesla\") produces “Tesla is a good pup!”. Here, I do not have a variable storing the string “Tesla”, but I can make the function run anyways. So “Tesla” here is an argument to goodpup but it is not a variable in my environment.\nThis is a confusing set of concepts and it’s ok if you only just sort of get what I’m trying to explain here. Hopefully it will become more clear as you write more code.\n\n\n\n\n\n\nTry it out: Function Parts\n\n\n\nFor each of the following blocks of code, identify the function name, function arguments, parameter names, and return statements. When the function is called, see if you can predict what the output will be. Also determine whether the function output is stored in memory or just printed to the command line.\n\n\nFunction 1\nAnswer\n\n\n\n\n\ndef hello_world():\n  print(\"Hello World\")\n\n\nhello_world()\n\n\n\n\nFunction name: hello_world\n\nFunction parameters: none\nFunction arguments: none\nFunction output:\n\n\nhello_world()\n## Hello World\n\n\nFunction output is not stored in memory and is printed to the command line.\n\n\n\n\n\n\nFunction 2\nAnswer\n\n\n\n\n\nmy_mean <- function(x) {\n  censor_x <- sample(x, size = length(x) - 2, replace = F)\n  mean(censor_x)\n}\n\n\nset.seed(3420523)\nx = my_mean(1:10)\nx\n\n\n\n\nFunction name: my_mean\n\nFunction parameters: x\nFunction arguments: 1:10\nFunction output: (varies each time the function is run unless you set the seed)\n\n\nset.seed(3420523)\nx = my_mean(1:10)\nx\n## [1] 6\n\n\nFunction output is saved to memory (x) and printed to the command line\n\n\n\n\n\n\n\n12.4.1 Named Arguments and Parameter Order\nIn the examples above, you didn’t have to worry about what order parameters were passed into the function, because there were 0 and 1 parameters, respectively. But what happens when we have a function with multiple parameters?\n\n\nR\nPython\n\n\n\n\n\ndivide <- function(x, y) {\n  x / y\n}\n\n\n\n\n\ndef divide(x, y):\n  return x / y\n\n\n\n\nIn this function, the order of the parameters matters! divide(3, 6) does not produce the same result as divide(6, 3). As you might imagine, this can quickly get confusing as the number of parameters in the function increases.\nIn this case, it can be simpler to use the parameter names when you pass in arguments.\n\n\nR\nPython\n\n\n\n\ndivide(3, 6)\n## [1] 0.5\n\ndivide(x = 3, y = 6)\n## [1] 0.5\n\ndivide(y = 6, x = 3)\n## [1] 0.5\n\ndivide(6, 3)\n## [1] 2\n\ndivide(x = 6, y = 3)\n## [1] 2\n\ndivide(y = 3, x = 6)\n## [1] 2\n\n\n\n\ndivide(3, 6)\n## 0.5\ndivide(x = 3, y = 6)\n## 0.5\ndivide(y = 6, x = 3)\n## 0.5\ndivide(6, 3)\n## 2.0\ndivide(x = 6, y = 3)\n## 2.0\ndivide(y = 3, x = 6)\n## 2.0\n\n\n\n\nAs you can see, the order of the arguments doesn’t much matter, as long as you use named arguments, but if you don’t name your arguments, the order very much matters.\n\n12.4.2 Input Validation\nWhen you write a function, you often assume that your parameters will be of a certain type. But you can’t guarantee that the person using your function knows that they need a certain type of input. In these cases, it’s best to validate your function input.\n\n\n\n\n\n\nInput Validation Example\n\n\n\n\n\nR\nPython\n\n\n\nIn R, you can use stopifnot() to check for certain essential conditions. If you want to provide a more illuminating error message, you can check your conditions using if() and then use stop(\"better error message\") in the body of the if statement.\n\nadd <- function(x, y) {\n  x + y\n}\n\nadd(\"tmp\", 3)\n## Error in x + y: non-numeric argument to binary operator\n\nadd <- function(x, y) {\n  stopifnot(is.numeric(x))\n  stopifnot(is.numeric(y))\n  x + y\n}\n\nadd(\"tmp\", 3)\n## Error in add(\"tmp\", 3): is.numeric(x) is not TRUE\nadd(3, 4)\n## [1] 7\n\n\n\nIn Python, the easiest way to handle errors is to use a try statement, which operates rather like an if statement: if the statement executes, then we’re good to go; if not, we can use except to handle different types of errors. The else clause is there to handle anything that needs to happen if the statement in the try clause executes without any errors.\n\n\ndef add(x, y):\n  x + y\n\nadd(\"tmp\", 3)\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: can only concatenate str (not \"int\") to str\ndef add(x, y):\n  try:\n    return x + y\n  except TypeError:\n    print(\"x and y must be add-able\")\n  else:\n    # We should never get here, because the try clause has a return statement\n    print(\"Else clause?\")\n  return\n\nadd(\"tmp\", 3)\n## x and y must be add-able\nadd(3, 4)\n## 7\n\nYou can read more about error handling in Python here\n\n\n\n\n\nInput validation is one aspect of defensive programming - programming in such a way that you try to ensure that your programs don’t error out due to unexpected bugs by anticipating ways your programs might be misunderstood or misused [3]."
  },
  {
    "objectID": "part-gen-prog/05-functions.html#scope",
    "href": "part-gen-prog/05-functions.html#scope",
    "title": "12  Writing Functions",
    "section": "\n12.5 Scope",
    "text": "12.5 Scope\nWhen talking about functions, for the first time we start to confront a critical concept in programming, which is scope. Scope is the part of the program where the name you’ve given a variable is valid - that is, where you can use a variable.\n\nA variable is only available from inside the region it is created.\n\nWhat do I mean by the part of a program? The lexical scope is the portion of the code (the set of lines of code) where the name is valid.\nThe concept of scope is best demonstrated through a series of examples, so in the rest of this section, I’ll show you some examples of how scope works and the concepts that help you figure out what “scope” actually means in practice.\n\n12.5.1 Name Masking\nScope is most clearly demonstrated when we use the same variable name inside and outside a function. Note that this is 1) bad programming practice, and 2) fairly easily avoided if you can make your names even slightly more creative than a, b, and so on. But, for the purposes of demonstration, I hope you’ll forgive my lack of creativity in this area so that you can see how name masking works.\n\n\n\n\n\n\nDanger\n\n\n\nWhat does this function return, 10 or 20?\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\na = 10\n\nmyfun = function() {\n  a = 20\n  return a\n}\n\nmyfun()\n\n\n\n\nA sketch of the global environment as well as the environment within myfun(). Because a=20 inside myfun(), when we call myfun(), we get the value of a within that environment, instead of within the global environment.\n\n\n\n\n\na <- 10\n\nmyfun <- function() {\n  a <- 20\n  a\n}\n\nmyfun()\n## [1] 20\n\n\n\n\n\na = 10\n\ndef myfun():\n  a = 20\n  return a\n\nmyfun()\n## 20\n\n\n\n\n\n\nThe lexical scope of the function is the area that is between the braces (in R) or the indented region (in python). Outside the function, a has the value of 10, but inside the function, a has the value of 20. So when we call myfun(), we get 20, because the scope of myfun is the local context where a is evaluated, and the value of a in that environment dominates.\nThis is an example of name masking, where names defined inside of a function mask names defined outside of a function.\n\n12.5.2 Environments and Scope\nAnother principle of scoping is that if you call a function and then call the same function again, the function’s environment is re-created each time. Each function call is unrelated to the next function call when the function is defined using local variables.\n\n\n\n\n\n\nDanger\n\n\n\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\nmyfun = function() {\n  if a is not defined\n    a = 1\n  else\n    a = a + 1\n}\n\nmyfun()\nmyfun()\n\nWhat does this output?\n\n\n\n\nWhen we define myfun, we create a template for an environment with variables and code to excecute. Each time myfun() is called, that template is used to create a new environment. This prevents successive calls to myfun() from affecting each other – which means a = 1 every time.\n\n\n\n\n\nmyfun <- function() {\n  if (!exists(\"aa\")) {\n    aa <- 1\n  } else {\n    aa <- aa + 1\n  }\n  return(aa)\n}\n\nmyfun()\n## [1] 1\nmyfun()\n## [1] 1\n\n\n\n\ndef myfun():\n  try: aa\n  except NameError: aa = 1\n  else: aa = aa + 1\n  return aa\n\nmyfun()\n## 1\nmyfun()\n## 1\n\nNote that the try command here is used to handle the case where a doesn’t exist. If there is a NameError (which will happen if aa is not defined) then we define aa = 1, if there is not a NameError, then aa = aa + 1.\nThis is necessary because Python does not have a built-in way to test if a variable exists before it is used [4], Ch 17.\n\n\n\n\n\n\n12.5.3 Dynamic Lookup\nScoping determines where to look for values – when, however, is determined by the sequence of steps in the code. When a function is called, the calling environment (the global environment or set of environments at the time the function is called) determines what values are used.\nIf an object doesn’t exist in the function’s environment, the global environment will be searched next; if there is no object in the global environment, the program will error out. This behavior, combined with changes in the calling environment over time, can mean that the output of a function can change based on objects outside of the function.\n\n\n\n\n\n\nDanger\n\n\n\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\nmyfun = function() x + 1\n\nx = 14\n\nmyfun()\n\nx = 20\n\nmyfun()\n\nWhat will the output be of this code?\n\n\n\n\nThe state of the global environment at the time the function is called (that is, the state of the calling environment) can change the results of the function\n\n\n\n\n\nmyfun <- function() {\n  x + 1\n}\n\nx <- 14\n\nmyfun()\n## [1] 15\n\nx <- 20\n\nmyfun()\n## [1] 21\n\n\n\n\n\ndef myfun():\n  return x + 1\n\n\nx = 14\n\nmyfun()\n## 15\nx = 20\n\nmyfun()\n## 21\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: Function Scope\n\n\n\nWhat does the following function return? Make a prediction, then run the code yourself. From [2, Ch. 6]\n\n\nR code\nR solution\nPython code\nPython solution\n\n\n\n\nf <- function(x) {\n  f <- function(x) {\n    f <- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n\n\n\nf <- function(x) {\n  f <- function(x) {\n    f <- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n## [1] 202\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ^ 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ** 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n## 202"
  },
  {
    "objectID": "part-gen-prog/05-functions.html#sec-functions-refs",
    "href": "part-gen-prog/05-functions.html#sec-functions-refs",
    "title": "12  Writing Functions",
    "section": "\n12.6 References",
    "text": "12.6 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[3] \nWikipedia Contributors, “Defensive programming,” Wikipedia. Wikimedia Foundation, Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Defensive_programming&oldid=1084121123. [Accessed: May 31, 2022]\n\n\n[4] \nA. Martelli and D. Ascher, Python Cookbook. O’Reilly Media, 2002 [Online]. Available: https://learning.oreilly.com/library/view/python-cookbook/0596001673/ch05s24.html. [Accessed: May 31, 2022]"
  },
  {
    "objectID": "part-gen-prog/06-debugging.html",
    "href": "part-gen-prog/06-debugging.html",
    "title": "13  Debugging",
    "section": "",
    "text": "14 I’ve deleted the intermediate chunks because they screw\n’ knitr::knit(text=indoc, output=“test.md”) rmarkdown::render(“test.md”)"
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#fa-bullseye-objectives",
    "href": "part-gen-prog/06-debugging.html#fa-bullseye-objectives",
    "title": "13  Debugging",
    "section": "\n13.1  Objectives",
    "text": "13.1  Objectives\n\nCreate reproducible examples of problems\nUse built in debugging tools to trace errors\nUse online resources to research errors\n\n\n\nThe faces of debugging (by Allison Horst)"
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#avoiding-errors-defensive-programming",
    "href": "part-gen-prog/06-debugging.html#avoiding-errors-defensive-programming",
    "title": "13  Debugging",
    "section": "\n13.2 Avoiding Errors: Defensive Programming",
    "text": "13.2 Avoiding Errors: Defensive Programming\nOne of the best debugging strategies (that isn’t a debugging strategy at all, really) is to code defensively [1]. By that, I mean, code in a way that you will make debugging things easier later.\n\nModularize your code. Each function should do only one task, ideally in the least-complex way possible.\nMake your code readable. If you can read the code easily, you’ll be able to narrow down the location of the bug more quickly.\nComment your code. This makes it more likely that you will be able to locate the spot where the bug is likely to have occurred, and will remind you how things are calculated. Remember, comments aren’t just for your collaborators or others who see the code. They’re for future you.\nDon’t duplicate code. If you have the same code (or essentially the same code) in two or three different places, put that code in a function and call the function instead. This will save you trouble when updating the code in the future, but also makes narrowing down the source of the bug less complex.\nReduce the number of dependencies you have on outside software packages. Often bugs are introduced when a dependency is updated and the functionality changes slightly. The tidyverse [2] is notorious for this.\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s ok to write code using lots of dependencies, but as you transition from “experimental” code to “production” code (you’re using the code without tinkering with it) you should work to reduce the dependencies, where possible. In addition, if you do need packages with lots of dependencies, try to make sure those packages are relatively popular, used by a lot of people, and currently maintained. (The tidyverse is a bit better from this perspective, because the constituent packages are some of the most installed R packages on CRAN.)\n\n\nAnother way to handle dependency management is to use the renv package [3], which creates a local package library with the appropriate versions of your packages stored in the same directory as your project. renv was inspired by the python concept of virtual environments, and it does also work with python if you’re using both R and python inside a project (e.g. this book uses renv). renv will at the very least help you minimize issues with code not working after unintentional package updates.\n\nAdd safeguards against unexpected inputs. Check to make sure inputs to the function are valid. Check to make sure intermediate results are reasonable (e.g. you don’t compute the derivative of a function and come up with “a”.)\nDon’t reinvent the wheel. If you have working, tested code for a task, use that! If someone else has working code that’s used by the community, don’t write your own unless you have a very good reason. The implementation of lm has been better tested than your homegrown linear regression.\nCollect your often-reused code in packages that you can easily load and make available to “future you”. The process of making a package often encourages you to document your code better than you would a script. A good resource for getting started making R packages is [4], and a similar python book is [5]."
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#working-through-errors",
    "href": "part-gen-prog/06-debugging.html#working-through-errors",
    "title": "13  Debugging",
    "section": "\n13.3 Working through Errors",
    "text": "13.3 Working through Errors\n\n13.3.1 First steps\n\n13.3.1.1 Get into the right mindset\nYou can’t debug something effectively if you’re upset. You have to be in a puzzle-solving, detective mindset to actually solve a problem. If you’re already stressed out, try to relieve that stress before you tackle the problem: take a shower, go for a walk, pet a puppy.\n\n\nA debugging manifesto [6]\n\n\n\n13.3.1.2 Check your spelling\nI’ll guess that 80% of my personal debugging comes down to spelling errors and misplaced punctuation.\n\n\nTitle: user.fist_name [7]\n\n\n\n13.3.2 General Debugging Strategies\n\n\n\nDebugging: Being the detective in a crime movie where you are also the murderer. - some t-shirt I saw once\n\nWhile defensive programming is a nice idea, if you’re already at the point where you have an error you can’t diagnose, then… it doesn’t help that much. At that point, you’ll need some general debugging strategies to work with. The overall process is well described in [8]; I’ve added some steps that are commonly overlooked and modified the context from the original package development to introductory programming. I’ve also integrated some lovely illustrations from Julia Evans (@b0rk) to lighten the mood.\n\nRealize that you have a bug\nRead the error message\n\n\n\nDebugging strategy: Reread the error message[9]\n\n\n\n\nGoogle! Seriously, just google the whole error message.\nIn R you can automate this with the errorist and searcher packages. Python is so commonly used that you’ll likely be able to find help for your issue if you are specific enough.\n\n\n\n\n\nDebugging strategy: Shorten your feedback loop [10]\n\n\n\n\nMake the error repeatable: This makes it easier to figure out what the error is, faster to iterate, and easier to ask for help.\n\nUse binary search (remove 1/2 of the code, see if the error occurs, if not go to the other 1/2 of the code. Repeat until you’ve isolated the error.)\nGenerate the error faster - use a minimal test dataset, if possible, so that you can ask for help easily and run code faster. This is worth the investment if you’ve been debugging the same error for a while. \nNote which inputs don’t generate the bug – this negative “data” is helpful when asking for help.\n\n\n\nDebugging strategy: Change working code into broken code [12]\n\n\n\nFigure out where it is. Debuggers may help with this, but you can also use the scientific method to explore the code, or the tried-and-true method of using lots of print() statements.\nCome up with one question. If you’re stuck, it can be helpful to break it down a bit and ask one tiny question about the bug.\n\n\n\nDebugging strategy: Come up with one question [13]\n\n\n\n\nFix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation. If you’re developing a package, unit test suites offer a more formalized way to test errors and you can automate your testing so that every time your code is changed, tests are run and checked.\n\n\n\n\n\nDebugging strategy: Write a unit test [14]\n\n\nThere are several other general strategies for debugging:\n\nRetype (from scratch) your code\nThis works well if it’s a short function or a couple of lines of code, but it’s less useful if you have a big script full of code to debug. However, it does sometimes fix really silly typos that are hard to spot, like having typed <-- instead of <- in R and then wondering why your answers are negative.\nVisualize your data as it moves through the program. This may be done using print() statements, or the debugger, or some other strategy depending on your application.\nTracing statements. Again, this is part of print() debugging, but these messages indicate progress - “got into function x”, “returning from function y”, and so on.\nRubber ducking. Have you ever tried to explain a problem you’re having to someone else, only to have a moment of insight and “oh, never mind”? Rubber ducking outsources the problem to a nonjudgmental entity, such as a rubber duck1. You simply explain, in terms simple enough for your rubber duck to understand, exactly what your code does, line by line, until you’ve found the problem. See [15] for a more thorough explanation.\n\nDo not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s well-known enough that it has its own xkcd comic. At some point, getting up and going for a walk may help. Redesigning your code to be more modular and more organized is also a good idea."
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#dividing-problems-into-smaller-parts",
    "href": "part-gen-prog/06-debugging.html#dividing-problems-into-smaller-parts",
    "title": "13  Debugging",
    "section": "\n13.4 Dividing Problems into Smaller Parts",
    "text": "13.4 Dividing Problems into Smaller Parts\n\n“Divide each difficulty into as many parts as is feasible and necessary to resolve it.” -René Descartes, Discourse on Method\n\nIn programming, as in life, big, general problems are very hard to solve effectively. Instead, the goal is to break a problem down into smaller pieces that may actually be solvable.\n\n\n\n\n\n\nExample: Exhaustion\n\n\n\nThis example inspired by [16].\n\n\nGeneral problem\nSpecific problem\nSubproblems\nBrainstorm\nSubproblem solutions\n\n\n\n“I’m exhausted all the time”\nOk, so this is a problem that many of us have from time to time (or all the time). If we get a little bit more specific at outlining the problem, though, we can sometimes get a bit more insight into how to solve it.\n\n\n“I wake up in the morning and I don’t have any energy to do anything. I want to go back to sleep, but I have too much to do to actually give in and sleep. I spend my days worrying about how I’m going to get all of the things on my to-do list done, and then I lie awake at night thinking about how many things there are to do tomorrow. I don’t have time for hobbies or exercise, so I drink a lot of coffee instead to make it through the day.”\nThis is a much more specific list of issues, and some of these issues are actually things we can approach separately.\n\n\nMoving through the list in the previous tab, we can isolate a few issues. Some of these issues are undoubtedly related to each other, but we can approach them separately (for the most part).\n\nPoor quality sleep (tired in the morning, lying awake at night)\nToo many things to do (to-do list)\nChemical solutions to low energy (coffee during the day)\nAnxiety about completing tasks (worrying, insomnia)\nLack of personal time for hobbies or exercise\n\n\n\n\nGet a check-up to rule out any other issues that could cause sleep quality degradation - depression, anxiety, sleep apnea, thyroid conditions, etc.\n\nAsk the doctor about taking melatonin supplements for a short time to ensure that sleep starts off well (note, don’t take medical advice from a stats textbook!)\n\n\nReformat your to-do list:\n\nSet time limits for things on the to-do list\nBreak the to-do list into smaller, manageable tasks that can be accomplished within a relatively short interval - such as an hour\nSort the to-do list by priority and level of “fun” so that each day has a few hard tasks and a couple of easy/fun tasks. Do the hard tasks first, and use the easy/fun tasks as a reward.\n\n\nSet a time limit for caffeine (e.g. no coffee after noon) so that caffeine doesn’t contribute to poor quality sleep\nAddress anxiety with medication (from 1), scheduled time for mindfulness meditation, and/or self-care activities\nScheduling time for exercise/hobbies\n\nscheduling exercise in the morning to take advantage of the endorphins generated by working out\nscheduling hobbies in the evening to reward yourself for a day’s work and wind down work well before bedtime\n\n\n\n\n\nWhen the sub-problem has a viable solution, move on to the next sub-problem. Don’t try to tackle everything at once. Here, that might look like this list, where each step is taken separately and you give each thing a few days to see how it affects your sleep quality. In programming, of course, this list would perhaps be a bit more sequential, but real life is messy and the results take a while to populate.\n\n[1] Make the doctor’s appointment.\n[5] While waiting for the appointment, schedule exercise early in the day and hobbies later in the day to create a “no-work” period before bedtime.\n[1] Go to the doctor’s appointment, follow up with any concerns.\n\n[1] If doctor approves, start taking melatonin according to directions\n\n\n[2] Work on reformatting the to-do list into manageable chunks. Schedule time to complete chunks using your favorite planning method.\n[4] If anxiety is still an issue after following up with the doctor, add some mindfullness meditation or self-care to the schedule in the mornings or evenings.\n[3] If sleep quality is still an issue, set a time limit for caffeine\n[2] Revise your to-do list and try a different tactic if what you were trying didn’t work."
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#minimal-working-or-reproducible-examples",
    "href": "part-gen-prog/06-debugging.html#minimal-working-or-reproducible-examples",
    "title": "13  Debugging",
    "section": "\n13.5 Minimal Working (or Reproducible) Examples",
    "text": "13.5 Minimal Working (or Reproducible) Examples\n\n\n\n\nThe reprex R package will help you make a reproducible example (drawing by Allison Horst)\n\n\nIf all else has failed, and you can’t figure out what is causing your error, it’s probably time to ask for help. If you have a friend or buddy that knows the language you’re working in, by all means ask for help sooner - use them as a rubber duck if you have to. But when you ask for help online, often you’re asking people who are much more knowledgeable about the topic - members of R core and really famous python developers browse stackoverflow and may drop in and help you out. Under those circumstances, it’s better to make the task of helping you as easy as possible because it shows respect for their time. The same thing goes for your supervisors and professors.\nThere are numerous resources for writing what’s called a “minimal working example”, “reproducible example” (commonly abbreviated reprex), or MCVE (minimal complete verifiable example). Much of this is lifted directly from the StackOverflow post describing a minimal reproducible example.\nThe goal is to reproduce the error message with information that is\n\n\nminimal - as little code as possible to still reproduce the problem\n\ncomplete - everything necessary to reproduce the issue is contained in the description/question\n\nreproducible - test the code you provide to reproduce the problem.\n\nYou should format your question to make it as easy as possible to help you. Make it so that code can be copied from your post directly and pasted into a terminal. Describe what you see and what you’d hope to see if the code were working.\n\n\n\n\n\n\nOther Minimum Working Example/Reprex resources\n\n\n\n\nreprex package: Do’s and Don’ts\n\nHow to use the reprex package - vignette with videos from Jenny Bryan\nreprex magic - Vignette adapted from a blog post by Nick Tierney\n\n\n\n\n\n\n\n\n\nExample: MWEs\n\n\n\nNote: You don’t need to know anything about SAS to understand this example.\n\nSAS markdown\n\n\nA long time ago, when this book covered R and SAS, I had issues with SAS graphs rendering in black and white most of the time.\nI started debugging the issue with the following code chunk:\n```{{r sas-cat-aes-map-07, engine=“sashtml”, engine.path=“sas”, fig.path = “image/”}} libname classdat “sas/”;\nPROC SGPLOT data=classdat.fbiwide; SCATTER x = Population y = Assault / markerattrs=(size=8pt symbol=circlefilled) group = Abb; /* maps to point color by default */ RUN; QUIT;\nPROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; /* dont generate a legend / SCATTER x = Population y = Assault / markercharattrs=(size=8) markerchar = Abb / specify marker character variable */ group = Abb ; RUN; QUIT;\n\n\nAfter running the code separately in SAS and getting a figure that looked like what I'd expected, I set out to construct a reproducible example so that I could post to the [SASmarkdown github issues page](https://github.com/Hemken/SASmarkdown/issues/14) and ask for help.\n\nThe first thing I did was strip out all of the extra stuff that didn't need to be in the chunk - this chunk generates 2 pictures; I only need one. \nThis chunk requires the fbiwide data from the `classdata` R package (that I exported to CSV); I replaced it with a dataset in the `sashelp` library.\n\nWhen I was done, the chunk looked like this:\n\nPROC SGPLOT data=sashelp.snacks; SCATTER x = date y = QtySold / markerattrs=(size=8pt symbol=circlefilled) group = product; /* maps to point color by default */ RUN; QUIT;\n\nThen, I started constructing my reproducible example. \nI ran `?sas_enginesetup` to get to a SASmarkdown help page, because I remembered it had a nice way to generate and run markdown files from R directly (without saving the Rmd file). \n\nI copied the example from that page:\n\n13.6 indoc <- ’\ntitle: “Basic SASmarkdown Doc” author: “Doug Hemken” output: html_document —\n\n\n15 everything up when I print this chunk out\n\n\nThen, I created several chunks which would do the following:\n1. Write the minimal example SAS code above to a [file](files/reprex.sas)\n2. Call that file in a SASmarkdown chunk using the `%include` macro, which dumps the listed file into the SAS program. This generates the [plot](files/SASmarkdown-reprex/unnamed-chunk-1.png) using SASmarkdown.\n3. Call the file using SAS batch mode    \n(this runs the code and produces a [plot](files/SASmarkdown-reprex/SGPlot.png) outside of SASmarkdown, to prove that the issue is SASmarkdown itself)\n\nFinally, I included the image generated from the batch mode call manually.\n\nYou can see the resulting code [here](https://github.com/Hemken/SASmarkdown/issues/14).\n\nI pasted my example into the issues page, and then included some additional information: \n\n1. A screenshot of the rendered page\n2. The image files themselves\n3. A description of what happened\n4. My suspicions (some obvious option I'm missing?)\n5. An additional line of R code that would delete any files created if someone ran my example. Because file clutter sucks.\n\nThis process took me about 45 minutes, but that was still much shorter than the time I'd spent rerunning code trying to get it to work with no success. \n\nIn less than 24 hours, the package maintainer responded with a (admittedly terse) explanation of what he thought caused the problem. \nI had to do some additional research to figure out what that meant, but once I had my reproducible example working in color, I posted that code (so that anyone else with the same problem would know what to do).\n\nThen, I had to tinker with the book a bit to figure out if there were easier ways to get the same result.\nThe end result, though, was that I got what I wanted - color figures throughout the book!\n\n#### Python/Quarto\n\nWhile converting the book from Rmarkdown to quarto, I ran into an issue setting up GitHub Actions (basically, when I push changes, GitHub rebuilds the book from scratch automatically). \n\n\nI found an issue describing the same segfault issue I had been getting, and so I made a [post there](https://github.com/rstudio/reticulate/issues/1133) with a new github repository containing a [minimal working example](https://github.com/srvanderplas/test-quarto) that I set up to test the problem. \n\nWithin 24h, I had gotten replies from people working at RStudio, and one of them had [diagnosed the problem](https://github.com/rstudio/reticulate/issues/1133#issuecomment-1021783041). \nAfter I asked a few more questions, one of them submitted a pull request to my repository with a solution.\n\nI didn't know enough python or enough about GitHub Actions to diagnose the problem myself, but because I managed to create a reproducible example, I got the answers I needed from people with more experience.\n\n:::\n\n:::\n\n::: callout-tip\n### Try It Out \n\nUse [this list of StackOverflow posts](files/Debugging_exercise.html) to try out your new debugging techniques. \nCan you figure out what's wrong? \nWhat information would you need from the poster in order to come up with a solution?\nHow much time did you spend trying to figure out what the poster was actually asking?\n:::\n\n\n\n## Debugging Tools\n\nNow that we've discussed general strategies for debugging that will work in any language, lets get down to the dirty details of debugging. \n\n### Low tech debugging with print() and other tools\nSometimes called \"tracing\" techniques, the most common, universal, and low tech strategy for debugging involves scattering messages throughout your code. \nWhen the code is executed, you get a window into what the variables look like during execution.\n\nThis is called **print debugging** and it is an incredibly useful tool.\n\n::: callout-caution\n#### Example: Nested Functions\n\n::: panel-tabset\n#### R\nImagine we start with this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = 1\ny = 2\nz = 0\n\naa <- function(x) {\n  bb <- function(y) {\n    cc <- function(z) {\n      z + y\n    }\n    cc(3) + 2\n  }\n  x + bb(4)\n}\n\naa(5)\n## [1] 14\n\n\n\n\nand the goal is to understand what’s happening in the code. We might add some lines:\n\nx = 1\ny = 2\nz = 0\n\naa <- function(x) {\n  print(paste(\"Entering aa(). x = \", x))\n  bb <- function(y) {\n    print(paste(\"Entering bb(). x = \", x, \"y = \", y))\n    cc <- function(z) {\n      print(paste(\"Entering cc(). x = \", x, \"y = \", y, \"z = \", z))\n      cres <- z + y\n      print(paste(\"Returning\", cres, \"from cc()\"))\n      cres\n    }\n    bres <- cc(3) + 2\n    print(paste(\"Returning\", bres, \"from bb()\"))\n    bres\n  }\n  ares <- x + bb(4)\n  print(paste(\"Returning\",ares, \"from aa()\"))\n  ares\n}\n\naa(5)\n## [1] \"Entering aa(). x =  5\"\n## [1] \"Entering bb(). x =  5 y =  4\"\n## [1] \"Entering cc(). x =  5 y =  4 z =  3\"\n## [1] \"Returning 7 from cc()\"\n## [1] \"Returning 9 from bb()\"\n## [1] \"Returning 14 from aa()\"\n## [1] 14\n\n\n15.0.0.1 Python\nImagine we start with this:\n\nx = 1\ny = 2\nz = 0\n\ndef aa(x):\n  def bb(y):\n    def cc(z):\n      return z + y\n    return cc(3) + 2\n  return x + bb(4)\n\naa(5)\n## 14\n\nand the goal is to understand what’s happening in the code. We might add some lines:\n\nx = 1\ny = 2\nz = 0\n\ndef aa(x):\n  print(\"Entering aa(). x = \" + str(x))\n  def bb(y):\n    print(\"Entering bb(). x = \" + str(x) + \", y = \" + str(y))\n    def cc(z):\n      print(\"Entering cc(). x = \" + str(x) + \", y = \" + str(y) + \", z = \" + str(z))\n      cres = z + y\n      print(\"Returning \" + str(cres) + \" from cc()\")\n      return cres\n    bres = cc(3) + 2\n    print(\"Returning \" + str(bres) + \" from bb()\")\n    return bres\n  ares = x + bb(4)\n  print(\"Returning \" + str(ares) + \" from aa()\")\n  return ares\n\naa(5)\n## Entering aa(). x = 5\n## Entering bb(). x = 5, y = 4\n## Entering cc(). x = 5, y = 4, z = 3\n## Returning 7 from cc()\n## Returning 9 from bb()\n## Returning 14 from aa()\n## 14\n\n\n\n\n:::\nFor more complex data structures, it can be useful to add str(), head(), or summary() functions.\n\n\n\n\n\n\nReal world example: Web Scraping\n\n\n\nIn fall 2020, I wrote a webscraper to get election polling data from the RealClearPolitics site as part of the electionViz package. I wrote the function search_for_parent() to get the parent HTML tag which matched the “tag” argument, that had the “node” argument as a descendant. I used print debugging to show the sequence of tags on the page.\nI was assuming that the order of the parents would be “html”, “body”, “div”, “table”, “tbody”, “tr” - descending from outer to inner (if you know anything about HTML/XML structure).\nTo prevent the site from changing on me (as websites tend to do…), I’ve saved the HTML file here.\n\n\nR\nPython\n\n\n\n\nlibrary(xml2) # read html\n\nsearch_for_parent <- function(node, tag) {\n  # Get all of the parent nodes \n  parents <- xml2::xml_parents(node)\n  # Get the tags of every parent node\n  tags <- purrr::map_chr(parents, rvest::html_name)\n  print(tags)\n  \n  # Find matching tags\n  matches <- which(tags == tag)\n  print(matches)\n  \n  # Take the minimum matching tag\n  min_match <- min(matches)\n  if (length(matches) == 1) return(parents[min_match]) else return(NULL)\n}\n\npage <- read_html(\"shorturl.at/jkS59\")\n## Error: 'shorturl.at/jkS59' does not exist in current working directory ('/home/susan/Projects/Class/stat-computing-r-python/part-gen-prog').\n# find all poll results in any table\npoll_results <- xml_find_all(page, \"//td[@class='lp-results']\") \n## Error in UseMethod(\"xml_find_all\"): no applicable method for 'xml_find_all' applied to an object of class \"function\"\n# find the table that contains it\nsearch_for_parent(poll_results[1], \"table\") \n## Error in nodeset_apply(x, function(x) .Call(node_parents, x)): object 'poll_results' not found\n\n\n\nYou may need to pip install lxml requests bs4 to run this code.\n\n# !pip install lxml requests bs4\nfrom bs4 import BeautifulSoup\nimport requests as req\nimport numpy as np\n\n\ndef search_for_parent(node, tag):\n  # Get all of the parent nodes\n  parents = node.find_parents()\n  # get tag type for each parent node\n  tags = [x.name for x in parents]\n  print(tags)\n  \n  # Find matching tags\n  matches = np.array([i for i, val in enumerate(tags) if val == tag])\n  print(matches)\n  \n  # Take the minimum matching tag\n  min_match = np.min(matches)\n  if matches.size == 1:\n    ret = parents[min_match]\n  \n  return ret\n\n\nhtml_file = open('shorturl.at/jkS59', 'r')\n## Error in py_call_impl(callable, dots$args, dots$keywords): FileNotFoundError: [Errno 2] No such file or directory: 'shorturl.at/jkS59'\npage = html_file.read() \n# Read the page as HTML\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'html_file' is not defined\nsoup = BeautifulSoup(page, 'html')\n# Find all poll results in any table\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'page' is not defined\npoll_results = soup.findAll('td', {'class': 'lp-results'})\n# Find the table that contains the first poll result\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'soup' is not defined\nsearch_for_parent(poll_results[0], 'table')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'poll_results' is not defined\n\n\n\n\nBy printing out all of the tags that contain node, I could see the order – inner to outer. I asked the function to return the location of the first table node, so the index (2nd value printed out) should match table in the character vector that was printed out first. I could then see that the HTML node that is returned is in fact the table node.\n\n\n\n\n\n\n\n\nTry it out: Hurricanes in R\n\n\n\nNot all bugs result in error messages, unfortunately, which makes higher-level techniques like traceback() less useful. The low-tech debugging tools, however, still work wonderfully.\n\n\nSetup\nBuggy code\nSolution 1: Identification\nSolution 2: Fixing\nSolution 3: Verifying\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(maps)\nlibrary(ggthemes)\nworldmap <- map_data(\"world\")\n\n# Load the data\ndata(storms, package = \"dplyr\")\n\n\n\nThe code below is supposed to print out a map of the tracks of all hurricanes of a specific category, 1 to 5, in 2013. Use print statements to figure out what’s wrong with my code.\n\n# Make base map to be used for each iteration\nbasemap <-  ggplot() + \n  # Country shapes\n  geom_polygon(aes(x = long, y = lat, group = group), \n               data = worldmap, fill = \"white\", color = \"black\") + \n  # Zoom in \n  coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + \n  # Don't need scales b/c maps provide their own geographic context...\n  theme_map()\n\nfor (i in 1:5) {\n  # Subset the data\n  subdata <- storms %>%\n    filter(year == 2013) %>%\n    filter(status == i)\n  \n  # Plot the data - path + points to show the observations\n  plot <- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  print(plot)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, lets split the setup from the loop.\n\n# Make base map to be used for each iteration\nbasemap <-  ggplot() + \n  # Country shapes\n  geom_polygon(aes(x = long, y = lat, group = group), \n               data = worldmap, fill = \"white\", color = \"black\") + \n  # Zoom in \n  coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + \n  # Don't need scales b/c maps provide their own geographic context...\n  theme_map()\n\nprint(basemap) # make sure the basemap is fine\n\n\n\n\n# Load the data\ndata(storms, package = \"dplyr\")\n\nstr(storms) # make sure the data exists and is formatted as expected\n## tibble [11,859 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ name                        : chr [1:11859] \"Amy\" \"Amy\" \"Amy\" \"Amy\" ...\n##  $ year                        : num [1:11859] 1975 1975 1975 1975 1975 ...\n##  $ month                       : num [1:11859] 6 6 6 6 6 6 6 6 6 6 ...\n##  $ day                         : int [1:11859] 27 27 27 27 28 28 28 28 29 29 ...\n##  $ hour                        : num [1:11859] 0 6 12 18 0 6 12 18 0 6 ...\n##  $ lat                         : num [1:11859] 27.5 28.5 29.5 30.5 31.5 32.4 33.3 34 34.4 34 ...\n##  $ long                        : num [1:11859] -79 -79 -79 -79 -78.8 -78.7 -78 -77 -75.8 -74.8 ...\n##  $ status                      : chr [1:11859] \"tropical depression\" \"tropical depression\" \"tropical depression\" \"tropical depression\" ...\n##  $ category                    : Ord.factor w/ 7 levels \"-1\"<\"0\"<\"1\"<\"2\"<..: 1 1 1 1 1 1 1 1 2 2 ...\n##  $ wind                        : int [1:11859] 25 25 25 25 25 25 25 30 35 40 ...\n##  $ pressure                    : int [1:11859] 1013 1013 1013 1013 1012 1012 1011 1006 1004 1002 ...\n##  $ tropicalstorm_force_diameter: int [1:11859] NA NA NA NA NA NA NA NA NA NA ...\n##  $ hurricane_force_diameter    : int [1:11859] NA NA NA NA NA NA NA NA NA NA ...\n\nEverything looks ok in the setup chunk…\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata <- storms %>%\n    filter(year == 2013) %>%\n    filter(status == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot <- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  # print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n\nOk, so from this we can see that something is going wrong with our filter statement - we have no rows of data.\n\n\n\nhead(storms)\n## # A tibble: 6 × 13\n##   name   year month   day  hour   lat  long status categ…¹  wind press…² tropi…³\n##   <chr> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>  <ord>   <int>   <int>   <int>\n## 1 Amy    1975     6    27     0  27.5 -79   tropi… -1         25    1013      NA\n## 2 Amy    1975     6    27     6  28.5 -79   tropi… -1         25    1013      NA\n## 3 Amy    1975     6    27    12  29.5 -79   tropi… -1         25    1013      NA\n## 4 Amy    1975     6    27    18  30.5 -79   tropi… -1         25    1013      NA\n## 5 Amy    1975     6    28     0  31.5 -78.8 tropi… -1         25    1012      NA\n## 6 Amy    1975     6    28     6  32.4 -78.7 tropi… -1         25    1012      NA\n## # … with 1 more variable: hurricane_force_diameter <int>, and abbreviated\n## #   variable names ¹​category, ²​pressure, ³​tropicalstorm_force_diameter\n\nWhoops. I meant “category” when I typed “status”.\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata <- storms %>%\n    filter(year == 2013) %>%\n    filter(category == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot <- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  # print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 13 ncol 13\"\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n\nOk, that’s something, at least. We now have some data for category 1 storms…\n\nfilter(storms, year == 2013) %>%\n  # Get max category for each named storm\n  group_by(name) %>%\n  filter(category == max(category)) %>%\n  ungroup() %>%\n  # See what categories exist\n  select(name, category) %>%\n  unique()\n## # A tibble: 14 × 2\n##    name      category\n##    <chr>     <ord>   \n##  1 Andrea    0       \n##  2 Barry     0       \n##  3 Chantal   0       \n##  4 Dorian    0       \n##  5 Erin      0       \n##  6 Fernand   0       \n##  7 Gabrielle 0       \n##  8 Eight     -1      \n##  9 Humberto  1       \n## 10 Ingrid    1       \n## 11 Jerry     0       \n## 12 Karen     0       \n## 13 Lorenzo   0       \n## 14 Melissa   0\n\nIt looks like 2013 was just an incredibly quiet year for tropical activity.\n\n\n2013 may have been a quiet year for tropical activity in the Atlantic, but 2004 was not. So let’s just make sure our code works by checking out 2004.\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata <- storms %>%\n    filter(year == 2004) %>%\n    filter(category == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot <- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 45 ncol 13\"\n\n\n\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 39 ncol 13\"\n\n\n\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 29 ncol 13\"\n\n\n\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 32 ncol 13\"\n\n\n\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 12 ncol 13\"\n\n\n\n\nIf we want to only print informative plots, we could add an if statement. Now that the code works, we can also comment out our print() statements (we could delete them, too, depending on whether we anticipate future problems with the code).\n\nfor (i in 1:5) {\n  # print(paste0(\"Category \", i, \" storms\"))\n  \n  # Subset the data\n  subdata <- storms %>%\n    filter(year == 2013) %>%\n    filter(category == i)\n  \n  # print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n  #       # str(subdata) works too, but produces more clutter. I started\n  #       # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot <- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  \n  if (nrow(subdata) > 0) print(plot) \n}\n\n\n\n\n\n\n\n\n\nOnce you’ve found your problem, go back and delete or comment out your print statements, as they’re no longer necessary. If you think you may need them again, comment them out, otherwise, just delete them so that your code is neat, clean, and concise.\n\n15.0.1 After an error has occurred - traceback()\n\ntraceback() can help you narrow down where an error occurs by taking you through the series of function calls that led up to the error. This may help you identify which function is actually causing the problem, which is especially useful when you have nested functions or are using package functions that depend on other packages.\n\n\n\n\n\n\nUsing traceback\n\n\n\n\n\nR\nPython\n\n\n\n\naa <- function(x) {\n  bb <- function(y) {\n    cc <- function(z) {\n     stop('there was a problem')  # This generates an error\n    }\n    cc()\n  }\n  bb()\n}\n\naa()\n## Error in cc(): there was a problem\n\nFor more information, you could run traceback\n\ntraceback()\n\nWhich will provide the following output:\n4: stop(\"there was a problem\") at #4\n3: c() at #6\n2: b() at #8\n1: a()\nReading through this, we see that a() was called, b() was called, c() was called, and then there was an error. It’s even kind enough to tell us that the error occurred at line 4 of the code.\nIf you are running this code interactively in RStudio, it’s even easier to run traceback() by clicking on the “Show Traceback” option that appears when there is an error.\n\n\nBoth Show Traceback and Rerun with Debug are useful tools\n\n\nIf you are using source() to run the code in Rstudio, it will even provide a link to the file and line location of the error. \n\n\n\nimport sys,traceback\n\ndef aa(x):\n  def bb(y):\n    def cc(z):\n      try: \n        return y + z + tuple()[0] # This generates an error\n      except IndexError:\n        exc_type, exc_value, exc_tb = sys.exc_info()\n        traceback.print_exception(exc_type, exc_value, exc_tb, file = sys.stdout)\n    return cc(3) + 2\n  return x + bb(4)\n\naa(5)\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\nPython’s traceback information is a bit more low-level and requires a bit more from the programmer than R’s version.\n\n\n\n\n\n\n15.0.2 Interactive Debugging\n\n\n\n\n\n\nR browser()\nPython\n\n\n\nThe browser() function is useful for debugging your own code. If you’re writing a function and something isn’t working quite right, you can insert a call to browser() in that function, and examine what’s going on.\n\n\n\n\n\n\nExample : browser()\n\n\n\nSuppose that I want to write a function that will plot an xkcd comic in R.\nI start with\n\nlibrary(png)\nlibrary(xml2)\nlibrary(dplyr)\n\n# get the most current xkcd\nget_xkcd <- function() {\n  url <- \"http://xkcd.com\"\n  page <- read_html(url)\n  # Find the comic\n  image <- xml_find_first(page, \"//div[@id='comic']/img\") %>%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  \n  \n  readPNG(source = image)\n}\n\nget_xkcd() %>%\n  as.raster() %>%\n  plot()\n## Error in readPNG(source = image): unable to open //imgs.xkcd.com/comics/washing_machine_settings.png\n\nHere’s the final function\n\nlibrary(png)\nlibrary(xml2)\n\n# get the most current xkcd\nget_xkcd <- function() {\n  \n  url <- \"http://xkcd.com\"\n  page <- read_html(url)\n  # Find the comic\n  image <- xml_find_first(page, \"//div[@id='comic']/img\") %>%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  \n  # Fix image address so that we can access the image\n  image <- substr(image, 3, nchar(image))\n  \n  # Download the file to a temp file and read from there\n  file_location <- tempfile(fileext = \".png\")\n  download.file(image, destfile = file_location, quiet = T)\n  \n  readPNG(source = file_location)\n}\n\nget_xkcd() %>%\n  as.raster() %>%\n  plot()\n\n\n\n\n\n\n\n\nIn python, the equivalent interactive debugger is ipdb. You can install it with pip install ipdb.\nIf you want to run Python in the interactive ipython console, then you can invoke the ipdb debugging with %debug get_xkcd(). This is similar to browser() in R. If you’re working in Python in RStudio, though, you have to get into debug mode in a more involved way.\nTo run code using ipdb when your code hits an error, add from ipdb import launch_ipdb_on_exception to the top of your python code chunk. Then, at the bottom, put any lines that may trigger the error after these two lines:\nif __name__ == \"__main__\":\n  with launch_ipdb_on_exception():\n    <your properly indented code goes here>\nThis ensures that ipdb is launched when an error is reached.\n\n\n\n\n\n\nExample using ipdb\n\n\n\nSuppose that I want to write a function that will plot an xkcd comic in python.\nI start with\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd():\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic > img')[0].get('src')\n  # Format as a numpy array\n  image = np.array(Image.open(urllib.request.urlopen(imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: unknown url type: '//imgs.xkcd.com/comics/washing_machine_settings.png'\nplt.show()\n\n\n\n\nHere’s the final function\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd():\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic > img')[0].get('src')\n  # Format as a numpy array\n  image = np.array(Image.open(urllib.request.urlopen('https:' + imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nEach xkcd has a corresponding ID number (ordered sequentially from 1 to 2722 at the time this was written). Modify the XKCD functions above to make use of the id parameter, so that you can pass in an ID number and get the relevant comic.\nUse interactive debugging tools to help you figure out what logic you need to add. You should not need to change the web scraping code - the only change should be to the URL.\nWhat things might you add to make this function “defensive programming” compatible?\n\n\n\n# get the most current xkcd or the specified number\nget_xkcd <- function(id = NULL) {\n  if (is.null(id)) {\n    # Have to get the location of the image ourselves\n    url <- \"http://xkcd.com\"\n  } else if (is.numeric(id)) {\n    url <- paste0(\"http://xkcd.com/\", id, \"/\")\n  } else {\n    # only allow numeric or null input\n    stop(\"To get current xkcd, pass in NULL, otherwise, pass in a valid comic number\")\n  }\n\n  page <- read_html(url)\n  # Find the comic\n  image <- xml_find_first(page, \"//div[@id='comic']/img\") %>%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  # Fix image address so that we can access the image\n  image <- substr(image, 3, nchar(image)) # cut the first 2 characters off\n\n  # make temp file\n  location <- tempfile(fileext = \"png\")\n  download.file(image, destfile = location, quiet = T)\n\n  # This checks to make sure we saved the file correctly\n  if (file.exists(location)) {\n    readPNG(source = location)\n  } else {\n    # Give a good informative error message\n    stop(paste(\"Something went wrong saving the image at \", image, \" to \", location))\n  }\n}\n\nget_xkcd(2259) %>%\n  as.raster() %>% \n  plot()\n\n\n\n\n\n\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd(id=''):\n  image = 0 # Defining a placeholder\n  \n  if id == '':\n    # Have to get the location of the image ourselves\n    url = \"http://xkcd.com\"\n  elif id.isnumeric():\n    url = \"http://xkcd.com/\" + id + \"/\"\n  else:\n    # only allow numeric or null input\n    raise TypeError(\"To get current xkcd, pass in an empty string, otherwise, pass in a valid integer comic number\")\n  \n  # Print debugging left in for your amusement\n  # print(type(id))\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imnode = soup.select('#comic > img')\n  \n  try:\n    imlink = imnode[0].get('src')\n  except:\n    raise Exception(\"No comic could be found with number \" + id + \" (url = \"+ url+ \" )\")\n  \n  try: \n    # Format as a numpy array\n    image = np.array(Image.open(urllib.request.urlopen('https:' + imlink)))\n    return image\n  except: \n    raise Exception(\"Reading the image failed. Check to make sure an image exists at \" + url)\n    return(None)\n\n\nres = get_xkcd('')\nplt.imshow(res)\nplt.show()\n\n\n\nres = get_xkcd('3000')\n\nError in py_call_impl(callable, dots$args, dots$keywords): urllib.error.HTTPError: HTTP Error 404: Not Found\n\nres = get_xkcd('abcd')\n\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: To get current xkcd, pass in an empty string, otherwise, pass in a valid integer comic number\n\n\n\n\n\n\n\n\n15.0.3 R debug()\n\nIn the traceback() Rstudio output, the other option is “rerun with debug”. In short, debug mode opens up a new interactive session inside the function evaluation environment. This lets you observe what’s going on in the function, pinpoint the error (and what causes it), and potentially fix the error, all in one neat workflow.\ndebug() is most useful when you’re working with code that you didn’t write yourself. So, if you can’t change the code in the function causing the error, debug() is the way to go. Otherwise, using browser() is generally easier. Essentially, debug() places a browser() statement at the first line of a function, but without having to actually alter the function’s source code.\n\n\n\n\n\n\ndebug() example\n\n\n\n\n\n\ndata(iris)\n\ntmp <- lm(Species ~ ., data = iris)\nsummary(tmp)\n## \n## Call:\n## lm(formula = Species ~ ., data = iris)\n## \n## Residuals:\n## Error in quantile.default(resid): (unordered) factors are not allowed\n\nWe get this weird warning, and then an error about factors when we use summary() to look at the coefficients.\n\ndebug(lm) # turn debugging on\n\n\ntmp <- lm(Species ~ ., data = iris)\nsummary(tmp)\n\nundebug(lm) # turn debugging off\n\n\n\nThe first thing I see when I run lm after turning on debug (screenshot)\n\n\n\n\nThe variables passed into the lm function are available as named and used in the function. In addition, we have some handy buttons in the console window that will let us ‘drive’ through the function\n\n\nAfter pressing “next” a few times, you can see that I’ve stepped through the first few lines of the lm function.\n\n\nStepping through the function. The arrow on the left side in the editor window shows which line of code we’re currently at.\n\n\nWe can see that once we’re at line 21, we get a warning about using type with a factor response, and that the warning occurs during a call to the model.response function. So, we’ve narrowed our problem down - we passed in a numeric variable as the response (y) variable, but it’s a factor, so our results aren’t going to mean much. We were using the function wrong.\nWe probably could have gotten there from reading the error message carefully, but this has allowed us to figure out exactly what happened, where it happened, and why it happened.\n\n\nI can hit “Stop” or type “Q” to exit the debug environment.\n\n\nBut, until I run undebug(lm), every call to lm will take me into the debug window.\n\n\n\nundebug(f) will remove the debug flag on the function f. debugonce(f) will only debug f the first time it is run.\n\n\n\n\n\n\nTry it out: debug in R\n\n\n\n\n\nProblem\nSolution\n\n\n\nlarger(x, y) is supposed to return the elementwise maximum of two vectors.\n\nlarger <- function(x, y) { \n  y.is.bigger <- y > x \n  x[y.is.bigger] <- y[y.is.bigger] \n  x\n} \n\nlarger(c(1, 5, 10), c(2, 4, 11))\n## [1]  2  5 11\n\n\nlarger(c(1, 5, 10), 6)\n## [1]  6 NA 10\n\nWhy is there an NA in the second example? It should be a 6. Figure out why this happens, then try to fix it.\n\n\nI’ll replicate “debug” in non-interactive mode by setting up an environment where x and y are defined\n\n\nx <- c(1, 5, 10)\ny <- 6\n\n# Inside of larger() with x = c(1, 5, 10), y = 6\n(y.is.bigger <- y > x ) # putting something in () prints it out\n## [1]  TRUE  TRUE FALSE\ny[y.is.bigger] # This isn't quite what we were going for, but it's what's causing the issue\n## [1]  6 NA\nx[y.is.bigger] # What gets replaced\n## [1] 1 5\n\n\n# Better option\nlarger <- function(x, y) { \n  y.is.bigger <- y > x \n  ifelse(y.is.bigger, y, x)\n} \n\n\n\n\n\n\n\n\n\n\n[1] \nWikipedia Contributors, “Defensive programming,” Wikipedia. Wikimedia Foundation, Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Defensive_programming&oldid=1084121123. [Accessed: May 31, 2022]\n\n\n[2] \nH. Wickham et al., “Welcome to the tidyverse,” Journal of Open Source Software, vol. 4, no. 43, p. 1686, 2019, doi: 10.21105/joss.01686. \n\n\n[3] \nK. Ushey, Renv: Project environments. 2022 [Online]. Available: https://CRAN.R-project.org/package=renv\n\n\n\n[4] \nH. Wickham and J. Bryan, R Packages: Organize, Test, Document, and Share Your Code, 1st ed. Sebastopol, CA: O’Reilly, 2015 [Online]. Available: https://r-pkgs.org/. [Accessed: Sep. 23, 2022]\n\n\n[5] \nT. Beuzen and T. Timbers, Python Packages, 1st edition. Boca Raton: Chapman; Hall/CRC, 2022 [Online]. Available: https://py-pkgs.org/\n\n\n\n[6] \nJ. Evans, “A debugging manifesto https://t.co/3eSOFQj1e1,” Twitter. Sep. 2022 [Online]. Available: https://twitter.com/b0rk/status/1570060516839641092. [Accessed: Sep. 21, 2022]\n\n\n[7] \nNasser_Junior, “User.fist_name https://t.co/lxrf3IFO4x,” Twitter. Aug. 2020 [Online]. Available: https://twitter.com/Nasser_Junior/status/1295805928315531264. [Accessed: Sep. 21, 2022]\n\n\n[8] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[9] \nJ. Evans, “Debugging strategy: Reread the error message https://t.co/2BZHhPg04h,” Twitter. Sep. 2022 [Online]. Available: https://twitter.com/b0rk/status/1570463473011920897. [Accessed: Sep. 21, 2022]\n\n\n[10] \nJ. Evans, “Debugging strategy: Shorten your feedback loop https://t.co/1cByDlafsK,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1549164800978059264. [Accessed: Sep. 21, 2022]\n\n\n[11] \nJ. Evans, “Debugging strategy: Write a tiny program https://t.co/Kajr5ZyeIp,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1547247776001654786. [Accessed: Sep. 21, 2022]\n\n\n[12] \nJ. Evans, “Debugging strategy: Change working code into broken code https://t.co/1T5uNDDFs0,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1545099244238946304. [Accessed: Sep. 21, 2022]\n\n\n[13] \nJ. Evans, “Debugging strategy: Come up with one question https://t.co/2Lytzl4laQ,” Twitter. Aug. 2022 [Online]. Available: https://twitter.com/b0rk/status/1554120424602193921. [Accessed: Sep. 21, 2022]\n\n\n[14] \nJ. Evans, “Debugging strategy: Write a unit test https://t.co/mC01DBNyM3,” Twitter. Aug. 2022 [Online]. Available: https://twitter.com/b0rk/status/1561718747504803842. [Accessed: Sep. 21, 2022]\n\n\n[15] \nT. Monteiro, “Improve how you code: Understanding rubber duck debugging. Duckly blog,” Oct. 31, 2019. [Online]. Available: https://duckly.com/blog/improve-how-to-code-with-rubber-duck-debugging/. [Accessed: Jan. 11, 2023]\n\n\n[16] \nS. Grimes, “This 500-Year-Old Piece of Advice Can Help You Solve Your Modern Problems,” Forge. Dec. 2019 [Online]. Available: https://forge.medium.com/the-500-year-old-piece-of-advice-that-will-change-your-life-1e580f115731. [Accessed: Sep. 21, 2022]"
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#indoc--",
    "href": "part-gen-prog/06-debugging.html#indoc--",
    "title": "13  Debugging",
    "section": "\n13.6 indoc <- ’",
    "text": "13.6 indoc <- ’\ntitle: “Basic SASmarkdown Doc” author: “Doug Hemken” output: html_document —"
  },
  {
    "objectID": "part-gen-prog/07-extra.html#mits-missing-semester",
    "href": "part-gen-prog/07-extra.html#mits-missing-semester",
    "title": "14  Other Helpful Programming Resources",
    "section": "14.1 MIT’s Missing Semester",
    "text": "14.1 MIT’s Missing Semester\nThis set of 11 1-hour lectures [1] covers topics that will help you develop general programming/computing skills. The topics covered are (mostly) adjacent to things covered in this book (with the exception of version control), but it seems like an excellent way to bone up on skills like how to work with the command line, how to accomplish basic tasks at the command line, and various other things that students tend to struggle with but that we don’t usually have time to go over in class in great detail."
  },
  {
    "objectID": "part-gen-prog/07-extra.html#references",
    "href": "part-gen-prog/07-extra.html#references",
    "title": "14  Other Helpful Programming Resources",
    "section": "14.2 References",
    "text": "14.2 References\n\n\n\n\n[1] Anish Athalye, Jon Gjengset, and Jose Javier, “The missing semester of your CS education. Missing semester.” [Online]. Available: https://missing.csail.mit.edu/. [Accessed: Apr. 20, 2023]"
  },
  {
    "objectID": "part-wrangling/00-wrangling.html#sec-wrangling-refs",
    "href": "part-wrangling/00-wrangling.html#sec-wrangling-refs",
    "title": "Part III: Data Wrangling",
    "section": "References",
    "text": "References\n\n\n\n\n[1] H. Wickham, “Tidy data,” The Journal of Statistical Software, vol. 59, 2014 [Online]. Available: http://www.jstatsoft.org/v59/i10/"
  },
  {
    "objectID": "part-wrangling/01-data-input.html#fa-bullseye-objectives",
    "href": "part-wrangling/01-data-input.html#fa-bullseye-objectives",
    "title": "15  Data Input",
    "section": "\n15.1  Objectives",
    "text": "15.1  Objectives\n\nRead in data from common formats into R or Python\nIdentify delimiters, headers, and other essential components of files\n\n\n\n\n\n\n\nCheatsheets!\n\n\n\nThese may be worth printing off as you work through this module.\n\nR - tidyverse\nPython"
  },
  {
    "objectID": "part-wrangling/01-data-input.html#overview-data-formats",
    "href": "part-wrangling/01-data-input.html#overview-data-formats",
    "title": "15  Data Input",
    "section": "\n15.2 Overview: Data Formats",
    "text": "15.2 Overview: Data Formats\nIn order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we’ll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables. This type of data can be stored on the computer in multiple ways:\n\nas raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable.\n\nas a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected.\n\nR, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as foreign in R will let you read data from other programs, and packages such as haven in R will let you write data into binary formats used by other programs.\n\n[1] describes why binary file formats exist, and why they’re not necessarily optimal.\n\n\nin a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not binary formats, but they’re also not raw text files either. They’re a hybrid - a special type of markup that is specific to the filetype and the program it’s designed to work with. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet.\n\n\n\n\n\n\n\nNote\n\n\n\nThere is a collection of spreadsheet horror stories here and a series of even more horrifying tweets here.\nAlso, there’s this amazing comic:\n\n\n\n\nin a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with.\n\nThere are, of course, many other non-tabular data formats – some open and easy to work with, some inpenetrable. A few which you may come across:\n\nWeb data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R or SAS, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis. See Chapter 25 for some tools to work with these files.\nSpatial files: Shapefiles are the most common version of spatial files, though there are a seemingly infinite number of different formats, and new formats pop up at the most inconvenient times. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information. Chapter 26 covers some of the tools available for working with spatial data.\n\nTo be minimally functional in R and Python, it’s important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files. We will briefly discuss binary files and databases, but it is less critical to remember how to read these in without consulting an online reference."
  },
  {
    "objectID": "part-wrangling/01-data-input.html#text-files",
    "href": "part-wrangling/01-data-input.html#text-files",
    "title": "15  Data Input",
    "section": "\n15.3 Text Files",
    "text": "15.3 Text Files\nThere are several different variants of text data which are relatively common, but for the most part, text data files can be broken down into fixed-width and delimited formats. What’s the difference, you say?\n\n15.3.1 Fixed-width files\nCol1    Col2    Col3\n 3.4     4.2     5.4\n27.3    -2.4    15.9\nIn a fixed-width text file, the position of the data indicates which field (variable/column) it belongs to. These files are fairly common outputs from older FORTRAN-based programs, but may be found elsewhere as well - if you have a very large amount of data, a fixed-width format may be more efficient to read, because you can select only the portions of the file which matter for a particular analysis (and so you don’t have to read the whole thing into memory).\n\n\n\n\n\n\nFixed Width File IO\n\n\n\n\n\nBase R\nreadr\nPython\n\n\n\nIn base R (no extra packages), you can read fixed-width files in using read.fwf, but you must specify the column breaks yourself, which can be painful.\n\n## url <- \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202206070000/mdf/TEXT/\"\ndata <- read.fwf(url, \n         skip = 3, # Skip the first 2 lines (useless) + header line\n         widths = c(5, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 8, 9, 6, 7, 7, 7, 7, 7, 7, \n7, 8, 8, 8)) # There is a row with the column names specified\n\ndata[1:6,] # first 6 rows\n##      V1  V2 V3 V4   V5  V6  V7  V8   V9 V10 V11   V12    V13 V14  V15 V16  V17\n## 1  ACME 110  0 60 29.9 4.4 4.3 111  9.0 0.8 6.4  0.00 959.37 267 29.6 3.6 25.4\n## 2  ADAX   1  0 69 29.3 1.7 1.6  98 24.9 0.6 3.4  0.00 971.26 251 29.0 0.6 24.6\n## 3  ALTU   2  0 52 31.7 5.5 5.4  89  7.6 1.0 7.8  0.00 956.12 287 31.3 3.5 26.5\n## 4  ALV2 116  0 57 30.1 2.5 2.4 108 10.3 0.5 3.6 55.63 954.01 266 30.1 1.7 23.3\n## 5  ANT2 135  0 75 29.1 1.1 1.1  44 21.1 0.3 2.0  0.00 985.35 121 28.9 0.5 25.9\n## 6  APAC 111  0 58 29.9 5.1 5.1 107  8.5 0.7 6.6  0.00 954.47 224 29.7 3.6 26.2\n##    V18  V19  V20    V21  V22  V23     V24\n## 1 29.4 27.4 22.5   20.6 1.55 1.48    1.40\n## 2 28.7 25.6 24.3 -998.0 1.46 1.52 -998.00\n## 3 32.1 27.6 24.0 -998.0 1.72 1.50 -998.00\n## 4 30.3 26.2 21.1 -998.0 1.49 1.40 -998.00\n## 5 29.0 26.3 22.8   21.4 1.51 1.39    1.41\n## 6 29.1 26.6 24.3   20.5 1.59 1.47    1.40\n\nYou can count all of those spaces by hand (not shown), you can use a different function, or you can write code to do it for you.\n\n\n\n\n\n\nCode for counting field width\n\n\n\n\n\n\n\n# I like to cheat a bit....\n# Read the first few lines in\ntmp <- readLines(url, n = 20)[-c(1:2)]\n\n# split each line into a series of single characters\ntmp_chars <- strsplit(tmp, '') \n\n# Bind the lines together into a character matrix\n# do.call applies a function to an entire list - so instead of doing 18 rbinds, \n# one command will put all 18 rows together\ntmp_chars <- do.call(\"rbind\", tmp_chars) # (it's ok if you don't get this line)\n\n# Make into a logical matrix where T = space, F = not space\ntmp_chars_space <- tmp_chars == \" \"\n\n# Add up the number of rows where there is a non-space character\n# space columns would have 0s/FALSE\ntmp_space <- colSums(!tmp_chars_space)\n\n# We need a nonzero column followed by a zero column\nbreaks <- which(tmp_space != 0 & c(tmp_space[-1], 0) == 0)\n\n# Then, we need to get the widths between the columns\nwidths <- diff(c(0, breaks))\n\n# Now we're ready to go\nmesodata <- read.fwf(url, skip = 3, widths = widths, header = F)\n# read header separately - if you use header = T, it errors for some reason.\n# It's easier just to work around the error than to fix it :)\nmesodata_names <- read.fwf(url, skip = 2, n = 1, widths = widths, header = F, \n                           stringsAsFactors = F)\nnames(mesodata) <- as.character(mesodata_names)\n\nmesodata[1:6,] # first 6 rows\n##    STID   STNM   TIME    RELH    TAIR    WSPD    WVEC   WDIR    WDSD    WSSD\n## 1  ACME    110      0      60    29.9     4.4     4.3    111     9.0     0.8\n## 2  ADAX      1      0      69    29.3     1.7     1.6     98    24.9     0.6\n## 3  ALTU      2      0      52    31.7     5.5     5.4     89     7.6     1.0\n## 4  ALV2    116      0      57    30.1     2.5     2.4    108    10.3     0.5\n## 5  ANT2    135      0      75    29.1     1.1     1.1     44    21.1     0.3\n## 6  APAC    111      0      58    29.9     5.1     5.1    107     8.5     0.7\n##      WMAX     RAIN      PRES   SRAD    TA9M    WS2M    TS10    TB10    TS05\n## 1     6.4     0.00    959.37    267    29.6     3.6    25.4    29.4    27.4\n## 2     3.4     0.00    971.26    251    29.0     0.6    24.6    28.7    25.6\n## 3     7.8     0.00    956.12    287    31.3     3.5    26.5    32.1    27.6\n## 4     3.6    55.63    954.01    266    30.1     1.7    23.3    30.3    26.2\n## 5     2.0     0.00    985.35    121    28.9     0.5    25.9    29.0    26.3\n## 6     6.6     0.00    954.47    224    29.7     3.6    26.2    29.1    26.6\n##      TS25    TS60     TR05     TR25     TR60\n## 1    22.5    20.6     1.55     1.48     1.40\n## 2    24.3  -998.0     1.46     1.52  -998.00\n## 3    24.0  -998.0     1.72     1.50  -998.00\n## 4    21.1  -998.0     1.49     1.40  -998.00\n## 5    22.8    21.4     1.51     1.39     1.41\n## 6    24.3    20.5     1.59     1.47     1.40\n\n\n\n\nYou can also write fixed-width files if you really want to:\n\nif (!\"gdata\" %in% installed.packages()) install.packages(\"gdata\")\nlibrary(gdata)\nwrite.fwf(mtcars, file = tempfile())\n\n\n\nThe readr package creates data-frame like objects called tibbles (a souped-up data frame), but it is much friendlier to use.\n\nlibrary(readr) # Better data importing in R\n\nread_table(url, skip = 2) # Gosh, that was much easier!\n## # A tibble: 120 × 24\n##    STID   STNM  TIME  RELH   TAIR  WSPD  WVEC  WDIR  WDSD  WSSD  WMAX  RAIN\n##    <chr> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1 ACME    110     0    60   29.9   4.4   4.3   111   9     0.8   6.4   0  \n##  2 ADAX      1     0    69   29.3   1.7   1.6    98  24.9   0.6   3.4   0  \n##  3 ALTU      2     0    52   31.7   5.5   5.4    89   7.6   1     7.8   0  \n##  4 ALV2    116     0    57   30.1   2.5   2.4   108  10.3   0.5   3.6  55.6\n##  5 ANT2    135     0    75   29.1   1.1   1.1    44  21.1   0.3   2     0  \n##  6 APAC    111     0    58   29.9   5.1   5.1   107   8.5   0.7   6.6   0  \n##  7 ARD2    126     0    61   31.2   3.3   3.2   109   9.1   0.6   4.3   0  \n##  8 ARNE      6     0    49   30.4   4.5   4.4   111  11.1   0.9   6.4   0  \n##  9 BEAV      8     0    42   30.5   6.1   6     127   8.7   0.9   7.9   0  \n## 10 BESS      9     0    53 -999     5.3   5.2   115   8.6   0.6   7     0  \n## # … with 110 more rows, and 12 more variables: PRES <dbl>, SRAD <dbl>,\n## #   TA9M <dbl>, WS2M <dbl>, TS10 <dbl>, TB10 <dbl>, TS05 <dbl>, TS25 <dbl>,\n## #   TS60 <dbl>, TR05 <dbl>, TR25 <dbl>, TR60 <dbl>\n\n\n\nBy default, pandas’ read_fwf will guess at the format of your fixed-width file.\n\nimport pandas as pd\nurl = \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/\"\ndata = pd.read_fwf(url, skiprows = 2) # Skip the first 2 lines (useless)\n\n\n\n\n\n\n\n15.3.2 Delimited Text Files\nDelimited text files are files where fields are separated by a specific character, such as space, comma, semicolon, tabs, etc. Often, delimited text files will have the column names as the first row in the file.\n\n\n\n\n\n\nComma Delimited Files\n\n\n\n\n\nBase R\nreadr\nPython\n\n\n\n\nurl <- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\n\npokemon_info <- read.csv(url, header = T, stringsAsFactors = F)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n\n  gen pokedex_no\n1   1          1\n2   1          2\n3   1          3\n4   1          3\n5   1          4\n6   1          5\n                                                               img_link\n1     https://img.pokemondb.net/sprites/sword-shield/icon/bulbasaur.png\n2       https://img.pokemondb.net/sprites/sword-shield/icon/ivysaur.png\n3      https://img.pokemondb.net/sprites/sword-shield/icon/venusaur.png\n4 https://img.pokemondb.net/sprites/sword-shield/icon/venusaur-mega.png\n5    https://img.pokemondb.net/sprites/sword-shield/icon/charmander.png\n6    https://img.pokemondb.net/sprites/sword-shield/icon/charmeleon.png\n        name variant         type\n1  Bulbasaur    <NA> Grass,Poison\n2    Ivysaur    <NA> Grass,Poison\n3   Venusaur    <NA> Grass,Poison\n4   Venusaur    Mega Grass,Poison\n5 Charmander    <NA>         Fire\n6 Charmeleon    <NA>         Fire\n\n\n\n\nThere is a family of read_xxx functions in readr to read files delimited with commas (read_csv), tabs (read_tsv), or generic delimited files (read_delim).\nThe most common delimited text format is CSV: comma-separated value.\n\nlibrary(readr)\nurl <- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\npokemon_info <- read_csv(url)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n\n# A tibble: 6 × 6\n    gen pokedex_no img_link                                  name  variant type \n  <dbl>      <dbl> <chr>                                     <chr> <chr>   <chr>\n1     1          1 https://img.pokemondb.net/sprites/sword-… Bulb… <NA>    Gras…\n2     1          2 https://img.pokemondb.net/sprites/sword-… Ivys… <NA>    Gras…\n3     1          3 https://img.pokemondb.net/sprites/sword-… Venu… <NA>    Gras…\n4     1          3 https://img.pokemondb.net/sprites/sword-… Venu… Mega    Gras…\n5     1          4 https://img.pokemondb.net/sprites/sword-… Char… <NA>    Fire \n6     1          5 https://img.pokemondb.net/sprites/sword-… Char… <NA>    Fire \n\n\n\n\nThere is a family of read_xxx functions in pandas including functions to read files delimited with commas (read_csv) as well as generic delimited files (read_table).\n\nimport pandas as pd\n\nurl <- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\n\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: bad operand type for unary -: 'str'\n\npokemon_info = pd.read_csv(url)\npokemon_info.iloc[:,2:51]\n\nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n\n[123 rows x 0 columns]\n\n\n\n\n\n\n\nSometimes, data is available in files that use other characters as delimiters. This can happen when commas are an important part of the data stored in the file, but can also just be a choice made by the person generating the file. Either way, we can’t let it keep us from accessing the data.\n\n\n\n\n\n\nOther Character Delimited Files\n\n\n\n\n\nBase R\nreadr\nPython\n\n\n\n\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = '../data/NE_Features.zip')\n# Unzip to `data/` folder\nunzip('../data/NE_Features.zip', exdir = '../data/')\n# List files matching the file type and pick the first one\nfname <- list.files(\"../data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\n# a file delimited with |\nnebraska_locations <- read.delim(fname, sep = \"|\", header = T)\nnebraska_locations[1:6, 1:6]\n##   FEATURE_ID     FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC\n## 1     171013      Peetz Table          Area          CO             8\n## 2     171029      Sidney Draw        Valley          NE            31\n## 3     182687   Highline Canal         Canal          CO             8\n## 4     182688 Cottonwood Creek        Stream          CO             8\n## 5     182689        Sand Draw        Valley          CO             8\n## 6     182690    Sedgwick Draw        Valley          CO             8\n##   COUNTY_NAME\n## 1       Logan\n## 2    Cheyenne\n## 3    Sedgwick\n## 4    Sedgwick\n## 5    Sedgwick\n## 6    Sedgwick\n\n\n\nThere is a family of read_xxx functions in readr to read files delimited with commas (read_csv), tabs (read_tsv), or generic delimited files (read_delim).\n\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = '../data/NE_Features.zip')\n# Unzip to `data/` folder\nunzip('../data/NE_Features.zip', exdir = '../data/')\n# List files matching the file type and pick the first one\nfname <- list.files(\"../data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\nnebraska_locations <- read_delim(fname, delim = \"|\")\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 × 6\n##   FEATURE_ID FEATURE_NAME     FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_N…¹\n##        <dbl> <chr>            <chr>         <chr>       <chr>         <chr>     \n## 1     171013 Peetz Table      Area          CO          08            Logan     \n## 2     171029 Sidney Draw      Valley        NE          31            Cheyenne  \n## 3     182687 Highline Canal   Canal         CO          08            Sedgwick  \n## 4     182688 Cottonwood Creek Stream        CO          08            Sedgwick  \n## 5     182689 Sand Draw        Valley        CO          08            Sedgwick  \n## 6     182690 Sedgwick Draw    Valley        CO          08            Sedgwick  \n## # … with abbreviated variable name ¹​COUNTY_NAME\n\nWe can actually read in the file without unzipping it, so long as we download it first - readr does not support reading remote zipped files, but it does support reading zipped files locally. If we know ahead of time what our delimiter is, this is the best choice as it reduces the amount of file clutter we have in our working directory.\n\nnebraska_locations <- read_delim(\"../data/NE_Features.zip\", delim = \"|\")\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 × 6\n##   FEATURE_ID FEATURE_NAME     FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_N…¹\n##        <dbl> <chr>            <chr>         <chr>       <chr>         <chr>     \n## 1     171013 Peetz Table      Area          CO          08            Logan     \n## 2     171029 Sidney Draw      Valley        NE          31            Cheyenne  \n## 3     182687 Highline Canal   Canal         CO          08            Sedgwick  \n## 4     182688 Cottonwood Creek Stream        CO          08            Sedgwick  \n## 5     182689 Sand Draw        Valley        CO          08            Sedgwick  \n## 6     182690 Sedgwick Draw    Valley        CO          08            Sedgwick  \n## # … with abbreviated variable name ¹​COUNTY_NAME\n\n\n\nThere is a family of read_xxx functions in pandas including functions to read files delimited with commas (read_csv) as well as generic delimited files (read_table).\nPandas can access zipped data files and unzip them while reading the data in, so we don’t have to download the file and unzip it first.\n\n# a file delimited with |\n\nurl = \"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\"\nnebraska_locations = pd.read_table(url, delimiter = \"|\")\nnebraska_locations\n##        FEATURE_ID  ... DATE_EDITED\n## 0          171013  ...         NaN\n## 1          171029  ...  03/08/2018\n## 2          182687  ...         NaN\n## 3          182688  ...  10/23/2009\n## 4          182689  ...  12/20/2017\n## ...           ...  ...         ...\n## 31473     2806916  ...  08/12/2021\n## 31474     2806917  ...  08/12/2021\n## 31475     2806918  ...  08/12/2021\n## 31476     2806919  ...         NaN\n## 31477     2806920  ...  08/12/2021\n## \n## [31478 rows x 20 columns]\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Reading CSV files\n\n\n\nRebrickable.com contains tables of almost any information imaginable concerning Lego sets, conveniently available at their download page. Because these data sets are comparatively large, they are available as compressed CSV files - that is, the .gz extension is a gzip compression applied to the CSV.\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe readr package and pandas can handle .csv.gz files with no problems. Try reading in the data using the appropriate function from that package. Can you save the data as an uncompressed csv?\n\n\n\nlibrary(readr)\nlegosets <- read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nwrite_csv(legosets, \"../data/lego_sets.csv\")\n\n\n\n\nimport pandas as pd\n\nlegosets = pd.read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nlegosets.to_csv(\"../data/lego_sets_py.csv\")"
  },
  {
    "objectID": "part-wrangling/01-data-input.html#spreadsheets",
    "href": "part-wrangling/01-data-input.html#spreadsheets",
    "title": "15  Data Input",
    "section": "\n15.4 Spreadsheets",
    "text": "15.4 Spreadsheets\n\n\n\n\n\n\nSpreadsheet IO\n\n\n\nThis example uses from NYC SLice. The author maintains a google sheet of the slices he has photographed, which we can download as an excel sheet and import.\n\n\nR readxl\nPython\n\n\n\nIn R, the easiest way to read Excel data in is to use the readxl package. There are many other packages with different features, however - I have used openxlsx in the past to format spreadsheets to send to clients, for instance. By far and away you are more likely to have problems with the arcane format of the Excel spreadsheet than with the package used to read the data in. It is usually helpful to open the spreadsheet up in a graphical program first to make sure the formatting is as you expected it to be.\n\nif (!\"readxl\" %in% installed.packages()) install.packages(\"readxl\")\nlibrary(readxl)\n\nurl <- \"https://docs.google.com/spreadsheets/d/1EY3oi9ttxybG0A0Obtwey6BFu7QLqdHe02JApijgztg/export?format=xlsx\"\n# Only download the data if it doesn't exist in the data folder\nif (!file.exists(\"../data/nyc_slice.xlsx\")) {\n  download.file(url, destfile = \"../data/nyc_slice.xlsx\", mode = \"wb\")\n}\n\n# Read in the downloaded data\npizza_data <- read_xlsx(\"../data/nyc_slice.xlsx\", sheet = 1)\npizza_data[1:10, 1:6]\n## # A tibble: 10 × 6\n##    `Link to IG Post`                        Name   locat…¹ locat…² Date  Date …³\n##    <chr>                                    <chr>  <chr>   <chr>   <chr> <chr>  \n##  1 https://www.instagram.com/p/CjszJ-fOP5o/ Angel… 40.623… -73.93… 2022… Oct 14…\n##  2 https://www.instagram.com/p/CjdcPNAufPj/ Ozone… 40.680… -73.84… 2022… Oct 8t…\n##  3 https://www.instagram.com/p/CjQdNsaOZlY/ Pino … 40.600… -73.99… 2022… Oct 3r…\n##  4 https://www.instagram.com/p/Ci5XblnOnMA/ La Ro… 40.713… -73.82… 2022… Sep 24…\n##  5 https://www.instagram.com/p/CiiLAtkON_1/ Rony'… 40.748… -73.99… 2022… Sep 15…\n##  6 https://www.instagram.com/p/CiS-44nucNE/ John … 40.854… -73.86… 2022… Sep 9t…\n##  7 https://www.instagram.com/p/CiSmQnjutQy/ Prego… 40.863… -73.85… 2022… Sep 9t…\n##  8 https://www.instagram.com/p/CiIO6oFuxpP/ N & D… 40.600… -73.94… 2022… Sep 5t…\n##  9 https://www.instagram.com/p/ChaZUsxuFsr/ Peppi… 40.903… -73.85… 2022… Aug 18…\n## 10 https://www.instagram.com/p/ChNd9wqOqGD/ Rocco… 40.867… -73.88… 2022… Aug 13…\n## # … with abbreviated variable names ¹​location_lat, ²​location_lng,\n## #   ³​`Date Expanded (times in EST)`\n\n\n\n\nimport pandas as pd\n\npizza_data = pd.read_excel(\"../data/nyc_slice.xlsx\")\npizza_data\n##                               Link to IG Post  ... Notes\n## 0    https://www.instagram.com/p/CjszJ-fOP5o/  ...   NaN\n## 1    https://www.instagram.com/p/CjdcPNAufPj/  ...   NaN\n## 2    https://www.instagram.com/p/CjQdNsaOZlY/  ...   NaN\n## 3    https://www.instagram.com/p/Ci5XblnOnMA/  ...   NaN\n## 4    https://www.instagram.com/p/CiiLAtkON_1/  ...   NaN\n## ..                                        ...  ...   ...\n## 459   https://www.instagram.com/p/rqCdE_hp3N/  ...   NaN\n## 460   https://www.instagram.com/p/rnT-kRhp4h/  ...   NaN\n## 461   https://www.instagram.com/p/rh17_NBp7a/  ...   NaN\n## 462   https://www.instagram.com/p/rfnZKmBp3B/  ...   NaN\n## 463   https://www.instagram.com/p/rfGr-RBp4U/  ...   NaN\n## \n## [464 rows x 11 columns]\n\n\n\n\n\n\nIn general, it is better to avoid working in Excel, as it is not easy to reproduce the results (and Excel is horrible about dates and times, among other issues). Saving your data in more reproducible formats will make writing reproducible code much easier.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe Nebraska Department of Motor Vehicles publishes a database of vehicle registrations by type of license plate. Link\nRead in the data using your language(s) of choice. Be sure to look at the structure of the excel file, so that you can read the data in properly!\n\n\n\nurl <- \"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\"\ndownload.file(url, destfile = \"../data/NE_Licensed_Drivers_by_Type_2021.xls\", mode = \"wb\")\nlibrary(readxl)\nne_plates <- read_xls(path = \"../data/NE_Licensed_Drivers_by_Type_2021.xls\", skip = 2)\nne_plates[1:10,1:6]\n## # A tibble: 10 × 6\n##    Age   `\\nOperator's \\nLicense -\\nClass O` Operator's\\…¹ Motor…² Comme…³ ...6 \n##    <chr>                               <dbl>         <dbl>   <dbl> <chr>   <chr>\n##  1 <NA>                                   NA            NA      NA CDL A   CDL B\n##  2 14                                      0             0       0 0       0    \n##  3 15                                      0             0       0 0       0    \n##  4 16                                      0             0       0 0       0    \n##  5 17                                    961            33       0 0       0    \n##  6 18                                  18903           174       0 25      3    \n##  7 19                                  22159           251       0 97      32   \n##  8 20                                  22844           326       1 144     47   \n##  9 21                                  21589           428       0 233     57   \n## 10 22                                  22478           588       0 292     81   \n## # … with abbreviated variable names\n## #   ¹​`Operator's\\nLicense - \\nClass O/\\nMotorcycle\\nClass M`,\n## #   ²​`Motor-\\ncycle\\nLicense /\\nClass M`, ³​`Commercial Driver's License`\n\n\n\nYou may need to install xlrd via pip for this code to work.\n\nimport pandas as pd\n\nne_plates = pd.read_excel(\"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\", skiprows = 2)\n## Error in py_call_impl(callable, dots$args, dots$keywords): ImportError: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.\nne_plates\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'ne_plates' is not defined\n\n\n\n\n\n\n\n15.4.1 Google Sheets\nOf course, some spreadsheets are available online via Google sheets. There are specific R and python packages to interface with Google sheets, and these can do more than just read data in - they can create, format, and otherwise manipulate Google sheets programmatically. We’re not going to get into the power of these packages just now, but it’s worth a look if you’re working with collaborators who use Google sheets.\n\n\n\n\n\n\nAdvanced\n\n\n\nThis section is provided for reference, but the details of API authentication are a bit too complicated to require of anyone who is just learning to program. Feel free to skip it and come back later if you need it.\nThe first two tabs below show authentication-free options for publicly available spreadsheets. For anything that is privately available, you will have to use API authentication via GSpread or googlesheets4 in python and R respectively.\n\n\n\n\n\n\n\n\nReading Google Sheets\n\n\n\nLet’s demonstrate reading in data from google sheets in R and python using the Data Is Plural archive.\n\n\nPython\nR\nR: googlesheets4\nPython: GSpread\n\n\n\nOne simple hack-ish way to read google sheets in Python (so long as the sheet is publicly available) is to modify the sheet url to export the data to CSV and then just read that into pandas as usual. This method is described in [2].\n\nimport pandas as pd\n\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n\ndata_is_plural = pd.read_csv(url)\n\nThis method would likely work just as well in R and would not require the googlesheets4 package.\n\n\nThis method is described in [2] for Python, but I have adapted the code to use in R.\n\nlibrary(readr)\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = sprintf(\"https://docs.google.com/spreadsheets/d/%s/gviz/tq?tqx=out:csv&sheet=%s\", sheet_id, sheet_name)\n\ndata_is_plural = read_csv(url)\n\n\n\nThis code is set not to run when the textbook is compiled because it requires some interactive authentication.\nCopy this code and run it on your computer to read in a sheet from google drive directly. You will see a prompt in the R console that you’ll have to interact with, and there may also be a browser pop-up where you will need to sign in to google.\n\n\nlibrary(googlesheets4)\ngs4_auth(scopes = \"https://www.googleapis.com/auth/drive.readonly\") # Read-only permissions\ndata_is_plural <- read_sheet(\"https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0\")\n\n\n\nThese instructions are derived from [3]. We will have to install the GSpread package: type pip install gspread into the terminal.\nThen, you will need to obtain a client token JSON file following these instructions.\n\nimport gspread as gs\nimport pandas as pd\n\nI’ve stopped here because I can’t get the authentication working, but the method seems solid if you’re willing to fiddle around with it. \n\n\n\n\n\n\n\n\n\n\n\nTry It Out!\n\n\n\n\n\nProblem\nSolution\n\n\n\nUsing a method of your choice, read in this spreadsheet of dog sizes and make a useful plot of dog height and weight ranges by breed.\n\n\nComing soon!"
  },
  {
    "objectID": "part-wrangling/01-data-input.html#binary-files",
    "href": "part-wrangling/01-data-input.html#binary-files",
    "title": "15  Data Input",
    "section": "\n15.5 Binary Files",
    "text": "15.5 Binary Files\nR has binary file formats which store data in a more compact form. It is relatively common for government websites, in particular, to provide SAS data in binary form. Python, as a more general computing language, has many different ways to interact with binary data files, as each programmer and application might want to save their data in binary form in a different way. As a result, there is not a general-purpose binary data format for Python data. If you are interested in reading binary data in Python, see [4].\n\n\n\n\n\n\nBinary File IO\n\n\n\n\n\nR formats in R\nR formats in Python\nSAS format in R\nSAS format in Python\n\n\n\n.Rdata is perhaps the most common R binary data format, and can store several objects (along with their names) in the same file.\n\nlegos <- read_csv(\"../data/lego_sets.csv\")\nmy_var <- \"This variable contains a string\"\nsave(legos, my_var, file = \"../data/R_binary.Rdata\")\n\nIf we look at the file sizes of lego_sets.csv (619 KB) and R_binary.Rdata(227.8 KB), the size difference between binary and flat file formats is obvious.\nWe can load the R binary file back in using the load() function.\n\nrm(legos, my_var) # clear the files out\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"legosets\"           \"mesodata\"          \n##  [7] \"mesodata_names\"     \"ne_plates\"          \"nebraska_locations\"\n## [10] \"pizza_data\"         \"pokemon_info\"       \"sheet_id\"          \n## [13] \"sheet_name\"         \"tmp\"                \"tmp_chars\"         \n## [16] \"tmp_chars_space\"    \"tmp_space\"          \"url\"               \n## [19] \"widths\"\n\nload(\"../data/R_binary.Rdata\")\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"legos\"              \"legosets\"          \n##  [7] \"mesodata\"           \"mesodata_names\"     \"my_var\"            \n## [10] \"ne_plates\"          \"nebraska_locations\" \"pizza_data\"        \n## [13] \"pokemon_info\"       \"sheet_id\"           \"sheet_name\"        \n## [16] \"tmp\"                \"tmp_chars\"          \"tmp_chars_space\"   \n## [19] \"tmp_space\"          \"url\"                \"widths\"\n\nAnother (less common) binary format used in R is the RDS format. Unlike Rdata, the RDS format does not save the object name - it only saves its contents (which also means you can save only one object at a time). As a result, when you read from an RDS file, you need to store the result of that function into a variable.\n\nsaveRDS(legos, \"../data/RDSlego.rds\")\n\nother_lego <- readRDS(\"../data/RDSlego.rds\")\n\nBecause RDS formats don’t save the object name, you can be sure that you’re not over-writing some object in your workspace by loading a different file. The downside to this is that you have to save each object to its own RDS file separately.\n\n\nWe first need to install the pyreadr package by running pip install pyreadr in the terminal.\n\nimport pyreadr\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'pyreadr'\nrdata_result = pyreadr.read_r('../data/R_binary.Rdata')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pyreadr' is not defined\nrdata_result[\"legos\"] # Access the variables using the variable name as a key\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'rdata_result' is not defined\nrdata_result[\"my_var\"]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'rdata_result' is not defined\nrds_result = pyreadr.read_r('../data/RDSlego.rds')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pyreadr' is not defined\nrds_result[None] # for RDS files, access the data using None as the key since RDS files have no object name.\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'rds_result' is not defined\n\n\n\nFirst, let’s download the NHTS data.\n\nlibrary(httr)\n# Download the file and write to disk\nres <- GET(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \n           write_disk(\"../data/cen10pub.sas7bdat\", overwrite = T))\n\nYou can see more information about this data here [5].\n\nif (!\"sas7bdat\" %in% installed.packages()) install.packages(\"sas7bdat\")\n\nlibrary(sas7bdat)\ndata <- read.sas7bdat(\"../data/cen10pub.sas7bdat\")\nhead(data)\n##    HOUSEID HH_CBSA10 RAIL10 CBSASIZE10 CBSACAT10 URBAN10 URBSIZE10 URBRUR10\n## 1 20000017     XXXXX     02         02        03      04        06       02\n## 2 20000231     XXXXX     02         03        03      01        03       01\n## 3 20000521     XXXXX     02         03        03      01        03       01\n## 4 20001283     35620     01         05        01      01        05       01\n## 5 20001603        -1     02         06        04      04        06       02\n## 6 20001649     XXXXX     02         03        03      01        02       01\n\nIf you are curious about what this data means, then by all means, take a look at the codebook (XLSX file). For now, it’s enough that we can see roughly how it’s structured.\n\n\nFirst, we need to download the SAS data file. This required writing a function to actually write the file downloaded from the URL, which is what this code chunk does.\n\n# Source: https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\nimport requests\ndef download_file(url, local_filename):\n  # NOTE the stream=True parameter below\n  with requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open(local_filename, 'wb') as f:\n      for chunk in r.iter_content(chunk_size=8192): \n        f.write(chunk)\n  return local_filename\n\ndownload_file(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \"../data/cen10pub.sas7bdat\")\n## '../data/cen10pub.sas7bdat'\n\nYou can see more information about this data here [5].\nTo read SAS files, we use the read_sas function in Pandas.\n\nimport pandas as pd\n\ndata = pd.read_sas(\"../data/cen10pub.sas7bdat\")\ndata\n##             HOUSEID HH_CBSA10 RAIL10  ... URBAN10 URBSIZE10 URBRUR10\n## 0       b'20000017'  b'XXXXX'  b'02'  ...   b'04'     b'06'    b'02'\n## 1       b'20000231'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 2       b'20000521'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 3       b'20001283'  b'35620'  b'01'  ...   b'01'     b'05'    b'01'\n## 4       b'20001603'     b'-1'  b'02'  ...   b'04'     b'06'    b'02'\n## ...             ...       ...    ...  ...     ...       ...      ...\n## 150142  b'69998896'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150143  b'69998980'  b'33100'  b'01'  ...   b'01'     b'05'    b'01'\n## 150144  b'69999718'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150145  b'69999745'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150146  b'69999811'  b'31080'  b'01'  ...   b'01'     b'05'    b'01'\n## \n## [150147 rows x 8 columns]\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nRead in two of the files from an earlier example, and save the results as an Rdata file with two objects. Then save each one as an RDS file. (Obviously, use R for this)\nIn RStudio, go to Session -> Clear Workspace. (This will clear your environment)\nNow, using your RDS files, load the objects back into R with different names.\nFinally, load your Rdata file. Are the two objects the same? (You can actually test this with all.equal() if you’re curious)\nThen, load the two RDS files and the Rdata file in Python. Are the objects the same?\n\n\n\nlibrary(readxl)\nlibrary(readr)\npizza <- read_xlsx(\"../data/nyc_slice.xlsx\", sheet = 1, guess_max = 7000)\nlegos <- read_csv(\"../data/lego_sets.csv\")\n\nsave(pizza, legos, file = \"../data/04_Try_Binary.Rdata\")\nsaveRDS(pizza, \"../data/04_Try_Binary1.rds\")\nsaveRDS(legos, \"../data/04_Try_Binary2.rds\")\n\nrm(pizza, legos) # Limited clearing of workspace... \n\n\nload(\"../data/04_Try_Binary.Rdata\")\n\npizza_compare <- readRDS(\"../data/04_Try_Binary1.rds\")\nlego_compare <- readRDS(\"../data/04_Try_Binary2.rds\")\n\nall.equal(pizza, pizza_compare)\n## [1] TRUE\nall.equal(legos, lego_compare)\n## [1] TRUE\n\n\n\n\nimport pyreadr\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'pyreadr'\nrobjs = pyreadr.read_r('data/04_Try_Binary.Rdata')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pyreadr' is not defined\npizza = robjs[\"pizza\"]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'robjs' is not defined\nlegos = robjs[\"legos\"] # Access the variables using the variable name as a key\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'robjs' is not defined\npizza_compare = pyreadr.read_r('data/04_Try_Binary1.rds')[None]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pyreadr' is not defined\nlego_compare = pyreadr.read_r('data/04_Try_Binary2.rds')[None]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pyreadr' is not defined\npizza.equals(pizza_compare)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pizza' is not defined\nlegos.equals(lego_compare)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'legos' is not defined\n\n\n\n\n\n\n\n\n\n\n\n\nLearn more\n\n\n\n\n\nSlides from Jenny Bryan’s talk on spreadsheets (sadly, no audio. It was a good talk.)\nThe vroom package works like read_csv but allows you to read in and write to many files at incredible speeds."
  },
  {
    "objectID": "part-wrangling/01-data-input.html#references",
    "href": "part-wrangling/01-data-input.html#references",
    "title": "15  Data Input",
    "section": "\n15.6 References",
    "text": "15.6 References\n\n\n\n\n[1] \nBetterExplained, “A little diddy about binary file formats – BetterExplained. Better explained,” 2017. [Online]. Available: https://betterexplained.com/articles/a-little-diddy-about-binary-file-formats/. [Accessed: Jan. 13, 2023]\n\n\n[2] \nM. Schäfer, “Read Data from Google Sheets into Pandas without the Google Sheets API,” Towards Data Science. Dec. 2020 [Online]. Available: https://towardsdatascience.com/read-data-from-google-sheets-into-pandas-without-the-google-sheets-api-5c468536550. [Accessed: Jun. 07, 2022]\n\n\n[3] \nM. Clarke, “How to read Google Sheets data in Pandas with GSpread,” Practical Data Science. Jun. 2021 [Online]. Available: https://practicaldatascience.co.uk/data-science/how-to-read-google-sheets-data-in-pandas-with-gspread. [Accessed: Jun. 07, 2022]\n\n\n[4] \nC. Maierle, “Loading binary data to NumPy/Pandas,” Towards Data Science. Jul. 2020 [Online]. Available: https://towardsdatascience.com/loading-binary-data-to-numpy-pandas-9caa03eb0672. [Accessed: Jun. 07, 2022]\n\n\n[5] \nUS Department of Transportation, “National Household Travel Survey (NHTS) 2009,” data.world. Mar. 2018 [Online]. Available: https://data.world/dot/national-household-travel-survey-nhts-2009. [Accessed: Jun. 13, 2022]"
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#fa-bullseye-objectives",
    "href": "part-wrangling/02-basic-data-vis.html#fa-bullseye-objectives",
    "title": "16  Data Visualization Basics",
    "section": "\n16.1  Objectives",
    "text": "16.1  Objectives\n\nUse ggplot2/plotnine to create a chart\nBegin to identify issues with data formatting"
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#first-steps",
    "href": "part-wrangling/02-basic-data-vis.html#first-steps",
    "title": "16  Data Visualization Basics",
    "section": "\n16.2 First Steps",
    "text": "16.2 First Steps\nNow that you can read data in to R and python and define new variables, you can create plots! Data visualization is a skill that takes a lifetime to learn, but for now, let’s start out easy: let’s talk about how to make (basic) plots in R (with ggplot2) and in python (with plotnine, which is a ggplot2 clone).\n\n16.2.1 Graphing HBCU Enrollment\nLet’s work with Historically Black College and University enrollment.\n\n\n\n\n\n\nLoading Libraries\n\n\n\n\n\nR\nPython\n\n\n\n\nhbcu_all <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-02/hbcu_all.csv')\n\nlibrary(ggplot2)\n\n\n\n\nimport pandas as pd\nfrom plotnine import *\n\nhbcu_all = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-02/hbcu_all.csv')\n\n\n\n\n\n\n\n16.2.2 Making a Line Chart\nggplot2 and plotnine work with data frames.\nIf you pass a data frame in as the data argument, you can refer to columns in that data with “bare” column names (you don’t have to reference the full data object using df$name or df.name; you can instead use name or \"name\").\n\n\n\n\n\n\nDemo\n\n\n\n\n\nR\nPython\n\n\n\n\n\nggplot(hbcu_all, aes(x = Year, y = `4-year`)) + geom_line() +\n  ggtitle(\"4-year HBCU College Enrollment\")\n\n\n\n\n\n\n\n\nggplot(hbcu_all, aes(x = \"Year\", y = \"4-year\")) + geom_line() + \\\n  ggtitle(\"4-year HBCU College Enrollment\")\n## <ggplot: (8750721456381)>\n\n\n\n\n\n\n\n\n\n\n16.2.3 Data Formatting\nIf your data is in the right format, ggplot2 is very easy to use; if your data aren’t formatted neatly, it can be a real pain. If you want to plot multiple lines, you need to either list each variable you want to plot, one by one, or (more likely) you want to get your data into “long form”. We’ll learn more about how to do this type of data transition when we talk about reshaping data.\n\n\n\n\n\n\nDemo\n\n\n\nYou don’t need to know exactly how this works, but it is helpful to see the difference in the two datasets:\n\n\nR\nPython\nOriginal Data\nLong Data\n\n\n\n\nlibrary(tidyr)\nhbcu_long <- pivot_longer(hbcu_all, -Year, names_to = \"type\", values_to = \"value\")\n\n\n\n\nhbcu_long = pd.melt(hbcu_all, id_vars = ['Year'], value_vars = hbcu_all.columns[1:11])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTotal enrollment\nMales\nFemales\n4-year\n2-year\nTotal - Public\n4-year - Public\n2-year - Public\nTotal - Private\n4-year - Private\n2-year - Private\n\n\n\n1976\n222613\n104669\n117944\n206676\n15937\n156836\n143528\n13308\n65777\n63148\n2629\n\n\n1980\n233557\n106387\n127170\n218009\n15548\n168217\n155085\n13132\n65340\n62924\n2416\n\n\n1982\n228371\n104897\n123474\n212017\n16354\n165871\n151472\n14399\n62500\n60545\n1955\n\n\n1984\n227519\n102823\n124696\n212844\n14675\n164116\n151289\n12827\n63403\n61555\n1848\n\n\n1986\n223275\n97523\n125752\n207231\n16044\n162048\n147631\n14417\n61227\n59600\n1627\n\n\n1988\n239755\n100561\n139194\n223250\n16505\n173672\n158606\n15066\n66083\n64644\n1439\n\n\n\n\n\n\n\n\n\n\n\nYear\ntype\nvalue\n\n\n\n1976\nTotal enrollment\n222613\n\n\n1976\nMales\n104669\n\n\n1976\nFemales\n117944\n\n\n1976\n4-year\n206676\n\n\n1976\n2-year\n15937\n\n\n1976\nTotal - Public\n156836\n\n\n\n\n\nIn the long form of the data, we have a row for each data point (year x measurement type), not for each year.\n\n\n\n\n\n\n16.2.4 Making a (Better) Line Chart\nIf we had wanted to show all of the available data before, we would have needed to add a separate line for each column, coloring each one manually, and then we would have wanted to create a legend manually (which is a pain). Converting the data to long form means we can use ggplot2/plotnine to do all of this for us with only a single geom_line statement. Having the data in the right form to plot is very important if you want to get the plot you’re imagining with relatively little effort.\n\n\n\n\n\n\nDemo\n\n\n\n\n\nR\nPython\n\n\n\n\n\nggplot(hbcu_long, aes(x = Year, y = value, color = type)) + geom_line() +\n  ggtitle(\"HBCU College Enrollment\")\n\n\n\n\n\n\n\n\nggplot(hbcu_long, aes(x = \"Year\", y = \"value\", color = \"variable\")) + geom_line() + \\\n  ggtitle(\"HBCU College Enrollment\") + \\\n  theme(subplots_adjust={'right':0.75}) # This moves the key so it takes up 25% of the area\n## <ggplot: (8750718345596)>"
  },
  {
    "objectID": "part-wrangling/02a-eda.html#fa-bullseye-objectives",
    "href": "part-wrangling/02a-eda.html#fa-bullseye-objectives",
    "title": "17  Exploratory Data Analysis",
    "section": "\n17.1  Objectives",
    "text": "17.1  Objectives\n\nUnderstand the main goals of exploratory data analysis\nGenerate and answer questions about a new dataset using charts, tables, and numerical summaries\n\n\n\n\n\n\n\nExtra Reading\n\n\n\nThe EDA chapter in R for Data Science [1] is very good at explaining what the goals of EDA are, and what types of questions you will typically need to answer in EDA. Much of the material in this chapter is based at least in part on the R4DS chapter.\n\n\n\nMajor components of Exploratory Data Analysis (EDA):\n\ngenerating questions about your data\nlook for answers to the questions (visualization, transformation, modeling)\nuse answers to refine the questions and generate new questions\n\nEDA is an iterative process. It is like brainstorming - you start with an idea or question you might have about the data, investigate, and then generate new ideas. EDA is useful even when you are relatively familiar with the type of data you’re working with: in any dataset, it is good to make sure that you know the quality of the data as well as the relationships between the variables in the dataset.\nEDA is important because it helps us to know what challenges a particular data set might bring, what we might do with it. Real data is often messy, with large amounts of cleaning that must be done before statistical analysis can commence.\nWhile in many classes you’ll be given mostly clean data, you do need to know how to clean your own data up so that you can use more interesting data sets for projects (and for fun!). EDA is an important component to learning how to work with messy data.\n\nIn this section, I will mostly be using the plot commands that come with base R/python and require no extra packages. The R for Data Science book [1] shows plot commands which use the ggplot2 library. I’ll show you some plots from ggplot here as well, but you don’t have to understand how to generate them yet. We will learn more about ggplot2 later, though if you want to start using it now, you may."
  },
  {
    "objectID": "part-wrangling/02a-eda.html#a-note-on-language-philosophies",
    "href": "part-wrangling/02a-eda.html#a-note-on-language-philosophies",
    "title": "17  Exploratory Data Analysis",
    "section": "\n17.2 A Note on Language Philosophies",
    "text": "17.2 A Note on Language Philosophies\nIt is usually relatively easy to get summary statistics from a dataset, but the “flow” of EDA is somewhat different depending on the language patterns.\n\nYou must realize that R is written by experts in statistics and statistical computing who, despite popular opinion, do not believe that everything in SAS and SPSS is worth copying. Some things done in such packages, which trace their roots back to the days of punched cards and magnetic tape when fitting a single linear model may take several days because your first 5 attempts failed due to syntax errors in the JCL or the SAS code, still reflect the approach of “give me every possible statistic that could be calculated from this model, whether or not it makes sense”. The approach taken in R is different. The underlying assumption is that the useR is thinking about the analysis while doing it. – Douglas Bates\n\nI provide this as a historical artifact, but it does explain the difference between the approach to EDA and model output in R and Python, and the approach in SAS, which you may see in your other statistics classes. This is not (at least, in my opinion) a criticism – the SAS philosophy dates back to the mainframe and punch card days, and the syntax and output still bear evidence of that – but it is worth noting.\nIn R and in Python, you will have to specify each piece of output you want, but in SAS you will get more than you ever wanted with a single command. Neither approach is wrong, but sometimes one is preferable over the other for a given problem."
  },
  {
    "objectID": "part-wrangling/02a-eda.html#generating-eda-questions",
    "href": "part-wrangling/02a-eda.html#generating-eda-questions",
    "title": "17  Exploratory Data Analysis",
    "section": "\n17.3 Generating EDA Questions",
    "text": "17.3 Generating EDA Questions\nI very much like the two quotes in the [1] section on EDA Questions:\n\nThere are no routine statistical questions, only questionable statistical routines. — Sir David Cox\n\n\nFar better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise. — John Tukey\n\nAs statisticians, we are concerned with variability by default. This is also true during EDA: we are interested in variability (or sometimes, lack thereof) in the variables in our dataset, including the co-variability between multiple variables.\nWe may assess variability using pictures or numerical summaries:\n\nhistograms or density plots (continuous variables)\ncolumn plots (categorical variables)\nboxplots\n5 number summaries (min, 25%, mean, 75%, max)\ntabular data summaries (for categorical variables)\n\nIn many cases, this gives us a picture of both variability and the “typical” value of our variable.\nSometimes we may also be interested in identifying unusual values: outliers, data entry errors, and other points which don’t conform to our expectations. These unusual values may show up when we generate pictures and the axis limits are much larger than expected.\nWe also are usually concerned with missing values - in many cases, not all observations are complete, and this missingness can interfere with statistical analyses. It can be helpful to keep track of how much missingness there is in any particular variable and any patterns of missingness that would impact the eventual data analysis1.\nIf you are having trouble getting started on EDA, [3] provides a nice checklist to get you thinking:\n\n\nWhat question(s) are you trying to solve (or prove wrong)?\nWhat kind of data do you have and how do you treat different types?\nWhat’s missing from the data and how do you deal with it?\nWhere are the outliers and why should you care about them?\nHow can you add, change or remove features to get more out of your data?"
  },
  {
    "objectID": "part-wrangling/02a-eda.html#useful-eda-techniques",
    "href": "part-wrangling/02a-eda.html#useful-eda-techniques",
    "title": "17  Exploratory Data Analysis",
    "section": "\n17.4 Useful EDA Techniques",
    "text": "17.4 Useful EDA Techniques\n\n\n\n\nNintendo, Creatures, Game Freak, The Pokémon Company, Public domain, via Wikimedia Commons\n\n\n\n\n\n\n\n\nExample: Generations of Pokemon\n\n\n\nSuppose we want to explore Pokemon. There’s not just the original 150 (gotta catch ’em all!) - now there are over 1000! Let’s start out by looking at the proportion of Pokemon added in each of the 9 generations.\n\n\nR setup\nPython setup\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n# Setup the data\npoke <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\", na = '.') %>%\n  mutate(generation = factor(gen))\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\npoke['generation'] = pd.Categorical(poke.gen)\n\n\n\n\n\n\nThis data has several categorical and continuous variables that should allow for a reasonable demonstration of a number of techniques for exploring data.\n\n17.4.1 Numerical Summary Statistics\n\n\nR: summary\nPython: describe\nR: skimr\npython: skimpy\n\n\n\nThe first, and most basic EDA command in R is summary().\nFor numeric variables, summary provides 5-number summaries plus the mean. For categorical variables, summary provides the length of the variable and the Class and Mode. For factors, summary provides a table of the most common values, as well as a catch-all “other” category.\n\n# Make types into factors to demonstrate the difference\npoke <- tidyr::separate(poke, type, into = c(\"type_1\", \"type_2\"), sep = \",\")\npoke$type_1 <- factor(poke$type_1)\npoke$type_2 <- factor(poke$type_2)\n\nsummary(poke)\n##       gen          pokedex_no       img_link             name          \n##  Min.   :1.000   Min.   :   1.0   Length:1526        Length:1526       \n##  1st Qu.:2.000   1st Qu.: 226.2   Class :character   Class :character  \n##  Median :4.000   Median : 484.0   Mode  :character   Mode  :character  \n##  Mean   :4.478   Mean   : 487.9                                        \n##  3rd Qu.:7.000   3rd Qu.: 726.8                                        \n##  Max.   :9.000   Max.   :1008.0                                        \n##                                                                        \n##    variant               type_1        type_2        total       \n##  Length:1526        Water   :179   Flying :157   Min.   : 175.0  \n##  Class :character   Normal  :156   Psychic: 61   1st Qu.: 345.8  \n##  Mode  :character   Psychic :123   Ghost  : 57   Median : 475.0  \n##                     Electric:119   Ground : 57   Mean   : 450.3  \n##                     Grass   :113   Steel  : 55   3rd Qu.: 525.0  \n##                     Bug     :107   (Other):466   Max.   :1125.0  \n##                     (Other) :729   NA's   :673                   \n##        hp             attack          defense         sp_attack     \n##  Min.   :  1.00   Min.   :  5.00   Min.   :  5.00   Min.   : 10.00  \n##  1st Qu.: 50.25   1st Qu.: 60.00   1st Qu.: 55.00   1st Qu.: 50.00  \n##  Median : 70.00   Median : 80.00   Median : 70.00   Median : 70.00  \n##  Mean   : 71.18   Mean   : 82.05   Mean   : 75.66   Mean   : 75.05  \n##  3rd Qu.: 85.00   3rd Qu.:100.00   3rd Qu.: 95.00   3rd Qu.: 98.00  \n##  Max.   :255.00   Max.   :190.00   Max.   :250.00   Max.   :194.00  \n##                                                                     \n##    sp_defense         speed         species             height_m     \n##  Min.   : 20.00   Min.   :  5.0   Length:1526        Min.   : 0.100  \n##  1st Qu.: 55.00   1st Qu.: 50.0   Class :character   1st Qu.: 0.500  \n##  Median : 70.00   Median : 70.0   Mode  :character   Median : 1.000  \n##  Mean   : 73.84   Mean   : 72.5                      Mean   : 1.233  \n##  3rd Qu.: 90.00   3rd Qu.: 95.0                      3rd Qu.: 1.500  \n##  Max.   :250.00   Max.   :200.0                      Max.   :20.000  \n##                                                                      \n##    weight_kg        generation \n##  Min.   :  0.10   1      :285  \n##  1st Qu.:  8.00   5      :237  \n##  Median : 29.25   3      :193  \n##  Mean   : 68.25   4      :178  \n##  3rd Qu.: 78.50   8      :134  \n##  Max.   :999.90   7      :133  \n##                   (Other):366\n\nOne common question in EDA is whether there are missing values or other inconsistencies that need to be handled. summary() provides you with the NA count for each variable, making it easy to identify what variables are likely to cause problems in an analysis. We can see in this summary that 673 pokemon don’t have a second type.\nWe also look for extreme values. There is at least one pokemon who appears to have a weight of 999.90 kg. Let’s investigate further:\n\npoke[poke$weight_kg > 999,] \n## # A tibble: 2 × 18\n##     gen pokedex…¹ img_l…² name  variant type_1 type_2 total    hp attack defense\n##   <dbl>     <dbl> <chr>   <chr> <chr>   <fct>  <fct>  <dbl> <dbl>  <dbl>   <dbl>\n## 1     7       790 https:… Cosm… NA      Psych… <NA>     400    43     29     131\n## 2     7       797 https:… Cele… NA      Steel  Flying   570    97    101     103\n## # … with 7 more variables: sp_attack <dbl>, sp_defense <dbl>, speed <dbl>,\n## #   species <chr>, height_m <dbl>, weight_kg <dbl>, generation <fct>, and\n## #   abbreviated variable names ¹​pokedex_no, ²​img_link\n# Show any rows where weight_kg is extreme\n\nThis is the last row of our data frame, and this pokemon appears to have many missing values.\n\n\nThe most basic EDA command in pandas is df.describe() (which operates on a DataFrame named df). Like summary() in R, describe() provides a 5-number summary for numeric variables. For categorical variables, describe() provides the number of unique values, the most common value, and the frequency of that common value.\n\n# Split types into two columns\npoke[['type_1', 'type_2']] = poke.type.str.split(\",\", expand = True)\n# Make each one categorical\npoke['type_1'] = pd.Categorical(poke.type_1)\npoke['type_2'] = pd.Categorical(poke.type_2)\n\npoke.iloc[:,:].describe() # describe only shows numeric variables by default\n\n# You can get categorical variables too if that's all you give it to show\n##                gen   pokedex_no  ...     height_m    weight_kg\n## count  1526.000000  1526.000000  ...  1526.000000  1526.000000\n## mean      4.477720   487.863041  ...     1.232962    68.249607\n## std       2.565182   290.328644  ...     1.289446   121.828015\n## min       1.000000     1.000000  ...     0.100000     0.100000\n## 25%       2.000000   226.250000  ...     0.500000     8.000000\n## 50%       4.000000   484.000000  ...     1.000000    29.250000\n## 75%       7.000000   726.750000  ...     1.500000    78.500000\n## max       9.000000  1008.000000  ...    20.000000   999.900000\n## \n## [8 rows x 11 columns]\npoke['type_1'].describe()\n## count      1526\n## unique       18\n## top       Water\n## freq        179\n## Name: type_1, dtype: object\npoke['type_2'].describe()\n## count        853\n## unique        18\n## top       Flying\n## freq         157\n## Name: type_2, dtype: object\n\n\n\nAn R package that is incredibly useful for this type of dataset exploration is skimr.\n\nlibrary(skimr)\nskim(poke)\n\n\nData summary\n\n\nName\npoke\n\n\nNumber of rows\n1526\n\n\nNumber of columns\n18\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nfactor\n3\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nimg_link\n0\n1\n59\n84\n0\n1192\n0\n\n\nname\n0\n1\n3\n12\n0\n1008\n0\n\n\nvariant\n0\n1\n2\n22\n0\n105\n0\n\n\nspecies\n0\n1\n11\n21\n0\n708\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ntype_1\n0\n1.00\nFALSE\n18\nWat: 179, Nor: 156, Psy: 123, Ele: 119\n\n\ntype_2\n673\n0.56\nFALSE\n18\nFly: 157, Psy: 61, Gho: 57, Gro: 57\n\n\ngeneration\n0\n1.00\nFALSE\n9\n1: 285, 5: 237, 3: 193, 4: 178\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ngen\n0\n1\n4.48\n2.57\n1.0\n2.00\n4.00\n7.00\n9.0\n▇▇▅▅▅\n\n\npokedex_no\n0\n1\n487.86\n290.33\n1.0\n226.25\n484.00\n726.75\n1008.0\n▇▆▇▇▆\n\n\ntotal\n0\n1\n450.29\n120.59\n175.0\n345.75\n475.00\n525.00\n1125.0\n▅▇▂▁▁\n\n\nhp\n0\n1\n71.18\n26.53\n1.0\n50.25\n70.00\n85.00\n255.0\n▃▇▁▁▁\n\n\nattack\n0\n1\n82.05\n32.41\n5.0\n60.00\n80.00\n100.00\n190.0\n▂▇▇▂▁\n\n\ndefense\n0\n1\n75.66\n30.21\n5.0\n55.00\n70.00\n95.00\n250.0\n▃▇▂▁▁\n\n\nsp_attack\n0\n1\n75.05\n33.88\n10.0\n50.00\n70.00\n98.00\n194.0\n▅▇▅▂▁\n\n\nsp_defense\n0\n1\n73.84\n27.72\n20.0\n55.00\n70.00\n90.00\n250.0\n▇▇▁▁▁\n\n\nspeed\n0\n1\n72.50\n30.74\n5.0\n50.00\n70.00\n95.00\n200.0\n▃▇▆▁▁\n\n\nheight_m\n0\n1\n1.23\n1.29\n0.1\n0.50\n1.00\n1.50\n20.0\n▇▁▁▁▁\n\n\nweight_kg\n0\n1\n68.25\n121.83\n0.1\n8.00\n29.25\n78.50\n999.9\n▇▁▁▁▁\n\n\n\n\n\nskim provides a beautiful table of summary statistics along with a sparklines-style histogram of values, giving you a sneak peek at the distribution.\n\n\nThere is a similar package to skimr in R called skimpy in Python.\n\nfrom skimpy import skim\nskim(poke)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 1526   │ │ int64       │ 9     │                       │\n## │ │ Number of columns │ 19     │ │ string      │ 5     │                       │\n## │ └───────────────────┴────────┘ │ category    │ 3     │                       │\n## │                                │ float64     │ 2     │                       │\n## │                                └─────────────┴───────┘                       │\n## │        Categories                                                            │\n## │ ┏━━━━━━━━━━━━━━━━━━━━━━━┓                                                    │\n## │ ┃ Categorical Variables ┃                                                    │\n## │ ┡━━━━━━━━━━━━━━━━━━━━━━━┩                                                    │\n## │ │ generation            │                                                    │\n## │ │ type_1                │                                                    │\n## │ │ type_2                │                                                    │\n## │ └───────────────────────┘                                                    │\n## │                                   number                                     │\n## │ ┏━━━━━━━━━━━━━━┳━━━━┳━━━━━━┳━━━━━━┳━━━━━┳━━━━━┳━━━━━┳━━━━━┳━━━━━━┳━━━━━━━━┓  │\n## │ ┃ column_name  ┃ NA ┃ NA % ┃ mean ┃ sd  ┃ p0  ┃ p25 ┃ p75 ┃ p100 ┃ hist   ┃  │\n## │ ┡━━━━━━━━━━━━━━╇━━━━╇━━━━━━╇━━━━━━╇━━━━━╇━━━━━╇━━━━━╇━━━━━╇━━━━━━╇━━━━━━━━┩  │\n## │ │ gen          │  0 │    0 │  4.5 │ 2.6 │   1 │   2 │   7 │    9 │ █▄▃▇▃▅ │  │\n## │ │ pokedex_no   │  0 │    0 │  490 │ 290 │   1 │ 230 │ 730 │ 1000 │ █▅▇▇▆▆ │  │\n## │ │ total        │  0 │    0 │  450 │ 120 │ 180 │ 350 │ 520 │ 1100 │  ▅█▇▁  │  │\n## │ │ hp           │  0 │    0 │   71 │  27 │   1 │  50 │  85 │  260 │  ▁█▃   │  │\n## │ │ attack       │  0 │    0 │   82 │  32 │   5 │  60 │ 100 │  190 │ ▂▇█▆▂▁ │  │\n## │ │ defense      │  0 │    0 │   76 │  30 │   5 │  55 │  95 │  250 │  ▂█▄▁  │  │\n## │ │ sp_attack    │  0 │    0 │   75 │  34 │  10 │  50 │  98 │  190 │ ▄█▆▃▁  │  │\n## │ │ sp_defense   │  0 │    0 │   74 │  28 │  20 │  55 │  90 │  250 │  ▅█▃   │  │\n## │ │ speed        │  0 │    0 │   73 │  31 │   5 │  50 │  95 │  200 │ ▃▆█▂▁  │  │\n## │ │ height_m     │  0 │    0 │  1.2 │ 1.3 │ 0.1 │ 0.5 │ 1.5 │   20 │   █    │  │\n## │ │ weight_kg    │  0 │    0 │   68 │ 120 │ 0.1 │   8 │  78 │ 1000 │   █▁   │  │\n## │ └──────────────┴────┴──────┴──────┴─────┴─────┴─────┴─────┴──────┴────────┘  │\n## │                                  category                                    │\n## │ ┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓  │\n## │ ┃ column_name          ┃ NA      ┃ NA %     ┃ ordered       ┃ unique      ┃  │\n## │ ┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩  │\n## │ │ generation           │       0 │        0 │ False         │           9 │  │\n## │ │ type_1               │       0 │        0 │ False         │          18 │  │\n## │ │ type_2               │     670 │       44 │ False         │          19 │  │\n## │ └──────────────────────┴─────────┴──────────┴───────────────┴─────────────┘  │\n## │                                   string                                     │\n## │ ┏━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓  │\n## │ ┃ column_name     ┃ NA     ┃ NA %   ┃ words per row      ┃ total words    ┃  │\n## │ ┡━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩  │\n## │ │ img_link        │      0 │      0 │                  1 │           1500 │  │\n## │ │ name            │      0 │      0 │                  1 │           1500 │  │\n## │ │ variant         │   1100 │     70 │                  1 │           1500 │  │\n## │ │ type            │      0 │      0 │                  1 │           1500 │  │\n## │ │ species         │      0 │      0 │                  1 │           1500 │  │\n## │ └─────────────────┴────────┴────────┴────────────────────┴────────────────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\n\n\n\n\n17.4.2 Assessing Distributions\nWe are often also interested in the distribution of values.\n\n17.4.2.1 Categorical Variables\nOne useful way to assess the distribution of values is to generate a cross-tabular view of the data. This is mostly important for variables with a relatively low number of categories - otherwise, it is usually easier to use a graphical summary method.\nTabular Summaries\n\n\nR\nPython\n\n\n\nWe can generate cross-tabs for variables that we know are discrete (such as generation, which will always be a whole number). We can even generate cross-tabular views for a combination of two variables (or theoretically more, but this gets hard to read and track).\n\ntable(poke$generation)\n## \n##   1   2   3   4   5   6   7   8   9 \n## 285 124 193 178 237 119 133 134 123\n\ntable(poke$type_1, poke$type_2)\n##           \n##            Bug Dark Dragon Electric Fairy Fighting Fire Flying Ghost Grass\n##   Bug        0    1      0        4     2        5    2     14     1     8\n##   Dark       0    0      4        0     3        2    4      8     2     2\n##   Dragon     0    1      0        1     1        2    1      6     3     0\n##   Electric   0    4      3        0     2        2    6     19     6    10\n##   Fairy      0    0      0        0     0        1    0      6     0     0\n##   Fighting   0    3      1        1     0        0    4      3     2     0\n##   Fire       2    1      2        0     0        7    0     11     7     0\n##   Flying     0    1      2        0     0        1    0      0     0     0\n##   Ghost      0    1      4        0     3        0    3      6     0    11\n##   Grass      0    5      6        0     5        7    1      8     4     0\n##   Ground     0    3      2        2     0        1    1      6     5     2\n##   Ice        2    0      0        0     2        0    4      3     1     0\n##   Normal     0    0      1        0     5        5    0     33     4     8\n##   Poison     1    7      4        0     2        3    2      3     0     0\n##   Psychic    0    2      3        0    11        3    1     14     9     4\n##   Rock       2    2      2        7     3        1    2      8     0     2\n##   Steel      0    0      9        0     4        1    0      2     7     0\n##   Water      2    9      6        2     4        6    0      7     6     3\n##           \n##            Ground Ice Normal Poison Psychic Rock Steel Water\n##   Bug           4   0      0     12       3    4    13     3\n##   Dark          1   4      9      3       2    0     3     0\n##   Dragon       13  12      1      0       4    0     0     9\n##   Electric      1   7      2      5       2    0     4     6\n##   Fairy         0   0      0      0       1    0     5     0\n##   Fighting      0   1      0      2       3    0     4     6\n##   Fire          3   0      2      1       6    5     1     1\n##   Flying        0   0      0      0       0    0     1     3\n##   Ghost         2   0      0      4       0    0     0     0\n##   Grass         1   3      3     15       3    0     3     0\n##   Ground        0   0      1      0       2    3     8     0\n##   Ice           3   0      0      0       5    2     4     4\n##   Normal        1   0      0      0       6    0     0     1\n##   Poison        5   0      2      0       4    0     0     3\n##   Psychic       0   3      4      0       0    0     4     0\n##   Rock          9   2      0      3       2    0     4     6\n##   Steel         2   0      0      2       7    3     0     0\n##   Water        12   4      0      4      11    6     1     0\n\n\n\n\nimport numpy as np\n# For only one factor, use .groupby('colname')['colname'].count()\npoke.groupby(['generation'])['generation'].count()\n\n# for two or more factors, use pd.crosstab\n## generation\n## 1    285\n## 2    124\n## 3    193\n## 4    178\n## 5    237\n## 6    119\n## 7    133\n## 8    134\n## 9    123\n## Name: generation, dtype: int64\npd.crosstab(index = poke['type_1'], columns = poke['type_2'])\n## type_2    Bug  Dark  Dragon  Electric  ...  Psychic  Rock  Steel  Water\n## type_1                                 ...                             \n## Bug         0     1       0         4  ...        3     4     13      3\n## Dark        0     0       4         0  ...        2     0      3      0\n## Dragon      0     1       0         1  ...        4     0      0      9\n## Electric    0     4       3         0  ...        2     0      4      6\n## Fairy       0     0       0         0  ...        1     0      5      0\n## Fighting    0     3       1         1  ...        3     0      4      6\n## Fire        2     1       2         0  ...        6     5      1      1\n## Flying      0     1       2         0  ...        0     0      1      3\n## Ghost       0     1       4         0  ...        0     0      0      0\n## Grass       0     5       6         0  ...        3     0      3      0\n## Ground      0     3       2         2  ...        2     3      8      0\n## Ice         2     0       0         0  ...        5     2      4      4\n## Normal      0     0       1         0  ...        6     0      0      1\n## Poison      1     7       4         0  ...        4     0      0      3\n## Psychic     0     2       3         0  ...        0     0      4      0\n## Rock        2     2       2         7  ...        2     0      4      6\n## Steel       0     0       9         0  ...        7     3      0      0\n## Water       2     9       6         2  ...       11     6      1      0\n## \n## [18 rows x 18 columns]\n\n\n\n\nFrequency Plots\n\n\nBase R\nR: ggplot2\nPython: matplotlib\nPython: plotnine\n\n\n\n\nplot(table(poke$generation)) # bar plot\n\n\n\n\n\n\nWe generate a bar chart using geom_bar. It helps to tell R that generation (despite appearances) is categorical by declaring it a factor variable. This ensures that we get a break on the x-axis at each generation.\n\nlibrary(ggplot2)\n\nggplot(poke, aes(x = factor(generation))) +\n  geom_bar() +\n  xlab(\"Generation\") + ylab(\"# Pokemon\")\n\n\n\n\n\n\nWe generate a bar chart using the contingency table we generated earlier combined with matplotlib’s plt.bar().\n\nimport matplotlib.pyplot as plt\n\ntab = poke.groupby(['generation'])['generation'].count()\n\nplt.bar(tab.keys(), tab.values, color = 'grey')\n## <BarContainer object of 9 artists>\nplt.xlabel(\"Generation\")\nplt.ylabel(\"# Pokemon\")\nplt.show()\n\n\n\n\n\n\nPlotnine is a ggplot2 clone for python, and for the most part, the code looks almost exactly the same, minus a few python-specific tweaks to account for different syntax conventions in each language.\nWe generate a bar chart using geom_bar. It helps to tell R that generation (despite appearances) is categorical by declaring it a factor variable. This ensures that we get a break on the x-axis at each generation.\n\nfrom plotnine import *\n\n(ggplot(mapping = aes(x = \"factor(generation)\"), data = poke) +\n  geom_bar() +\n  xlab(\"Generation\") + ylab(\"# Pokemon\"))\n## <ggplot: (8779212617725)>\n\n\n\n\n\n\n\n\n17.4.2.2 Quantitative Variables\nWe covered some numerical summary statistics in the numerical summary statistic section above. In this section, we will primarily focus on visualization methods for assessing the distribution of quantitative variables.\n\n\n\n\n\n\nNote: R pipe\n\n\n\nThe code in this section uses the R pipe, %>%. The left side of the pipe is passed as an argument to the right side. This makes code easier to read because it becomes a step-wise “recipe” instead of a nested mess of functions and parentheses.\n\n\nIn each step, the left hand side of the pipe is put into the first argument of the function. Source: Arthur Welle (Github)\n\n\n\n\nWe can generate histograms2 or kernel density plots (a continuous version of the histogram) to show us the distribution of a continuous variable.\n\n\nBase R\nPython: matplotlib\nR: ggplot2\nPython: plotnine\n\n\n\nBy default, R uses ranges of \\((a, b]\\) in histograms, so we specify which breaks will give us a desirable result. If we do not specify breaks, R will pick them for us.\n\nhist(poke$hp)\n\n\n\n\nFor continuous variables, we can use histograms, or we can examine kernel density plots.\n\nlibrary(magrittr) # This provides the pipe command, %>%\n\nhist(poke$weight_kg)\n\npoke$weight_kg %>%\n  log10() %>% # Take the log - will transformation be useful w/ modeling?\n  hist(main = \"Histogram of Log10 Weight (Kg)\") # create a histogram\n\npoke$weight_kg %>%\n  density(na.rm = T) %>% # First, we compute the kernel density\n  # (na.rm = T says to ignore NA values)\n  plot(main = \"Density of Weight (Kg)\") # Then, we plot the result\n\n\npoke$weight_kg %>%\n  log10() %>% # Transform the variable\n  density(na.rm = T) %>% # Compute the density ignoring missing values\n  plot(main = \"Density of Log10 pokemon weight in Kg\") # Plot the result,\n    # changing the title of the plot to a meaningful value\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\n\nHistogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Create a 2x2 grid of plots with separate axes\n# This uses python multi-assignment to assign figures, axes\n# variables all in one go\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n\npoke.weight_kg.plot.hist(ax = ax1) # first plot\nax1.set_title(\"Histogram of Weight (kg)\")\n\n\nnp.log10(poke.weight_kg).plot.hist(ax = ax2)\nax2.set_title(\"Histogram of Log10 Weight (kg)\")\n\npoke.weight_kg.plot.density(ax = ax3)\nax3.set_title(\"Density of Weight (kg)\")\n\nnp.log10(poke.weight_kg).plot.density(ax = ax4)\nax4.set_title(\"Density of Log10 Weight (kg)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\nHistogram and density plots of weight and log10 weight of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\n\n\n\nlibrary(ggplot2)\nggplot(poke, aes(x = height_m)) +\n  geom_histogram(bins = 30)\nggplot(poke, aes(x = height_m)) +\n  geom_histogram(bins = 30) +\n  scale_x_log10()\nggplot(poke, aes(x = height_m)) +\n  geom_density()\nggplot(poke, aes(x = height_m)) +\n  geom_density() +\n  scale_x_log10()\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\n\nHistogram and density plots of height and log10 height of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\nNotice that in ggplot2/plotnine, we transform the axes instead of the data. This means that the units on the axis are true to the original, unlike in base R and matplotlib.\n\n\n\nggplot(poke, aes(x = 'height_m')) + geom_histogram(bins = 30)\n## <ggplot: (8779212572249)>\n(ggplot(poke, aes(x = 'height_m')) +\n  geom_histogram(bins = 30) +\n  scale_x_log10())\n## <ggplot: (8779209653879)>\n(ggplot(poke, aes(x = 'height_m')) +\n  geom_density())\n## <ggplot: (8779209690695)>\n(ggplot(poke, aes(x = 'height_m')) +\n  geom_density() +\n  scale_x_log10())\n## <ggplot: (8779215468686)>\n\n\n\n\n\nHistogram of Pokemon Height (m)\n\n\n\n\n\n\nHistogram of Pokemon Height (m, log 10)\n\n\n\n\n\n\n\n\nDensity of Pokemon Height (m)\n\n\n\n\n\n\nDensity of Pokemon Height (m, log 10)\n\n\n\n\n\n\nHistogram and density plots of height and log10 height of different pokemon. The untransformed data are highly skewed, the transformed data are significantly less skewed.\n\n\n\n\nNotice that in ggplot2/plotnine, we transform the axes instead of the data. This means that the units on the axis are true to the original, unlike in base R and matplotlib.\n\n\n\n\n17.4.3 Relationships Between Variables\n\n17.4.3.1 Categorical - Categorical Relationships\n\n\nR: ggplot2\nBase R\nPython: matplotlib\nPython: plotnine\n\n\n\nWe can generate a (simple) mosaic plot (the equivalent of a 2-dimensional cross-tabular view) using geom_bar with position = 'fill', which scales each bar so that it ends at 1. I’ve flipped the axes using coord_flip so that you can read the labels more easily.\n\nlibrary(ggplot2)\n\nggplot(poke, aes(x = factor(type_1), fill = factor(type_2))) +\n  geom_bar(color = \"black\", position = \"fill\") +\n  xlab(\"Type 1\") + ylab(\"Proportion of Pokemon w/ Type 2\") +\n  coord_flip()\n\n\n\n\nAnother way to look at this data is to bin it in x and y and shade the resulting bins by the number of data points in each bin. We can even add in labels so that this is at least as clear as the tabular view!\n\nggplot(poke, aes(x = factor(type_1), y = factor(type_2))) +\n  # Shade tiles according to the number of things in the bin\n  geom_tile(aes(fill = after_stat(count)), stat = \"bin2d\") +\n  # Add the number of things in the bin to the top of the tile as text\n  geom_text(aes(label = after_stat(count)), stat = 'bin2d') +\n  # Scale the tile fill\n  scale_fill_gradient2(limits = c(0, 100), low = \"white\", high = \"blue\", na.value = \"white\") + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n\n\n\n\n\n\nBase R mosaic plots aren’t nearly as pretty as the ggplot version, but I will at least show you how to create them.\n\nplot(table(poke$type_1, poke$type_2)) \n\n\n\n# mosaic plot - hard to read b/c too many categories\n\n\n\nTo get a mosaicplot, we need an additional library, called statsmodels, which we install with pip install statsmodels in the terminal.\n\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(poke, ['type_1', 'type_2'], title = \"Pokemon Types\")\n## (<Figure size 700x500 with 3 Axes>, {('Grass', 'Poison'): (0.0, 0.0, 0.06915143624291603, 0.22180599369085174), ('Grass', 'Flying'): (0.0, 0.22496056782334384, 0.06915143624291603, 0.11829652996845426), ('Grass', 'Dragon'): (0.0, 0.34641167192429023, 0.06915143624291603, 0.0887223974763407), ('Grass', 'Normal'): (0.0, 0.438288643533123, 0.06915143624291603, 0.04436119873817035), ('Grass', 'Psychic'): (0.0, 0.48580441640378547, 0.06915143624291603, 0.04436119873817035), ('Grass', 'Steel'): (0.0, 0.533320189274448, 0.06915143624291603, 0.04436119873817035), ('Grass', 'Ground'): (0.0, 0.5808359621451105, 0.06915143624291603, 0.014787066246056782), ('Grass', 'Fairy'): (0.0, 0.5987776025236593, 0.06915143624291603, 0.07393533123028391), ('Grass', 'Grass'): (0.0, 0.6758675078864353, 0.06915143624291603, 0.0), ('Grass', 'Rock'): (0.0, 0.6790220820189274, 0.06915143624291603, 0.0), ('Grass', 'Fighting'): (0.0, 0.6821766561514195, 0.06915143624291603, 0.10350946372239749), ('Grass', 'Electric'): (0.0, 0.7888406940063091, 0.06915143624291603, 0.0), ('Grass', 'Ice'): (0.0, 0.7919952681388013, 0.06915143624291603, 0.04436119873817035), ('Grass', 'Dark'): (0.0, 0.8395110410094637, 0.06915143624291603, 0.07393533123028391), ('Grass', 'Ghost'): (0.0, 0.9166009463722398, 0.06915143624291603, 0.05914826498422713), ('Grass', 'Fire'): (0.0, 0.9789037854889591, 0.06915143624291603, 0.014787066246056782), ('Grass', 'Water'): (0.0, 0.9968454258675078, 0.06915143624291603, 0.0), ('Grass', 'Bug'): (0.0, 1.0, 0.06915143624291603, 0.0), ('Fire', 'Poison'): (0.0737597311737916, 0.0, 0.05294406837348259, 0.019313719178523144), ('Fire', 'Flying'): (0.0737597311737916, 0.02246829331101526, 0.05294406837348259, 0.2124509109637546), ('Fire', 'Dragon'): (0.0737597311737916, 0.23807377840726196, 0.05294406837348259, 0.03862743835704628), ('Fire', 'Normal'): (0.0737597311737916, 0.27985579089680035, 0.05294406837348259, 0.03862743835704628), ('Fire', 'Psychic'): (0.0737597311737916, 0.32163780338633874, 0.05294406837348259, 0.11588231507113889), ('Fire', 'Steel'): (0.0737597311737916, 0.4406746925899698, 0.05294406837348259, 0.019313719178523165), ('Fire', 'Ground'): (0.0737597311737916, 0.46314298590098507, 0.05294406837348259, 0.05794115753556944), ('Fire', 'Fairy'): (0.0737597311737916, 0.5242387175690466, 0.05294406837348259, 0.0), ('Fire', 'Grass'): (0.0737597311737916, 0.5273932917015387, 0.05294406837348259, 0.0), ('Fire', 'Rock'): (0.0737597311737916, 0.5305478658340308, 0.05294406837348259, 0.09656859589261572), ('Fire', 'Fighting'): (0.0737597311737916, 0.6302710358591387, 0.05294406837348259, 0.13519603424966195), ('Fire', 'Electric'): (0.0737597311737916, 0.7686216442412926, 0.05294406837348259, 0.0), ('Fire', 'Ice'): (0.0737597311737916, 0.7717762183737849, 0.05294406837348259, 0.0), ('Fire', 'Dark'): (0.0737597311737916, 0.7749307925062769, 0.05294406837348259, 0.019313719178523165), ('Fire', 'Ghost'): (0.0737597311737916, 0.7973990858172922, 0.05294406837348259, 0.13519603424966206), ('Fire', 'Fire'): (0.0737597311737916, 0.9357496941994464, 0.05294406837348259, 0.0), ('Fire', 'Water'): (0.0737597311737916, 0.9389042683319385, 0.05294406837348259, 0.01931371917852306), ('Fire', 'Bug'): (0.0737597311737916, 0.9613725616429537, 0.05294406837348259, 0.03862743835704633), ('Bug', 'Poison'): (0.1313120944781498, 0.0, 0.08211733053846279, 0.14942719574962643), ('Bug', 'Flying'): (0.1313120944781498, 0.15258176988211852, 0.08211733053846279, 0.1743317283745642), ('Bug', 'Dragon'): (0.1313120944781498, 0.33006807238917485, 0.08211733053846279, 0.0), ('Bug', 'Normal'): (0.1313120944781498, 0.33322264652166694, 0.08211733053846279, 0.0), ('Bug', 'Psychic'): (0.1313120944781498, 0.33637722065415904, 0.08211733053846279, 0.03735679893740662), ('Bug', 'Steel'): (0.1313120944781498, 0.3768885937240578, 0.08211733053846279, 0.16187946206209533), ('Bug', 'Ground'): (0.1313120944781498, 0.5419226299186453, 0.08211733053846279, 0.049809065249875425), ('Bug', 'Fairy'): (0.1313120944781498, 0.5948862693010127, 0.08211733053846279, 0.024904532624937713), ('Bug', 'Grass'): (0.1313120944781498, 0.6229453760584426, 0.08211733053846279, 0.09961813049975095), ('Bug', 'Rock'): (0.1313120944781498, 0.7257180806906857, 0.08211733053846279, 0.04980906524987553), ('Bug', 'Fighting'): (0.1313120944781498, 0.7786817200730534, 0.08211733053846279, 0.062261331562344334), ('Bug', 'Electric'): (0.1313120944781498, 0.8440976257678897, 0.08211733053846279, 0.04980906524987553), ('Bug', 'Ice'): (0.1313120944781498, 0.8970612651502574, 0.08211733053846279, 0.0), ('Bug', 'Dark'): (0.1313120944781498, 0.9002158392827495, 0.08211733053846279, 0.012452266312468803), ('Bug', 'Ghost'): (0.1313120944781498, 0.9158226797277104, 0.08211733053846279, 0.012452266312468908), ('Bug', 'Fire'): (0.1313120944781498, 0.9314295201726716, 0.08211733053846279, 0.024904532624937713), ('Bug', 'Water'): (0.1313120944781498, 0.9594886269301012, 0.08211733053846279, 0.03735679893740662), ('Bug', 'Bug'): (0.1313120944781498, 1.0, 0.08211733053846279, 0.0), ('Normal', 'Poison'): (0.21803771994748813, 0.0, 0.06915143624291603, 0.0), ('Normal', 'Flying'): (0.21803771994748813, 0.003154574132492114, 0.06915143624291603, 0.48797318611987384), ('Normal', 'Dragon'): (0.21803771994748813, 0.4942823343848581, 0.06915143624291603, 0.014787066246056782), ('Normal', 'Normal'): (0.21803771994748813, 0.512223974763407, 0.06915143624291603, 0.0), ('Normal', 'Psychic'): (0.21803771994748813, 0.5153785488958991, 0.06915143624291603, 0.0887223974763407), ('Normal', 'Steel'): (0.21803771994748813, 0.607255520504732, 0.06915143624291603, 0.0), ('Normal', 'Ground'): (0.21803771994748813, 0.610410094637224, 0.06915143624291603, 0.014787066246056782), ('Normal', 'Fairy'): (0.21803771994748813, 0.6283517350157729, 0.06915143624291603, 0.07393533123028391), ('Normal', 'Grass'): (0.21803771994748813, 0.7054416403785488, 0.06915143624291603, 0.11829652996845426), ('Normal', 'Rock'): (0.21803771994748813, 0.8268927444794953, 0.06915143624291603, 0.0), ('Normal', 'Fighting'): (0.21803771994748813, 0.8300473186119874, 0.06915143624291603, 0.07393533123028391), ('Normal', 'Electric'): (0.21803771994748813, 0.9071372239747634, 0.06915143624291603, 0.0), ('Normal', 'Ice'): (0.21803771994748813, 0.9102917981072556, 0.06915143624291603, 0.0), ('Normal', 'Dark'): (0.21803771994748813, 0.9134463722397477, 0.06915143624291603, 0.0), ('Normal', 'Ghost'): (0.21803771994748813, 0.9166009463722398, 0.06915143624291603, 0.05914826498422713), ('Normal', 'Fire'): (0.21803771994748813, 0.9789037854889591, 0.06915143624291603, 0.0), ('Normal', 'Water'): (0.21803771994748813, 0.982058359621451, 0.06915143624291603, 0.014787066246056782), ('Normal', 'Bug'): (0.21803771994748813, 1.0, 0.06915143624291603, 0.0), ('Dark', 'Poison'): (0.29179745112127975, 0.0, 0.05078308599089144, 0.060406738707295786), ('Dark', 'Flying'): (0.29179745112127975, 0.0635613128397879, 0.05078308599089144, 0.1610846365527888), ('Dark', 'Dragon'): (0.29179745112127975, 0.2278005235250688, 0.05078308599089144, 0.0805423182763944), ('Dark', 'Normal'): (0.29179745112127975, 0.31149741593395536, 0.05078308599089144, 0.18122021612188732), ('Dark', 'Psychic'): (0.29179745112127975, 0.4958722061883347, 0.05078308599089144, 0.04027115913819725), ('Dark', 'Steel'): (0.29179745112127975, 0.5392979394590242, 0.05078308599089144, 0.06040673870729577), ('Dark', 'Ground'): (0.29179745112127975, 0.602859252298812, 0.05078308599089144, 0.020135579569098627), ('Dark', 'Fairy'): (0.29179745112127975, 0.6261494060004027, 0.05078308599089144, 0.06040673870729577), ('Dark', 'Grass'): (0.29179745112127975, 0.6897107188401906, 0.05078308599089144, 0.04027115913819715), ('Dark', 'Rock'): (0.29179745112127975, 0.73313645211088, 0.05078308599089144, 0.0), ('Dark', 'Fighting'): (0.29179745112127975, 0.7362910262433721, 0.05078308599089144, 0.04027115913819725), ('Dark', 'Electric'): (0.29179745112127975, 0.7797167595140614, 0.05078308599089144, 0.0), ('Dark', 'Ice'): (0.29179745112127975, 0.7828713336465536, 0.05078308599089144, 0.0805423182763944), ('Dark', 'Dark'): (0.29179745112127975, 0.86656822605544, 0.05078308599089144, 0.0), ('Dark', 'Ghost'): (0.29179745112127975, 0.8697228001879321, 0.05078308599089144, 0.04027115913819715), ('Dark', 'Fire'): (0.29179745112127975, 0.9131485334586215, 0.05078308599089144, 0.0805423182763944), ('Dark', 'Water'): (0.29179745112127975, 0.9968454258675078, 0.05078308599089144, 0.0), ('Dark', 'Bug'): (0.29179745112127975, 1.0, 0.05078308599089144, 0.0), ('Electric', 'Poison'): (0.3471888320430468, 0.0, 0.08535880411234952, 0.05989697719921735), ('Electric', 'Flying'): (0.3471888320430468, 0.06305155133170946, 0.08535880411234952, 0.22760851335702592), ('Electric', 'Dragon'): (0.3471888320430468, 0.2938146388212275, 0.08535880411234952, 0.03593818631953039), ('Electric', 'Normal'): (0.3471888320430468, 0.33290739927325, 0.08535880411234952, 0.023958790879686963), ('Electric', 'Psychic'): (0.3471888320430468, 0.36002076428542906, 0.08535880411234952, 0.02395879087968691), ('Electric', 'Steel'): (0.3471888320430468, 0.3871341292976081, 0.08535880411234952, 0.04791758175937387), ('Electric', 'Ground'): (0.3471888320430468, 0.43820628518947413, 0.08535880411234952, 0.011979395439843481), ('Electric', 'Fairy'): (0.3471888320430468, 0.45334025476180967, 0.08535880411234952, 0.023958790879686963), ('Electric', 'Grass'): (0.3471888320430468, 0.4804536197739887, 0.08535880411234952, 0.1197939543984347), ('Electric', 'Rock'): (0.3471888320430468, 0.6034021483049156, 0.08535880411234952, 0.0), ('Electric', 'Fighting'): (0.3471888320430468, 0.6065567224374077, 0.08535880411234952, 0.023958790879686963), ('Electric', 'Electric'): (0.3471888320430468, 0.6336700874495867, 0.08535880411234952, 0.0), ('Electric', 'Ice'): (0.3471888320430468, 0.636824661582079, 0.08535880411234952, 0.08385576807890421), ('Electric', 'Dark'): (0.3471888320430468, 0.7238350037934752, 0.08535880411234952, 0.047917581759373926), ('Electric', 'Ghost'): (0.3471888320430468, 0.7749071596853412, 0.08535880411234952, 0.07187637263906078), ('Electric', 'Fire'): (0.3471888320430468, 0.8499381064568942, 0.08535880411234952, 0.07187637263906088), ('Electric', 'Water'): (0.3471888320430468, 0.9249690532284471, 0.08535880411234952, 0.07187637263906078), ('Electric', 'Bug'): (0.3471888320430468, 1.0, 0.08535880411234952, 0.0), ('Ice', 'Poison'): (0.4371559310862718, 0.0, 0.03241473573886688, 0.0), ('Ice', 'Flying'): (0.4371559310862718, 0.003154574132492114, 0.03241473573886688, 0.09463722397476342), ('Ice', 'Dragon'): (0.4371559310862718, 0.10094637223974764, 0.03241473573886688, 0.0), ('Ice', 'Normal'): (0.4371559310862718, 0.10410094637223975, 0.03241473573886688, 0.0), ('Ice', 'Psychic'): (0.4371559310862718, 0.10725552050473187, 0.03241473573886688, 0.15772870662460567), ('Ice', 'Steel'): (0.4371559310862718, 0.26813880126182965, 0.03241473573886688, 0.12618296529968456), ('Ice', 'Ground'): (0.4371559310862718, 0.39747634069400634, 0.03241473573886688, 0.09463722397476339), ('Ice', 'Fairy'): (0.4371559310862718, 0.4952681388012618, 0.03241473573886688, 0.06309148264984225), ('Ice', 'Grass'): (0.4371559310862718, 0.5615141955835962, 0.03241473573886688, 0.0), ('Ice', 'Rock'): (0.4371559310862718, 0.5646687697160884, 0.03241473573886688, 0.06309148264984225), ('Ice', 'Fighting'): (0.4371559310862718, 0.6309148264984227, 0.03241473573886688, 0.0), ('Ice', 'Electric'): (0.4371559310862718, 0.6340694006309148, 0.03241473573886688, 0.0), ('Ice', 'Ice'): (0.4371559310862718, 0.637223974763407, 0.03241473573886688, 0.0), ('Ice', 'Dark'): (0.4371559310862718, 0.6403785488958991, 0.03241473573886688, 0.0), ('Ice', 'Ghost'): (0.4371559310862718, 0.6435331230283912, 0.03241473573886688, 0.03154574132492113), ('Ice', 'Fire'): (0.4371559310862718, 0.6782334384858044, 0.03241473573886688, 0.12618296529968462), ('Ice', 'Water'): (0.4371559310862718, 0.8075709779179812, 0.03241473573886688, 0.1261829652996845), ('Ice', 'Bug'): (0.4371559310862718, 0.9369085173501578, 0.03241473573886688, 0.06309148264984225), ('Poison', 'Poison'): (0.4741789617560143, 0.0, 0.03889768288664027, 0.0), ('Poison', 'Flying'): (0.4741789617560143, 0.003154574132492114, 0.03889768288664027, 0.07886435331230283), ('Poison', 'Dragon'): (0.4741789617560143, 0.08517350157728706, 0.03889768288664027, 0.10515247108307046), ('Poison', 'Normal'): (0.4741789617560143, 0.19348054679284965, 0.03889768288664027, 0.052576235541535225), ('Poison', 'Psychic'): (0.4741789617560143, 0.24921135646687695, 0.03889768288664027, 0.10515247108307045), ('Poison', 'Steel'): (0.4741789617560143, 0.35751840168243953, 0.03889768288664027, 0.0), ('Poison', 'Ground'): (0.4741789617560143, 0.3606729758149317, 0.03889768288664027, 0.13144058885383808), ('Poison', 'Fairy'): (0.4741789617560143, 0.4952681388012618, 0.03889768288664027, 0.05257623554153525), ('Poison', 'Grass'): (0.4741789617560143, 0.5509989484752892, 0.03889768288664027, 0.0), ('Poison', 'Rock'): (0.4741789617560143, 0.5541535226077814, 0.03889768288664027, 0.0), ('Poison', 'Fighting'): (0.4741789617560143, 0.5573080967402734, 0.03889768288664027, 0.07886435331230277), ('Poison', 'Electric'): (0.4741789617560143, 0.6393270241850683, 0.03889768288664027, 0.0), ('Poison', 'Ice'): (0.4741789617560143, 0.6424815983175605, 0.03889768288664027, 0.0), ('Poison', 'Dark'): (0.4741789617560143, 0.6456361724500526, 0.03889768288664027, 0.18401682439537337), ('Poison', 'Ghost'): (0.4741789617560143, 0.832807570977918, 0.03889768288664027, 0.0), ('Poison', 'Fire'): (0.4741789617560143, 0.8359621451104102, 0.03889768288664027, 0.05257623554153515), ('Poison', 'Water'): (0.4741789617560143, 0.8916929547844374, 0.03889768288664027, 0.07886435331230288), ('Poison', 'Bug'): (0.4741789617560143, 0.9737118822292324, 0.03889768288664027, 0.026288117770767627), ('Ground', 'Poison'): (0.5176849395735302, 0.0, 0.03889768288664027, 0.0), ('Ground', 'Flying'): (0.5176849395735302, 0.003154574132492114, 0.03889768288664027, 0.15772870662460567), ('Ground', 'Dragon'): (0.5176849395735302, 0.16403785488958988, 0.03889768288664027, 0.052576235541535225), ('Ground', 'Normal'): (0.5176849395735302, 0.21976866456361724, 0.03889768288664027, 0.026288117770767627), ('Ground', 'Psychic'): (0.5176849395735302, 0.24921135646687695, 0.03889768288664027, 0.05257623554153525), ('Ground', 'Steel'): (0.5176849395735302, 0.30494216614090436, 0.03889768288664027, 0.2103049421661409), ('Ground', 'Ground'): (0.5176849395735302, 0.5184016824395373, 0.03889768288664027, 0.0), ('Ground', 'Fairy'): (0.5176849395735302, 0.5215562565720294, 0.03889768288664027, 0.0), ('Ground', 'Grass'): (0.5176849395735302, 0.5247108307045215, 0.03889768288664027, 0.05257623554153525), ('Ground', 'Rock'): (0.5176849395735302, 0.580441640378549, 0.03889768288664027, 0.07886435331230277), ('Ground', 'Fighting'): (0.5176849395735302, 0.6624605678233438, 0.03889768288664027, 0.026288117770767627), ('Ground', 'Electric'): (0.5176849395735302, 0.6919032597266035, 0.03889768288664027, 0.05257623554153525), ('Ground', 'Ice'): (0.5176849395735302, 0.7476340694006309, 0.03889768288664027, 0.0), ('Ground', 'Dark'): (0.5176849395735302, 0.750788643533123, 0.03889768288664027, 0.07886435331230288), ('Ground', 'Ghost'): (0.5176849395735302, 0.832807570977918, 0.03889768288664027, 0.13144058885383803), ('Ground', 'Fire'): (0.5176849395735302, 0.9674027339642481, 0.03889768288664027, 0.026288117770767627), ('Ground', 'Water'): (0.5176849395735302, 0.9968454258675078, 0.03889768288664027, 0.0), ('Ground', 'Bug'): (0.5176849395735302, 1.0, 0.03889768288664027, 0.0), ('Water', 'Poison'): (0.561190917391046, 0.0, 0.08968076887753176, 0.045608300710729355), ('Water', 'Flying'): (0.561190917391046, 0.04876287484322147, 0.08968076887753176, 0.07981452624377637), ('Water', 'Dragon'): (0.561190917391046, 0.13173197521948996, 0.08968076887753176, 0.06841245106609402), ('Water', 'Normal'): (0.561190917391046, 0.2032990004180761, 0.08968076887753176, 0.0), ('Water', 'Psychic'): (0.561190917391046, 0.2064535745505682, 0.08968076887753176, 0.1254228269545057), ('Water', 'Steel'): (0.561190917391046, 0.335030975637566, 0.08968076887753176, 0.011402075177682375), ('Water', 'Ground'): (0.561190917391046, 0.34958762494774054, 0.08968076887753176, 0.13682490213218804), ('Water', 'Fairy'): (0.561190917391046, 0.4895671012124207, 0.08968076887753176, 0.04560830071072934), ('Water', 'Grass'): (0.561190917391046, 0.538329976055642, 0.08968076887753176, 0.03420622553304702), ('Water', 'Rock'): (0.561190917391046, 0.5756907757211812, 0.08968076887753176, 0.06841245106609405), ('Water', 'Fighting'): (0.561190917391046, 0.6472578009197674, 0.08968076887753176, 0.06841245106609405), ('Water', 'Electric'): (0.561190917391046, 0.7188248261183535, 0.08968076887753176, 0.022804150355364646), ('Water', 'Ice'): (0.561190917391046, 0.7447835506062104, 0.08968076887753176, 0.045608300710729396), ('Water', 'Dark'): (0.561190917391046, 0.7935464254494318, 0.08968076887753176, 0.10261867659914106), ('Water', 'Ghost'): (0.561190917391046, 0.899319676181065, 0.08968076887753176, 0.06841245106609405), ('Water', 'Fire'): (0.561190917391046, 0.970886701379651, 0.08968076887753176, 0.0), ('Water', 'Water'): (0.561190917391046, 0.9740412755121433, 0.08968076887753176, 0.0), ('Water', 'Bug'): (0.561190917391046, 0.9771958496446355, 0.08968076887753176, 0.022804150355364646), ('Rock', 'Poison'): (0.6554799811994534, 0.0, 0.059427015521255885, 0.051620303986234586), ('Rock', 'Flying'): (0.6554799811994534, 0.0547748781187267, 0.059427015521255885, 0.13765414396329226), ('Rock', 'Dragon'): (0.6554799811994534, 0.19558359621451105, 0.059427015521255885, 0.034413535990823044), ('Rock', 'Normal'): (0.6554799811994534, 0.23315170633782623, 0.059427015521255885, 0.0), ('Rock', 'Psychic'): (0.6554799811994534, 0.23630628047031832, 0.059427015521255885, 0.034413535990823044), ('Rock', 'Steel'): (0.6554799811994534, 0.2738743905936335, 0.059427015521255885, 0.06882707198164614), ('Rock', 'Ground'): (0.6554799811994534, 0.34585603670777176, 0.059427015521255885, 0.15486091195870372), ('Rock', 'Fairy'): (0.6554799811994534, 0.5038715227989675, 0.059427015521255885, 0.05162030398623461), ('Rock', 'Grass'): (0.6554799811994534, 0.5586464009176942, 0.059427015521255885, 0.03441353599082307), ('Rock', 'Rock'): (0.6554799811994534, 0.5962145110410095, 0.059427015521255885, 0.0), ('Rock', 'Fighting'): (0.6554799811994534, 0.5993690851735015, 0.059427015521255885, 0.017206767995411536), ('Rock', 'Electric'): (0.6554799811994534, 0.6197304273014052, 0.059427015521255885, 0.12044737596788074), ('Rock', 'Ice'): (0.6554799811994534, 0.7433323774017782, 0.059427015521255885, 0.03441353599082307), ('Rock', 'Dark'): (0.6554799811994534, 0.7809004875250932, 0.059427015521255885, 0.03441353599082307), ('Rock', 'Ghost'): (0.6554799811994534, 0.8184685976484084, 0.059427015521255885, 0.0), ('Rock', 'Fire'): (0.6554799811994534, 0.8216231717809006, 0.059427015521255885, 0.03441353599082297), ('Rock', 'Water'): (0.6554799811994534, 0.8591912819042157, 0.059427015521255885, 0.10324060797246921), ('Rock', 'Bug'): (0.6554799811994534, 0.9655864640091769, 0.059427015521255885, 0.03441353599082307), ('Psychic', 'Poison'): (0.7195152916515849, 0.0, 0.06266848909514271, 0.0), ('Psychic', 'Flying'): (0.7195152916515849, 0.003154574132492114, 0.06266848909514271, 0.22843467855977376), ('Psychic', 'Dragon'): (0.7195152916515849, 0.23474382682475797, 0.06266848909514271, 0.048950288262808624), ('Psychic', 'Normal'): (0.7195152916515849, 0.28684868922005874, 0.06266848909514271, 0.06526705101707823), ('Psychic', 'Psychic'): (0.7195152916515849, 0.355270314369629, 0.06266848909514271, 0.0), ('Psychic', 'Steel'): (0.7195152916515849, 0.3584248885021212, 0.06266848909514271, 0.06526705101707819), ('Psychic', 'Ground'): (0.7195152916515849, 0.4268465136516915, 0.06266848909514271, 0.0), ('Psychic', 'Fairy'): (0.7195152916515849, 0.4300010877841836, 0.06266848909514271, 0.17948439029696514), ('Psychic', 'Grass'): (0.7195152916515849, 0.6126400522136408, 0.06266848909514271, 0.06526705101707823), ('Psychic', 'Rock'): (0.7195152916515849, 0.6810616773632112, 0.06266848909514271, 0.0), ('Psychic', 'Fighting'): (0.7195152916515849, 0.6842162514957033, 0.06266848909514271, 0.04895028826280865), ('Psychic', 'Electric'): (0.7195152916515849, 0.7363211138910041, 0.06266848909514271, 0.0), ('Psychic', 'Ice'): (0.7195152916515849, 0.7394756880234962, 0.06266848909514271, 0.04895028826280865), ('Psychic', 'Dark'): (0.7195152916515849, 0.7915805504187969, 0.06266848909514271, 0.03263352550853907), ('Psychic', 'Ghost'): (0.7195152916515849, 0.8273686500598281, 0.06266848909514271, 0.14685086478842596), ('Psychic', 'Fire'): (0.7195152916515849, 0.9773740889807462, 0.06266848909514271, 0.016316762754269586), ('Psychic', 'Water'): (0.7195152916515849, 0.9968454258675078, 0.06266848909514271, 0.0), ('Psychic', 'Bug'): (0.7195152916515849, 1.0, 0.06266848909514271, 0.0), ('Ghost', 'Poison'): (0.7867920756776031, 0.0, 0.036736700504049126, 0.11133791055854518), ('Ghost', 'Flying'): (0.7867920756776031, 0.11449248469103729, 0.036736700504049126, 0.1670068658378178), ('Ghost', 'Dragon'): (0.7867920756776031, 0.2846539246613472, 0.036736700504049126, 0.11133791055854515), ('Ghost', 'Normal'): (0.7867920756776031, 0.3991464093523845, 0.036736700504049126, 0.0), ('Ghost', 'Psychic'): (0.7867920756776031, 0.4023009834848766, 0.036736700504049126, 0.0), ('Ghost', 'Steel'): (0.7867920756776031, 0.4054555576173687, 0.036736700504049126, 0.0), ('Ghost', 'Ground'): (0.7867920756776031, 0.4086101317498608, 0.036736700504049126, 0.055668955279272604), ('Ghost', 'Fairy'): (0.7867920756776031, 0.46743366116162555, 0.036736700504049126, 0.08350343291890891), ('Ghost', 'Grass'): (0.7867920756776031, 0.5540916682130265, 0.036736700504049126, 0.3061792540359992), ('Ghost', 'Rock'): (0.7867920756776031, 0.8634254963815179, 0.036736700504049126, 0.0), ('Ghost', 'Fighting'): (0.7867920756776031, 0.86658007051401, 0.036736700504049126, 0.0), ('Ghost', 'Electric'): (0.7867920756776031, 0.8697346446465021, 0.036736700504049126, 0.0), ('Ghost', 'Ice'): (0.7867920756776031, 0.8728892187789943, 0.036736700504049126, 0.0), ('Ghost', 'Dark'): (0.7867920756776031, 0.8760437929114864, 0.036736700504049126, 0.027834477639636302), ('Ghost', 'Ghost'): (0.7867920756776031, 0.9070328446836148, 0.036736700504049126, 0.0), ('Ghost', 'Fire'): (0.7867920756776031, 0.9101874188161069, 0.036736700504049126, 0.08350343291890891), ('Ghost', 'Water'): (0.7867920756776031, 0.9968454258675078, 0.036736700504049126, 0.0), ('Ghost', 'Bug'): (0.7867920756776031, 1.0, 0.036736700504049126, 0.0), ('Fighting', 'Poison'): (0.8281370711125278, 0.0, 0.032414735738866926, 0.06309148264984227), ('Fighting', 'Flying'): (0.8281370711125278, 0.06624605678233438, 0.032414735738866926, 0.0946372239747634), ('Fighting', 'Dragon'): (0.8281370711125278, 0.16403785488958988, 0.032414735738866926, 0.031545741324921155), ('Fighting', 'Normal'): (0.8281370711125278, 0.19873817034700317, 0.032414735738866926, 0.0), ('Fighting', 'Psychic'): (0.8281370711125278, 0.2018927444794953, 0.032414735738866926, 0.09463722397476339), ('Fighting', 'Steel'): (0.8281370711125278, 0.29968454258675076, 0.032414735738866926, 0.12618296529968456), ('Fighting', 'Ground'): (0.8281370711125278, 0.4290220820189275, 0.032414735738866926, 0.0), ('Fighting', 'Fairy'): (0.8281370711125278, 0.4321766561514196, 0.032414735738866926, 0.0), ('Fighting', 'Grass'): (0.8281370711125278, 0.4353312302839117, 0.032414735738866926, 0.0), ('Fighting', 'Rock'): (0.8281370711125278, 0.43848580441640383, 0.032414735738866926, 0.0), ('Fighting', 'Fighting'): (0.8281370711125278, 0.44164037854889593, 0.032414735738866926, 0.0), ('Fighting', 'Electric'): (0.8281370711125278, 0.444794952681388, 0.032414735738866926, 0.03154574132492113), ('Fighting', 'Ice'): (0.8281370711125278, 0.47949526813880133, 0.032414735738866926, 0.03154574132492113), ('Fighting', 'Dark'): (0.8281370711125278, 0.5141955835962145, 0.032414735738866926, 0.09463722397476339), ('Fighting', 'Ghost'): (0.8281370711125278, 0.61198738170347, 0.032414735738866926, 0.06309148264984225), ('Fighting', 'Fire'): (0.8281370711125278, 0.6782334384858044, 0.032414735738866926, 0.12618296529968462), ('Fighting', 'Water'): (0.8281370711125278, 0.8075709779179812, 0.032414735738866926, 0.18927444794952678), ('Fighting', 'Bug'): (0.8281370711125278, 1.0, 0.032414735738866926, 0.0), ('Dragon', 'Poison'): (0.8651601017822702, 0.0, 0.05834652432996041, 0.0), ('Dragon', 'Flying'): (0.8651601017822702, 0.003154574132492114, 0.05834652432996041, 0.10515247108307045), ('Dragon', 'Dragon'): (0.8651601017822702, 0.11146161934805468, 0.05834652432996041, 0.0), ('Dragon', 'Normal'): (0.8651601017822702, 0.11461619348054679, 0.05834652432996041, 0.01752541184717841), ('Dragon', 'Psychic'): (0.8651601017822702, 0.13529617946021733, 0.05834652432996041, 0.07010164738871363), ('Dragon', 'Steel'): (0.8651601017822702, 0.20855240098142305, 0.05834652432996041, 0.0), ('Dragon', 'Ground'): (0.8651601017822702, 0.21170697511391517, 0.05834652432996041, 0.2278303540133193), ('Dragon', 'Fairy'): (0.8651601017822702, 0.44269190325972657, 0.05834652432996041, 0.017525411847178433), ('Dragon', 'Grass'): (0.8651601017822702, 0.46337188923939715, 0.05834652432996041, 0.0), ('Dragon', 'Rock'): (0.8651601017822702, 0.4665264633718893, 0.05834652432996041, 0.0), ('Dragon', 'Fighting'): (0.8651601017822702, 0.46968103750438134, 0.05834652432996041, 0.03505082369435682), ('Dragon', 'Electric'): (0.8651601017822702, 0.5078864353312302, 0.05834652432996041, 0.01752541184717838), ('Dragon', 'Ice'): (0.8651601017822702, 0.5285664213109008, 0.05834652432996041, 0.2103049421661409), ('Dragon', 'Dark'): (0.8651601017822702, 0.7420259376095338, 0.05834652432996041, 0.017525411847178488), ('Dragon', 'Ghost'): (0.8651601017822702, 0.7627059235892044, 0.05834652432996041, 0.05257623554153515), ('Dragon', 'Fire'): (0.8651601017822702, 0.8184367332632317, 0.05834652432996041, 0.017525411847178488), ('Dragon', 'Water'): (0.8651601017822702, 0.8391167192429023, 0.05834652432996041, 0.15772870662460564), ('Dragon', 'Bug'): (0.8651601017822702, 1.0, 0.05834652432996041, 0.0), ('Fairy', 'Poison'): (0.9281149210431063, 0.0, 0.014046385486842264, 0.0), ('Fairy', 'Flying'): (0.9281149210431063, 0.003154574132492114, 0.014046385486842264, 0.4367871875758311), ('Fairy', 'Dragon'): (0.9281149210431063, 0.44309633584081537, 0.014046385486842264, 0.0), ('Fairy', 'Normal'): (0.9281149210431063, 0.4462509099733075, 0.014046385486842264, 0.0), ('Fairy', 'Psychic'): (0.9281149210431063, 0.44940548410579956, 0.014046385486842264, 0.0727978645959718), ('Fairy', 'Steel'): (0.9281149210431063, 0.5253579228342635, 0.014046385486842264, 0.36398932297985936), ('Fairy', 'Ground'): (0.9281149210431063, 0.892501819946615, 0.014046385486842264, 0.0), ('Fairy', 'Fairy'): (0.9281149210431063, 0.895656394079107, 0.014046385486842264, 0.0), ('Fairy', 'Grass'): (0.9281149210431063, 0.8988109682115991, 0.014046385486842264, 0.0), ('Fairy', 'Rock'): (0.9281149210431063, 0.9019655423440913, 0.014046385486842264, 0.0), ('Fairy', 'Fighting'): (0.9281149210431063, 0.9051201164765834, 0.014046385486842264, 0.0727978645959718), ('Fairy', 'Electric'): (0.9281149210431063, 0.9810725552050473, 0.014046385486842264, 0.0), ('Fairy', 'Ice'): (0.9281149210431063, 0.9842271293375395, 0.014046385486842264, 0.0), ('Fairy', 'Dark'): (0.9281149210431063, 0.9873817034700316, 0.014046385486842264, 0.0), ('Fairy', 'Ghost'): (0.9281149210431063, 0.9905362776025236, 0.014046385486842264, 0.0), ('Fairy', 'Fire'): (0.9281149210431063, 0.9936908517350158, 0.014046385486842264, 0.0), ('Fairy', 'Water'): (0.9281149210431063, 0.9968454258675078, 0.014046385486842264, 0.0), ('Fairy', 'Bug'): (0.9281149210431063, 1.0, 0.014046385486842264, 0.0), ('Steel', 'Poison'): (0.9467696014608241, 0.0, 0.03997817407793585, 0.05115525620257482), ('Steel', 'Flying'): (0.9467696014608241, 0.054309830335066936, 0.03997817407793585, 0.05115525620257482), ('Steel', 'Dragon'): (0.9467696014608241, 0.10861966067013387, 0.03997817407793585, 0.23019865291158667), ('Steel', 'Normal'): (0.9467696014608241, 0.3419728877142127, 0.03997817407793585, 0.0), ('Steel', 'Psychic'): (0.9467696014608241, 0.34512746184670473, 0.03997817407793585, 0.17904339670901187), ('Steel', 'Steel'): (0.9467696014608241, 0.5273254326882088, 0.03997817407793585, 0.0), ('Steel', 'Ground'): (0.9467696014608241, 0.5304800068207008, 0.03997817407793585, 0.05115525620257482), ('Steel', 'Fairy'): (0.9467696014608241, 0.5847898371557678, 0.03997817407793585, 0.10231051240514964), ('Steel', 'Grass'): (0.9467696014608241, 0.6902549236934095, 0.03997817407793585, 0.0), ('Steel', 'Rock'): (0.9467696014608241, 0.6934094978259017, 0.03997817407793585, 0.07673288430386217), ('Steel', 'Fighting'): (0.9467696014608241, 0.7732969562622559, 0.03997817407793585, 0.025577628101287463), ('Steel', 'Electric'): (0.9467696014608241, 0.8020291584960355, 0.03997817407793585, 0.0), ('Steel', 'Ice'): (0.9467696014608241, 0.8051837326285277, 0.03997817407793585, 0.0), ('Steel', 'Dark'): (0.9467696014608241, 0.8083383067610198, 0.03997817407793585, 0.0), ('Steel', 'Ghost'): (0.9467696014608241, 0.8114928808935118, 0.03997817407793585, 0.17904339670901182), ('Steel', 'Fire'): (0.9467696014608241, 0.9936908517350158, 0.03997817407793585, 0.0), ('Steel', 'Water'): (0.9467696014608241, 0.9968454258675078, 0.03997817407793585, 0.0), ('Steel', 'Bug'): (0.9467696014608241, 1.0, 0.03997817407793585, 0.0), ('Flying', 'Poison'): (0.9913560704696356, 0.0, 0.008643929530364493, 0.0), ('Flying', 'Flying'): (0.9913560704696356, 0.003154574132492114, 0.008643929530364493, 0.0), ('Flying', 'Dragon'): (0.9913560704696356, 0.006309148264984228, 0.008643929530364493, 0.23659305993690852), ('Flying', 'Normal'): (0.9913560704696356, 0.24605678233438488, 0.008643929530364493, 0.0), ('Flying', 'Psychic'): (0.9913560704696356, 0.24921135646687695, 0.008643929530364493, 0.0), ('Flying', 'Steel'): (0.9913560704696356, 0.25236593059936907, 0.008643929530364493, 0.11829652996845426), ('Flying', 'Ground'): (0.9913560704696356, 0.37381703470031546, 0.008643929530364493, 0.0), ('Flying', 'Fairy'): (0.9913560704696356, 0.37697160883280756, 0.008643929530364493, 0.0), ('Flying', 'Grass'): (0.9913560704696356, 0.3801261829652997, 0.008643929530364493, 0.0), ('Flying', 'Rock'): (0.9913560704696356, 0.38328075709779186, 0.008643929530364493, 0.0), ('Flying', 'Fighting'): (0.9913560704696356, 0.3864353312302839, 0.008643929530364493, 0.11829652996845426), ('Flying', 'Electric'): (0.9913560704696356, 0.5078864353312302, 0.008643929530364493, 0.0), ('Flying', 'Ice'): (0.9913560704696356, 0.5110410094637224, 0.008643929530364493, 0.0), ('Flying', 'Dark'): (0.9913560704696356, 0.5141955835962145, 0.008643929530364493, 0.11829652996845426), ('Flying', 'Ghost'): (0.9913560704696356, 0.6356466876971608, 0.008643929530364493, 0.0), ('Flying', 'Fire'): (0.9913560704696356, 0.638801261829653, 0.008643929530364493, 0.0), ('Flying', 'Water'): (0.9913560704696356, 0.6419558359621451, 0.008643929530364493, 0.3548895899053628), ('Flying', 'Bug'): (0.9913560704696356, 1.0, 0.008643929530364493, 0.0)})\nplt.show()\n\n\n\n\nThis obviously needs a bit of cleaning up to remove extra labels, but it’s easy to get to and relatively functional. Notice that it does not, by default, show NA values.\n\n\nWe can generate a mosaic plot (the equivalent of a 2-dimensional cross-tabular view) using geom_bar with position = 'fill', which scales each bar so that it ends at 1. I’ve flipped the axes using coord_flip so that you can read the labels more easily.\n\n# Convert everything to categorical/factor variable ahead of time\n# this stops an error: TypeError: '<' not supported between instances of 'float' and 'str'\npoke['type_1'] = pd.Categorical(poke['type_1'].astype(str))\npoke['type_2'] = pd.Categorical(poke['type_2'].astype(str))\n\n( ggplot(mapping = aes(x = 'type_1', fill = 'type_2'), data = poke) +\n  geom_bar(color = \"black\", position = \"fill\") +\n  xlab(\"Type 1\") + ylab(\"Proportion of Pokemon w/ Type 2\") +\n  coord_flip() +\n  # This says 85% of the plot is for the main plot and 15% is for the legend.\n  theme(subplots_adjust={'right':0.85})\n  )\n## <ggplot: (8779209682191)>\n\n\n\n\nAnother way to look at this data is to bin it in x and y and shade the resulting bins by the number of data points in each bin. We can even add in labels so that this is at least as clear as the tabular view!\n\n(ggplot(mapping = aes(x = 'type_1', y = 'type_2'), data = poke) +\n  # Shade tiles according to the number of things in the bin\n  stat_bin2d(aes(fill = after_stat('count')), geom = 'tile'))\n## <ggplot: (8779203614387)>\n\n\n\n\n\n\n\n\n17.4.3.2 Categorical - Continuous Relationships\n\n\nBase R\nR: ggplot2\nPython: matplotlib\nPython: plotnine\n\n\n\nIn R, most models are specified as y ~ x1 + x2 + x3, where the information on the left side of the tilde is the dependent variable, and the information on the right side are any explanatory variables. Interactions are specified using x1*x2 to get all combinations of x1 and x2 (x1, x2, x1*x2); single interaction terms are specified as e.g. x1:x2 and do not include any component terms.\nTo examine the relationship between a categorical variable and a continuous variable, we might look at box plots:\n\npar(mfrow = c(1, 2)) # put figures in same row\nboxplot(log10(height_m) ~ type_1, data = poke)\nboxplot(total ~ generation, data = poke)\n\n\n\n\n\n\n\nIn the second box plot, there are far too many categories to be able to resolve the relationship clearly, but the plot is still effective in that we can identify that there are one or two species which have a much higher point range than other species. EDA isn’t usually about creating pretty plots (or we’d be using ggplot right now) but rather about identifying things which may come up in the analysis later.\n\n\n\nggplot(data = poke, aes(x = type_1, y = height_m)) + \n  geom_boxplot() + \n  scale_y_log10()\n\nggplot(data = poke, aes(x = factor(generation), y = total)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.figure()\n\n# Create a list of vectors of height_m by type_1\npoke['height_m_log'] = np.log(poke.height_m)\nheight_by_type = poke.groupby('type_1', group_keys = True).height_m_log.apply(list)\n\n# Plot each object in the list\nplt.boxplot(height_by_type, labels = height_by_type.index)\n## {'whiskers': [<matplotlib.lines.Line2D object at 0x7fc12051d870>, <matplotlib.lines.Line2D object at 0x7fc12051eec0>, <matplotlib.lines.Line2D object at 0x7fc1204de230>, <matplotlib.lines.Line2D object at 0x7fc1204dd0c0>, <matplotlib.lines.Line2D object at 0x7fc1204df3a0>, <matplotlib.lines.Line2D object at 0x7fc1204dc6a0>, <matplotlib.lines.Line2D object at 0x7fc114fe7100>, <matplotlib.lines.Line2D object at 0x7fc114fe6980>, <matplotlib.lines.Line2D object at 0x7fc11d8c6aa0>, <matplotlib.lines.Line2D object at 0x7fc11d8c6500>, <matplotlib.lines.Line2D object at 0x7fc11d8c6140>, <matplotlib.lines.Line2D object at 0x7fc11d8c7d30>, <matplotlib.lines.Line2D object at 0x7fc120572e00>, <matplotlib.lines.Line2D object at 0x7fc1205723e0>, <matplotlib.lines.Line2D object at 0x7fc120570850>, <matplotlib.lines.Line2D object at 0x7fc120573730>, <matplotlib.lines.Line2D object at 0x7fc11ada3610>, <matplotlib.lines.Line2D object at 0x7fc11ada21a0>, <matplotlib.lines.Line2D object at 0x7fc11ada0ee0>, <matplotlib.lines.Line2D object at 0x7fc11ad1bf40>, <matplotlib.lines.Line2D object at 0x7fc11ac87100>, <matplotlib.lines.Line2D object at 0x7fc11ac87d90>, <matplotlib.lines.Line2D object at 0x7fc11ac86350>, <matplotlib.lines.Line2D object at 0x7fc11ac84c40>, <matplotlib.lines.Line2D object at 0x7fc11ac872b0>, <matplotlib.lines.Line2D object at 0x7fc11ac862f0>, <matplotlib.lines.Line2D object at 0x7fc11add21d0>, <matplotlib.lines.Line2D object at 0x7fc11add3100>, <matplotlib.lines.Line2D object at 0x7fc11add1e10>, <matplotlib.lines.Line2D object at 0x7fc11add2230>, <matplotlib.lines.Line2D object at 0x7fc11add05e0>, <matplotlib.lines.Line2D object at 0x7fc11add0e20>, <matplotlib.lines.Line2D object at 0x7fc11ac1ca60>, <matplotlib.lines.Line2D object at 0x7fc11ac1fc70>, <matplotlib.lines.Line2D object at 0x7fc11d9a7d90>, <matplotlib.lines.Line2D object at 0x7fc11d9a6e00>], 'caps': [<matplotlib.lines.Line2D object at 0x7fc12051fbe0>, <matplotlib.lines.Line2D object at 0x7fc12051cca0>, <matplotlib.lines.Line2D object at 0x7fc1204dc4f0>, <matplotlib.lines.Line2D object at 0x7fc1204dc220>, <matplotlib.lines.Line2D object at 0x7fc114fe7af0>, <matplotlib.lines.Line2D object at 0x7fc114fe6560>, <matplotlib.lines.Line2D object at 0x7fc114fe4d90>, <matplotlib.lines.Line2D object at 0x7fc11d8c4370>, <matplotlib.lines.Line2D object at 0x7fc11d8c7e20>, <matplotlib.lines.Line2D object at 0x7fc11d8c6ec0>, <matplotlib.lines.Line2D object at 0x7fc120572620>, <matplotlib.lines.Line2D object at 0x7fc120573790>, <matplotlib.lines.Line2D object at 0x7fc120571f00>, <matplotlib.lines.Line2D object at 0x7fc1205733a0>, <matplotlib.lines.Line2D object at 0x7fc11ada38b0>, <matplotlib.lines.Line2D object at 0x7fc11ada1510>, <matplotlib.lines.Line2D object at 0x7fc11ada1ff0>, <matplotlib.lines.Line2D object at 0x7fc11ada3f70>, <matplotlib.lines.Line2D object at 0x7fc11abf80a0>, <matplotlib.lines.Line2D object at 0x7fc11abf9060>, <matplotlib.lines.Line2D object at 0x7fc11ac847c0>, <matplotlib.lines.Line2D object at 0x7fc11ac86440>, <matplotlib.lines.Line2D object at 0x7fc11ac85c00>, <matplotlib.lines.Line2D object at 0x7fc11ac85900>, <matplotlib.lines.Line2D object at 0x7fc11ac87af0>, <matplotlib.lines.Line2D object at 0x7fc11ac87df0>, <matplotlib.lines.Line2D object at 0x7fc11add10c0>, <matplotlib.lines.Line2D object at 0x7fc11add2110>, <matplotlib.lines.Line2D object at 0x7fc11add2ce0>, <matplotlib.lines.Line2D object at 0x7fc11add1360>, <matplotlib.lines.Line2D object at 0x7fc11add2290>, <matplotlib.lines.Line2D object at 0x7fc11add21a0>, <matplotlib.lines.Line2D object at 0x7fc11ac1cd90>, <matplotlib.lines.Line2D object at 0x7fc11ac1c490>, <matplotlib.lines.Line2D object at 0x7fc11d9a4220>, <matplotlib.lines.Line2D object at 0x7fc11d9a49d0>], 'boxes': [<matplotlib.lines.Line2D object at 0x7fc12051f310>, <matplotlib.lines.Line2D object at 0x7fc1204df190>, <matplotlib.lines.Line2D object at 0x7fc1204ddf90>, <matplotlib.lines.Line2D object at 0x7fc114fe61a0>, <matplotlib.lines.Line2D object at 0x7fc11d8c6b90>, <matplotlib.lines.Line2D object at 0x7fc11d8c43a0>, <matplotlib.lines.Line2D object at 0x7fc120570580>, <matplotlib.lines.Line2D object at 0x7fc120570340>, <matplotlib.lines.Line2D object at 0x7fc11ada3a60>, <matplotlib.lines.Line2D object at 0x7fc11ada0a90>, <matplotlib.lines.Line2D object at 0x7fc11ac84430>, <matplotlib.lines.Line2D object at 0x7fc1205304c0>, <matplotlib.lines.Line2D object at 0x7fc11ac85420>, <matplotlib.lines.Line2D object at 0x7fc11add2470>, <matplotlib.lines.Line2D object at 0x7fc11add17e0>, <matplotlib.lines.Line2D object at 0x7fc11add0670>, <matplotlib.lines.Line2D object at 0x7fc11ac1fe80>, <matplotlib.lines.Line2D object at 0x7fc11d9a5e40>], 'medians': [<matplotlib.lines.Line2D object at 0x7fc12051c130>, <matplotlib.lines.Line2D object at 0x7fc1204de380>, <matplotlib.lines.Line2D object at 0x7fc114fe6500>, <matplotlib.lines.Line2D object at 0x7fc11d8c68f0>, <matplotlib.lines.Line2D object at 0x7fc11d8c7520>, <matplotlib.lines.Line2D object at 0x7fc120571b10>, <matplotlib.lines.Line2D object at 0x7fc120573220>, <matplotlib.lines.Line2D object at 0x7fc11ada0700>, <matplotlib.lines.Line2D object at 0x7fc11ada0280>, <matplotlib.lines.Line2D object at 0x7fc11ac84ca0>, <matplotlib.lines.Line2D object at 0x7fc114fb82b0>, <matplotlib.lines.Line2D object at 0x7fc11ac84c70>, <matplotlib.lines.Line2D object at 0x7fc11ac85600>, <matplotlib.lines.Line2D object at 0x7fc11add0580>, <matplotlib.lines.Line2D object at 0x7fc11add00d0>, <matplotlib.lines.Line2D object at 0x7fc11add3cd0>, <matplotlib.lines.Line2D object at 0x7fc11ac1fb80>, <matplotlib.lines.Line2D object at 0x7fc11d9a5060>], 'fliers': [<matplotlib.lines.Line2D object at 0x7fc12051c730>, <matplotlib.lines.Line2D object at 0x7fc1204df490>, <matplotlib.lines.Line2D object at 0x7fc114fe5b70>, <matplotlib.lines.Line2D object at 0x7fc11d8c71f0>, <matplotlib.lines.Line2D object at 0x7fc11d8c63e0>, <matplotlib.lines.Line2D object at 0x7fc1205716f0>, <matplotlib.lines.Line2D object at 0x7fc120573e80>, <matplotlib.lines.Line2D object at 0x7fc11ada2d40>, <matplotlib.lines.Line2D object at 0x7fc11ada01f0>, <matplotlib.lines.Line2D object at 0x7fc11ac84460>, <matplotlib.lines.Line2D object at 0x7fc11acacd00>, <matplotlib.lines.Line2D object at 0x7fc11ac84160>, <matplotlib.lines.Line2D object at 0x7fc11ac87970>, <matplotlib.lines.Line2D object at 0x7fc11add2b60>, <matplotlib.lines.Line2D object at 0x7fc11add0040>, <matplotlib.lines.Line2D object at 0x7fc11ac1e890>, <matplotlib.lines.Line2D object at 0x7fc11d9a4d00>, <matplotlib.lines.Line2D object at 0x7fc11d9a52a0>], 'means': []}\nplt.show()\n\n\n\n\n\n\nplt.figure()\n\n# Create a list of vectors of total by generation\ntotal_by_gen = poke.groupby('generation', group_keys = True).total.apply(list)\n\n# Plot each object in the list\nplt.boxplot(total_by_gen, labels = total_by_gen.index)\n## {'whiskers': [<matplotlib.lines.Line2D object at 0x7fc1151863b0>, <matplotlib.lines.Line2D object at 0x7fc1151866b0>, <matplotlib.lines.Line2D object at 0x7fc1151876a0>, <matplotlib.lines.Line2D object at 0x7fc1151865f0>, <matplotlib.lines.Line2D object at 0x7fc115185420>, <matplotlib.lines.Line2D object at 0x7fc115184310>, <matplotlib.lines.Line2D object at 0x7fc114ea5090>, <matplotlib.lines.Line2D object at 0x7fc114ea4af0>, <matplotlib.lines.Line2D object at 0x7fc114ea6140>, <matplotlib.lines.Line2D object at 0x7fc114ea65c0>, <matplotlib.lines.Line2D object at 0x7fc114ea40d0>, <matplotlib.lines.Line2D object at 0x7fc114ea6e60>, <matplotlib.lines.Line2D object at 0x7fc114ea64a0>, <matplotlib.lines.Line2D object at 0x7fc114ea78b0>, <matplotlib.lines.Line2D object at 0x7fc114ec57b0>, <matplotlib.lines.Line2D object at 0x7fc114ec5a80>, <matplotlib.lines.Line2D object at 0x7fc114ec6b60>, <matplotlib.lines.Line2D object at 0x7fc114ec7490>], 'caps': [<matplotlib.lines.Line2D object at 0x7fc115187f70>, <matplotlib.lines.Line2D object at 0x7fc115187130>, <matplotlib.lines.Line2D object at 0x7fc1151869b0>, <matplotlib.lines.Line2D object at 0x7fc1151864d0>, <matplotlib.lines.Line2D object at 0x7fc115184d00>, <matplotlib.lines.Line2D object at 0x7fc1151845b0>, <matplotlib.lines.Line2D object at 0x7fc114ea5000>, <matplotlib.lines.Line2D object at 0x7fc114ea44c0>, <matplotlib.lines.Line2D object at 0x7fc114ea6320>, <matplotlib.lines.Line2D object at 0x7fc114ea6da0>, <matplotlib.lines.Line2D object at 0x7fc114ea7e80>, <matplotlib.lines.Line2D object at 0x7fc114ea5f60>, <matplotlib.lines.Line2D object at 0x7fc114ec4100>, <matplotlib.lines.Line2D object at 0x7fc114ec4f10>, <matplotlib.lines.Line2D object at 0x7fc114ec63b0>, <matplotlib.lines.Line2D object at 0x7fc114ec6260>, <matplotlib.lines.Line2D object at 0x7fc114ec7340>, <matplotlib.lines.Line2D object at 0x7fc114ec73d0>], 'boxes': [<matplotlib.lines.Line2D object at 0x7fc115184c10>, <matplotlib.lines.Line2D object at 0x7fc115187010>, <matplotlib.lines.Line2D object at 0x7fc115185f30>, <matplotlib.lines.Line2D object at 0x7fc115184760>, <matplotlib.lines.Line2D object at 0x7fc114ea5c30>, <matplotlib.lines.Line2D object at 0x7fc114ea7610>, <matplotlib.lines.Line2D object at 0x7fc114ea76a0>, <matplotlib.lines.Line2D object at 0x7fc114ec4bb0>, <matplotlib.lines.Line2D object at 0x7fc114ec6ad0>], 'medians': [<matplotlib.lines.Line2D object at 0x7fc115186560>, <matplotlib.lines.Line2D object at 0x7fc115184c70>, <matplotlib.lines.Line2D object at 0x7fc115184ac0>, <matplotlib.lines.Line2D object at 0x7fc114ea47c0>, <matplotlib.lines.Line2D object at 0x7fc114ea6680>, <matplotlib.lines.Line2D object at 0x7fc114ea7850>, <matplotlib.lines.Line2D object at 0x7fc114ec4fa0>, <matplotlib.lines.Line2D object at 0x7fc114ec62f0>, <matplotlib.lines.Line2D object at 0x7fc114ec7d00>], 'fliers': [<matplotlib.lines.Line2D object at 0x7fc1151872e0>, <matplotlib.lines.Line2D object at 0x7fc115185bd0>, <matplotlib.lines.Line2D object at 0x7fc115185720>, <matplotlib.lines.Line2D object at 0x7fc114ea55a0>, <matplotlib.lines.Line2D object at 0x7fc114ea4d90>, <matplotlib.lines.Line2D object at 0x7fc114ea6e30>, <matplotlib.lines.Line2D object at 0x7fc114ec5930>, <matplotlib.lines.Line2D object at 0x7fc114ec6c20>, <matplotlib.lines.Line2D object at 0x7fc114ec7bb0>], 'means': []}\nplt.show()\n\n\n\n\n\n\n\nggplot(mapping = aes(x = \"type_1\", y = \"height_m\"), data = poke) +\\\ngeom_boxplot()\n## <ggplot: (8779209654008)>\n\n\n\nggplot(mapping = aes(x = \"generation\", y = \"total\"), data = poke) +\\\ngeom_boxplot()\n## <ggplot: (8779215416723)>\n\n\n\n\n\n\n\nYou can find more on boxplots and ways to customize boxplots in the Graphics chapter.\n\n17.4.3.3 Continuous - Continuous Relationships\n\n\nBase R\nR: ggplot2\nPython: pandas\nPython: plotnine\n\n\n\nTo look at the relationship between numeric variables, we could compute a numeric correlation, but a plot may be more useful, because it allows us to see outliers as well.\n\nplot(defense ~ attack, data = poke, type = \"p\")\n\n\n\n\n\n\n\ncor(poke$defense, poke$attack)\n## [1] 0.4259168\n\nSometimes, we discover that a numeric variable which may seem to be continuous is actually relatively quantized - there are only a few values of base_friendship in the whole dataset.\n\nplot(x = poke$base_experience, y = poke$base_friendship, type = \"p\")\n## Error in plot.window(...): need finite 'xlim' values\n\n\n\n\nA scatterplot matrix can also be a useful way to visualize relationships between several variables.\n\npairs(poke[,16:20]) # hp - sp_defense columns\n## Error in `poke[, 16:20]`:\n## ! Can't subset columns past the end.\n## ℹ Locations 19 and 20 don't exist.\n## ℹ There are only 18 columns.\n\n\n\n\n\n\n\nNote\n\n\n\nThere’s more information on how to customize base R scatterplot matrices here.\n\n\n\n\nTo look at the relationship between numeric variables, we could compute a numeric correlation, but a plot may be more useful, because it allows us to see outliers as well.\n\nlibrary(ggplot2)\nggplot(poke, aes(x = attack, y = defense)) + geom_point()\n\n\n\n\nSometimes, we discover that a numeric variable which may seem to be continuous is actually relatively quantized - there are only a few values of base_friendship in the whole dataset. When this happens, it can be a good idea to use geom_jitter to provide some “wiggle” in the data so that you can still see the point density. Changing the point transparency (alpha = .5) can also help with overplotting.\n\nggplot(poke, aes(x = base_experience, y = base_friendship)) + geom_point()\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error in `FUN()`:\n## ! object 'base_experience' not found\nggplot(poke, aes(x = base_experience, y = base_friendship)) + geom_jitter(alpha = 0.5)\n## Error in `geom_jitter()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error in `FUN()`:\n## ! object 'base_experience' not found\n\n\nlibrary(GGally) # an extension to ggplot2\nggpairs(poke[,16:20], # hp - sp_defense columns\n        lower = list(continuous = wrap(\"points\", alpha = .15)),\n        progress = F) \n## Error in `poke[, 16:20]`:\n## ! Can't subset columns past the end.\n## ℹ Locations 19 and 20 don't exist.\n## ℹ There are only 18 columns.\n\nggpairs can also handle continuous variables, if you want to explore the options available.\n\n\nBelieve it or not, you don’t have to go to matplotlib to get plots in python - you can get some plots from pandas directly, even if you are still using matplotlib under the hood (this is why you have to run plt.show() to get the plot to appear if you’re working in markdown).\n\nimport matplotlib.pyplot as plt\n\npoke.plot.scatter(x = 'attack', y = 'defense')\nplt.show()\n\n\n\n\nPandas also includes a nice scatterplot matrix method.\n\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\n\nscatter_matrix(poke.iloc[:,15:19], alpha = 0.2, figsize = (6, 6), diagonal = 'kde')\n## array([[<AxesSubplot:xlabel='weight_kg', ylabel='weight_kg'>]],\n##       dtype=object)\nplt.show()\n\n\n\n\n\n\n\nfrom plotnine import *\n\nggplot(poke, aes(x = \"base_experience\", y = \"base_friendship\")) + geom_point()\n# jitter in plotnine seems to use width and height jointly instead of \n# marginally\n## Error in py_call_impl(callable, dots$args, dots$keywords): plotnine.exceptions.PlotnineError: \"Could not evaluate the 'x' mapping: 'base_experience' (original error: name 'base_experience' is not defined)\"\nggplot(poke, aes(x = \"base_experience\", y = \"base_friendship\")) + geom_jitter(alpha = 0.5, height = 5)\n## Error in py_call_impl(callable, dots$args, dots$keywords): plotnine.exceptions.PlotnineError: \"Could not evaluate the 'x' mapping: 'base_experience' (original error: name 'base_experience' is not defined)\"\n\nWhile plotnine doesn’t have scatterplot matrices by default, you can create them using some clever code. This is obviously not as fancy as ggpairs but it works well enough.\n\nfrom plotnine import *\nimport itertools\n\ndef plot_matrix(df, columns):\n  pdf = []\n  for a1, b1 in itertools.combinations(columns, 2):\n    for (a,b) in ((a1, b1), (b1, a1)):\n      sub = df[[a, b]].rename(columns={a: \"x\", b: \"y\"}).assign(a=a, b=b)\n      pdf.append(sub)\n  \n  g = ggplot(pd.concat(pdf))\n  g += geom_point(aes('x','y'))\n  g += facet_grid('b~a', scales='free')\n  return g\n\n\nplot_matrix(poke, poke.columns[15:19])\n## <ggplot: (8779209684629)>\n\n\n\n\n\n\n\nIf you want summary statistics by group, you can get that using the dplyr package functions select and group_by, which we will learn more about in the next section. (I’m cheating a bit by mentioning it now, but it’s just so useful!)\n\n\n\n\n\n\nTry it out: EDA\n\n\n\n\n\nProblem\nR solution\nPython solution\n\n\n\nExplore the variables present in the Lancaster County Assessor Housing Sales Data Documentation.\nNote that some variables may be too messy to handle with the things that you have seen thus far - that is ok. As you find irregularities, document them - these are things you may need to clean up in the dataset before you conduct a formal analysis.\n\nif (!\"readxl\" %in% installed.packages()) install.packages(\"readxl\")\nlibrary(readxl)\ndownload.file(\"https://github.com/srvanderplas/datasets/blob/main/raw/Lancaster%20County,%20NE%20-%20Assessor.xlsx?raw=true\", destfile = \"../data/lancaster-housing.xlsx\")\nhousing_lincoln <- read_xlsx(\"../data/lancaster-housing.xlsx\", sheet = 1, guess_max = 7000)\n\n\nimport pandas as pd\nhousing_lincoln = pd.read_excel(\"../data/lancaster-housing.xlsx\")\n\n\n\n\nhousing_lincoln$TLA <- readr::parse_number(housing_lincoln$`TLA (Sqft)`)\nhousing_lincoln$Assd_Value <- readr::parse_number(housing_lincoln$Assd_Value)\n\nskim(housing_lincoln)\n\n\nData summary\n\n\nName\nhousing_lincoln\n\n\nNumber of rows\n6918\n\n\nNumber of columns\n9\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nParcel_ID\n0\n1\n17\n17\n0\n6740\n0\n\n\nAddress\n0\n1\n29\n50\n0\n6740\n0\n\n\nOwner\n0\n1\n6\n67\n0\n6435\n0\n\n\nOwner Address\n0\n1\n25\n93\n0\n6184\n0\n\n\nImp_Type\n0\n1\n2\n3\n0\n39\n0\n\n\nTLA (Sqft)\n0\n1\n3\n5\n0\n1767\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nYr_Blt\n0\n1\n1950.85\n22.55\n1900\n1933\n1954.0\n1963\n2023\n▂▃▇▂▁\n\n\nAssd_Value\n0\n1\n229956.23\n96272.53\n36500\n174925\n214500.0\n259175\n1404800\n▇▁▁▁▁\n\n\nTLA\n0\n1\n1379.08\n599.09\n400\n966\n1235.5\n1612\n6819\n▇▂▁▁▁\n\n\n\n\n\nLet’s examine the numeric variables first:\n\nhist(housing_lincoln$Assd_Value)\n\n\n\n\nhist(housing_lincoln$Yr_Blt)\n\n\n\n\nLet’s look at the years the houses were built and the Imp_Types. We can find more data on what the Improvement Types mean here, where the various abbreviations are defined.\n\nhousing_lincoln$decade <- 10*floor(housing_lincoln$Yr_Blt/10)\n\ntable(housing_lincoln$decade, useNA = 'ifany')\n## \n## 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 \n##  245  295  885  414  647 1949 1429  282  550  116   72   24   10\ntable(housing_lincoln$Imp_Type)\n## \n##   BL   BN   C1   C2   CA   CB  CXF  CXU  CYF  CYU   D1   D2   D3   D4   D5   D6 \n##  163  764   11   39   48   17    4    3    7   25    2   40    9  232   45   10 \n##   DA   HC   M1   R1   R2   RA   RB   RR   RS  RXF  RXU  RYF  RYU   T1   T2   T3 \n##    2  132    1 3165  492  628   23   15  218  257   79   31   73  160    9   14 \n##   T4   T5   T6   T7   TA   TS  TYF \n##   17   37    5   21  110    9    1\ntable(housing_lincoln$Imp_Type, housing_lincoln$decade)\n##      \n##       1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020\n##   BL     0    0    0    0    0    0  119   42    0    1    1    0    0\n##   BN    77   84  413  176   14    0    0    0    0    0    0    0    0\n##   C1     1    0    8    0    1    0    0    0    1    0    0    0    0\n##   C2    14    9   12    1    1    2    0    0    0    0    0    0    0\n##   CA     7   14   17    5    4    1    0    0    0    0    0    0    0\n##   CB     5   11    0    0    0    1    0    0    0    0    0    0    0\n##   CXF    0    1    2    0    1    0    0    0    0    0    0    0    0\n##   CXU    0    0    1    2    0    0    0    0    0    0    0    0    0\n##   CYF    0    5    2    0    0    0    0    0    0    0    0    0    0\n##   CYU    7    9    7    2    0    0    0    0    0    0    0    0    0\n##   D1     0    0    0    0    0    0    1    1    0    0    0    0    0\n##   D2     0    0    0    0    0    5   26    8    1    0    0    0    0\n##   D3     0    0    0    0    0    0    1    1    2    3    2    0    0\n##   D4     0    0    1    3   29   91   87    8    5    5    1    2    0\n##   D5     0    2    1   10    9   12    0    5    1    2    3    0    0\n##   D6     0    0    0    0    0    2    0    2    0    6    0    0    0\n##   DA     0    0    1    0    0    1    0    0    0    0    0    0    0\n##   HC     0    0    0    0    0    0   83    8   41    0    0    0    0\n##   M1     0    0    0    1    0    0    0    0    0    0    0    0    0\n##   R1     3    1   10    8  382 1641  902  133   52   10    3   10   10\n##   R2    33   46   76   42   19   11   48   25  163   27    0    2    0\n##   RA    47   51  165   87  115  104   13    7   26    6    4    3    0\n##   RB     4    5    8    5    1    0    0    0    0    0    0    0    0\n##   RR     0    0    0    0    1    9    3    0    0    1    1    0    0\n##   RS     0    0    2    0    4   33  145   26    8    0    0    0    0\n##   RXF   16   13  101   53   45   28    1    0    0    0    0    0    0\n##   RXU    3    8   31   12   18    7    0    0    0    0    0    0    0\n##   RYF   12   11    6    0    1    1    0    0    0    0    0    0    0\n##   RYU   16   25   21    7    2    0    0    0    2    0    0    0    0\n##   T1     0    0    0    0    0    0    0    6  124    8   22    0    0\n##   T2     0    0    0    0    0    0    0    0    3    0    0    6    0\n##   T3     0    0    0    0    0    0    0    4   10    0    0    0    0\n##   T4     0    0    0    0    0    0    0    0   16    0    0    1    0\n##   T5     0    0    0    0    0    0    0    0    8   29    0    0    0\n##   T6     0    0    0    0    0    0    0    0    4    1    0    0    0\n##   T7     0    0    0    0    0    0    0    0    5   13    3    0    0\n##   TA     0    0    0    0    0    0    0    0   74    4   32    0    0\n##   TS     0    0    0    0    0    0    0    6    3    0    0    0    0\n##   TYF    0    0    0    0    0    0    0    0    1    0    0    0    0\n\nplot(table(housing_lincoln$decade, housing_lincoln$Imp_Type),\n     main = \"Year Built and Improvement Type\")\n\n\n\n\nWe can also look at the square footage for each improvement type:\n\nhousing_lincoln %>%\n  subset(Imp_Type %in% c(\"BN\", \"R1\", \"R2\", \"RA\")) %>%\n  boxplot(TLA ~ Imp_Type, data = .)\n\n\n\n\nThis makes sense - there are relatively few bungalows (BN), but R1 means 1 story house, R2 means 2 story house, and RA is a so-called 1.5 story house.\n\n\n\nhousing_lincoln[\"TLA\"] = housing_lincoln[\"TLA (Sqft)\"].str.replace(\"[,\\$]\", \"\", regex = True)\n# For some reason, things without a comma just get NaN'd, so fix that\nhousing_lincoln.loc[housing_lincoln[\"TLA\"].isna(), \"TLA\"] = housing_lincoln.loc[housing_lincoln[\"TLA\"].isna(), \"TLA (Sqft)\"]\nhousing_lincoln[\"TLA\"] = pd.to_numeric(housing_lincoln[\"TLA\"], errors = 'coerce')\n\nhousing_lincoln[\"Assessed\"] = housing_lincoln[\"Assd_Value\"].str.replace(\"[,\\$]\", \"\", regex = True)\n# For some reason, things without a comma just get NaN'd, so fix that\nhousing_lincoln.loc[housing_lincoln[\"Assessed\"].isna(), \"Assessed\"] = housing_lincoln.loc[housing_lincoln[\"Assessed\"].isna(), \"Assd_Value\"]\nhousing_lincoln[\"Assessed\"] = pd.to_numeric(housing_lincoln[\"Assessed\"], errors = 'coerce')\n\nhousing_lincoln = housing_lincoln.drop([\"TLA (Sqft)\", \"Assd_Value\"], axis = 1)\n\n# housing_lincoln.describe()\nskim(housing_lincoln)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 6918   │ │ string      │ 5     │                       │\n## │ │ Number of columns │ 8      │ │ int64       │ 3     │                       │\n## │ └───────────────────┴────────┘ └─────────────┴───────┘                       │\n## │                                   number                                     │\n## │ ┏━━━━━━━┳━━━━┳━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━━┳━━━━━━┓  │\n## │ ┃ colum ┃    ┃      ┃       ┃       ┃       ┃       ┃      ┃       ┃      ┃  │\n## │ ┃ n_nam ┃    ┃      ┃       ┃       ┃       ┃       ┃      ┃       ┃      ┃  │\n## │ ┃ e     ┃ NA ┃ NA % ┃ mean  ┃ sd    ┃ p0    ┃ p25   ┃ p75  ┃ p100  ┃ hist ┃  │\n## │ ┡━━━━━━━╇━━━━╇━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━━╇━━━━━━┩  │\n## │ │ Yr_Bl │  0 │    0 │  2000 │    23 │  1900 │  1900 │ 2000 │  2000 │ ▃▃█▄ │  │\n## │ │ t     │    │      │       │       │       │       │      │       │  ▂   │  │\n## │ │ TLA   │  0 │    0 │  1400 │   600 │   400 │   970 │ 1600 │  6800 │ █▃▁  │  │\n## │ │ Asses │  0 │    0 │ 23000 │ 96000 │ 36000 │ 17000 │ 2600 │ 14000 │  █▂  │  │\n## │ │ sed   │    │      │     0 │       │       │     0 │   00 │    00 │      │  │\n## │ └───────┴────┴──────┴───────┴───────┴───────┴───────┴──────┴───────┴──────┘  │\n## │                                   string                                     │\n## │ ┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓  │\n## │ ┃ column_name        ┃ NA   ┃ NA %   ┃ words per row     ┃ total words    ┃  │\n## │ ┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩  │\n## │ │ Parcel_ID          │    0 │      0 │                 1 │           6900 │  │\n## │ │ Address            │    0 │      0 │                 1 │           6900 │  │\n## │ │ Owner              │    0 │      0 │                 1 │           6900 │  │\n## │ │ Owner Address      │    0 │      0 │                 1 │           6900 │  │\n## │ │ Imp_Type           │    0 │      0 │                 1 │           6900 │  │\n## │ └────────────────────┴──────┴────────┴───────────────────┴────────────────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\nLet’s examine the numeric and date variables first:\n\nhousing_lincoln[\"TLA\"].plot.hist()\nplt.show()\n\n\n\nhousing_lincoln[\"Yr_Blt\"].plot.hist()\nplt.show()\n\n\n\n\nLet’s look at the years the houses were built and the Imp_Types. We can find more data on what the Improvement Types mean here, where the various abbreviations are defined.\n\nimport numpy as np\nhousing_lincoln['decade'] = 10*np.floor(housing_lincoln.Yr_Blt/10)\n\nhousing_lincoln[\"decade\"].groupby(housing_lincoln[\"decade\"]).count()\n## decade\n## 1900.0     245\n## 1910.0     295\n## 1920.0     885\n## 1930.0     414\n## 1940.0     647\n## 1950.0    1949\n## 1960.0    1429\n## 1970.0     282\n## 1980.0     550\n## 1990.0     116\n## 2000.0      72\n## 2010.0      24\n## 2020.0      10\n## Name: decade, dtype: int64\nhousing_lincoln[\"Imp_Type\"].groupby(housing_lincoln[\"Imp_Type\"]).count()\n## Imp_Type\n## BL      163\n## BN      764\n## C1       11\n## C2       39\n## CA       48\n## CB       17\n## CXF       4\n## CXU       3\n## CYF       7\n## CYU      25\n## D1        2\n## D2       40\n## D3        9\n## D4      232\n## D5       45\n## D6       10\n## DA        2\n## HC      132\n## M1        1\n## R1     3165\n## R2      492\n## RA      628\n## RB       23\n## RR       15\n## RS      218\n## RXF     257\n## RXU      79\n## RYF      31\n## RYU      73\n## T1      160\n## T2        9\n## T3       14\n## T4       17\n## T5       37\n## T6        5\n## T7       21\n## TA      110\n## TS        9\n## TYF       1\n## Name: Imp_Type, dtype: int64\npd.crosstab(index = housing_lincoln[\"decade\"], columns = housing_lincoln[\"Imp_Type\"])\n## Imp_Type   BL   BN  C1  C2  CA  CB  CXF  CXU  ...  T3  T4  T5  T6  T7  TA  TS  TYF\n## decade                                        ...                                 \n## 1900.0      0   77   1  14   7   5    0    0  ...   0   0   0   0   0   0   0    0\n## 1910.0      0   84   0   9  14  11    1    0  ...   0   0   0   0   0   0   0    0\n## 1920.0      0  413   8  12  17   0    2    1  ...   0   0   0   0   0   0   0    0\n## 1930.0      0  176   0   1   5   0    0    2  ...   0   0   0   0   0   0   0    0\n## 1940.0      0   14   1   1   4   0    1    0  ...   0   0   0   0   0   0   0    0\n## 1950.0      0    0   0   2   1   1    0    0  ...   0   0   0   0   0   0   0    0\n## 1960.0    119    0   0   0   0   0    0    0  ...   0   0   0   0   0   0   0    0\n## 1970.0     42    0   0   0   0   0    0    0  ...   4   0   0   0   0   0   6    0\n## 1980.0      0    0   1   0   0   0    0    0  ...  10  16   8   4   5  74   3    1\n## 1990.0      1    0   0   0   0   0    0    0  ...   0   0  29   1  13   4   0    0\n## 2000.0      1    0   0   0   0   0    0    0  ...   0   0   0   0   3  32   0    0\n## 2010.0      0    0   0   0   0   0    0    0  ...   0   1   0   0   0   0   0    0\n## 2020.0      0    0   0   0   0   0    0    0  ...   0   0   0   0   0   0   0    0\n## \n## [13 rows x 39 columns]\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(housing_lincoln, [\"decade\", \"Imp_Type\"], title = \"Housing Built by Type, Decade\")\n## (<Figure size 700x500 with 3 Axes>, {('1900.0', 'BN'): (0.0, 0.0, 0.03341024508119372, 0.27895181741335584), ('1900.0', 'CA'): (0.0, 0.28191039729501266, 0.03341024508119372, 0.025359256128486905), ('1900.0', 'RXU'): (0.0, 0.31022823330515636, 0.03341024508119372, 0.010868252626494367), ('1900.0', 'D5'): (0.0, 0.32405506581330756, 0.03341024508119372, 0.0), ('1900.0', 'C2'): (0.0, 0.32701364569496433, 0.03341024508119372, 0.05071851225697381), ('1900.0', 'R2'): (0.0, 0.38069073783359497, 0.03341024508119372, 0.11955077889143824), ('1900.0', 'RXF'): (0.0, 0.5032000966066901, 0.03341024508119372, 0.057964014007970056), ('1900.0', 'R1'): (0.0, 0.5641226904963168, 0.03341024508119372, 0.010868252626494415), ('1900.0', 'CB'): (0.0, 0.577949523004468, 0.03341024508119372, 0.018113754377490562), ('1900.0', 'RYU'): (0.0, 0.5990218572636155, 0.03341024508119372, 0.057964014007970056), ('1900.0', 'CYF'): (0.0, 0.6599444511532423, 0.03341024508119372, 0.0), ('1900.0', 'RA'): (0.0, 0.6629030310348991, 0.03341024508119372, 0.1702692911484121), ('1900.0', 'CYU'): (0.0, 0.8361309020649681, 0.03341024508119372, 0.025359256128486808), ('1900.0', 'RYF'): (0.0, 0.8644487380751116, 0.03341024508119372, 0.043473010505977565), ('1900.0', 'C1'): (0.0, 0.9108803284627459, 0.03341024508119372, 0.0036227508754981715), ('1900.0', 'D3'): (0.0, 0.917461659219901, 0.03341024508119372, 0.0), ('1900.0', 'HC'): (0.0, 0.9204202391015577, 0.03341024508119372, 0.0), ('1900.0', 'RB'): (0.0, 0.9233788189832146, 0.03341024508119372, 0.01449100350199249), ('1900.0', 'D4'): (0.0, 0.9408284023668639, 0.03341024508119372, 0.0), ('1900.0', 'T2'): (0.0, 0.9437869822485206, 0.03341024508119372, 0.0), ('1900.0', 'D2'): (0.0, 0.9467455621301775, 0.03341024508119372, 0.0), ('1900.0', 'BL'): (0.0, 0.9497041420118344, 0.03341024508119372, 0.0), ('1900.0', 'RS'): (0.0, 0.952662721893491, 0.03341024508119372, 0.0), ('1900.0', 'RR'): (0.0, 0.9556213017751479, 0.03341024508119372, 0.0), ('1900.0', 'T7'): (0.0, 0.9585798816568047, 0.03341024508119372, 0.0), ('1900.0', 'TA'): (0.0, 0.9615384615384615, 0.03341024508119372, 0.0), ('1900.0', 'T1'): (0.0, 0.9644970414201183, 0.03341024508119372, 0.0), ('1900.0', 'TS'): (0.0, 0.9674556213017752, 0.03341024508119372, 0.0), ('1900.0', 'T3'): (0.0, 0.9704142011834319, 0.03341024508119372, 0.0), ('1900.0', 'T5'): (0.0, 0.9733727810650887, 0.03341024508119372, 0.0), ('1900.0', 'T6'): (0.0, 0.9763313609467456, 0.03341024508119372, 0.0), ('1900.0', 'T4'): (0.0, 0.9792899408284023, 0.03341024508119372, 0.0), ('1900.0', 'D1'): (0.0, 0.9822485207100592, 0.03341024508119372, 0.0), ('1900.0', 'D6'): (0.0, 0.985207100591716, 0.03341024508119372, 0.0), ('1900.0', 'M1'): (0.0, 0.9881656804733727, 0.03341024508119372, 0.0), ('1900.0', 'DA'): (0.0, 0.9911242603550295, 0.03341024508119372, 0.0), ('1900.0', 'TYF'): (0.0, 0.9940828402366865, 0.03341024508119372, 0.0), ('1900.0', 'CXF'): (0.0, 0.9970414201183432, 0.03341024508119372, 0.0), ('1900.0', 'CXU'): (0.0, 1.0, 0.03341024508119372, 0.0), ('1910.0', 'BN'): (0.03812722621326918, 0.0, 0.040228662444702634, 0.2527329254839033), ('1910.0', 'CA'): (0.03812722621326918, 0.2556915053655601, 0.040228662444702634, 0.042122154247317196), ('1910.0', 'RXU'): (0.03812722621326918, 0.30077223949453413, 0.040228662444702634, 0.024069802427038427), ('1910.0', 'D5'): (0.03812722621326918, 0.3278006218032294, 0.040228662444702634, 0.006017450606759607), ('1910.0', 'C2'): (0.03812722621326918, 0.3367766522916457, 0.040228662444702634, 0.027078527730418227), ('1910.0', 'R2'): (0.03812722621326918, 0.3668137599037208, 0.040228662444702634, 0.13840136395547079), ('1910.0', 'RXF'): (0.03812722621326918, 0.5081737037408484, 0.040228662444702634, 0.03911342894393749), ('1910.0', 'R1'): (0.03812722621326918, 0.5502457125664427, 0.040228662444702634, 0.003008725303379754), ('1910.0', 'CB'): (0.03812722621326918, 0.5562130177514792, 0.040228662444702634, 0.033095978337177784), ('1910.0', 'RYU'): (0.03812722621326918, 0.5922675759703139, 0.040228662444702634, 0.07521813258449513), ('1910.0', 'CYF'): (0.03812722621326918, 0.6704442884364658, 0.040228662444702634, 0.015043626516898967), ('1910.0', 'RA'): (0.03812722621326918, 0.6884464948350215, 0.040228662444702634, 0.1534449904723698), ('1910.0', 'CYU'): (0.03812722621326918, 0.8448500651890481, 0.040228662444702634, 0.027078527730418276), ('1910.0', 'RYF'): (0.03812722621326918, 0.8748871728011233, 0.040228662444702634, 0.033095978337177784), ('1910.0', 'C1'): (0.03812722621326918, 0.9109417310199579, 0.040228662444702634, 0.0), ('1910.0', 'D3'): (0.03812722621326918, 0.9139003109016146, 0.040228662444702634, 0.0), ('1910.0', 'HC'): (0.03812722621326918, 0.9168588907832714, 0.040228662444702634, 0.0), ('1910.0', 'RB'): (0.03812722621326918, 0.9198174706649284, 0.040228662444702634, 0.015043626516899064), ('1910.0', 'D4'): (0.03812722621326918, 0.9378196770634841, 0.040228662444702634, 0.0), ('1910.0', 'T2'): (0.03812722621326918, 0.9407782569451408, 0.040228662444702634, 0.0), ('1910.0', 'D2'): (0.03812722621326918, 0.9437368368267978, 0.040228662444702634, 0.0), ('1910.0', 'BL'): (0.03812722621326918, 0.9466954167084546, 0.040228662444702634, 0.0), ('1910.0', 'RS'): (0.03812722621326918, 0.9496539965901113, 0.040228662444702634, 0.0), ('1910.0', 'RR'): (0.03812722621326918, 0.9526125764717681, 0.040228662444702634, 0.0), ('1910.0', 'T7'): (0.03812722621326918, 0.955571156353425, 0.040228662444702634, 0.0), ('1910.0', 'TA'): (0.03812722621326918, 0.9585297362350816, 0.040228662444702634, 0.0), ('1910.0', 'T1'): (0.03812722621326918, 0.9614883161167386, 0.040228662444702634, 0.0), ('1910.0', 'TS'): (0.03812722621326918, 0.9644468959983954, 0.040228662444702634, 0.0), ('1910.0', 'T3'): (0.03812722621326918, 0.9674054758800521, 0.040228662444702634, 0.0), ('1910.0', 'T5'): (0.03812722621326918, 0.970364055761709, 0.040228662444702634, 0.0), ('1910.0', 'T6'): (0.03812722621326918, 0.9733226356433659, 0.040228662444702634, 0.0), ('1910.0', 'T4'): (0.03812722621326918, 0.9762812155250226, 0.040228662444702634, 0.0), ('1910.0', 'D1'): (0.03812722621326918, 0.9792397954066794, 0.040228662444702634, 0.0), ('1910.0', 'D6'): (0.03812722621326918, 0.9821983752883363, 0.040228662444702634, 0.0), ('1910.0', 'M1'): (0.03812722621326918, 0.9851569551699929, 0.040228662444702634, 0.0), ('1910.0', 'DA'): (0.03812722621326918, 0.9881155350516498, 0.040228662444702634, 0.0), ('1910.0', 'TYF'): (0.03812722621326918, 0.9910741149333067, 0.040228662444702634, 0.0), ('1910.0', 'CXF'): (0.03812722621326918, 0.9940326948149634, 0.040228662444702634, 0.003008725303379754), ('1910.0', 'CXU'): (0.03812722621326918, 1.0, 0.040228662444702634, 0.0), ('1970.0', 'BN'): (0.08307286979004729, 0.0, 0.03845587393019031, 0.0), ('1970.0', 'CA'): (0.08307286979004729, 0.0029585798816568047, 0.03845587393019031, 0.0), ('1970.0', 'RXU'): (0.08307286979004729, 0.005917159763313609, 0.03845587393019031, 0.0), ('1970.0', 'D5'): (0.08307286979004729, 0.008875739644970414, 0.03845587393019031, 0.015737127030089386), ('1970.0', 'C2'): (0.08307286979004729, 0.027571446556716606, 0.03845587393019031, 0.0), ('1970.0', 'R2'): (0.08307286979004729, 0.03053002643837341, 0.03845587393019031, 0.07868563515044694), ('1970.0', 'RXF'): (0.08307286979004729, 0.11217424147047715, 0.03845587393019031, 0.0), ('1970.0', 'R1'): (0.08307286979004729, 0.11513282135213394, 0.03845587393019031, 0.4186075790003777), ('1970.0', 'CB'): (0.08307286979004729, 0.5366989802341684, 0.03845587393019031, 0.0), ('1970.0', 'RYU'): (0.08307286979004729, 0.5396575601158252, 0.03845587393019031, 0.0), ('1970.0', 'CYF'): (0.08307286979004729, 0.542616139997482, 0.03845587393019031, 0.0), ('1970.0', 'RA'): (0.08307286979004729, 0.5455747198791389, 0.03845587393019031, 0.022031977842125124), ('1970.0', 'CYU'): (0.08307286979004729, 0.5705652776029209, 0.03845587393019031, 0.0), ('1970.0', 'RYF'): (0.08307286979004729, 0.5735238574845776, 0.03845587393019031, 0.0), ('1970.0', 'C1'): (0.08307286979004729, 0.5764824373662344, 0.03845587393019031, 0.0), ('1970.0', 'D3'): (0.08307286979004729, 0.5794410172478912, 0.03845587393019031, 0.0031474254060179312), ('1970.0', 'HC'): (0.08307286979004729, 0.5855470225355659, 0.03845587393019031, 0.025179403248142957), ('1970.0', 'RB'): (0.08307286979004729, 0.6136850056653657, 0.03845587393019031, 0.0), ('1970.0', 'D4'): (0.08307286979004729, 0.6166435855470226, 0.03845587393019031, 0.025179403248143054), ('1970.0', 'T2'): (0.08307286979004729, 0.6447815686768223, 0.03845587393019031, 0.0), ('1970.0', 'D2'): (0.08307286979004729, 0.6477401485584792, 0.03845587393019031, 0.025179403248143054), ('1970.0', 'BL'): (0.08307286979004729, 0.6758781316882789, 0.03845587393019031, 0.13219186705275085), ('1970.0', 'RS'): (0.08307286979004729, 0.8110285786226867, 0.03845587393019031, 0.08183306055646473), ('1970.0', 'RR'): (0.08307286979004729, 0.8958202190608083, 0.03845587393019031, 0.0), ('1970.0', 'T7'): (0.08307286979004729, 0.8987787989424649, 0.03845587393019031, 0.0), ('1970.0', 'TA'): (0.08307286979004729, 0.9017373788241219, 0.03845587393019031, 0.0), ('1970.0', 'T1'): (0.08307286979004729, 0.9046959587057785, 0.03845587393019031, 0.01888455243610729), ('1970.0', 'TS'): (0.08307286979004729, 0.9265390910235428, 0.03845587393019031, 0.01888455243610729), ('1970.0', 'T3'): (0.08307286979004729, 0.9483822233413068, 0.03845587393019031, 0.012589701624071527), ('1970.0', 'T5'): (0.08307286979004729, 0.9639305048470352, 0.03845587393019031, 0.0), ('1970.0', 'T6'): (0.08307286979004729, 0.966889084728692, 0.03845587393019031, 0.0), ('1970.0', 'T4'): (0.08307286979004729, 0.9698476646103488, 0.03845587393019031, 0.0), ('1970.0', 'D1'): (0.08307286979004729, 0.9728062444920055, 0.03845587393019031, 0.0031474254060178323), ('1970.0', 'D6'): (0.08307286979004729, 0.9789122497796802, 0.03845587393019031, 0.0062948508120357636), ('1970.0', 'M1'): (0.08307286979004729, 0.9881656804733727, 0.03845587393019031, 0.0), ('1970.0', 'DA'): (0.08307286979004729, 0.9911242603550295, 0.03845587393019031, 0.0), ('1970.0', 'TYF'): (0.08307286979004729, 0.9940828402366865, 0.03845587393019031, 0.0), ('1970.0', 'CXF'): (0.08307286979004729, 0.9970414201183432, 0.03845587393019031, 0.0), ('1970.0', 'CXU'): (0.08307286979004729, 1.0, 0.03845587393019031, 0.0), ('1920.0', 'BN'): (0.12624572485231306, 0.0, 0.1206859873341079, 0.4142011834319527), ('1920.0', 'CA'): (0.12624572485231306, 0.41715976331360954, 0.1206859873341079, 0.017049443385818888), ('1920.0', 'RXU'): (0.12624572485231306, 0.43716778658108524, 0.1206859873341079, 0.031090161468257972), ('1920.0', 'D5'): (0.12624572485231306, 0.471216527931, 0.1206859873341079, 0.0010029084344598852), ('1920.0', 'C2'): (0.12624572485231306, 0.4751780162471167, 0.1206859873341079, 0.012034901213519215), ('1920.0', 'R2'): (0.12624572485231306, 0.49017149734229276, 0.1206859873341079, 0.07622104101895504), ('1920.0', 'RXF'): (0.12624572485231306, 0.5693511182429045, 0.1206859873341079, 0.10129375188045324), ('1920.0', 'R1'): (0.12624572485231306, 0.6736034500050146, 0.1206859873341079, 0.010029084344599346), ('1920.0', 'CB'): (0.12624572485231306, 0.6865911142312707, 0.1206859873341079, 0.0), ('1920.0', 'RYU'): (0.12624572485231306, 0.6895496941129275, 0.1206859873341079, 0.021061077123658675), ('1920.0', 'CYF'): (0.12624572485231306, 0.713569351118243, 0.1206859873341079, 0.0020058168689198693), ('1920.0', 'RA'): (0.12624572485231306, 0.7185337478688196, 0.1206859873341079, 0.16547989168588906), ('1920.0', 'CYU'): (0.12624572485231306, 0.8869722194363656, 0.1206859873341079, 0.007020359041219591), ('1920.0', 'RYF'): (0.12624572485231306, 0.896951158359242, 0.1206859873341079, 0.0060174506067596075), ('1920.0', 'C1'): (0.12624572485231306, 0.9059271888476583, 0.1206859873341079, 0.008023267475679477), ('1920.0', 'D3'): (0.12624572485231306, 0.9169090362049948, 0.1206859873341079, 0.0), ('1920.0', 'HC'): (0.12624572485231306, 0.9198676160866514, 0.1206859873341079, 0.0), ('1920.0', 'RB'): (0.12624572485231306, 0.9228261959683083, 0.1206859873341079, 0.008023267475679477), ('1920.0', 'D4'): (0.12624572485231306, 0.9338080433256446, 0.1206859873341079, 0.0010029084344598852), ('1920.0', 'T2'): (0.12624572485231306, 0.9377695316417612, 0.1206859873341079, 0.0), ('1920.0', 'D2'): (0.12624572485231306, 0.9407281115234182, 0.1206859873341079, 0.0), ('1920.0', 'BL'): (0.12624572485231306, 0.9436866914050748, 0.1206859873341079, 0.0), ('1920.0', 'RS'): (0.12624572485231306, 0.9466452712867317, 0.1206859873341079, 0.0020058168689198693), ('1920.0', 'RR'): (0.12624572485231306, 0.9516096680373083, 0.1206859873341079, 0.0), ('1920.0', 'T7'): (0.12624572485231306, 0.9545682479189652, 0.1206859873341079, 0.0), ('1920.0', 'TA'): (0.12624572485231306, 0.9575268278006219, 0.1206859873341079, 0.0), ('1920.0', 'T1'): (0.12624572485231306, 0.9604854076822787, 0.1206859873341079, 0.0), ('1920.0', 'TS'): (0.12624572485231306, 0.9634439875639357, 0.1206859873341079, 0.0), ('1920.0', 'T3'): (0.12624572485231306, 0.9664025674455923, 0.1206859873341079, 0.0), ('1920.0', 'T5'): (0.12624572485231306, 0.9693611473272492, 0.1206859873341079, 0.0), ('1920.0', 'T6'): (0.12624572485231306, 0.972319727208906, 0.1206859873341079, 0.0), ('1920.0', 'T4'): (0.12624572485231306, 0.9752783070905627, 0.1206859873341079, 0.0), ('1920.0', 'D1'): (0.12624572485231306, 0.9782368869722197, 0.1206859873341079, 0.0), ('1920.0', 'D6'): (0.12624572485231306, 0.9811954668538765, 0.1206859873341079, 0.0), ('1920.0', 'M1'): (0.12624572485231306, 0.9841540467355332, 0.1206859873341079, 0.0), ('1920.0', 'DA'): (0.12624572485231306, 0.98711262661719, 0.1206859873341079, 0.0010029084344599839), ('1920.0', 'TYF'): (0.12624572485231306, 0.9910741149333069, 0.1206859873341079, 0.0), ('1920.0', 'CXF'): (0.12624572485231306, 0.9940326948149636, 0.1206859873341079, 0.0020058168689198693), ('1920.0', 'CXU'): (0.12624572485231306, 0.9989970915655402, 0.1206859873341079, 0.0010029084344598852), ('1950.0', 'BN'): (0.25164869331849643, 0.0, 0.26578190882957775, 0.0), ('1950.0', 'CA'): (0.25164869331849643, 0.0029585798816568047, 0.26578190882957775, 0.00045539967393383343), ('1950.0', 'RXU'): (0.25164869331849643, 0.006372559437247443, 0.26578190882957775, 0.003187797717536834), ('1950.0', 'D5'): (0.25164869331849643, 0.01251893703644108, 0.26578190882957775, 0.005464796087206001), ('1950.0', 'C2'): (0.25164869331849643, 0.020942313005303886, 0.26578190882957775, 0.0009107993478676663), ('1950.0', 'R2'): (0.25164869331849643, 0.024811692234828358, 0.26578190882957775, 0.00500939641327217), ('1950.0', 'RXF'): (0.25164869331849643, 0.03277966852975733, 0.26578190882957775, 0.012751190870147337), ('1950.0', 'R1'): (0.25164869331849643, 0.04848943928156147, 0.26578190882957775, 0.7473108649254208), ('1950.0', 'CB'): (0.25164869331849643, 0.7987588840886389, 0.26578190882957775, 0.0004553996739337523), ('1950.0', 'RYU'): (0.25164869331849643, 0.8021728636442296, 0.26578190882957775, 0.0), ('1950.0', 'CYF'): (0.25164869331849643, 0.8051314435258864, 0.26578190882957775, 0.0), ('1950.0', 'RA'): (0.25164869331849643, 0.8080900234075431, 0.26578190882957775, 0.04736156608911871), ('1950.0', 'CYU'): (0.25164869331849643, 0.8584101693783187, 0.26578190882957775, 0.0), ('1950.0', 'RYF'): (0.25164869331849643, 0.8613687492599755, 0.26578190882957775, 0.00045539967393385083), ('1950.0', 'C1'): (0.25164869331849643, 0.8647827288155662, 0.26578190882957775, 0.0), ('1950.0', 'D3'): (0.25164869331849643, 0.867741308697223, 0.26578190882957775, 0.0), ('1950.0', 'HC'): (0.25164869331849643, 0.8706998885788798, 0.26578190882957775, 0.0), ('1950.0', 'RB'): (0.25164869331849643, 0.8736584684605365, 0.26578190882957775, 0.0), ('1950.0', 'D4'): (0.25164869331849643, 0.8766170483421935, 0.26578190882957775, 0.04144137032797885), ('1950.0', 'T2'): (0.25164869331849643, 0.9210169985518291, 0.26578190882957775, 0.0), ('1950.0', 'D2'): (0.25164869331849643, 0.9239755784334859, 0.26578190882957775, 0.0022769983696691557), ('1950.0', 'BL'): (0.25164869331849643, 0.9292111566848118, 0.26578190882957775, 0.0), ('1950.0', 'RS'): (0.25164869331849643, 0.9321697365664685, 0.26578190882957775, 0.015028189239816487), ('1950.0', 'RR'): (0.25164869331849643, 0.9501565056879419, 0.26578190882957775, 0.004098597065404559), ('1950.0', 'T7'): (0.25164869331849643, 0.9572136826350032, 0.26578190882957775, 0.0), ('1950.0', 'TA'): (0.25164869331849643, 0.9601722625166601, 0.26578190882957775, 0.0), ('1950.0', 'T1'): (0.25164869331849643, 0.9631308423983169, 0.26578190882957775, 0.0), ('1950.0', 'TS'): (0.25164869331849643, 0.9660894222799736, 0.26578190882957775, 0.0), ('1950.0', 'T3'): (0.25164869331849643, 0.9690480021616306, 0.26578190882957775, 0.0), ('1950.0', 'T5'): (0.25164869331849643, 0.9720065820432872, 0.26578190882957775, 0.0), ('1950.0', 'T6'): (0.25164869331849643, 0.9749651619249441, 0.26578190882957775, 0.0), ('1950.0', 'T4'): (0.25164869331849643, 0.9779237418066009, 0.26578190882957775, 0.0), ('1950.0', 'D1'): (0.25164869331849643, 0.9808823216882576, 0.26578190882957775, 0.0), ('1950.0', 'D6'): (0.25164869331849643, 0.9838409015699144, 0.26578190882957775, 0.0009107993478676031), ('1950.0', 'M1'): (0.25164869331849643, 0.9877102807994389, 0.26578190882957775, 0.0), ('1950.0', 'DA'): (0.25164869331849643, 0.9906688606810957, 0.26578190882957775, 0.00045539967393385083), ('1950.0', 'TYF'): (0.25164869331849643, 0.9940828402366865, 0.26578190882957775, 0.0), ('1950.0', 'CXF'): (0.25164869331849643, 0.9970414201183432, 0.26578190882957775, 0.0), ('1950.0', 'CXU'): (0.25164869331849643, 1.0, 0.26578190882957775, 0.0), ('1960.0', 'BN'): (0.5221475832801497, 0.0, 0.1948703682490849, 0.0), ('1960.0', 'CA'): (0.5221475832801497, 0.0029585798816568047, 0.1948703682490849, 0.0), ('1960.0', 'RXU'): (0.5221475832801497, 0.005917159763313609, 0.1948703682490849, 0.0), ('1960.0', 'D5'): (0.5221475832801497, 0.008875739644970414, 0.1948703682490849, 0.0), ('1960.0', 'C2'): (0.5221475832801497, 0.011834319526627219, 0.1948703682490849, 0.0), ('1960.0', 'R2'): (0.5221475832801497, 0.014792899408284023, 0.1948703682490849, 0.02981354114475716), ('1960.0', 'RXF'): (0.5221475832801497, 0.047565020434697985, 0.1948703682490849, 0.0006211154405157738), ('1960.0', 'R1'): (0.5221475832801497, 0.05114471575687057, 0.1948703682490849, 0.5602461273452284), ('1960.0', 'CB'): (0.5221475832801497, 0.6143494229837557, 0.1948703682490849, 0.0), ('1960.0', 'RYU'): (0.5221475832801497, 0.6173080028654125, 0.1948703682490849, 0.0), ('1960.0', 'CYF'): (0.5221475832801497, 0.6202665827470694, 0.1948703682490849, 0.0), ('1960.0', 'RA'): (0.5221475832801497, 0.6232251626287261, 0.1948703682490849, 0.00807450072670503), ('1960.0', 'CYU'): (0.5221475832801497, 0.634258243237088, 0.1948703682490849, 0.0), ('1960.0', 'RYF'): (0.5221475832801497, 0.6372168231187448, 0.1948703682490849, 0.0), ('1960.0', 'C1'): (0.5221475832801497, 0.6401754030004015, 0.1948703682490849, 0.0), ('1960.0', 'D3'): (0.5221475832801497, 0.6431339828820585, 0.1948703682490849, 0.000621115440515817), ('1960.0', 'HC'): (0.5221475832801497, 0.646713678204231, 0.1948703682490849, 0.05155258156280926), ('1960.0', 'RB'): (0.5221475832801497, 0.7012248396486971, 0.1948703682490849, 0.0), ('1960.0', 'D4'): (0.5221475832801497, 0.7041834195303539, 0.1948703682490849, 0.05403704332487233), ('1960.0', 'T2'): (0.5221475832801497, 0.7611790427368831, 0.1948703682490849, 0.0), ('1960.0', 'D2'): (0.5221475832801497, 0.7641376226185398, 0.1948703682490849, 0.016149001453410156), ('1960.0', 'BL'): (0.5221475832801497, 0.7832452039536069, 0.1948703682490849, 0.07391273742137719), ('1960.0', 'RS'): (0.5221475832801497, 0.8601165212566408, 0.1948703682490849, 0.09006173887478724), ('1960.0', 'RR'): (0.5221475832801497, 0.9531368400130849, 0.1948703682490849, 0.0018633463215472538), ('1960.0', 'T7'): (0.5221475832801497, 0.9579587662162888, 0.1948703682490849, 0.0), ('1960.0', 'TA'): (0.5221475832801497, 0.9609173460979458, 0.1948703682490849, 0.0), ('1960.0', 'T1'): (0.5221475832801497, 0.9638759259796026, 0.1948703682490849, 0.0), ('1960.0', 'TS'): (0.5221475832801497, 0.9668345058612593, 0.1948703682490849, 0.0), ('1960.0', 'T3'): (0.5221475832801497, 0.9697930857429161, 0.1948703682490849, 0.0), ('1960.0', 'T5'): (0.5221475832801497, 0.9727516656245728, 0.1948703682490849, 0.0), ('1960.0', 'T6'): (0.5221475832801497, 0.9757102455062296, 0.1948703682490849, 0.0), ('1960.0', 'T4'): (0.5221475832801497, 0.9786688253878866, 0.1948703682490849, 0.0), ('1960.0', 'D1'): (0.5221475832801497, 0.9816274052695433, 0.1948703682490849, 0.000621115440515817), ('1960.0', 'D6'): (0.5221475832801497, 0.985207100591716, 0.1948703682490849, 0.0), ('1960.0', 'M1'): (0.5221475832801497, 0.9881656804733727, 0.1948703682490849, 0.0), ('1960.0', 'DA'): (0.5221475832801497, 0.9911242603550295, 0.1948703682490849, 0.0), ('1960.0', 'TYF'): (0.5221475832801497, 0.9940828402366865, 0.1948703682490849, 0.0), ('1960.0', 'CXF'): (0.5221475832801497, 0.9970414201183432, 0.1948703682490849, 0.0), ('1960.0', 'CXU'): (0.5221475832801497, 1.0, 0.1948703682490849, 0.0), ('1990.0', 'BN'): (0.7217349326613101, 0.0, 0.0158187282833407, 0.0), ('1990.0', 'CA'): (0.7217349326613101, 0.0029585798816568047, 0.0158187282833407, 0.0), ('1990.0', 'RXU'): (0.7217349326613101, 0.005917159763313609, 0.0158187282833407, 0.0), ('1990.0', 'D5'): (0.7217349326613101, 0.008875739644970414, 0.0158187282833407, 0.015302999387880024), ('1990.0', 'C2'): (0.7217349326613101, 0.027137318914507245, 0.0158187282833407, 0.0), ('1990.0', 'R2'): (0.7217349326613101, 0.03009589879616405, 0.0158187282833407, 0.2065904917363803), ('1990.0', 'RXF'): (0.7217349326613101, 0.23964497041420119, 0.0158187282833407, 0.0), ('1990.0', 'R1'): (0.7217349326613101, 0.24260355029585798, 0.0158187282833407, 0.07651499693940013), ('1990.0', 'CB'): (0.7217349326613101, 0.32207712711691494, 0.0158187282833407, 0.0), ('1990.0', 'RYU'): (0.7217349326613101, 0.32503570699857176, 0.0158187282833407, 0.0), ('1990.0', 'CYF'): (0.7217349326613101, 0.3279942868802285, 0.0158187282833407, 0.0), ('1990.0', 'RA'): (0.7217349326613101, 0.33095286676188534, 0.0158187282833407, 0.045908998163640066), ('1990.0', 'CYU'): (0.7217349326613101, 0.3798204448071822, 0.0158187282833407, 0.0), ('1990.0', 'RYF'): (0.7217349326613101, 0.382779024688839, 0.0158187282833407, 0.0), ('1990.0', 'C1'): (0.7217349326613101, 0.38573760457049583, 0.0158187282833407, 0.0), ('1990.0', 'D3'): (0.7217349326613101, 0.3886961844521526, 0.0158187282833407, 0.022954499081820033), ('1990.0', 'HC'): (0.7217349326613101, 0.4146092634156295, 0.0158187282833407, 0.0), ('1990.0', 'RB'): (0.7217349326613101, 0.4175678432972863, 0.0158187282833407, 0.0), ('1990.0', 'D4'): (0.7217349326613101, 0.42052642317894307, 0.0158187282833407, 0.038257498469700085), ('1990.0', 'T2'): (0.7217349326613101, 0.4617425015302999, 0.0158187282833407, 0.0), ('1990.0', 'D2'): (0.7217349326613101, 0.46470108141195676, 0.0158187282833407, 0.0), ('1990.0', 'BL'): (0.7217349326613101, 0.4676596612936136, 0.0158187282833407, 0.007651499693939978), ('1990.0', 'RS'): (0.7217349326613101, 0.4782697408692103, 0.0158187282833407, 0.0), ('1990.0', 'RR'): (0.7217349326613101, 0.4812283207508672, 0.0158187282833407, 0.007651499693940027), ('1990.0', 'T7'): (0.7217349326613101, 0.4918384003264639, 0.0158187282833407, 0.09946949602122011), ('1990.0', 'TA'): (0.7217349326613101, 0.5942664762293409, 0.0158187282833407, 0.030605998775760108), ('1990.0', 'T1'): (0.7217349326613101, 0.6278310548867578, 0.0158187282833407, 0.06121199755152012), ('1990.0', 'TS'): (0.7217349326613101, 0.6920016323199347, 0.0158187282833407, 0.0), ('1990.0', 'T3'): (0.7217349326613101, 0.6949602122015915, 0.0158187282833407, 0.0), ('1990.0', 'T5'): (0.7217349326613101, 0.6979187920832484, 0.0158187282833407, 0.22189349112426035), ('1990.0', 'T6'): (0.7217349326613101, 0.9227708630891656, 0.0158187282833407, 0.007651499693939978), ('1990.0', 'T4'): (0.7217349326613101, 0.9333809426647622, 0.0158187282833407, 0.0), ('1990.0', 'D1'): (0.7217349326613101, 0.9363395225464191, 0.0158187282833407, 0.0), ('1990.0', 'D6'): (0.7217349326613101, 0.939298102428076, 0.0158187282833407, 0.045908998163640066), ('1990.0', 'M1'): (0.7217349326613101, 0.9881656804733727, 0.0158187282833407, 0.0), ('1990.0', 'DA'): (0.7217349326613101, 0.9911242603550295, 0.0158187282833407, 0.0), ('1990.0', 'TYF'): (0.7217349326613101, 0.9940828402366865, 0.0158187282833407, 0.0), ('1990.0', 'CXF'): (0.7217349326613101, 0.9970414201183432, 0.0158187282833407, 0.0), ('1990.0', 'CXU'): (0.7217349326613101, 1.0, 0.0158187282833407, 0.0), ('2000.0', 'BN'): (0.7422706420767262, 0.0, 0.009818521003452865, 0.0), ('2000.0', 'CA'): (0.7422706420767262, 0.0029585798816568047, 0.009818521003452865, 0.0), ('2000.0', 'RXU'): (0.7422706420767262, 0.005917159763313609, 0.009818521003452865, 0.0), ('2000.0', 'D5'): (0.7422706420767262, 0.008875739644970414, 0.009818521003452865, 0.036982248520710054), ('2000.0', 'C2'): (0.7422706420767262, 0.04881656804733728, 0.009818521003452865, 0.0), ('2000.0', 'R2'): (0.7422706420767262, 0.051775147928994084, 0.009818521003452865, 0.0), ('2000.0', 'RXF'): (0.7422706420767262, 0.05473372781065088, 0.009818521003452865, 0.0), ('2000.0', 'R1'): (0.7422706420767262, 0.057692307692307696, 0.009818521003452865, 0.036982248520710054), ('2000.0', 'CB'): (0.7422706420767262, 0.09763313609467456, 0.009818521003452865, 0.0), ('2000.0', 'RYU'): (0.7422706420767262, 0.10059171597633135, 0.009818521003452865, 0.0), ('2000.0', 'CYF'): (0.7422706420767262, 0.10355029585798817, 0.009818521003452865, 0.0), ('2000.0', 'RA'): (0.7422706420767262, 0.10650887573964496, 0.009818521003452865, 0.04930966469428009), ('2000.0', 'CYU'): (0.7422706420767262, 0.15877712031558186, 0.009818521003452865, 0.0), ('2000.0', 'RYF'): (0.7422706420767262, 0.16173570019723865, 0.009818521003452865, 0.0), ('2000.0', 'C1'): (0.7422706420767262, 0.16469428007889547, 0.009818521003452865, 0.0), ('2000.0', 'D3'): (0.7422706420767262, 0.16765285996055226, 0.009818521003452865, 0.024654832347140027), ('2000.0', 'HC'): (0.7422706420767262, 0.1952662721893491, 0.009818521003452865, 0.0), ('2000.0', 'RB'): (0.7422706420767262, 0.1982248520710059, 0.009818521003452865, 0.0), ('2000.0', 'D4'): (0.7422706420767262, 0.2011834319526627, 0.009818521003452865, 0.012327416173570025), ('2000.0', 'T2'): (0.7422706420767262, 0.21646942800788954, 0.009818521003452865, 0.0), ('2000.0', 'D2'): (0.7422706420767262, 0.21942800788954636, 0.009818521003452865, 0.0), ('2000.0', 'BL'): (0.7422706420767262, 0.22238658777120313, 0.009818521003452865, 0.012327416173570025), ('2000.0', 'RS'): (0.7422706420767262, 0.23767258382642997, 0.009818521003452865, 0.0), ('2000.0', 'RR'): (0.7422706420767262, 0.2406311637080868, 0.009818521003452865, 0.012327416173570025), ('2000.0', 'T7'): (0.7422706420767262, 0.2559171597633136, 0.009818521003452865, 0.03698224852071005), ('2000.0', 'TA'): (0.7422706420767262, 0.2958579881656805, 0.009818521003452865, 0.3944773175542406), ('2000.0', 'T1'): (0.7422706420767262, 0.6932938856015779, 0.009818521003452865, 0.27120315581854043), ('2000.0', 'TS'): (0.7422706420767262, 0.9674556213017752, 0.009818521003452865, 0.0), ('2000.0', 'T3'): (0.7422706420767262, 0.9704142011834319, 0.009818521003452865, 0.0), ('2000.0', 'T5'): (0.7422706420767262, 0.9733727810650887, 0.009818521003452865, 0.0), ('2000.0', 'T6'): (0.7422706420767262, 0.9763313609467456, 0.009818521003452865, 0.0), ('2000.0', 'T4'): (0.7422706420767262, 0.9792899408284023, 0.009818521003452865, 0.0), ('2000.0', 'D1'): (0.7422706420767262, 0.9822485207100592, 0.009818521003452865, 0.0), ('2000.0', 'D6'): (0.7422706420767262, 0.985207100591716, 0.009818521003452865, 0.0), ('2000.0', 'M1'): (0.7422706420767262, 0.9881656804733727, 0.009818521003452865, 0.0), ('2000.0', 'DA'): (0.7422706420767262, 0.9911242603550295, 0.009818521003452865, 0.0), ('2000.0', 'TYF'): (0.7422706420767262, 0.9940828402366865, 0.009818521003452865, 0.0), ('2000.0', 'CXF'): (0.7422706420767262, 0.9970414201183432, 0.009818521003452865, 0.0), ('2000.0', 'CXU'): (0.7422706420767262, 1.0, 0.009818521003452865, 0.0), ('1940.0', 'BN'): (0.7568061442122546, 0.0, 0.08823032068380544, 0.01920561901539193), ('1940.0', 'CA'): (0.7568061442122546, 0.022164198897048737, 0.08823032068380544, 0.00548731971868341), ('1940.0', 'RXU'): (0.7568061442122546, 0.03061009849738895, 0.08823032068380544, 0.02469293873407534), ('1940.0', 'D5'): (0.7568061442122546, 0.05826161711312109, 0.08823032068380544, 0.012346469367037666), ('1940.0', 'C2'): (0.7568061442122546, 0.07356666636181557, 0.08823032068380544, 0.00137182992967086), ('1940.0', 'R2'): (0.7568061442122546, 0.07789707617314323, 0.08823032068380544, 0.026064768663746195), ('1940.0', 'RXF'): (0.7568061442122546, 0.10692042471854624, 0.08823032068380544, 0.06173234683518835), ('1940.0', 'R1'): (0.7568061442122546, 0.17161135143539138, 0.08823032068380544, 0.5240390331342655), ('1940.0', 'CB'): (0.7568061442122546, 0.6986089644513137, 0.08823032068380544, 0.0), ('1940.0', 'RYU'): (0.7568061442122546, 0.7015675443329705, 0.08823032068380544, 0.0027436598593417448), ('1940.0', 'CYF'): (0.7568061442122546, 0.707269784073969, 0.08823032068380544, 0.0), ('1940.0', 'RA'): (0.7568061442122546, 0.7102283639556258, 0.08823032068380544, 0.157760441912148), ('1940.0', 'CYU'): (0.7568061442122546, 0.8709473857494308, 0.08823032068380544, 0.0), ('1940.0', 'RYF'): (0.7568061442122546, 0.8739059656310875, 0.08823032068380544, 0.0013718299296708232), ('1940.0', 'C1'): (0.7568061442122546, 0.8782363754424151, 0.08823032068380544, 0.0013718299296708232), ('1940.0', 'D3'): (0.7568061442122546, 0.8825667852537428, 0.08823032068380544, 0.0), ('1940.0', 'HC'): (0.7568061442122546, 0.8855253651353996, 0.08823032068380544, 0.0), ('1940.0', 'RB'): (0.7568061442122546, 0.8884839450170564, 0.08823032068380544, 0.0013718299296709216), ('1940.0', 'D4'): (0.7568061442122546, 0.892814354828384, 0.08823032068380544, 0.03978306796045466), ('1940.0', 'T2'): (0.7568061442122546, 0.9355560026704954, 0.08823032068380544, 0.0), ('1940.0', 'D2'): (0.7568061442122546, 0.9385145825521524, 0.08823032068380544, 0.0), ('1940.0', 'BL'): (0.7568061442122546, 0.9414731624338092, 0.08823032068380544, 0.0), ('1940.0', 'RS'): (0.7568061442122546, 0.9444317423154659, 0.08823032068380544, 0.0054873197186833915), ('1940.0', 'RR'): (0.7568061442122546, 0.9528776419158063, 0.08823032068380544, 0.0013718299296709216), ('1940.0', 'T7'): (0.7568061442122546, 0.9572080517271339, 0.08823032068380544, 0.0), ('1940.0', 'TA'): (0.7568061442122546, 0.9601666316087907, 0.08823032068380544, 0.0), ('1940.0', 'T1'): (0.7568061442122546, 0.9631252114904476, 0.08823032068380544, 0.0), ('1940.0', 'TS'): (0.7568061442122546, 0.9660837913721043, 0.08823032068380544, 0.0), ('1940.0', 'T3'): (0.7568061442122546, 0.9690423712537611, 0.08823032068380544, 0.0), ('1940.0', 'T5'): (0.7568061442122546, 0.9720009511354178, 0.08823032068380544, 0.0), ('1940.0', 'T6'): (0.7568061442122546, 0.9749595310170747, 0.08823032068380544, 0.0), ('1940.0', 'T4'): (0.7568061442122546, 0.9779181108987316, 0.08823032068380544, 0.0), ('1940.0', 'D1'): (0.7568061442122546, 0.9808766907803882, 0.08823032068380544, 0.0), ('1940.0', 'D6'): (0.7568061442122546, 0.9838352706620451, 0.08823032068380544, 0.0), ('1940.0', 'M1'): (0.7568061442122546, 0.986793850543702, 0.08823032068380544, 0.0), ('1940.0', 'DA'): (0.7568061442122546, 0.9897524304253587, 0.08823032068380544, 0.0), ('1940.0', 'TYF'): (0.7568061442122546, 0.9927110103070156, 0.08823032068380544, 0.0), ('1940.0', 'CXF'): (0.7568061442122546, 0.9956695901886724, 0.08823032068380544, 0.0013718299296708232), ('1940.0', 'CXU'): (0.7568061442122546, 1.0, 0.08823032068380544, 0.0), ('2010.0', 'BN'): (0.8497534460281355, 0.0, 0.0032728403344842536, 0.0), ('2010.0', 'CA'): (0.8497534460281355, 0.0029585798816568047, 0.0032728403344842536, 0.0), ('2010.0', 'RXU'): (0.8497534460281355, 0.005917159763313609, 0.0032728403344842536, 0.0), ('2010.0', 'D5'): (0.8497534460281355, 0.008875739644970414, 0.0032728403344842536, 0.0), ('2010.0', 'C2'): (0.8497534460281355, 0.011834319526627219, 0.0032728403344842536, 0.0), ('2010.0', 'R2'): (0.8497534460281355, 0.014792899408284023, 0.0032728403344842536, 0.07396449704142011), ('2010.0', 'RXF'): (0.8497534460281355, 0.09171597633136094, 0.0032728403344842536, 0.0), ('2010.0', 'R1'): (0.8497534460281355, 0.09467455621301774, 0.0032728403344842536, 0.3698224852071006), ('2010.0', 'CB'): (0.8497534460281355, 0.4674556213017751, 0.0032728403344842536, 0.0), ('2010.0', 'RYU'): (0.8497534460281355, 0.47041420118343197, 0.0032728403344842536, 0.0), ('2010.0', 'CYF'): (0.8497534460281355, 0.47337278106508873, 0.0032728403344842536, 0.0), ('2010.0', 'RA'): (0.8497534460281355, 0.4763313609467455, 0.0032728403344842536, 0.11094674556213018), ('2010.0', 'CYU'): (0.8497534460281355, 0.5902366863905325, 0.0032728403344842536, 0.0), ('2010.0', 'RYF'): (0.8497534460281355, 0.5931952662721893, 0.0032728403344842536, 0.0), ('2010.0', 'C1'): (0.8497534460281355, 0.5961538461538461, 0.0032728403344842536, 0.0), ('2010.0', 'D3'): (0.8497534460281355, 0.599112426035503, 0.0032728403344842536, 0.0), ('2010.0', 'HC'): (0.8497534460281355, 0.6020710059171598, 0.0032728403344842536, 0.0), ('2010.0', 'RB'): (0.8497534460281355, 0.6050295857988165, 0.0032728403344842536, 0.0), ('2010.0', 'D4'): (0.8497534460281355, 0.6079881656804734, 0.0032728403344842536, 0.07396449704142015), ('2010.0', 'T2'): (0.8497534460281355, 0.6849112426035503, 0.0032728403344842536, 0.22189349112426035), ('2010.0', 'D2'): (0.8497534460281355, 0.9097633136094675, 0.0032728403344842536, 0.0), ('2010.0', 'BL'): (0.8497534460281355, 0.9127218934911242, 0.0032728403344842536, 0.0), ('2010.0', 'RS'): (0.8497534460281355, 0.9156804733727811, 0.0032728403344842536, 0.0), ('2010.0', 'RR'): (0.8497534460281355, 0.918639053254438, 0.0032728403344842536, 0.0), ('2010.0', 'T7'): (0.8497534460281355, 0.9215976331360947, 0.0032728403344842536, 0.0), ('2010.0', 'TA'): (0.8497534460281355, 0.9245562130177515, 0.0032728403344842536, 0.0), ('2010.0', 'T1'): (0.8497534460281355, 0.9275147928994082, 0.0032728403344842536, 0.0), ('2010.0', 'TS'): (0.8497534460281355, 0.930473372781065, 0.0032728403344842536, 0.0), ('2010.0', 'T3'): (0.8497534460281355, 0.9334319526627219, 0.0032728403344842536, 0.0), ('2010.0', 'T5'): (0.8497534460281355, 0.9363905325443788, 0.0032728403344842536, 0.0), ('2010.0', 'T6'): (0.8497534460281355, 0.9393491124260355, 0.0032728403344842536, 0.0), ('2010.0', 'T4'): (0.8497534460281355, 0.9423076923076924, 0.0032728403344842536, 0.036982248520710026), ('2010.0', 'D1'): (0.8497534460281355, 0.9822485207100592, 0.0032728403344842536, 0.0), ('2010.0', 'D6'): (0.8497534460281355, 0.985207100591716, 0.0032728403344842536, 0.0), ('2010.0', 'M1'): (0.8497534460281355, 0.9881656804733727, 0.0032728403344842536, 0.0), ('2010.0', 'DA'): (0.8497534460281355, 0.9911242603550295, 0.0032728403344842536, 0.0), ('2010.0', 'TYF'): (0.8497534460281355, 0.9940828402366865, 0.0032728403344842536, 0.0), ('2010.0', 'CXF'): (0.8497534460281355, 0.9970414201183432, 0.0032728403344842536, 0.0), ('2010.0', 'CXU'): (0.8497534460281355, 1.0, 0.0032728403344842536, 0.0), ('1980.0', 'BN'): (0.8577432674946952, 0.0, 0.07500259099859818, 0.0), ('1980.0', 'CA'): (0.8577432674946952, 0.0029585798816568047, 0.07500259099859818, 0.0), ('1980.0', 'RXU'): (0.8577432674946952, 0.005917159763313609, 0.07500259099859818, 0.0), ('1980.0', 'D5'): (0.8577432674946952, 0.008875739644970414, 0.07500259099859818, 0.0016137708445400753), ('1980.0', 'C2'): (0.8577432674946952, 0.013448090371167294, 0.07500259099859818, 0.0), ('1980.0', 'R2'): (0.8577432674946952, 0.0164066702528241, 0.07500259099859818, 0.2630446476600322), ('1980.0', 'RXF'): (0.8577432674946952, 0.28240989779451314, 0.07500259099859818, 0.0), ('1980.0', 'R1'): (0.8577432674946952, 0.28536847767616996, 0.07500259099859818, 0.0839160839160839), ('1980.0', 'CB'): (0.8577432674946952, 0.3722431414739107, 0.07500259099859818, 0.0), ('1980.0', 'RYU'): (0.8577432674946952, 0.3752017213555675, 0.07500259099859818, 0.0032275416890801714), ('1980.0', 'CYF'): (0.8577432674946952, 0.38138784292630445, 0.07500259099859818, 0.0), ('1980.0', 'RA'): (0.8577432674946952, 0.3843464228079613, 0.07500259099859818, 0.04195804195804198), ('1980.0', 'CYU'): (0.8577432674946952, 0.42926304464766, 0.07500259099859818, 0.0), ('1980.0', 'RYF'): (0.8577432674946952, 0.43222162452931684, 0.07500259099859818, 0.0), ('1980.0', 'C1'): (0.8577432674946952, 0.43518020441097366, 0.07500259099859818, 0.0016137708445400365), ('1980.0', 'D3'): (0.8577432674946952, 0.4397525551371705, 0.07500259099859818, 0.0032275416890801714), ('1980.0', 'HC'): (0.8577432674946952, 0.4459386767079075, 0.07500259099859818, 0.06616460462614311), ('1980.0', 'RB'): (0.8577432674946952, 0.5150618612157074, 0.07500259099859818, 0.0), ('1980.0', 'D4'): (0.8577432674946952, 0.5180204410973642, 0.07500259099859818, 0.008068854222700329), ('1980.0', 'T2'): (0.8577432674946952, 0.5290478752017214, 0.07500259099859818, 0.004841312533620257), ('1980.0', 'D2'): (0.8577432674946952, 0.5368477676169984, 0.07500259099859818, 0.0016137708445400857), ('1980.0', 'BL'): (0.8577432674946952, 0.5414201183431954, 0.07500259099859818, 0.0), ('1980.0', 'RS'): (0.8577432674946952, 0.5443786982248521, 0.07500259099859818, 0.012910166756320587), ('1980.0', 'RR'): (0.8577432674946952, 0.5602474448628295, 0.07500259099859818, 0.0), ('1980.0', 'T7'): (0.8577432674946952, 0.5632060247444862, 0.07500259099859818, 0.008068854222700329), ('1980.0', 'TA'): (0.8577432674946952, 0.5742334588488435, 0.07500259099859818, 0.11941904249596565), ('1980.0', 'T1'): (0.8577432674946952, 0.6966110812264659, 0.07500259099859818, 0.20010758472296925), ('1980.0', 'TS'): (0.8577432674946952, 0.899677245831092, 0.07500259099859818, 0.004841312533620257), ('1980.0', 'T3'): (0.8577432674946952, 0.907477138246369, 0.07500259099859818, 0.01613770844540076), ('1980.0', 'T5'): (0.8577432674946952, 0.9265734265734266, 0.07500259099859818, 0.012910166756320587), ('1980.0', 'T6'): (0.8577432674946952, 0.9424421732114039, 0.07500259099859818, 0.006455083378160343), ('1980.0', 'T4'): (0.8577432674946952, 0.9518558364712211, 0.07500259099859818, 0.025820333512641173), ('1980.0', 'D1'): (0.8577432674946952, 0.980634749865519, 0.07500259099859818, 0.0), ('1980.0', 'D6'): (0.8577432674946952, 0.9835933297471758, 0.07500259099859818, 0.0), ('1980.0', 'M1'): (0.8577432674946952, 0.9865519096288328, 0.07500259099859818, 0.0), ('1980.0', 'DA'): (0.8577432674946952, 0.9895104895104894, 0.07500259099859818, 0.0), ('1980.0', 'TYF'): (0.8577432674946952, 0.9924690693921463, 0.07500259099859818, 0.0016137708445400857), ('1980.0', 'CXF'): (0.8577432674946952, 0.9970414201183432, 0.07500259099859818, 0.0), ('1980.0', 'CXU'): (0.8577432674946952, 1.0, 0.07500259099859818, 0.0), ('1930.0', 'BN'): (0.9374628396253689, 0.0, 0.0564564957698539, 0.37732612983449104), ('1930.0', 'CA'): (0.9374628396253689, 0.38028470971614786, 0.0564564957698539, 0.010719492324843493), ('1930.0', 'RXU'): (0.9374628396253689, 0.3939627819226481, 0.0564564957698539, 0.025726781579624405), ('1930.0', 'D5'): (0.9374628396253689, 0.42264814338392936, 0.0564564957698539, 0.021438984649686986), ('1930.0', 'C2'): (0.9374628396253689, 0.44704570791527315, 0.0564564957698539, 0.0021438984649687085), ('1930.0', 'R2'): (0.9374628396253689, 0.4521481862618986, 0.0564564957698539, 0.09004373552868532), ('1930.0', 'RXF'): (0.9374628396253689, 0.5451505016722408, 0.0564564957698539, 0.11362661864334106), ('1930.0', 'R1'): (0.9374628396253689, 0.6617357001972386, 0.0564564957698539, 0.01715118771974957), ('1930.0', 'CB'): (0.9374628396253689, 0.681845467798645, 0.0564564957698539, 0.0), ('1930.0', 'RYU'): (0.9374628396253689, 0.6848040476803018, 0.0564564957698539, 0.01500728925478096), ('1930.0', 'CYF'): (0.9374628396253689, 0.7027699168167396, 0.0564564957698539, 0.0), ('1930.0', 'RA'): (0.9374628396253689, 0.7057284966983963, 0.0564564957698539, 0.18651916645227676), ('1930.0', 'CYU'): (0.9374628396253689, 0.8952062430323299, 0.0564564957698539, 0.004287796929937417), ('1930.0', 'RYF'): (0.9374628396253689, 0.9024526198439241, 0.0564564957698539, 0.0), ('1930.0', 'C1'): (0.9374628396253689, 0.905411199725581, 0.0564564957698539, 0.0), ('1930.0', 'D3'): (0.9374628396253689, 0.9083697796072377, 0.0564564957698539, 0.0), ('1930.0', 'HC'): (0.9374628396253689, 0.9113283594888946, 0.0564564957698539, 0.0), ('1930.0', 'RB'): (0.9374628396253689, 0.9142869393705515, 0.0564564957698539, 0.010719492324843543), ('1930.0', 'D4'): (0.9374628396253689, 0.9279650115770517, 0.0564564957698539, 0.0064316953949060274), ('1930.0', 'T2'): (0.9374628396253689, 0.9373552868536145, 0.0564564957698539, 0.0), ('1930.0', 'D2'): (0.9374628396253689, 0.9403138667352714, 0.0564564957698539, 0.0), ('1930.0', 'BL'): (0.9374628396253689, 0.9432724466169282, 0.0564564957698539, 0.0), ('1930.0', 'RS'): (0.9374628396253689, 0.9462310264985849, 0.0564564957698539, 0.0), ('1930.0', 'RR'): (0.9374628396253689, 0.9491896063802417, 0.0564564957698539, 0.0), ('1930.0', 'T7'): (0.9374628396253689, 0.9521481862618987, 0.0564564957698539, 0.0), ('1930.0', 'TA'): (0.9374628396253689, 0.9551067661435553, 0.0564564957698539, 0.0), ('1930.0', 'T1'): (0.9374628396253689, 0.9580653460252122, 0.0564564957698539, 0.0), ('1930.0', 'TS'): (0.9374628396253689, 0.961023925906869, 0.0564564957698539, 0.0), ('1930.0', 'T3'): (0.9374628396253689, 0.9639825057885257, 0.0564564957698539, 0.0), ('1930.0', 'T5'): (0.9374628396253689, 0.9669410856701826, 0.0564564957698539, 0.0), ('1930.0', 'T6'): (0.9374628396253689, 0.9698996655518395, 0.0564564957698539, 0.0), ('1930.0', 'T4'): (0.9374628396253689, 0.9728582454334962, 0.0564564957698539, 0.0), ('1930.0', 'D1'): (0.9374628396253689, 0.975816825315153, 0.0564564957698539, 0.0), ('1930.0', 'D6'): (0.9374628396253689, 0.9787754051968099, 0.0564564957698539, 0.0), ('1930.0', 'M1'): (0.9374628396253689, 0.9817339850784665, 0.0564564957698539, 0.0021438984649687085), ('1930.0', 'DA'): (0.9374628396253689, 0.9868364634250921, 0.0564564957698539, 0.0), ('1930.0', 'TYF'): (0.9374628396253689, 0.9897950433067491, 0.0564564957698539, 0.0), ('1930.0', 'CXF'): (0.9374628396253689, 0.9927536231884058, 0.0564564957698539, 0.0), ('1930.0', 'CXU'): (0.9374628396253689, 0.9957122030700626, 0.0564564957698539, 0.004287796929937417), ('2020.0', 'BN'): (0.9986363165272981, 0.0, 0.0013636834727017374, 0.0), ('2020.0', 'CA'): (0.9986363165272981, 0.0029585798816568047, 0.0013636834727017374, 0.0), ('2020.0', 'RXU'): (0.9986363165272981, 0.005917159763313609, 0.0013636834727017374, 0.0), ('2020.0', 'D5'): (0.9986363165272981, 0.008875739644970414, 0.0013636834727017374, 0.0), ('2020.0', 'C2'): (0.9986363165272981, 0.011834319526627219, 0.0013636834727017374, 0.0), ('2020.0', 'R2'): (0.9986363165272981, 0.014792899408284023, 0.0013636834727017374, 0.0), ('2020.0', 'RXF'): (0.9986363165272981, 0.01775147928994083, 0.0013636834727017374, 0.0), ('2020.0', 'R1'): (0.9986363165272981, 0.020710059171597635, 0.0013636834727017374, 0.8875739644970414), ('2020.0', 'CB'): (0.9986363165272981, 0.9112426035502957, 0.0013636834727017374, 0.0), ('2020.0', 'RYU'): (0.9986363165272981, 0.9142011834319527, 0.0013636834727017374, 0.0), ('2020.0', 'CYF'): (0.9986363165272981, 0.9171597633136095, 0.0013636834727017374, 0.0), ('2020.0', 'RA'): (0.9986363165272981, 0.9201183431952662, 0.0013636834727017374, 0.0), ('2020.0', 'CYU'): (0.9986363165272981, 0.9230769230769231, 0.0013636834727017374, 0.0), ('2020.0', 'RYF'): (0.9986363165272981, 0.92603550295858, 0.0013636834727017374, 0.0), ('2020.0', 'C1'): (0.9986363165272981, 0.9289940828402367, 0.0013636834727017374, 0.0), ('2020.0', 'D3'): (0.9986363165272981, 0.9319526627218935, 0.0013636834727017374, 0.0), ('2020.0', 'HC'): (0.9986363165272981, 0.9349112426035502, 0.0013636834727017374, 0.0), ('2020.0', 'RB'): (0.9986363165272981, 0.937869822485207, 0.0013636834727017374, 0.0), ('2020.0', 'D4'): (0.9986363165272981, 0.9408284023668639, 0.0013636834727017374, 0.0), ('2020.0', 'T2'): (0.9986363165272981, 0.9437869822485206, 0.0013636834727017374, 0.0), ('2020.0', 'D2'): (0.9986363165272981, 0.9467455621301775, 0.0013636834727017374, 0.0), ('2020.0', 'BL'): (0.9986363165272981, 0.9497041420118344, 0.0013636834727017374, 0.0), ('2020.0', 'RS'): (0.9986363165272981, 0.952662721893491, 0.0013636834727017374, 0.0), ('2020.0', 'RR'): (0.9986363165272981, 0.9556213017751479, 0.0013636834727017374, 0.0), ('2020.0', 'T7'): (0.9986363165272981, 0.9585798816568047, 0.0013636834727017374, 0.0), ('2020.0', 'TA'): (0.9986363165272981, 0.9615384615384615, 0.0013636834727017374, 0.0), ('2020.0', 'T1'): (0.9986363165272981, 0.9644970414201183, 0.0013636834727017374, 0.0), ('2020.0', 'TS'): (0.9986363165272981, 0.9674556213017752, 0.0013636834727017374, 0.0), ('2020.0', 'T3'): (0.9986363165272981, 0.9704142011834319, 0.0013636834727017374, 0.0), ('2020.0', 'T5'): (0.9986363165272981, 0.9733727810650887, 0.0013636834727017374, 0.0), ('2020.0', 'T6'): (0.9986363165272981, 0.9763313609467456, 0.0013636834727017374, 0.0), ('2020.0', 'T4'): (0.9986363165272981, 0.9792899408284023, 0.0013636834727017374, 0.0), ('2020.0', 'D1'): (0.9986363165272981, 0.9822485207100592, 0.0013636834727017374, 0.0), ('2020.0', 'D6'): (0.9986363165272981, 0.985207100591716, 0.0013636834727017374, 0.0), ('2020.0', 'M1'): (0.9986363165272981, 0.9881656804733727, 0.0013636834727017374, 0.0), ('2020.0', 'DA'): (0.9986363165272981, 0.9911242603550295, 0.0013636834727017374, 0.0), ('2020.0', 'TYF'): (0.9986363165272981, 0.9940828402366865, 0.0013636834727017374, 0.0), ('2020.0', 'CXF'): (0.9986363165272981, 0.9970414201183432, 0.0013636834727017374, 0.0), ('2020.0', 'CXU'): (0.9986363165272981, 1.0, 0.0013636834727017374, 0.0)})\nplt.show()\n\n\n\n\nWe can also look at the square footage for each improvement type:\n\nhousing_subcat = [\"BN\", \"R1\", \"RA\", \"R2\"]\n\nhousing_sub = housing_lincoln.loc[housing_lincoln[\"Imp_Type\"].isin(housing_subcat)]\nhousing_sub = housing_sub.assign(Imp_cat = pd.Categorical(housing_sub[\"Imp_Type\"], categories = housing_subcat))\n\nhousing_sub.boxplot(\"TLA\", by = \"Imp_cat\")\nplt.show()\n\n\n\n\nThis makes sense - there are relatively few bungalows (BN), but R1 means 1 story house, R2 means 2 story house, and RA is a so-called 1.5 story house; we would expect an increase in square footage with each additional floor of the house (broadly speaking).\n\n\n\n\n\n\n\n\n\n\n\nLearn More: Janitor R package\n\n\n\nThe janitor package [4] has some very convenient functions for cleaning up messy data. One of its best features is the clean_names() function, which creates names based on a capitalization/separation scheme of your choosing.\n\n\njanitor and clean_names() by Allison Horst"
  },
  {
    "objectID": "part-wrangling/02a-eda.html#references",
    "href": "part-wrangling/02a-eda.html#references",
    "title": "17  Exploratory Data Analysis",
    "section": "\n17.5 References",
    "text": "17.5 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nN. Tierney, D. Cook, M. McBain, and C. Fay, Naniar: Data structures, summaries, and visualisations for missing data. 2021 [Online]. Available: https://CRAN.R-project.org/package=naniar\n\n\n\n[3] \nDaniel Bourke, “A Gentle Introduction to Exploratory Data Analysis,” Daniel Bourke. Jan. 2019 [Online]. Available: https://www.mrdbourke.com/a-gentle-introduction-to-exploratory-data-analysis/. [Accessed: Jun. 13, 2022]\n\n\n[4] \nS. Firke, Janitor: Simple tools for examining and cleaning dirty data. 2021 [Online]. Available: https://CRAN.R-project.org/package=janitor"
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#fa-bullseye-objectives",
    "href": "part-wrangling/02b-graphics.html#fa-bullseye-objectives",
    "title": "18  Data Visualization",
    "section": "\n18.1  Objectives",
    "text": "18.1  Objectives\n\nCreate charts designed to communicate specific aspects of the data\nDescribe charts using the grammar of graphics\nCreate layered graphics that highlight multiple aspects of the data"
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#introduction",
    "href": "part-wrangling/02b-graphics.html#introduction",
    "title": "18  Data Visualization",
    "section": "\n18.2 Introduction",
    "text": "18.2 Introduction\nThere are a lot of different types of charts, and equally many ways to categorize and describe the different types of charts. I’m going to be opinionated on this one - while I will provide code for several different plotting programs, this chapter is organized based on the grammar of graphics specifically.\n\n\nVisualization and statistical graphics are also my research area, so I’m more passionate about this material, which means there’s going to be more to read. Sorry about that in advance. I’ll do my best to indicate which content is actually mission-critical and which content you can skip if you’re not that interested.\nThis is going to be a fairly extensive chapter (in terms of content) because I want you to have a resource to access later, if you need it. That’s why I’m showing you code for many different plotting libraries - I want you to be able to make charts in any program you may need to use for your research.\n\n\n\n\n\n\nGuides and Resources\n\n\n\n\n\nGraph galleries contain sample code to create many different types of charts. Similar galaries are available in R and Python.\nCheat Sheets:\n\nPython\nGgplot2\nBase R\n\nYoutube Playlist: a bunch of different “how to plot” tutorials on YouTube I found helpful\n\n\n\n\n\n\n\n\n18.2.1 Installing Graphics Libraries\nThis chapter will cover the following graphics libraries:\n\n\nggplot2 in R [1]\n\n\nplotnine in python [2], which is a clone of ggplot2\n\n\nseaborn in python [3], with some information about its next-generation interface that is much closer to a grammar-of-graphics implementation [4]\n\n\nTo a lesser degree, we will also cover some details of more basic plotting libraries:\n\n\nmatplotlib in python [5]\n\nBase R plotting libraries [6]\n\n\nTo install the relevant python libraries, run the following code in your system terminal:\n\npip3 install matplotlib plotnine seaborn\n\nTo install ggplot2, run the following in R:\n\ninstall.packages(\"ggplot2\")"
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#why-do-we-create-graphics",
    "href": "part-wrangling/02b-graphics.html#why-do-we-create-graphics",
    "title": "18  Data Visualization",
    "section": "\n18.3 Why do we create graphics?",
    "text": "18.3 Why do we create graphics?\n\nThe greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (“Why, that scatter diagram has a hole in the middle!”) – John W. Tukey [7]\n\nFundamentally, charts are easier to understand than raw data.\nWhen you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the “real world” into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is “tangible” in some way – if not physical and literally touch-able, at least something we can view and “wrap our heads around”.\n\n\n\n\n\n\nThought Experiment\n\n\n\nYou have a simple data set - 2 variables, about 150 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options:\n\nPrint out the data set\nCreate some summary statistics of each variable and perhaps the covariance between the two variables\nDraw a scatter plot of the two variables\n\nWhich one would you rather use? Why?\n\n\nDataset\nSummary statistics\nPlot\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur brains are very good at processing large amounts of visual information quickly. Evolution is good at optimizing for survival, and it’s important to be able to survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we’re not nearly as equipped to process quickly (and in parallel).\nIn addition, it’s a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out."
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#thinking-critically-about-graphics",
    "href": "part-wrangling/02b-graphics.html#thinking-critically-about-graphics",
    "title": "18  Data Visualization",
    "section": "\n18.4 Thinking Critically About Graphics",
    "text": "18.4 Thinking Critically About Graphics\nWhen we create graphics, we want to enable people to think about relationships between the variables in our dataset.\nFor this example, let’s consider the relationship between different physical measurements of penguins. We’ll use the palmerpenguins package in R, which is also available in python.\n\n\n\n\n\n\nLoad (and examine) the data\n\n\n\n\n\nR\nPython\n\n\n\n\n## install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nhead(penguins)\n## # A tibble: 6 × 8\n##   species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n##   <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n## 1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n## 2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n## 3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n## 4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n## 5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n## 6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n## # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\nIn your system console:\n\npip3 install palmerpenguins\n\nIn Python:\n\nfrom palmerpenguins import load_penguins\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'palmerpenguins'\npenguins = load_penguins()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'load_penguins' is not defined\npenguins.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'penguins' is not defined\n\n\n\n\n\n\nHow do physical measurements (bill width, bill depth, flipper length, and body mass) relate to each other?\n\n\n\n\n\n\nInitial plots\n\n\n\n\n\nR - ggplot2\nPy - plotnine\nPy - Seaborn\nPy - Seaborn Objects\nR base\nPy - Matplotlib\n\n\n\n\nlibrary(ggplot2)\nggplot(data = penguins, aes(x = body_mass_g, y = bill_length_mm)) + geom_point()\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\nggplot(data = penguins, aes(x = body_mass_g, y = bill_length_mm, color = species)) + geom_point()\n\n\n\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in ggplot2 is mapped to a plot feature using the aes() (aesthetic) function. This function automatically constructs a scale that e.g. converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in red, Chinstrap in green, and Gentoo in blue.\nThat is, ggplot2 allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\n\n\n\nfrom plotnine import ggplot,aes,geom_point\nggplot(data = penguins, \n       mapping = aes(x = \"body_mass_g\", y = \"bill_length_mm\")) +\\\n  geom_point()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'penguins' is not defined\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWith ggplot2/plotnine, you begin to construct a plot by identifying the primary relationship of interest: you define what variables go on each axis (body mass on x, bill length on y) and then create a layer that shows the actual data (geom_point). Note that the mapping argument must be explicitly labeled in python because of some differences in requirements for function arguments between R and python. In addition, when in python, the variable names must be passed in as strings.\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\nggplot(data = penguins, \n       mapping = aes(x = \"body_mass_g\", y = \"bill_length_mm\", color = \"species\")) +\\\n  geom_point()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'penguins' is not defined\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in ggplot2/plotnine is mapped to a plot feature using the aes() (aesthetic) function. This function automatically constructs a scale that e.g. converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in red, Chinstrap in green, and Gentoo in blue. Notice that the shades are slightly different, as plotnine uses matplotlib as a base plotting library, and the default colors are part of that library.\nThat is, ggplot2/plotnine allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\n\n\n\nimport seaborn as sns\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'seaborn'\nimport matplotlib.pyplot\n\nsns.set_theme() # default theme\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'sns' is not defined\nsns.relplot(\n  data = penguins,\n  x = \"body_mass_g\", y = \"bill_length_mm\"\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'sns' is not defined\nmatplotlib.pyplot.show() # include this line to show the plot in quarto\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). In seaborn, this is called a relational plot - that is, it shows the relationship between the variables on the x and y axis. We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWith seaborn, you begin to construct a plot by identifying the type of plot you want - whether it’s a relational plot, like a scatterplot or a lineplot, a distributional plot, like a histogram, density, CDF, or rug plot, or a categorical plot.\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\nsns.relplot(\n  data = penguins,\n  x = \"body_mass_g\", y = \"bill_length_mm\", hue = \"species\"\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'sns' is not defined\nmatplotlib.pyplot.show() # include this line to show the plot in quarto\n\n\n\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in a seaborn relational plot is mapped to a plot feature. This function automatically constructs a scale that e.g. converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in blue, Chinstrap in green, and Gentoo in red.\nAs with ggplot2 and plotnine, seaborn allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\n\n\nIn Version 0.12, Seaborn introduced a new interface, the objects interface, that is much more grammar-of-graphics like than the default Seaborn interface.\n\nimport seaborn.objects as so\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'seaborn'\n(\n  so.Plot(penguins, x = \"body_mass_g\", y = \"bill_length_mm\")\n  .add(so.Dot())\n  .show()\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'so' is not defined\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWith seaborn objects interface, you begin to construct a plot by identifying the primary relationship of interest: you define what variables go on each axis (body mass on x, bill length on y) and then create a layer that shows the actual data (so.Dot()).\nWe can extend this process a bit by adding in additional information from the dataset and mapping that information to an additional variable - for instance, we can color the points by the species of penguin.\n\n(\n  so.Plot(penguins, x = \"body_mass_g\", y = \"bill_length_mm\", color = \"species\")\n  .add(so.Dot())\n  .show()\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'so' is not defined\n\nAdding in this additional information allows us to see that chinstrap penguins are the cluster we noticed in the previous plot. Adelie and gentoo penguins have a similar relationship between body mass and bill length, but chinstrap penguins tend to have longer bills and lower body mass.\nEach variable in a seaborn objects plot declaration is mapped to a plot feature. Seaborn then automatically constructs a scale that e.g. converts between body mass and the x axis of the plot, which is measured on a (0,1) scale for plotting. Similarly, when we add a categorical variable and color the points of the plot by that variable, the mapping function automatically decides that Adelie penguins will be plotted in blue, Chinstrap in green, and Gentoo in red.\nAs with ggplot2 and plotnine, seaborn’s objects interface allows the programmer to focus on the relationship between the data and the plot, without having to get into the specifics of how that mapping occurs. This allows the programmer to consider these relationships when constructing the plot, choosing the relationships which are most important for the audience and mapping those variables to important dimensions of the plot, such as the x and y axis.\nYou may notice that this is much more similar to ggplot2 in syntax than the default Seaborn interface.\n\n\n\nplot(penguins$body_mass_g, penguins$bill_length_mm)\n\n\n\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nNotice that in base R, you have to reference each column of the data frame separately, as there is not an included data argument in the plot function.\nWhen we add color to the plot, it gets a little more complicated - to get a legend, we have to know a bit about how base R plots are constructed. When we pass in a factor as a color, R will automatically assign colors to each level of the factor.\n\nplot(penguins$body_mass_g, penguins$bill_length_mm, col = penguins$species)\nlegend(5500, 40,unique(penguins$species),col=1:3,pch=1)\n\n\n\n\nTo create a legend, we need to then identify where the legend should go (x = 5500, y = 40), and then the factor labels (unique(penguins$species)), and then the color levels assigned to those labels (1:3), and finally the point shape (pch = 1).\nThis manual legend creation process is a bit odd if you start from a grammar-of-graphics approach, but was for a long time the only way to make graphics in essentially any plotting system.\nPersonally, I still think the base plotting system creates charts that look a bit … ancient …, but there are some people who very much prefer it to ggplot2 [8].\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots() # Create a new plot\n\nax.scatter(penguins.body_mass_g, penguins.bill_length_mm)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'penguins' is not defined\nplt.show()\n\n\n\nplt.close() # close the figure\n\nHere, we start with a basic plot of bill length (mm) against body mass (g). We can see that there is an overall positive relationship between the two - penguins with higher body mass tend to have longer bills as well. However, it is also clear that there is considerable variation that is not explained by this relationship: there is a group of penguins who seem to have lower body mass and longer bills in the top left quadrant of the plot.\nWhen we add color to the plot, it gets a little more complicated. Matplotlib does not automatically assign factors to colors - we have to instead create a dictionary of colors for each factor label, and then use the .map function to apply our dictionary to our factor variable.\nTo create a legend, we define handles and use the dictionary to create the legend. Then we have to manually position the legend in the lower right of the plot. Note that here, the legend position is defined using bbox_to_anchor in plot coordinates (x = 1, y = 0), and then the orientation of that box is defined using the loc parameter.\n\nfrom matplotlib.lines import Line2D  # for legend handle\nfig, ax = plt.subplots() # Create a new plot\n\n# Define a color mapping\ncolors = {'Adelie':'tab:blue', 'Gentoo':'tab:orange', 'Chinstrap':'tab:green'}\n\nax.scatter(x = penguins.body_mass_g, y = penguins.bill_length_mm, c = penguins.species.map(colors))\n# add a legend\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'penguins' is not defined\nhandles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=v, label=k, markersize=8) for k, v in colors.items()]\nax.legend(title='Species', handles=handles, bbox_to_anchor=(1, 0), loc='lower right')\n\nplt.show()\n\n\n\nplt.close() # close the figure\n\nMatplotlib is prettier than the Base R graphics, but we again have to manually create our legend, which is not ideal. Matplotlib is great for lower-level control over your plot, but that means you have to do a lot of the work manually. Personally, I much prefer to focus my attention on creating the right plot, and let sensible defaults take over whenever possible - this is the idea with both ggplot2 and seaborn objects.\n\n\n\n\n\n\n\n\n\n\n\nA graphing template\n\n\n\nIt would be convenient if we could create a template for each of these libraries that would help us create many different types of plots. Unfortunately, that’s going to be a bit difficult, because there are a number of libraries covered here that are not built on the idea of a grammar of graphics - that is, a set of vocabulary that can define a bunch of different types of charts. In this section, I’ll show you basic templates for charts where such templates exist, and otherwise, I will try to give you a set of functions that may get you started.\n\n\nggplot2/plotnine\nSeaborn\nSeaborn Objects\nR base\nMatplotlib\n\n\n\nThe gg in ggplot2 stands for the grammar of graphics. In ggplot2/plotnine, we can describe any plot using a template that looks like this:\nggplot(data = <DATA>) + \n  <GEOM>(mapping = aes(<MAPPINGS>), \n         position = <POSITION>, \n         stat = <STAT>) + \n  <FACET> + \n  <COORD> + \n  <THEME>\nGeoms are geometric objects, like points (geom_point), lines (geom_line), and rectangles (geom_rect, geom_bar, geom_column). Mappings are relationships between plot characteristics and variables in the dataset - setting x, y, color, fill, size, shape, linetype, and more. Critically, when a visual characteristic, such as color, fill, shape, or size, is a constant, it is set outside of an aes() statement; when it is mapped to a variable, it must be provided inside the aes() statement. Read more about aesthetics\nIn ggplot2/plotnine, statistics can be used within geoms to summarize data. Additional modifications of plots in ggplot2 include changing the coordinate system, adjusting positions, and adding subplots using facets. Not all of these are fully implemented in plotnine, but most features are available in both systems.\n\n\nIn seaborn, we can describe plots using three different basic templates:\n\n\nSeaborn API module organization. Source\n\n\nsns.relplot(data = <DATA>, x = <X var>, y = <Y var>, \n            kind = <PLOT TYPE>, <ADDITIONAL ARGS>)\n\nsns.displot(data = <DATA>, x = <X var>, y = <Y var, optional>, \n            kind = <PLOT TYPE>, <ADDITIONAL ARGS>)\n\nsns.catplot(data = <DATA>, x = <Categorical X var>, y = <Y var>, \n            kind = <PLOT TYPE>, <ADDITIONAL ARGS>)\n\n# General version\nsns.<PLOTTYPE>(data = <DATA>, <MAPPINGS>, \n               kind = <PLOT SUBTYPE>, <ADDITIONAL ARGS>)\nHere, the kind argument is somewhat similar to the geom argument in ggplot2, but there are different functions used for different purposes. This seems to be a way to handle statistical transformations without the full grammar-of-graphics implementation of statistics that is found in ggplot2. Read more about statistical relationships, distributions, and categorical data in the seaborn tutorial.\n\n\nIn Version 0.12, Seaborn introduced a new interface, the objects interface, that is much more grammar-of-graphics like than the default Seaborn interface.\nWe can come up with a generic plot template that looks something like this:\n(\n  so.Plot(<DATA>, <MAPPINGS>, <GROUPS>)\n  .add(so.<GEOM>(<ARGS>), so.<TRANSFORMATION>, so.<POSITION>)\n  .show()\n)\n\n\nR’s base graphics library is decidedly non-grammar-of-graphics like. Each plot is defined using plot-specific arguments that are not particularly consistent across different functions.\nThe following is as close as I can get, but it’s still not accurate for many plots.\n<PLOT NAME>(<MAPPINGS>, <ARGS>)\n<LEGEND>(<MAPPINGS>, <ARGS>)\nAny statistics are either computed as part of the plot function (e.g. hist()) or there are plot methods to accompany the statistic calculation (plot(density(...))). Facets/subplots are sometimes created using par(mfrow=(...)) to define sub-plots, and are sometimes created by the plot command itself, in the case of e.g. passing a numeric matrix to plot(), which creates a scatterplot matrix.\nUseful commands: plot() for scatterplots, lines (type = ‘l’), scatterplot matrices, etc. hist() for histograms. plot(density(...)) for density plots. boxplot() for boxplots, barplot() for bar plots, …\n\n\nI’m not even going to try to create a grammar summary for matplotlib… there just isn’t one. It wasn’t designed with the grammar of graphics in mind, and was built by computer scientists [9], not statisticians. As a result, even retrofitting some sort of “grammar” interpretation doesn’t work so well.\nFor a primer on matplotlib, I recommend [10], Ch 9."
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#advanced-charts-graphics-and-the-grammar-of-graphics",
    "href": "part-wrangling/02b-graphics.html#advanced-charts-graphics-and-the-grammar-of-graphics",
    "title": "18  Data Visualization",
    "section": "\n18.5 Advanced: Charts, Graphics, and the Grammar of Graphics",
    "text": "18.5 Advanced: Charts, Graphics, and the Grammar of Graphics\nThere are two general approaches to generating statistical graphics computationally:\n\nManually specify the plot that you want, possibly doing the preprocessing and summarizing before you create the plot.\nBase R, matplotlib, old-style SAS graphics\nDescribe the relationship between the plot and the data, using sensible defaults that can be customized for common operations.\nggplot2, plotnine, seaborn (sort of), seaborn objects\n\n\n\nThere is a difference between low-level plotting libraries (base R, matplotlib) and high-level plotting libraries (ggplot2, plotnine, seaborn). Grammar of graphics libraries are usually high level, but it is entirely possible to have a high level library that does not follow the grammar of graphics. In general, if you have to manually add a legend, it’s probably a low level library.\nIn the introduction to The Grammar of Graphics [11], Leland Wilkinson suggests that the first approach is what we would call “charts” - pie charts, line charts, bar charts - objects that are “instances of much more general objects”. His argument is that elegant graphical design means we have to think about an underlying theory of graphics, rather than how to create specific charts. The 2nd approach is called the “grammar of graphics”.\n\n\nThere are other graphics systems (namely, lattice in R, seaborn in Python, and some web-based rendering engines like Observable or d3) that you could explore, but it’s far more important that you know how to functionally create plots in R and/or Python. I don’t recommend you try to become proficient in all of them. Pick one (two at most) and get familiar with those libraries, then google for the rest.\nBefore we delve into the grammar of graphics, let’s motivate the philosophy using a simple task. Suppose we want to create a pie chart using some data. Pie charts are terrible, and we’ve known it for 100 years[12], so in the interests of showing that we know that pie charts are awful, we’ll also create a stacked bar chart, which is the most commonly promoted alternative to a pie chart. We’ll talk about what makes pie charts terrible at the end of this module in Creating Good charts.\n\n\n\n\n\n\nExample: Generations of Pokemon\n\n\n\n\n\n\nSuppose we want to explore Pokemon. There’s not just the original 150 (gotta catch ’em all!) - now there are over 1000! Let’s start out by looking at the proportion of Pokemon added in each of the 9 generations.\n\n\nR setup\nPython setup\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n# Setup the data\npoke <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\", na = '.') %>%\n  mutate(generation = factor(gen))\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\npoke['generation'] = pd.Categorical(poke.gen)\n\n\n\n\nOnce the data is read in, we can start plotting:\n\n\nggplot2\nBase R\nMatplotlib\nPlotnine\nSeaborn\n\n\n\nIn ggplot2, we start by specifying which variables we want to be mapped to which features of the data.\nIn a pie or stacked bar chart, we don’t care about the x coordinate - the whole chart is centered at (0,0) or is contained in a single “stack”. So it’s easiest to specify our x variable as a constant, ““. We care about the fill of the slices, though - we want each generation to have a different fill color, so we specify generation as our fill variable.\nThen, we want to summarize our data by the number of objects in each category - this is basically a stacked bar chart. Any variables specified in the plot statement are used to implicitly calculate the statistical summary we want – that is, to count the rows (so if we had multiple x variables, the summary would be computed for both the x and fill variables). ggplot is smart enough to know that when we use geom_bar, we generally want the y variable to be the count, so we can get away with leaving that part out. We just have to specify that we want the bars to be stacked on top of one another (instead of next to each other, “dodge”).\n\nlibrary(ggplot2)\n\nggplot(aes(x = \"\", fill = generation), data = poke) + \n  geom_bar(position = \"stack\") \n\n\n\n\nIf we want a pie chart, we can get one very easily - we transform the coordinate plane from Cartesian coordinates to polar coordinates. We specify that we want angle to correspond to the “y” coordinate, and that we want to start at \\(\\theta = 0\\).\n\nggplot(aes(x = \"\", fill = generation), data = poke) + \n  geom_bar(position = \"stack\") + \n  coord_polar(\"y\", start = 0)\n\n\n\n\nNotice how the syntax and arguments to the functions didn’t change much between the bar chart and the pie chart? That’s because the ggplot package uses what’s called the grammar of graphics, which is a way to describe plots based on the underlying mathematical relationships between data and plotted objects. In base R and in matplotlib in Python, different types of plots will have different syntax, arguments, etc., but in ggplot2, the arguments are consistently named, and for plots which require similar transformations and summary observations, it’s very easy to switch between plot types by changing one word or adding one transformation.\n\n\nLet’s start with what we want: for each generation, we want the total number of pokemon.\nTo get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of pokemon in that generation.\n\n# Create summary of pokemon by type\ntmp <- poke %>%\n  group_by(generation) %>%\n  count() \n\npie(tmp$n, labels = tmp$generation)\n\n\n\n\nWe could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion.\n\n# Create summary of pokemon by type\ntmp <- poke %>%\n  group_by(generation) %>%\n  count() \n\n# Matrix is necessary for a stacked bar chart\nmatrix(tmp$n, nrow = 9, ncol = 1, dimnames = list(tmp$generation)) %>%\nbarplot(beside = F, legend.text = T, main = \"Generations of Pokemon\")\n\n\n\n\nThere’s not a huge amount of similarity between the code for a pie chart and a bar plot, even though the underlying statistics required to create the two charts are very similar. The appearance of the two charts is also very different.\n\n\nLet’s start with what we want: for each generation, we want the total number of pokemon.\nTo get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of Pokemon in that generation.\n\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts(normalize=True)*100\n\n# Draw the plot\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels = labels, autopct='%1.1f%%', startangle = 90)\n## ([<matplotlib.patches.Wedge object at 0x7f595ad990d0>, <matplotlib.patches.Wedge object at 0x7f595adaa700>, <matplotlib.patches.Wedge object at 0x7f595adaae20>, <matplotlib.patches.Wedge object at 0x7f595adb8580>, <matplotlib.patches.Wedge object at 0x7f595adb8ca0>, <matplotlib.patches.Wedge object at 0x7f595ad46400>, <matplotlib.patches.Wedge object at 0x7f595ad46b20>, <matplotlib.patches.Wedge object at 0x7f595ad55280>, <matplotlib.patches.Wedge object at 0x7f595ad559a0>], [Text(-0.6090072830464104, 0.9160295460280905, '1'), Text(-1.0954901625854225, -0.0995052947262842, '2'), Text(-0.6165300405105602, -0.9109833747923434, '3'), Text(0.18481484283490274, -1.0843631651194678, '4'), Text(0.7975738857504167, -0.7575459700697914, '5'), Text(1.0758366105622164, -0.22929367059298023, '6'), Text(1.044469949497543, 0.3450833589099894, '7'), Text(0.7443080435855409, 0.8099416869465756, '8'), Text(0.2667977873990754, 1.0671546001582704, '9')], [Text(-0.3321857907525874, 0.4996524796516857, '18.7%'), Text(-0.5975400886829577, -0.05427561530524592, '15.5%'), Text(-0.3362891130057601, -0.4969000226140054, '12.6%'), Text(0.10080809609176512, -0.5914708173378915, '11.7%'), Text(0.43504030131840904, -0.4132068927653407, '8.8%'), Text(0.5868199693975725, -0.1250692748688983, '8.7%'), Text(0.5697108815441142, 0.18822728667817604, '8.1%'), Text(0.40598620559211324, 0.44178637469813214, '8.1%'), Text(0.14552606585404113, 0.5820843273590565, '7.8%')])\nax1.axis('equal')\n## (-1.1114088153158663, 1.1045180850389171, -1.106683057272638, 1.1003182408225065)\nplt.show()\n\n\n\n\nWe could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion.\n\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts()\nsizes = sizes.sort_index()\n\n# Find location of bottom of the bar for each bar\ncumulative_sizes = sizes.cumsum() - sizes\nwidth = 1\n\nfig, ax = plt.subplots()\n\nfor i in sizes.index:\n  ax.bar(\"Generation\", sizes[i-1], width, label=i, bottom = cumulative_sizes[i-1])\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: 0\nax.set_ylabel('# Pokemon')\nax.set_title('Pokemon Distribution by Generation')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\nAs of January 2023, pie charts are still not supported in plotnine. So this demo will fall a bit flat.\n\nfrom plotnine import *\nplt.cla() # clear out matplotlib buffer\n\nggplot(poke, aes(x = \"1\", fill = \"generation\")) + geom_bar(position = \"stack\")\n## <ggplot: (8751359448001)>\n\n\n\n\n\n\nAs with Plotnine, seaborn does not support pie charts due to the same underlying issue. The best option is to create a pie chart in matplotlib and use seaborn colors [13].\n\n# Load the seaborn package\n# the alias \"sns\" stands for Samuel Norman Seaborn\n# from \"The West Wing\" television show\nimport seaborn as sns\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'seaborn'\nimport matplotlib.pyplot as plt\n\n# Initialize seaborn styling; context\nsns.set_style('white')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'sns' is not defined\nsns.set_context('notebook')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'sns' is not defined\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts(normalize=True)*100\n\n#define Seaborn color palette to use\ncolors = sns.color_palette()[0:9]\n\n# Draw the plot\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'sns' is not defined\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels = labels, colors = colors, autopct='%1.1f%%', startangle = 90)\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: Invalid RGBA argument: 'Adelie'\nax1.axis('equal')\n## (-0.05500000000000001, 0.05500000000000001, -0.05500000000000001, 0.05500000000000001)\nplt.show()\n\n\n\n\nSeaborn doesn’t have the autopct option that matplotlib uses, so we have to aggregate the data ourselves before creating a barchart.\n\npoke['generation'] = pd.Categorical(poke['generation'])\n\nplt.cla() # clear out matplotlib buffer\nsns.catplot(data = poke, x = 'generation', kind = \"count\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'sns' is not defined\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nNintendo, Creatures, Game Freak, The Pokémon Company, Public domain, via Wikimedia Commons\n\n\nThe grammar of graphics is an approach first introduced in Leland Wilkinson’s book [11]. Unlike other graphics classification schemes, the grammar of graphics makes an attempt to describe how the dataset itself relates to the components of the chart.\n\n\nBuilding a masterpiece, by Allison Horst\n\n\nThis has a few advantages:\n\nIt’s relatively easy to represent the same dataset with different types of plots (and to find their strengths and weaknesses)\nGrammar leads to a concise description of the plot and its contents\nWe can add layers to modify the graphics, each with their own basic grammar (just like we combine sentences and clauses to build a rich, descriptive paragraph)\n\n\n\nA pyramid view of the major components of the grammar of graphics, with data as the base, aesthetics building on data, scales building on aesthetics, geometric objects, statistics, facets, and the coordinate system at the top of the pyramid. Source: [14]\n\n\n\n\n\n\n\n\nNote\n\n\n\nI have turned off warnings for all of the code chunks in this chapter. When you run the code you may get warnings about e.g. missing points - this is normal, I just didn’t want to have to see them over and over again - I want you to focus on the changes in the code.\n\n\nWhen creating a grammar of graphics chart, we start with the data (this is consistent with the data-first tidyverse philosophy).\n\nIdentify the dimensions of your dataset you want to visualize.\n\nDecide what aesthetics you want to map to different variables. For instance, it may be natural to put time on the \\(x\\) axis, or the experimental response variable on the \\(y\\) axis. You may want to think about other aesthetics, such as color, size, shape, etc. at this step as well.\n\nIt may be that your preferred representation requires some summary statistics in order to work. At this stage, you would want to determine what variables you feed in to those statistics, and then how the statistics relate to the geoms that you’re envisioning. You may want to think in terms of layers - showing the raw data AND a summary geom.\n\n\nIn most cases, ggplot will determine the scale for you, but sometimes you want finer control over the scale - for instance, there may be specific, meaningful bounds for a variable that you want to directly set.\nCoordinate system: Are you going to use a polar coordinate system? (Please say no, for reasons we’ll get into later!)\nFacets: Do you want to show subplots based on specific categorical variable values?\n\n(this list modified from [14]).\n\n\nSketch\nggplot2\nplotnine\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\n\nlibrary(ggplot2)\ndata(txhousing)\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point(alpha = .2)\n\n\n\n\n\n\n\nfrom plotnine import *\nfrom plotnine.data import txhousing\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) + geom_point(alpha = .1)\n## <ggplot: (8751359333994)>\n## \n## /home/susan/.local/lib/python3.8/site-packages/plotnine/layer.py:412: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\n\n\n\nfrom plotnine.data import txhousing\nimport seaborn.objects as so\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'seaborn'\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\")\n  .add(so.Dot(alpha = 0.1))\n  .show()\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'so' is not defined"
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#exploratory-data-analysis-and-plot-customization",
    "href": "part-wrangling/02b-graphics.html#exploratory-data-analysis-and-plot-customization",
    "title": "18  Data Visualization",
    "section": "\n18.6 Exploratory Data Analysis and Plot Customization",
    "text": "18.6 Exploratory Data Analysis and Plot Customization\nWhen you first get a new dataset, it is critical that you, as the analyst, get a feel for the data. John Tukey, in his book [15], likens exploratory data analysis to detective work, and says that in both cases the analyst needs two things: tools, and understanding. Tools, like fingerprint powder or types of charts, are essential to collect evidence; understanding helps the analyst to know where to apply the tools and what to look out for.\nTukey claims that while tools are constantly evolving, and there may not be one set of “best” tools, understanding is a bit different. There are situational elements – you might need different base knowledge to solve a crime in a small village in the UK than you need to solve a crime in New York City or Buenos Aires. However, the broad strokes of what questions are asked during an investigation will be similar across different locations and types of crimes.\nThe same thing is true in exploratory data analysis - while it is helpful to have a basic understanding of the dataset, and being intimately familiar with the type of data and data collection processes can give the analyst an advantage, the same basic detective skills will be useful across a wide variety of data sets.\nThere is one other aspect of the analogy between EDA and detective work that is useful: the detective gathers evidence, but does not try the case or make decisions about what should happen to the accused. Similarly, the individual conducting an exploratory data analysis should not move too quickly to hypothesis testing and other confirmatory data analysis techniques. In exploratory data analysis, the goal is to lay the foundation, assess the evidence (data) for interesting clues, and to try to understand the whole story. Only once this process is complete should we move to any sort of confirmatory analysis.\nIn this section, we’ll primarily use charts as a tool for exploratory data analysis. Pay close attention not only to the tools, but to the process of inquiry.\n\n18.6.1 Texas Housing Data\nLet’s explore the txhousing data a bit more thoroughly by adding some complexity to our chart. This example will give me an opportunity to show you how an exploratory data analysis might work in practice, while also demonstrating some of the features of each plotting library.\n\n18.6.2 Starting Chart\nBefore we start exploring, let’s start with a basic scatterplot, and add a title and label our axes, so that we’re creating good, informative charts.\n\n\nSketch\nggplot2\nplotnine\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) +\\\ngeom_point() +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## <ggplot: (8751359291218)>\n## \n## /home/susan/.local/lib/python3.8/site-packages/plotnine/layer.py:412: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\n\n\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\")\n  .add(so.Dot(alpha = 0.1))\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'so' is not defined\n\n\n\n\n\n18.6.3 Exploring Trends\nFirst, we may want to show some sort of overall trend line. We can start with a linear regression, but it may be better to use a loess smooth (loess regression is a fancy weighted average and can create curves without too much additional effort on your part).\n\n\nSketch\nggplot2\nplotnine\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) + geom_point() +\\\ngeom_smooth(method = \"lm\", color = \"blue\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n# By default, geom_smooth in plotnine has a black line you can't see well\n## <ggplot: (8751359356483)>\n## \n## /home/susan/.local/lib/python3.8/site-packages/plotnine/layer.py:412: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) + geom_point() +\\\ngeom_smooth(method = \"loess\", color = \"blue\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\n\n\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\")\n  .add(so.Dot(alpha = 0.1))\n  .add(so.Line(color = \"black\"), so.PolyFit(order = 2))\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'so' is not defined\n\nI haven’t yet figured out a way to do locally weighted regression in Seaborn using the objects interface, but I’m sure that will come as the interface develops.\n\n\n\n\n18.6.4 Adding Complexity with Moderating Variables\nLooking at the plots here, it’s clear that there are small sub-groupings (see, for instance, the almost continuous line of points at the very top of the group between 2000 and 2005). Let’s see if we can figure out what those additional variables are…\nAs it happens, the best viable option is City.\n\n\nSketch\nggplot2\nplotnine\nSeaborn\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nThat’s a really crowded graph! It’s slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) +\n  # geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nggplot(txhousing, aes(x = \"date\", y = \"median\", color = \"city\")) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\nThat’s a really crowded graph! It’s slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(txhousing, aes(x = \"date\", y = \"median\", color = \"city\")) +\\\ngeom_smooth(method = \"loess\") +\\\ntheme(subplots_adjust={'right': 0.5}) +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\nThis is one of the first places we see differences in Python and R’s graphs - python doesn’t allocate sufficient space for the legend by default. In Python, you have to manually adjust the theme to show the legend (or plot the legend separately).\n\n\n\n(\n  so.Plot(txhousing, x = \"date\", y = \"median\", color = \"city\")\n  .add(so.Line())\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'so' is not defined\n\n\n\n\n\n18.6.5 Data Reduction Strategies\nIn reality, though, you should not ever map color to something with more than about 7 categories if your goal is to allow people to trace the category back to the label. It just doesn’t work well perceptually.\nSo let’s work with a smaller set of data: Houston, Dallas, Fort worth, Austin, and San Antonio (the major cities).\nAnother way to show this data is to plot each city as its own subplot. In ggplot2 lingo, these subplots are called “facets”. In visualization terms, we call this type of plot “small multiples” - we have many small charts, each showing the trend for a subset of the data.\n\n\nggplot2\nplotnine\nSeaborn\n\n\n\n\ncitylist <- c(\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\")\nhousingsub <- dplyr::filter(txhousing, city %in% citylist)\n\nggplot(data = housingsub, aes(x = date, y = median, color = city)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nHere’s the facetted version of the chart:\n\nggplot(data = housingsub, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  facet_wrap(~city) +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\nggplot(housingsub, aes(x = \"date\", y = \"median\", color = \"city\")) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\") +\\\ntheme(subplots_adjust={'right': 0.75}) +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\nHere’s the facetted version of the chart:\n\nggplot(housingsub, aes(x = \"date\", y = \"median\")) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\", color = \"blue\") +\\\nfacet_wrap(\"city\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\n(\n  so.Plot(housingsub, x = \"date\", y = \"median\", color = \"city\")\n  .add(so.Line())\n  .label(x = \"Date\", y = \"Median Home Price\", title = \"Texas Housing Prices\")\n  .show()\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'so' is not defined\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\n(\n  so.Plot(housingsub, x = \"date\", y = \"median\")\n  .add(so.Line())\n  .label(x = \"Date\", y = \"Median Home Price\")\n  .facet(col = \"city\")\n  .show()\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'so' is not defined\n\n\n\n\n\n18.6.6 Adding Additional Complexity\nNow that we’ve simplified our charts a bit, we can explore a couple of the other quantitative variables by mapping them to additional aesthetics:\n\n\nggplot2\nplotnine\nSeaborn\n\n\n\n\nggplot(data = housingsub, aes(x = date, y = median, size = sales)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  facet_wrap(~city) +\n  # Remove extra information from the legend -\n  # line and error bands aren't what we want to show\n  # Also add a title\n  guides(size = guide_legend(title = 'Number of Sales',\n                             override.aes = list(linetype = NA,\n                                                 fill = 'transparent'))) +\n  # Move legend to bottom right of plot\n  theme(legend.position = c(1, 0), legend.justification = c(1, 0)) +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\n( # This is used to group lines together in python\nggplot(housingsub, aes(x = \"date\", y = \"median\", size = \"sales\"))\n+ geom_point(alpha = .15) # Make points transparent\n+ geom_smooth(method = \"loess\")\n+ facet_wrap(\"city\")\n+ guides(size = guide_legend(title = 'Number of Sales'))\n+ xlab(\"Date\") + ylab(\"Median Home Price\")\n+ ggtitle(\"Texas Housing Prices\")\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\nNot all of the features we used in R are available in plotnine in Python (in part because of limitations of the underlying graphics interface that plotnine uses). This does somewhat limit the customization we can do with python, but for the most part we can still get the same basic information back out.\n\n\nComing soon!\n\n\n\n\n18.6.7 Exploring Other Variables and Relationships\nUp to this point, we’ve used the same position information - date for the y axis, median sale price for the y axis. Let’s switch that up a bit so that we can play with some transformations on the x and y axis and add variable mappings to a continuous variable.\n\n\nggplot2\nplotnine\n\n\n\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nThe points for Fort Worth are compressed pretty tightly relative to the points for Houston and Dallas. When we get this type of difference, it is sometimes common to use a log transformation1. Here, I have transformed both the x and y axis, since the number of sales seems to be proportional to the number of listings.\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n( # This is used to group lines together in python\nggplot(housingsub, aes(x = \"listings\", y = \"sales\", color = \"city\"))\n+ geom_point(alpha = .15) # Make points transparent\n+ geom_smooth(method = \"loess\")\n+ scale_x_log10()\n+ scale_y_log10()\n+ xlab(\"Date\") + ylab(\"Median Home Price\")\n+ ggtitle(\"Texas Housing Prices\")\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): plotnine.exceptions.PlotnineError: \"For loess smoothing, install 'scikit-misc'\"\n\nNotice that the gridlines included in python by default are different than those in ggplot2 by default (personally, I vastly prefer the python version - it makes it obvious that we’re using a log scale).\n\n\n\n\n18.6.8 Adding Additional Moderating Variables\nFor the next demonstration, let’s look at just Houston’s data. We can examine the inventory’s relationship to the number of sales by looking at the inventory-date relationship in x and y, and mapping the size or color of the point to number of sales.\n\n\nggplot2\nplotnine\n\n\n\n\nhouston <- dplyr::filter(txhousing, city == \"Houston\")\n\nggplot(data = houston, aes(x = date, y = inventory, size = sales)) +\n  geom_point(shape = 1) +\n  xlab(\"Date\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_legend(title = \"Number of Sales\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\nggplot(data = houston, aes(x = date, y = inventory, color = sales)) +\n  geom_point() +\n  xlab(\"Date\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_colorbar(title = \"Number of Sales\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\nWhich is easier to read?\nWhat happens if we move the variables around and map date to the point color?\n\nggplot(data = houston, aes(x = sales, y = inventory, color = date)) +\n  geom_point() +\n  xlab(\"Number of Sales\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_colorbar(title = \"Date\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\nIs that easier or harder to read?\n\n\n\nhouston = txhousing[txhousing.city==\"Houston\"]\n\n(\n  ggplot(houston, aes(x = \"date\", y = \"inventory\", size = \"sales\"))\n  + geom_point(shape = 'o', fill = 'none')\n  + xlab(\"Date\") + ylab(\"Median Home Price\")\n  + guides(size = guide_legend(title = \"Number of Sales\"))\n  + ggtitle(\"Houston Housing Data\")\n)\n## <ggplot: (8751365968381)>\n## \n## /home/susan/.local/lib/python3.8/site-packages/plotnine/layer.py:412: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\nIn plotnine, we have to use matplotlib marker syntax.\n\n\n(\n  ggplot(houston, aes(x = \"date\", y = \"inventory\", color = \"sales\"))\n  + geom_point()\n  + xlab(\"Date\") + ylab(\"Median Home Price\")\n  + guides(size = guide_legend(title = \"Number of Sales\"))\n  + ggtitle(\"Houston Housing Data\")\n)\n## <ggplot: (8751359570571)>\n## \n## /home/susan/.local/lib/python3.8/site-packages/plotnine/layer.py:412: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\nPlotnine also defaults to different color schemes than ggplot2 – just something to know if you want the plot to be exactly the same. Personally, I prefer the viridis color scheme (what plotnine uses) to the ggplot2 defaults.\nWhat happens if we move the variables around and map date to the point color?\n\n(\nggplot(houston, aes(x = \"sales\", y = \"inventory\", color = \"date\"))\n  + geom_point()\n  + xlab(\"Number of Sales\") + ylab(\"Months of Inventory\")\n  + guides(size = guide_colorbar(title = \"Date\"))\n  + ggtitle(\"Houston Housing Data\")\n  + theme(subplots_adjust={'right': 0.75})\n)\n## <ggplot: (8751359687367)>\n## \n## /home/susan/.local/lib/python3.8/site-packages/plotnine/layer.py:412: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\nIs that easier or harder to read?"
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#what-type-of-chart-to-use",
    "href": "part-wrangling/02b-graphics.html#what-type-of-chart-to-use",
    "title": "18  Data Visualization",
    "section": "\n18.7 What type of chart to use?",
    "text": "18.7 What type of chart to use?\nIt can be hard to know what type of chart to use for a particular type of data. I recommend figuring out what you want to show first, and then thinking about how to show that data with an appropriate plot type.\nConsider the following factors:\n\nWhat type of variable is x? Categorical? Continuous? Discrete?\nWhat type of variable is y?\nHow many observations do I have for each x/y variable?\nAre there any important moderating variables?\nDo I have data that might be best shown in small multiples? E.g. a categorical moderating variable and a lot of data, where the categorical variable might be important for showing different features of the data?\n\nOnce you’ve thought through this, take a look through catalogs like the R Graph Gallery or the Python Graph Gallery to see what visualizations match your data and use-case.\nChapter 19 talks in more depth about considerations for creating good charts; these considerations may also inform your decisions.\n\n\n\n\n\n\nAdditional Reading\n\n\n\nR graphics\n\nggplot2 cheat sheet\n\nggplot2 aesthetics cheat sheet - aesthetic mapping one page cheatsheet\nggplot2 reference guide\nR graph cookbook\n\nData Visualization in R (@ramnathv)\nPython graphics\n\nPlotnine documentation\n\nMatplotlib documentation - Matplotlib is the base that plotnine uses to replicate ggplot2 functionality\n\n\nVisualization with Matplotlib chapter of Python Data Science\n\n\nScientific Visualization with Python"
  },
  {
    "objectID": "part-wrangling/02b-graphics.html#references",
    "href": "part-wrangling/02b-graphics.html#references",
    "title": "18  Data Visualization",
    "section": "\n18.8 References",
    "text": "18.8 References\n\n\n\n\n[1] \nH. Wickham, ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016 [Online]. Available: https://ggplot2.tidyverse.org\n\n\n\n[2] \nH. Kibirige, “A grammar of graphics for python. Plotnine 0.10.1 documentation,” 2022. [Online]. Available: https://plotnine.readthedocs.io/en/stable/. [Accessed: Feb. 06, 2023]\n\n\n[3] \nM. Waskom, “An introduction to seaborn. Seaborn 0.12.2 documentation,” 2022. [Online]. Available: https://seaborn.pydata.org/tutorial/introduction.html. [Accessed: Feb. 06, 2023]\n\n\n[4] \nM. Waskom, “Next-generation seaborn interface. Seaborn nextgen documentation,” 2022. [Online]. Available: https://seaborn.pydata.org/nextgen/. [Accessed: Aug. 29, 2022]\n\n\n[5] \nThe matplotlib development team, “Matplotlib — visualization with python. Matplotlib,” 2023. [Online]. Available: https://matplotlib.org/. [Accessed: Feb. 06, 2023]\n\n\n[6] \nR. D. Peng, “The base plotting system,” in Exploratory data analysis with r, 1st ed., leanpub, 2020 [Online]. Available: https://bookdown.org/rdpeng/exdata/. [Accessed: Feb. 06, 2023]\n\n\n[7] \nJ. W. Tukey, “Data-Based Graphics: Visual Display in the Decades to Come,” Statistical Science, vol. 5, no. 3, pp. 327–339, Aug. 1990, doi: 10.1214/ss/1177012101. [Online]. Available: https://projecteuclid.org/journals/statistical-science/volume-5/issue-3/Data-Based-Graphics--Visual-Display-in-the-Decades-to/10.1214/ss/1177012101.full. [Accessed: Aug. 22, 2022]\n\n\n[8] \nN. Yau, “Comparing ggplot2 and r base graphics. FlowingData,” Mar. 22, 2016. [Online]. Available: https://flowingdata.com/2016/03/22/comparing-ggplot2-and-r-base-graphics/. [Accessed: Feb. 06, 2023]\n\n\n[9] \nW. Koehrsen, “The next level of data visualization in python. Medium,” Jan. 24, 2019. [Online]. Available: https://towardsdatascience.com/the-next-level-of-data-visualization-in-python-dd6e99039d5e. [Accessed: Feb. 06, 2023]\n\n\n[10] \nW. McKinney, Python for data analysis, 3rd ed. O’Reilly, 2022 [Online]. Available: https://wesmckinney.com/book/. [Accessed: Feb. 06, 2023]\n\n\n[11] \nL. Wilkinson, The grammar of graphics. New York: Springer, 1999 [Online]. Available: http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=3085765. [Accessed: Jan. 29, 2020]\n\n\n[12] \nF. E. Croxton and R. E. Stryker, “Bar Charts Versus Circle Diagrams,” Journal of the American Statistical Association, vol. 22, no. 160, pp. 473–482, 1927, doi: 10.2307/2276829. [Online]. Available: https://www.jstor.org/stable/2276829. [Accessed: Aug. 22, 2022]\n\n\n[13] \nZach, “How to Create a Pie Chart in Seaborn,” Statology. Jul. 2021 [Online]. Available: https://www.statology.org/seaborn-pie-chart/. [Accessed: Sep. 19, 2022]\n\n\n[14] \nD. (DJ). Sarkar, “A Comprehensive Guide to the Grammar of Graphics for Effective Visualization of Multi-dimensional…,” Medium. Sep. 2018 [Online]. Available: https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149. [Accessed: Apr. 11, 2022]\n\n\n[15] \nJ. Tukey, Exploratory data analysis. Addison-Wesley Publishing Company, 1977."
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#fa-bullseye-objectives",
    "href": "part-wrangling/02c-good-graphics.html#fa-bullseye-objectives",
    "title": "19  Creating Good Charts",
    "section": "\n19.1  Objectives",
    "text": "19.1  Objectives\n\nEvaluate existing charts and develop new versions that improve accessibility and readability\n\nA chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g. the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate.\nUnfortunately, while our visual system is amazing, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms."
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#perceptual-and-cognitive-factors",
    "href": "part-wrangling/02c-good-graphics.html#perceptual-and-cognitive-factors",
    "title": "19  Creating Good Charts",
    "section": "\n19.2 Perceptual and Cognitive Factors",
    "text": "19.2 Perceptual and Cognitive Factors\n\n19.2.1 Color\nOur eyes are optimized for perceiving the yellow/green region of the color spectrum. Why? Well, our sun produces yellow light, and plants tend to be green. It’s pretty important to be able to distinguish different shades of green (evolutionarily speaking) because it impacts your ability to feed yourself. There aren’t that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum.\n\n\nSensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons)\n\n\nNot everyone perceives color in the same way. Some individuals are colorblind or color deficient. We have 3 cones used for color detection, as well as cells called rods which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, <1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances.\n\n\n\n\n\n\nColorblindness Test\n\n\n\n\n\nYou can take a test designed to screen for colorblindness here\nYour monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. A different test is available here.\n\n In reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown.\n\n\n\nIn addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context.\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\nOur brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly “remove” the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown above - because we’re correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade.\nImplications and Guidelines\n\nDo not use rainbow color gradient schemes - because of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance.\nAvoid any scheme that uses green-yellow-red signaling if you have a target audience that may include colorblind people.\nTo “colorblind-proof” a graphic, you can use a couple of strategies:\n\ndouble encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out\nIf you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it!\nIf you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -> dark by colorblind people, so it will be correctly perceived no matter what color you use.\nIf you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is purple - white - orange. In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly.\n\n\nBe conscious of what certain colors “mean”\n\nLeveraging common associations can make it easier to read a color scale and remember what it stands for (e.g. blue for cold, orange/red for hot is a natural scale, red = Republican and blue = Democrat in the US, white -> blue gradients for showing rainfall totals)\nSome colors can can provoke emotional responses that may not be desirable.1\n\nIt is also important to be conscious of the social baggage that certain color schemes may have - the pink/blue color scheme often used to denote gender can be unnecessarily polarizing, and it may be easier to use a colder color (blue or purple) for men and a warmer color (yellow, orange, lighter green) for women2.\n\n\nThere are packages such as RColorBrewer and dichromat that have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer). You can also take a look at other ways to find nice color palettes.\n\n19.2.2 Short Term Memory\nWe have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time.\n\n\n\n\n\n\nTry it out!\n\n\n\n\nClick here, read the information, and then click to hide it.\n1 4 2 2 3 9 8 0 7 8\n\nWait a few seconds, then expand this section\nWhat was the third number?\n\n\n\nWithout rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 “bits” of information.\nIn charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart.\nWhere possible, try to keep your legends to 6 or 7 characteristics.\nImplications and Guidelines\n\n\nLimit the number of categories in your legends to minimize the short term memory demands on your reader.\n\nWhen using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude.\n\n\nUse colors and symbols which have implicit meaning to minimize the need to refer to the legend.\nAdd annotations on the plot, where possible, to reduce the need to re-read captions.\n\n19.2.3 Grouping and Sense-making\nImposing order on visual chaos.\n\n\nAmbiguous Images\nIllusory Contours\nFigure/Ground\n\n\n\nWhat does the figure below look like to you?\n\n\nIs it a rabbit, or a duck?\n\n\nWhen faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options.\n\n\n\n\nConsider this image - what do you see?\n\n\nDid you see something like “3 circles, a triangle with a black outline, and a white triangle on top of that”? In reality, there are 3 angles and 3 pac-man shapes. But, it’s much more likely that we’re seeing layers of information, where some of the information is obscured (like the “mouth” of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience.\n\n\nNow, look at the logo for the Pittsburgh Zoo.\n\nDo you see the gorilla and lionness? Or do you see a tree? Here, we’re not entirely sure which part of the image is the figure and which is the background.\n\n\n\nThe ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs.\nThe catchphrase of Gestalt psychology is\n\nThe whole is greater than the sum of the parts\n\nThat is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene.\n\n\nThe Gestalt Heuristics help us to impose order on ambiguous visual stimuli\n\n\nYou can read about the gestalt rules here, but they are also demonstrated in the figure above.\nIn graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving “with” the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders.\n\n\n\n\n\n\nDanger\n\n\n\nSuppose I want to emphasize the change in the murder rate between 1980 and 2010.\nI could use a bar chart (showing only the first 4 states alphabetically for space)\n\n\nR\nPython\n\n\n\n\nfbiwide <- read.csv(\"https://github.com/srvanderplas/Stat151/raw/main/data/fbiwide.csv\")\nlibrary(dplyr)\n\nfbiwide %>%\n  filter(Year %in% c(1980, 2010)) %>%\n  filter(State %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\")) %>%\n  ggplot(aes(x = State, y = Murder/Population*100000, fill = factor(Year))) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Murders per 100,000 residents\")\n## Error in ggplot(., aes(x = State, y = Murder/Population * 1e+05, fill = factor(Year))): could not find function \"ggplot\"\n\n\n\n\nimport pandas as pd\nfbiwide = r.fbiwide\nfbiwide = fbiwide.assign(YearFactor = pd.Categorical(fbiwide.Year))\nfbiwide = fbiwide.assign(Murder100k = fbiwide.Murder/fbiwide.Population * 100000)\n\nyr1980_2010 = fbiwide[fbiwide.Year.isin([1980,2010])]\nsubdata = yr1980_2010[yr1980_2010.State.isin([\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\"])]\n\n(\nggplot(subdata, aes(x = \"State\", y = \"Murder100k\", fill = \"YearFactor\")) +\n  geom_col(stat='identity', position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Murders per 100,000 residents\")\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'ggplot' is not defined\n\n\n\n\nOr, I could use a line chart\n\n\nR\nPython\n\n\n\n\nfbiwide %>%\n  filter(Year %in% c(1980, 2010)) %>%\n  ggplot(aes(x = Year, y = Murder/Population*100000, group = State)) +\n  geom_line() +\n  ylab(\"Murders per 100,000 residents\")\n## Error in ggplot(., aes(x = Year, y = Murder/Population * 1e+05, group = State)): could not find function \"ggplot\"\n\n\n\n\n(\nggplot(yr1980_2010, aes(x = \"Year\", y = \"Murder100k\", group = \"State\")) +\n  geom_line() +\n  ylab(\"Murders per 100,000 residents\")\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'ggplot' is not defined\n\n\n\n\nOr, I could use a box plot\n\n\nR\nPython\n\n\n\n\nfbiwide %>%\n  filter(Year %in% c(1980, 2010)) %>%\n  ggplot(aes(x = factor(Year), y = Murder/Population*100000)) +\n  geom_boxplot() +\n  ylab(\"Murders per 100,000 residents\")\n## Error in ggplot(., aes(x = factor(Year), y = Murder/Population * 1e+05)): could not find function \"ggplot\"\n\n\n\n\n\n(\nggplot(yr1980_2010, aes(x = \"YearFactor\", y = \"Murder100k\")) +\n  geom_boxplot() +\n  ylab(\"Murders per 100,000 residents\")\n)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'ggplot' is not defined\n\n\n\n\nWhich one best demonstrates that in every state and region, the murder rate decreased?\nThe line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g. almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn’t allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can’t see the individual relationships.\n\n\nThe aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) can easily make when examining those plots.3"
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#general-guidelines-for-accuracy",
    "href": "part-wrangling/02c-good-graphics.html#general-guidelines-for-accuracy",
    "title": "19  Creating Good Charts",
    "section": "\n19.3 General guidelines for accuracy",
    "text": "19.3 General guidelines for accuracy\nThere are certain tasks which are easier for us relative to other, similar tasks.\n\n## Error in ggplot(segs, aes(x = x, xend = x, y = y1, yend = y2)): could not find function \"ggplot\"\n\nWhen making judgments corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level.4\n\nPosition (common scale)\nPosition (non-aligned scale)\nLength, Direction, Angle, Slope\nArea\nVolume, Density, Curvature\nShading, Color Saturation, Color Hue\n\nIf we compare a pie chart and a stacked bar chart, the bar chart asks readers to make judgements of position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not preferable – they make it harder on the reader, and as a result we are less accurate when reading information from pie charts.\nWhen creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc.\nThere is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area.\n.\nTed ED: How to spot a misleading graph - Lea Gaslowitz\nBusiness Insider: The Worst Graphs Ever\nExtra dimensions and other annotations are sometimes called “chartjunk” and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration)."
  },
  {
    "objectID": "part-wrangling/02c-good-graphics.html#references",
    "href": "part-wrangling/02c-good-graphics.html#references",
    "title": "19  Creating Good Charts",
    "section": "\n19.4 References",
    "text": "19.4 References"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#fa-bullseye-objectives",
    "href": "part-wrangling/03-data-cleaning.html#fa-bullseye-objectives",
    "title": "20  Data Cleaning",
    "section": "\n20.1  Objectives",
    "text": "20.1  Objectives\n\nIdentify required sequence of steps for data cleaning\nDescribe step-by-step data cleaning process in lay terms appropriately\nApply data manipulation verbs to prepare data for analysis\nUnderstand the consequences of data cleaning steps for statistical analysis\nCreate summaries of data appropriate for analysis or display using data manipulation techniques"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#introduction",
    "href": "part-wrangling/03-data-cleaning.html#introduction",
    "title": "20  Data Cleaning",
    "section": "\n20.2 Introduction",
    "text": "20.2 Introduction\nIn this section, we’re going start learning how to work with data. Generally speaking, data doesn’t come in a form suitable for analysis1 - you have to clean it up, create the variables you care about, get rid of those you don’t care about, and so on.\n\n\n\n\nData wrangling (by Allison Horst)\n\n\nSome people call the process of cleaning and organizing your data “data wrangling”, which is a fantastic way to think about chasing down all of the issues in the data.\nIn R, we’ll be using the tidyverse for this. It’s a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy2 and interface (basically, the commands will use predictable argument names and structure). You’ve already been introduced to parts of the tidyverse - specifically, readr and ggplot2.\ndplyr (one of the packages in the tidyverse) creates a “grammar of data manipulation” to make it easier to describe different operations. I find the dplyr grammar to be extremely useful when talking about data operations, so I’m going to attempt to show you how to do the same operations in R with dplyr, and in Python (without the underlying framework).\nEach dplyr verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, data comes first – literally, as it’s the first argument to any function. In addition, you don’t use df$variable to access a variable - you refer to the variable by its name alone (“bare” names). This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy.\nIn Python, most data manipulation tasks are handled using pandas[1]. In the interests of using a single consistent “language” for describing data manipulation tasks, I’ll use the tidyverse “verbs” to describe operations in both languages. The goal of this is to help focus your attention on the essentials of the operations, instead of the specific syntax.\nThere is also the datar python package[2], which attempts to port the dplyr grammar of data wrangling into python. While pandas tends to be fairly similar to base R in basic operation, datar may be more useful if you prefer the dplyr way of handling things using a data-first API.\n\n\nI haven’t had the chance to add the datar package to this book, but it looks promising and may be worth your time to figure out. It’s a bit too new for me to teach right now - I want packages that will be maintained long-term if I’m going to teach them to others.\n\n\n\n\n\n\nNote\n\n\n\nThere is an excellent dplyr cheatsheet available from RStudio. You may want to print it out to have a copy to reference as you work through this chapter.\nHere is a data wrangling with pandas cheatsheet that is formatted similarly to the dplyr cheat sheet."
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#tidy-data",
    "href": "part-wrangling/03-data-cleaning.html#tidy-data",
    "title": "20  Data Cleaning",
    "section": "\n20.3 Tidy Data",
    "text": "20.3 Tidy Data\nThere are infinitely many ways to configure “messy” data, but data that is “tidy” has 3 attributes:\n\nEach variable has its own column\nEach observation has its own row\nEach value has its own cell\n\nThese attributes aren’t sufficient to define “clean” data, but they work to define “tidy” data (in the same way that you can have a “tidy” room because all of your clothes are folded, but they aren’t clean just because they’re folded; you could have folded a pile of dirty clothes).\nWe’ll get more into how to work with different “messy” data configurations in Chapter 22 and Chapter 23, but it’s worth keeping rules 1 and 3 in mind while working through this module."
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#filter-subset-rows",
    "href": "part-wrangling/03-data-cleaning.html#filter-subset-rows",
    "title": "20  Data Cleaning",
    "section": "\n20.4 Filter: Subset rows",
    "text": "20.4 Filter: Subset rows\nFilter allows us to work with a subset of a larger data frame, keeping only the rows we’re interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from filter(). Note that unless we store the result from filter() in the original object, we don’t change the original.\n\n\ndplyr filter() by Allison Horst\n\n\n\n\n\n\n\n\nExample: starwars\n\n\n\nLet’s explore how it works, using the starwars dataset, which contains a comprehensive list of the characters in the Star Wars movies.\nIn the interests of demonstrating the process on the same data, I’ve exported the starwars data to a CSV file using the readr package. I had to remove the list-columns (films, vehicles, starships) because that format isn’t supported by CSV files. You can access the csv data here.\n\n\nR\nPython\n\n\n\nThis data set is included in the dplyr package, so we load that package and then use the data() function to load dataset into memory. The loading isn’t complete until we actually use the dataset though… so let’s print the first few rows.\n\nlibrary(dplyr)\ndata(starwars)\nstarwars\n## # A tibble: 87 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 C-3PO          167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n##  3 R2-D2           96    32 <NA>    white,… red        33   none  mascu… Naboo  \n##  4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  8 R5-D4           97    32 <NA>    white,… red        NA   none  mascu… Tatooi…\n##  9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n## # … with 77 more rows, 4 more variables: species <chr>, films <list>,\n## #   vehicles <list>, starships <list>, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\nWe have to use the exported CSV data in python.\n\nimport pandas as pd\nstarwars = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/starwars.csv\")\nstarwars\n##               name  height   mass  ...     gender homeworld species\n## 0   Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 1            C-3PO   167.0   75.0  ...  masculine  Tatooine   Droid\n## 2            R2-D2    96.0   32.0  ...  masculine     Naboo   Droid\n## 3      Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 4      Leia Organa   150.0   49.0  ...   feminine  Alderaan   Human\n## ..             ...     ...    ...  ...        ...       ...     ...\n## 82             Rey     NaN    NaN  ...   feminine       NaN   Human\n## 83     Poe Dameron     NaN    NaN  ...  masculine       NaN   Human\n## 84             BB8     NaN    NaN  ...  masculine       NaN   Droid\n## 85  Captain Phasma     NaN    NaN  ...        NaN       NaN     NaN\n## 86   Padmé Amidala   165.0   45.0  ...   feminine     Naboo   Human\n## \n## [87 rows x 11 columns]\nfrom skimpy import skim\nskim(starwars)\n## ╭─────────────────────────────── skimpy summary ───────────────────────────────╮\n## │          Data Summary                Data Types                              │\n## │ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                       │\n## │ ┃ dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                       │\n## │ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                       │\n## │ │ Number of rows    │ 87     │ │ string      │ 8     │                       │\n## │ │ Number of columns │ 11     │ │ float64     │ 3     │                       │\n## │ └───────────────────┴────────┘ └─────────────┴───────┘                       │\n## │                                   number                                     │\n## │ ┏━━━━━━━━━━━━━━┳━━━━━┳━━━━━━┳━━━━━━┳━━━━━┳━━━━┳━━━━━┳━━━━━┳━━━━━━┳━━━━━━━━┓  │\n## │ ┃ column_name  ┃ NA  ┃ NA % ┃ mean ┃ sd  ┃ p0 ┃ p25 ┃ p75 ┃ p100 ┃ hist   ┃  │\n## │ ┡━━━━━━━━━━━━━━╇━━━━━╇━━━━━━╇━━━━━━╇━━━━━╇━━━━╇━━━━━╇━━━━━╇━━━━━━╇━━━━━━━━┩  │\n## │ │ height       │   6 │  6.9 │  170 │  35 │ 66 │ 170 │ 190 │  260 │ ▁ ▁█▂  │  │\n## │ │ mass         │  28 │   32 │   97 │ 170 │ 15 │  56 │  84 │ 1400 │   █    │  │\n## │ │ birth_year   │  44 │   51 │   88 │ 150 │  8 │  35 │  72 │  900 │   █    │  │\n## │ └──────────────┴─────┴──────┴──────┴─────┴────┴─────┴─────┴──────┴────────┘  │\n## │                                   string                                     │\n## │ ┏━━━━━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓  │\n## │ ┃ column_name      ┃ NA   ┃ NA %   ┃ words per row      ┃ total words     ┃  │\n## │ ┡━━━━━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩  │\n## │ │ name             │    0 │      0 │                1.8 │             160 │  │\n## │ │ hair_color       │    5 │    5.7 │                1.8 │             160 │  │\n## │ │ skin_color       │    0 │      0 │                1.8 │             160 │  │\n## │ │ eye_color        │    0 │      0 │                1.8 │             160 │  │\n## │ │ sex              │    4 │    4.6 │                1.8 │             160 │  │\n## │ │ gender           │    4 │    4.6 │                1.8 │             160 │  │\n## │ │ homeworld        │   10 │     11 │                1.8 │             160 │  │\n## │ │ species          │    4 │    4.6 │                1.8 │             160 │  │\n## │ └──────────────────┴──────┴────────┴────────────────────┴─────────────────┘  │\n## ╰──────────────────────────────────── End ─────────────────────────────────────╯\n\n\n\n\nOnce the data is set up, filtering the data (selecting certain rows) is actually very simple. Of course, we’ve talked about how to use logical indexing before in Section 9.4.1, but here we’ll focus on using specific functions to perform the same operation.\n\n\nR: dplyr\nPython\nBase R\n\n\n\nThe dplyr verb for selecting rows is filter. filter takes a set of one or more logical conditions, using bare column names and logical operators. Each provided condition is combined using AND.\n\n# Get only the people\nfilter(starwars, species == \"Human\")\n## # A tibble: 35 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  3 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  4 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  5 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  6 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n##  7 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n##  8 Anakin Sky…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n##  9 Wilhuff Ta…    180    NA auburn… fair    blue       64   male  mascu… Eriadu \n## 10 Han Solo       180    80 brown   fair    brown      29   male  mascu… Corell…\n## # … with 25 more rows, 4 more variables: species <chr>, films <list>,\n## #   vehicles <list>, starships <list>, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n# Get only the people who come from Tatooine\nfilter(starwars, species == \"Human\", homeworld == \"Tatooine\")\n## # A tibble: 8 × 14\n##   name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##   <chr>         <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n## 1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n## 2 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n## 3 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n## 4 Beru Whites…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n## 5 Biggs Darkl…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 6 Anakin Skyw…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n## 7 Shmi Skywal…    163    NA black   fair    brown      72   fema… femin… Tatooi…\n## 8 Cliegg Lars     183    NA brown   fair    blue       82   male  mascu… Tatooi…\n## # … with 4 more variables: species <chr>, films <list>, vehicles <list>,\n## #   starships <list>, and abbreviated variable names ¹​hair_color, ²​skin_color,\n## #   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\n\n# Get only the people\nstarwars.query(\"species == 'Human'\")\n\n# Get only the people who come from Tattoine\n##                    name  height   mass  ...     gender     homeworld species\n## 0        Luke Skywalker   172.0   77.0  ...  masculine      Tatooine   Human\n## 3           Darth Vader   202.0  136.0  ...  masculine      Tatooine   Human\n## 4           Leia Organa   150.0   49.0  ...   feminine      Alderaan   Human\n## 5             Owen Lars   178.0  120.0  ...  masculine      Tatooine   Human\n## 6    Beru Whitesun lars   165.0   75.0  ...   feminine      Tatooine   Human\n## 8     Biggs Darklighter   183.0   84.0  ...  masculine      Tatooine   Human\n## 9        Obi-Wan Kenobi   182.0   77.0  ...  masculine       Stewjon   Human\n## 10     Anakin Skywalker   188.0   84.0  ...  masculine      Tatooine   Human\n## 11       Wilhuff Tarkin   180.0    NaN  ...  masculine        Eriadu   Human\n## 13             Han Solo   180.0   80.0  ...  masculine      Corellia   Human\n## 16       Wedge Antilles   170.0   77.0  ...  masculine      Corellia   Human\n## 17     Jek Tono Porkins   180.0  110.0  ...  masculine    Bestine IV   Human\n## 19            Palpatine   170.0   75.0  ...  masculine         Naboo   Human\n## 20            Boba Fett   183.0   78.2  ...  masculine        Kamino   Human\n## 23     Lando Calrissian   177.0   79.0  ...  masculine       Socorro   Human\n## 24                Lobot   175.0   79.0  ...  masculine        Bespin   Human\n## 26           Mon Mothma   150.0    NaN  ...   feminine     Chandrila   Human\n## 27         Arvel Crynyd     NaN    NaN  ...  masculine           NaN   Human\n## 30         Qui-Gon Jinn   193.0   89.0  ...  masculine           NaN   Human\n## 32        Finis Valorum   170.0    NaN  ...  masculine     Coruscant   Human\n## 40       Shmi Skywalker   163.0    NaN  ...   feminine      Tatooine   Human\n## 47           Mace Windu   188.0   84.0  ...  masculine    Haruun Kal   Human\n## 56         Gregar Typho   185.0   85.0  ...  masculine         Naboo   Human\n## 57                Cordé   157.0    NaN  ...   feminine         Naboo   Human\n## 58          Cliegg Lars   183.0    NaN  ...  masculine      Tatooine   Human\n## 62                Dormé   165.0    NaN  ...   feminine         Naboo   Human\n## 63                Dooku   193.0   80.0  ...  masculine       Serenno   Human\n## 64  Bail Prestor Organa   191.0    NaN  ...  masculine      Alderaan   Human\n## 65           Jango Fett   183.0   79.0  ...  masculine  Concord Dawn   Human\n## 70           Jocasta Nu   167.0    NaN  ...   feminine     Coruscant   Human\n## 78      Raymus Antilles   188.0   79.0  ...  masculine      Alderaan   Human\n## 81                 Finn     NaN    NaN  ...  masculine           NaN   Human\n## 82                  Rey     NaN    NaN  ...   feminine           NaN   Human\n## 83          Poe Dameron     NaN    NaN  ...  masculine           NaN   Human\n## 86        Padmé Amidala   165.0   45.0  ...   feminine         Naboo   Human\n## \n## [35 rows x 11 columns]\nstarwars.query(\"species == 'Human' & homeworld == 'Tatooine'\")\n\n# This is another option if you prefer to keep the queries separate\n# starwars.query(\"species == 'Human'\").query(\"homeworld == 'Tatooine'\")\n##                   name  height   mass  ...     gender homeworld species\n## 0       Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 3          Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 5            Owen Lars   178.0  120.0  ...  masculine  Tatooine   Human\n## 6   Beru Whitesun lars   165.0   75.0  ...   feminine  Tatooine   Human\n## 8    Biggs Darklighter   183.0   84.0  ...  masculine  Tatooine   Human\n## 10    Anakin Skywalker   188.0   84.0  ...  masculine  Tatooine   Human\n## 40      Shmi Skywalker   163.0    NaN  ...   feminine  Tatooine   Human\n## 58         Cliegg Lars   183.0    NaN  ...  masculine  Tatooine   Human\n## \n## [8 rows x 11 columns]\n\n\n\nIn base R, you would perform a filtering operation using subset\n\n# Get only the people\nsubset(starwars, species == \"Human\")\n## # A tibble: 35 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  3 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  4 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  5 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  6 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n##  7 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n##  8 Anakin Sky…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n##  9 Wilhuff Ta…    180    NA auburn… fair    blue       64   male  mascu… Eriadu \n## 10 Han Solo       180    80 brown   fair    brown      29   male  mascu… Corell…\n## # … with 25 more rows, 4 more variables: species <chr>, films <list>,\n## #   vehicles <list>, starships <list>, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n# Get only the people who come from Tatooine\nsubset(starwars, species == \"Human\" & homeworld == \"Tatooine\")\n## # A tibble: 8 × 14\n##   name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##   <chr>         <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n## 1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n## 2 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n## 3 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n## 4 Beru Whites…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n## 5 Biggs Darkl…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 6 Anakin Skyw…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n## 7 Shmi Skywal…    163    NA black   fair    brown      72   fema… femin… Tatooi…\n## 8 Cliegg Lars     183    NA brown   fair    blue       82   male  mascu… Tatooi…\n## # … with 4 more variables: species <chr>, films <list>, vehicles <list>,\n## #   starships <list>, and abbreviated variable names ¹​hair_color, ²​skin_color,\n## #   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\nNotice that with subset, you have to use & to join two logical statements; it does not by default take multiple successive arguments.\n\n\n\n\n\n\n20.4.1 Common Row Selection Tasks\nIn dplyr, there are a few helper functions which may be useful when constructing filter statements. In base R or python, these tasks are still important, and so I’ll do my best to show you easy ways to handle each task in each language.\n\n\n\n\n\n\nFiltering by row number\n\n\n\n\n\n\n\nR: dplyr\nPython\nBase R\n\n\n\nrow_number() is a helper function that is only used inside of another dplyr function (e.g. filter). You might want to keep only even rows, or only the first 10 rows in a table.\n\nlibrary(tidyverse)\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\nfilter(poke, (row_number() %% 2 == 0)) \n## # A tibble: 763 × 16\n##      gen pokede…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##    <dbl>    <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n##  1     1        2 https:… Ivys… <NA>    Gras…   405    60     62      63      80\n##  2     1        3 https:… Venu… Mega    Gras…   625    80    100     123     122\n##  3     1        5 https:… Char… <NA>    Fire    405    58     64      58      80\n##  4     1        6 https:… Char… Mega  X Fire…   634    78    130     111     130\n##  5     1        7 https:… Squi… <NA>    Water   314    44     48      65      50\n##  6     1        9 https:… Blas… <NA>    Water   530    79     83     100      85\n##  7     1       10 https:… Cate… <NA>    Bug     195    45     30      35      20\n##  8     1       12 https:… Butt… <NA>    Bug,…   395    60     45      50      90\n##  9     1       14 https:… Kaku… <NA>    Bug,…   205    45     25      50      25\n## 10     1       15 https:… Beed… Mega    Bug,…   495    65    150      40      15\n## # … with 753 more rows, 5 more variables: sp_defense <dbl>, speed <dbl>,\n## #   species <chr>, height_m <dbl>, weight_kg <dbl>, and abbreviated variable\n## #   names ¹​pokedex_no, ²​img_link, ³​sp_attack\n# There are several pokemon who have multiple entries in the table,\n# so the pokedex_number doesn't line up with the row number.\n\n\n\nIn python, the easiest way to accomplish filtering by row number is by using .iloc. But, up until now, we’ve only talked about how Python creates slices using start:(end+1) notation. There is an additional option with slicing - start:(end+1):by. So if we want to get only even rows, we can use the index [::2], which will give us row 0, 2, 4, 6, … through the end of the dataset, because we didn’t specify the start and end portions of the slice.\nBecause Python is 0-indexed, using ::2 will give us the opposite set of rows from that returned in R, which is 1-indexed.\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\npoke.iloc[0::2]\n##       gen  pokedex_no  ... height_m weight_kg\n## 0       1           1  ...      0.7       6.9\n## 2       1           3  ...      2.0     100.0\n## 4       1           4  ...      0.6       8.5\n## 6       1           6  ...      1.7      90.5\n## 8       1           6  ...      1.7      90.5\n## ...   ...         ...  ...      ...       ...\n## 1516    9         999  ...      0.3       5.0\n## 1518    9        1001  ...      1.5      74.2\n## 1520    9        1003  ...      2.7     699.7\n## 1522    9        1005  ...      2.0     380.0\n## 1524    9        1007  ...      2.5     303.0\n## \n## [763 rows x 16 columns]\n\nIf we want to get only odd rows, we can use the index [1::2], which will start at row 1 and give us 1, 3, 5, …\n\n\nIn base R, we’d use seq() to create an index vector instead of using the approach in filter and evaluating the whole index for a logical condition. Alternately, we can use subset, which requires a logical condition, and use 1:nrow(poke) to create an index which we then use for deciding whether each row is even or odd.\n\npoke[seq(1, nrow(poke), 2),]\n## # A tibble: 763 × 16\n##      gen pokede…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##    <dbl>    <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n##  1     1        1 https:… Bulb… <NA>    Gras…   318    45     49      49      65\n##  2     1        3 https:… Venu… <NA>    Gras…   525    80     82      83     100\n##  3     1        4 https:… Char… <NA>    Fire    309    39     52      43      60\n##  4     1        6 https:… Char… <NA>    Fire…   534    78     84      78     109\n##  5     1        6 https:… Char… Mega  Y Fire…   634    78    104      78     159\n##  6     1        8 https:… Wart… <NA>    Water   405    59     63      80      65\n##  7     1        9 https:… Blas… Mega    Water   630    79    103     120     135\n##  8     1       11 https:… Meta… <NA>    Bug     205    50     20      55      25\n##  9     1       13 https:… Weed… <NA>    Bug,…   195    40     35      30      20\n## 10     1       15 https:… Beed… <NA>    Bug,…   395    65     90      40      45\n## # … with 753 more rows, 5 more variables: sp_defense <dbl>, speed <dbl>,\n## #   species <chr>, height_m <dbl>, weight_kg <dbl>, and abbreviated variable\n## #   names ¹​pokedex_no, ²​img_link, ³​sp_attack\n\nsubset(poke, 1:nrow(poke) %% 2 == 0)\n## # A tibble: 763 × 16\n##      gen pokede…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##    <dbl>    <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n##  1     1        2 https:… Ivys… <NA>    Gras…   405    60     62      63      80\n##  2     1        3 https:… Venu… Mega    Gras…   625    80    100     123     122\n##  3     1        5 https:… Char… <NA>    Fire    405    58     64      58      80\n##  4     1        6 https:… Char… Mega  X Fire…   634    78    130     111     130\n##  5     1        7 https:… Squi… <NA>    Water   314    44     48      65      50\n##  6     1        9 https:… Blas… <NA>    Water   530    79     83     100      85\n##  7     1       10 https:… Cate… <NA>    Bug     195    45     30      35      20\n##  8     1       12 https:… Butt… <NA>    Bug,…   395    60     45      50      90\n##  9     1       14 https:… Kaku… <NA>    Bug,…   205    45     25      50      25\n## 10     1       15 https:… Beed… Mega    Bug,…   495    65    150      40      15\n## # … with 753 more rows, 5 more variables: sp_defense <dbl>, speed <dbl>,\n## #   species <chr>, height_m <dbl>, weight_kg <dbl>, and abbreviated variable\n## #   names ¹​pokedex_no, ²​img_link, ³​sp_attack\n\nThis is less fun than using dplyr because you have to repeat the name of the dataset at least twice using base R, but either option will get you where you’re going. The real power of dplyr is in the collection of the full set of verbs with a consistent user interface; nothing done in dplyr is so special that it can’t be done in base R as well.\n\n\n\n\n\n\n\n\n\n\n\n\nSorting rows by variable values\n\n\n\n\n\nAnother common operation is to sort your data frame by the values of one or more variables.\n\n\nR: dplyr\nPython\nBase R\n\n\n\narrange() is a dplyr verb for sort rows in the table by one or more variables. It is often used with a helper function, desc(), which reverses the order of a variable, sorting it in descending order. Multiple arguments can be passed to arrange to sort the data frame by multiple columns hierarchically; each column can be modified with desc() separately.\n\narrange(poke, desc(total))\n## # A tibble: 1,526 × 16\n##      gen pokede…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##    <dbl>    <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n##  1     8      890 https:… Eter… Eterna… Pois…  1125   255    115     250     125\n##  2     1      150 https:… Mewt… Mega  X Psyc…   780   106    190     100     154\n##  3     1      150 https:… Mewt… Mega  Y Psyc…   780   106    150      70     194\n##  4     3      384 https:… Rayq… Mega    Drag…   780   105    180     100     180\n##  5     3      382 https:… Kyog… Primal  Water   770   100    150      90     180\n##  6     3      383 https:… Grou… Primal  Grou…   770   100    180     160     150\n##  7     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n##  8     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n##  9     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n## 10     4      493 https:… Arce… <NA>    Norm…   720   120    120     120     120\n## # … with 1,516 more rows, 5 more variables: sp_defense <dbl>, speed <dbl>,\n## #   species <chr>, height_m <dbl>, weight_kg <dbl>, and abbreviated variable\n## #   names ¹​pokedex_no, ²​img_link, ³​sp_attack\n\n\n\nIn pandas, we use the sort_values function, which has an argument ascending. Multiple columns can be passed in to sort by multiple columns in a hierarchical manner.\n\npoke.sort_values(['total'], ascending = False)\n##       gen  pokedex_no  ... height_m weight_kg\n## 1370    8         890  ...     20.0     950.0\n## 282     1         150  ...      2.0     122.0\n## 283     1         150  ...      2.0     122.0\n## 584     3         384  ...      7.0     206.5\n## 580     3         382  ...      4.5     352.0\n## ...   ...         ...  ...      ...       ...\n## 1336    8         872  ...      0.3       3.8\n## 328     2         191  ...      0.3       1.8\n## 1283    8         824  ...      0.4       8.0\n## 1187    7         746  ...      0.2       0.3\n## 1188    7         746  ...      0.2       0.3\n## \n## [1526 rows x 16 columns]\n\n\n\nThe sort() function in R can be used to sort a vector, but when sorting a data frame we usually want to use the order() function instead. This is because sort() orders the values of the argument directly, where order() returns a sorted index.\n\nx <- c(32, 25, 98, 45, 31, 19, 5)\nsort(x)\n## [1]  5 19 25 31 32 45 98\norder(x)\n## [1] 7 6 2 5 1 4 3\n\nWhen working with a data frame, we want to sort the entire data frame’s rows by the variables we choose; it is easiest to do this using an index to reorder the rows.\n\npoke[order(poke$total, decreasing = T),]\n## # A tibble: 1,526 × 16\n##      gen pokede…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##    <dbl>    <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n##  1     8      890 https:… Eter… Eterna… Pois…  1125   255    115     250     125\n##  2     1      150 https:… Mewt… Mega  X Psyc…   780   106    190     100     154\n##  3     1      150 https:… Mewt… Mega  Y Psyc…   780   106    150      70     194\n##  4     3      384 https:… Rayq… Mega    Drag…   780   105    180     100     180\n##  5     3      382 https:… Kyog… Primal  Water   770   100    150      90     180\n##  6     3      383 https:… Grou… Primal  Grou…   770   100    180     160     150\n##  7     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n##  8     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n##  9     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n## 10     4      493 https:… Arce… <NA>    Norm…   720   120    120     120     120\n## # … with 1,516 more rows, 5 more variables: sp_defense <dbl>, speed <dbl>,\n## #   species <chr>, height_m <dbl>, weight_kg <dbl>, and abbreviated variable\n## #   names ¹​pokedex_no, ²​img_link, ³​sp_attack\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeep the top \\(n\\) values of a variable\n\n\n\n\n\n\n\nR: dplyr\nPython\nBase R\n\n\n\nslice_max() will keep the top values of a specified variable. This is like a filter statement, but it’s a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a few more lines of code.\n\nslice_max(poke, order_by = total, n = 5)\n## # A tibble: 6 × 16\n##     gen pokedex…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##   <dbl>     <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n## 1     8       890 https:… Eter… Eterna… Pois…  1125   255    115     250     125\n## 2     1       150 https:… Mewt… Mega  X Psyc…   780   106    190     100     154\n## 3     1       150 https:… Mewt… Mega  Y Psyc…   780   106    150      70     194\n## 4     3       384 https:… Rayq… Mega    Drag…   780   105    180     100     180\n## 5     3       382 https:… Kyog… Primal  Water   770   100    150      90     180\n## 6     3       383 https:… Grou… Primal  Grou…   770   100    180     160     150\n## # … with 5 more variables: sp_defense <dbl>, speed <dbl>, species <chr>,\n## #   height_m <dbl>, weight_kg <dbl>, and abbreviated variable names\n## #   ¹​pokedex_no, ²​img_link, ³​sp_attack\n\nBy default, slice_max() returns values tied with the nth value as well, which is why our result has 6 rows.\n\nslice_max(poke, order_by = total, n = 5, with_ties = F) \n## # A tibble: 5 × 16\n##     gen pokedex…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##   <dbl>     <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n## 1     8       890 https:… Eter… Eterna… Pois…  1125   255    115     250     125\n## 2     1       150 https:… Mewt… Mega  X Psyc…   780   106    190     100     154\n## 3     1       150 https:… Mewt… Mega  Y Psyc…   780   106    150      70     194\n## 4     3       384 https:… Rayq… Mega    Drag…   780   105    180     100     180\n## 5     3       382 https:… Kyog… Primal  Water   770   100    150      90     180\n## # … with 5 more variables: sp_defense <dbl>, speed <dbl>, species <chr>,\n## #   height_m <dbl>, weight_kg <dbl>, and abbreviated variable names\n## #   ¹​pokedex_no, ²​img_link, ³​sp_attack\n\nOf course, there is a similar slice_min() function as well:\n\nslice_min(poke, order_by = total, n = 5)\n## # A tibble: 5 × 16\n##     gen pokedex…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##   <dbl>     <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n## 1     7       746 https:… Wish… Solo    Water   175    45     20      20      25\n## 2     7       746 https:… Wish… Solo    Water   175    45     20      20      25\n## 3     2       191 https:… Sunk… <NA>    Grass   180    30     30      30      30\n## 4     8       824 https:… Blip… <NA>    Bug     180    25     20      20      25\n## 5     8       872 https:… Snom  <NA>    Ice,…   185    30     25      35      45\n## # … with 5 more variables: sp_defense <dbl>, speed <dbl>, species <chr>,\n## #   height_m <dbl>, weight_kg <dbl>, and abbreviated variable names\n## #   ¹​pokedex_no, ²​img_link, ³​sp_attack\n\nslice_max and slice_min also take a prop argument that gives you a certain proportion of the values:\n\nslice_max(poke, order_by = total, prop = .01)\n## # A tibble: 30 × 16\n##      gen pokede…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##    <dbl>    <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n##  1     8      890 https:… Eter… Eterna… Pois…  1125   255    115     250     125\n##  2     1      150 https:… Mewt… Mega  X Psyc…   780   106    190     100     154\n##  3     1      150 https:… Mewt… Mega  Y Psyc…   780   106    150      70     194\n##  4     3      384 https:… Rayq… Mega    Drag…   780   105    180     100     180\n##  5     3      382 https:… Kyog… Primal  Water   770   100    150      90     180\n##  6     3      383 https:… Grou… Primal  Grou…   770   100    180     160     150\n##  7     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n##  8     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n##  9     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n## 10     4      493 https:… Arce… <NA>    Norm…   720   120    120     120     120\n## # … with 20 more rows, 5 more variables: sp_defense <dbl>, speed <dbl>,\n## #   species <chr>, height_m <dbl>, weight_kg <dbl>, and abbreviated variable\n## #   names ¹​pokedex_no, ²​img_link, ³​sp_attack\n\n\n\nIn Python, nlargest and nsmallest work roughly the same as dplyr’s slice_max and slice_min for integer counts.\n\npoke.nlargest(5, 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1370    8         890  ...     20.0     950.0\n## 282     1         150  ...      2.0     122.0\n## 283     1         150  ...      2.0     122.0\n## 584     3         384  ...      7.0     206.5\n## 580     3         382  ...      4.5     352.0\n## \n## [5 rows x 16 columns]\npoke.nsmallest(5, 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1187    7         746  ...      0.2       0.3\n## 1188    7         746  ...      0.2       0.3\n## 328     2         191  ...      0.3       1.8\n## 1283    8         824  ...      0.4       8.0\n## 1336    8         872  ...      0.3       3.8\n## \n## [5 rows x 16 columns]\n\nTo get proportions, though, we have to do some math:\n\npoke.nlargest(int(len(poke)*0.01), 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1370    8         890  ...     20.0     950.0\n## 282     1         150  ...      2.0     122.0\n## 283     1         150  ...      2.0     122.0\n## 584     3         384  ...      7.0     206.5\n## 580     3         382  ...      4.5     352.0\n## 582     3         383  ...      3.5     950.0\n## 1257    7         800  ...      2.4     230.0\n## 1258    7         800  ...      2.4     230.0\n## 1259    7         800  ...      2.4     230.0\n## 779     4         493  ...      3.2     320.0\n## 1126    6         718  ...      5.0     305.0\n## 1127    6         718  ...      5.0     305.0\n## 1128    6         718  ...      5.0     305.0\n## 405     2         248  ...      2.0     202.0\n## 567     3         373  ...      1.5     102.6\n## \n## [15 rows x 16 columns]\npoke.nsmallest(int(len(poke)*0.01), 'total')\n##       gen  pokedex_no  ... height_m weight_kg\n## 1187    7         746  ...      0.2       0.3\n## 1188    7         746  ...      0.2       0.3\n## 328     2         191  ...      0.3       1.8\n## 1283    8         824  ...      0.4       8.0\n## 1336    8         872  ...      0.3       3.8\n## 465     3         298  ...      0.2       2.0\n## 616     4         401  ...      0.3       2.2\n## 13      1          10  ...      0.3       2.9\n## 16      1          13  ...      0.3       3.2\n## 431     3         265  ...      0.3       3.6\n## 446     3         280  ...      0.4       6.6\n## 248     1         129  ...      0.9      10.0\n## 524     3         349  ...      0.6       7.4\n## 1032    6         664  ...      0.3       2.5\n## 1237    7         789  ...      0.2       0.1\n## \n## [15 rows x 16 columns]\n\n\n\nThe simplest way to do this type of task with base R is to combine the order() function and indexing. In the case of selecting the top 1% of rows, we need to use round(nrow(poke)*.01) to get an integer.\n\npoke[order(poke$total, decreasing = T)[1:5],]\n## # A tibble: 5 × 16\n##     gen pokedex…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##   <dbl>     <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n## 1     8       890 https:… Eter… Eterna… Pois…  1125   255    115     250     125\n## 2     1       150 https:… Mewt… Mega  X Psyc…   780   106    190     100     154\n## 3     1       150 https:… Mewt… Mega  Y Psyc…   780   106    150      70     194\n## 4     3       384 https:… Rayq… Mega    Drag…   780   105    180     100     180\n## 5     3       382 https:… Kyog… Primal  Water   770   100    150      90     180\n## # … with 5 more variables: sp_defense <dbl>, speed <dbl>, species <chr>,\n## #   height_m <dbl>, weight_kg <dbl>, and abbreviated variable names\n## #   ¹​pokedex_no, ²​img_link, ³​sp_attack\npoke[order(poke$total, decreasing = T)[1:round(nrow(poke)*.01)],]\n## # A tibble: 15 × 16\n##      gen pokede…¹ img_l…² name  variant type  total    hp attack defense sp_at…³\n##    <dbl>    <dbl> <chr>   <chr> <chr>   <chr> <dbl> <dbl>  <dbl>   <dbl>   <dbl>\n##  1     8      890 https:… Eter… Eterna… Pois…  1125   255    115     250     125\n##  2     1      150 https:… Mewt… Mega  X Psyc…   780   106    190     100     154\n##  3     1      150 https:… Mewt… Mega  Y Psyc…   780   106    150      70     194\n##  4     3      384 https:… Rayq… Mega    Drag…   780   105    180     100     180\n##  5     3      382 https:… Kyog… Primal  Water   770   100    150      90     180\n##  6     3      383 https:… Grou… Primal  Grou…   770   100    180     160     150\n##  7     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n##  8     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n##  9     7      800 https:… Necr… Ultra   Psyc…   754    97    167      97     167\n## 10     4      493 https:… Arce… <NA>    Norm…   720   120    120     120     120\n## 11     6      718 https:… Zyga… Comple… Drag…   708   216    100     121      91\n## 12     6      718 https:… Zyga… Comple… Drag…   708   216    100     121      91\n## 13     6      718 https:… Zyga… Comple… Drag…   708   216    100     121      91\n## 14     2      248 https:… Tyra… Mega    Rock…   700   100    164     150      95\n## 15     3      373 https:… Sala… Mega    Drag…   700    95    145     130     120\n## # … with 5 more variables: sp_defense <dbl>, speed <dbl>, species <chr>,\n## #   height_m <dbl>, weight_kg <dbl>, and abbreviated variable names\n## #   ¹​pokedex_no, ²​img_link, ³​sp_attack\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Filtering\n\n\n\n\n\nProblem\nR: dplyr\nPython\nBase R\n\n\n\nUse the Pokemon data to accomplish the following:\n\ncreate a new data frame that has only water type Pokemon\nwrite a filter statement that looks for any Pokemon which has water type for either type1 or type2\n\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\nfilter(poke, type_1 == \"Water\")\n## Error in `filter()`:\n## ! Problem while computing `..1 = type_1 == \"Water\"`.\n## Caused by error in `mask$eval_all_filter()`:\n## ! object 'type_1' not found\n\nfilter(poke, type_1 == \"Water\" | type_2 == \"Water\")\n## Error in `filter()`:\n## ! Problem while computing `..1 = type_1 == \"Water\" | type_2 == \"Water\"`.\n## Caused by error in `mask$eval_all_filter()`:\n## ! object 'type_1' not found\n# The conditions have to be separated by |, which means \"or\"\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke.query(\"type_1=='Water'\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): pandas.core.computation.ops.UndefinedVariableError: name 'type_1' is not defined\npoke.query(\"type_1=='Water'|type_2=='Water'\")\n# The conditions have to be separated by |, which means \"or\"\n## Error in py_call_impl(callable, dots$args, dots$keywords): pandas.core.computation.ops.UndefinedVariableError: name 'type_1' is not defined\n\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\nsubset(poke, type_1 == \"Water\")\n## Error in eval(e, x, parent.frame()): object 'type_1' not found\n\nsubset(poke, type_1 == \"Water\" | type_2 == \"Water\")\n## Error in eval(e, x, parent.frame()): object 'type_1' not found\n# The conditions have to be separated by |, which means \"or\""
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#select-pick-columns",
    "href": "part-wrangling/03-data-cleaning.html#select-pick-columns",
    "title": "20  Data Cleaning",
    "section": "\n20.5 Select: Pick columns",
    "text": "20.5 Select: Pick columns\nSometimes, we don’t want to work with a set of 50 variables when we’re only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g. df[, c(1, 3, 5)]), but this can get tedious.\n\n\nR: dplyr\nPython\nBase R\n\n\n\nIn dplyr, the function to pick a few columns is select(). The syntax from the help file (?select) looks deceptively simple.\n\nselect(.data, …)\n\nSo as with just about every other tidyverse function, the first argument in a select statement is the data. After that, though, you can put just about anything that R can interpret. ... means something along the lines of “put in any additional arguments that make sense in context or might be passed on to other functions”.\nSo what can go in there?\n\n\n\n\n\n\nWays to select variables in dplyr\n\n\n\n\n\nFirst, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).3\nMost dplyr commands work with “bare” variable names - you don’t need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they’re very explicitly exceptions.\n\nvar3:var5: select(df, var3:var5) will give you a data frame with columns var3, anything between var3 and var 5, and var5\n\n!(<set of variables>) will give you any columns that aren’t in the set of variables in parentheses\n\n\n(<set of vars 1>) & (<set of vars 2>) will give you any variables that are in both set 1 and set 2. (<set of vars 1>) | (<set of vars 2>) will give you any variables that are in either set 1 or set 2.\n\nc() combines sets of variables.\n\n\n\ndplyr also defines a lot of variable selection “helpers” that can be used inside select() statements. These statements work with bare column names (so you don’t have to put quotes around the column names when you use them).\n\n\neverything() matches all variables\n\nlast_col() matches the last variable. last_col(offset = n) selects the n-th to last variable.\n\nstarts_with(\"xyz\") will match any columns with names that start with xyz. Similarly, ends_with() does exactly what you’d expect as well.\n\ncontains(\"xyz\") will match any columns with names containing the literal string “xyz”. Note, contains does not work with regular expressions (you don’t need to know what that means right now).\n\nmatches(regex) takes a regular expression as an argument and returns all columns matching that expression.\n\nnum_range(prefix, range) selects any columns that start with prefix and have numbers matching the provided numerical range.\n\nThere are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables.\n\n\nall_of(char) matches all variable names in the character vector char. If one of the variables doesn’t exist, this will return an error.\n\nany_of(char) matches the contents of the character vector char, but does not throw an error if the variable doesn’t exist in the data set.\n\nThere’s one final selector -\n\n\nwhere() applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative.\n\n\n\n\nLet’s try these selector functions out and see what we can accomplish!\n\nlibrary(nycflights13)\ndata(flights)\nstr(flights)\n## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n##  $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\nWe’ll start out with the nycflights13 package, which contains information on all flights that left a NYC airport to destinations in the US, Puerto Rico, and the US Virgin Islands.\n\n\n\n\n\n\nTip\n\n\n\nYou might want to try out your EDA (Exploratory Data Analysis) skills to see what you can find out about the dataset, before seeing how select() works.\n\n\nWe could get a data frame of departure information for each flight:\n\nselect(flights, flight, year:day, tailnum, origin, matches(\"dep\"))\n## # A tibble: 336,776 × 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     <int> <int> <int> <int> <chr>   <chr>     <int>          <int>     <dbl>\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # … with 336,766 more rows\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\nflights %>%\n  select(carrier:dest, everything())\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    <chr>    <int> <chr>   <chr>  <chr> <int> <int> <int>   <int>   <int>   <dbl>\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time <int>,\n## #   sched_arr_time <int>, arr_delay <dbl>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n\nNote that everything() won’t duplicate columns you’ve already added.\nExploring the difference between bare name selection and all_of()/any_of()\n\nflights %>%\n  select(carrier, flight, tailnum, matches(\"time\"))\n## # A tibble: 336,776 × 9\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    <chr>    <int> <chr>      <int>          <int>    <int>         <int>   <dbl>\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, 1 more variable: time_hour <dttm>, and abbreviated\n## #   variable names ¹​sched_arr_time, ²​air_time\n\nvarlist <- c(\"carrier\", \"flight\", \"tailnum\",\n             \"dep_time\", \"sched_dep_time\", \"arr_time\", \"sched_arr_time\",\n             \"air_time\")\n\nflights %>%\n  select(all_of(varlist))\n## # A tibble: 336,776 × 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    <chr>    <int> <chr>      <int>          <int>    <int>         <int>   <dbl>\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, and abbreviated variable names ¹​sched_arr_time,\n## #   ²​air_time\n\nvarlist <- c(varlist, \"whoops\")\n\nflights %>%\n  select(all_of(varlist)) # this errors out b/c whoops doesn't exist\n## Error in `select()`:\n## ! Problem while evaluating `all_of(varlist)`.\n\nflights %>%\nselect(any_of(varlist)) # this runs just fine\n## # A tibble: 336,776 × 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    <chr>    <int> <chr>      <int>          <int>    <int>         <int>   <dbl>\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, and abbreviated variable names ¹​sched_arr_time,\n## #   ²​air_time\n\nSo for now, at least in R, you know how to cut your data down to size rowwise (with filter) and column-wise (with select).\n\n\nFirst, let’s install the nycflights13 package[3] in python by typing the following into your system terminal.\n\npip install nycflights13\n## Defaulting to user installation because normal site-packages is not writeable\n## Requirement already satisfied: nycflights13 in /home/susan/.local/lib/python3.10/site-packages (0.0.3)\n## Requirement already satisfied: pandas>=0.24.0 in /home/susan/.local/lib/python3.10/site-packages (from nycflights13) (1.3.5)\n## Requirement already satisfied: pytz>=2017.3 in /home/susan/.local/lib/python3.10/site-packages (from pandas>=0.24.0->nycflights13) (2019.3)\n## Requirement already satisfied: numpy>=1.21.0 in /home/susan/.local/lib/python3.10/site-packages (from pandas>=0.24.0->nycflights13) (1.24.2)\n## Requirement already satisfied: python-dateutil>=2.7.3 in /home/susan/.local/lib/python3.10/site-packages (from pandas>=0.24.0->nycflights13) (2.8.1)\n## Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->nycflights13) (1.16.0)\n\nThen, we can load the flights data from the nycflights13 package.\n\nfrom nycflights13 import flights\n\nSelect operations are not as easy in python as they are when using dplyr::select() with helpers, but of course you can accomplish the same tasks.\n\ncols = flights.columns\n\n# Rearrange column order by manual indexing\nx = cols[9:13].append(cols[0:9])\nx = x.append(cols[13:19])\n\n# Then use the index to rearrange the columns\nflights.loc[:,x]\n##        carrier  flight tailnum  ... hour  minute             time_hour\n## 0           UA    1545  N14228  ...    5      15  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...    5      29  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...    5      40  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...    5      45  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...    6       0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...  ...     ...                   ...\n## 336771      9E    3393     NaN  ...   14      55  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...   22       0  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...   12      10  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...   11      59  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...    8      40  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]\n\nList Comprehensions\nIn Python, there are certain shorthands called “list comprehensions” [4] that can perform similar functions to e.g. the matches() function in dplyr.\nSuppose we want to get all columns containing the word ‘time’. We could iterate through the list of columns (flights.columns) and add the column name any time we detect the word ‘time’ within. That is essentially what the following code does:\n\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\ntimecols\n## ['dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour']\n\nExplaining the code:\n\n\nfor col in flights.columns iterates through the list of columns, storing each column name in the variable col\n\n\nif 'time' in col detects the presence of the word ‘time’ in the column name stored in col\n\nthe col out front adds the column name in the variable col to the array of columns to keep\nSelecting columns in Python\n\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\n# Other columns\nselcols = [\"carrier\", \"flight\", \"tailnum\"]\n# Combine the two lists\nselcols.extend(timecols)\n\n# Subset the data frame\nflights.loc[:,selcols]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\nselcols.extend([\"whoops\"])\nselcols\n\n# Subset the data frame\n## ['carrier', 'flight', 'tailnum', 'dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour', 'whoops']\nflights.loc[:,selcols]\n\n# Error-tolerance - use list comprehension to check if \n# variable names are in the data frame\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: \"['whoops'] not in index\"\nselcols_fixed = [x for x in selcols if x in flights.columns]\nflights.loc[:,selcols_fixed]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\n\n\n\n\nIn base R, we typically select columns by name or index directly. This is nowhere near as convenient, of course, but there are little shorthand ways to replicate the functionality of e.g. matches in dplyr.\ngrepl is a shorthand function for grep, which searches for a pattern in a vector of strings. grepl returns a logical vector indicating whether the pattern (\"dep\", in this case) was found in the vector (names(flights), in this case).\n\n\ndepcols <- names(flights)[grepl(\"dep\", names(flights))]\ncollist <- c(\"flight\", \"year\", \"month\", \"day\", \"tailnum\", \"origin\", depcols)\n\nflights[,collist]\n## # A tibble: 336,776 × 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     <int> <int> <int> <int> <chr>   <chr>     <int>          <int>     <dbl>\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # … with 336,766 more rows\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\nnew_order <- names(flights)\nnew_order <- new_order[c(10:14, 1:9, 15:19)]\n\nflights[,new_order]\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    <chr>    <int> <chr>   <chr>  <chr> <int> <int> <int>   <int>   <int>   <dbl>\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time <int>,\n## #   sched_arr_time <int>, arr_delay <dbl>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n\nThis is less convenient than dplyr::everything in part because it depends on us to get the column indexes right.\n\n\n\n\n\n\n\n\n\nRearranging Columns\n\n\n\n\n\n\n\ndplyr::relocate\nPython\n\n\n\nAnother handy dplyr function is relocate; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I’m covering relocate here mostly because it also comes with this amazing cartoon illustration.\n\n\nrelocate lets you rearrange columns (by Allison Horst)\n\n\n\n# Move flight specific info to the front\ndata(flights, package = \"nycflights13\")\nrelocate(flights, carrier:dest, everything())\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    <chr>    <int> <chr>   <chr>  <chr> <int> <int> <int>   <int>   <int>   <dbl>\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time <int>,\n## #   sched_arr_time <int>, arr_delay <dbl>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n\n# move numeric variables to the front\nflights %>% relocate(where(is.numeric))\n## # A tibble: 336,776 × 19\n##     year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ flight\n##    <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl>  <int>\n##  1  2013     1     1      517         515       2     830     819      11   1545\n##  2  2013     1     1      533         529       4     850     830      20   1714\n##  3  2013     1     1      542         540       2     923     850      33   1141\n##  4  2013     1     1      544         545      -1    1004    1022     -18    725\n##  5  2013     1     1      554         600      -6     812     837     -25    461\n##  6  2013     1     1      554         558      -4     740     728      12   1696\n##  7  2013     1     1      555         600      -5     913     854      19    507\n##  8  2013     1     1      557         600      -3     709     723     -14   5708\n##  9  2013     1     1      557         600      -3     838     846      -8     79\n## 10  2013     1     1      558         600      -2     753     745       8    301\n## # … with 336,766 more rows, 9 more variables: air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, carrier <chr>, tailnum <chr>, origin <chr>,\n## #   dest <chr>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\n\nThere are similar ways to rearrange columns in pandas, but they are a bit harder to work with - you have to specify the column names (in some way) and then perform the operation yourself.\n\nimport numpy as np\ncols = list(flights.columns.values) # get column names\n\n# Move flight specific info to the front\nflightcols = ['carrier', 'flight', 'tailnum', 'origin', 'dest']\nflights[flightcols + list(flights.drop(flightcols, axis = 1))]\n\n# move numeric variables to the front\n##        carrier  flight tailnum  ... hour minute             time_hour\n## 0           UA    1545  N14228  ...    5     15  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...    5     29  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...    5     40  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...    5     45  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...    6      0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...  ...    ...                   ...\n## 336771      9E    3393     NaN  ...   14     55  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...   22      0  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...   12     10  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...   11     59  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...    8     40  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]\nnumcols = list(flights.select_dtypes(include = np.number).columns.values)\nflights[numcols + list(flights.drop(numcols, axis = 1))]\n##         year  month  day  dep_time  ...  tailnum  origin  dest             time_hour\n## 0       2013      1    1     517.0  ...   N14228     EWR   IAH  2013-01-01T10:00:00Z\n## 1       2013      1    1     533.0  ...   N24211     LGA   IAH  2013-01-01T10:00:00Z\n## 2       2013      1    1     542.0  ...   N619AA     JFK   MIA  2013-01-01T10:00:00Z\n## 3       2013      1    1     544.0  ...   N804JB     JFK   BQN  2013-01-01T10:00:00Z\n## 4       2013      1    1     554.0  ...   N668DN     LGA   ATL  2013-01-01T11:00:00Z\n## ...      ...    ...  ...       ...  ...      ...     ...   ...                   ...\n## 336771  2013      9   30       NaN  ...      NaN     JFK   DCA  2013-09-30T18:00:00Z\n## 336772  2013      9   30       NaN  ...      NaN     LGA   SYR  2013-10-01T02:00:00Z\n## 336773  2013      9   30       NaN  ...   N535MQ     LGA   BNA  2013-09-30T16:00:00Z\n## 336774  2013      9   30       NaN  ...   N511MQ     LGA   CLE  2013-09-30T15:00:00Z\n## 336775  2013      9   30       NaN  ...   N839MQ     LGA   RDU  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#mutate-add-and-transform-variables",
    "href": "part-wrangling/03-data-cleaning.html#mutate-add-and-transform-variables",
    "title": "20  Data Cleaning",
    "section": "\n20.6 Mutate: Add and transform variables",
    "text": "20.6 Mutate: Add and transform variables\nUp to this point, we’ve been primarily focusing on how to decrease the dimensionality of our dataset in various ways. But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is mutate, but in base R and python, we’ll simply use assignment to add columns to our data frames.\n\n\n\n\nMutate (by Allison Horst)\n\n\nWe’ll use the Pokemon data to demonstrate. Some Pokemon have a single “type”, which is usually elemental, such as Water, Ice, Fire, etc., but others have two. Let’s add a column that indicates how many types a pokemon has.\n\n\nBase R\nR: dplyr\nPython\n\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke$no_types <- 1 # set a default value\npoke$no_types[!is.na(poke$type_2)] <- 2 # set the value if type_2 is not NA\n\n# This is a bit faster\npoke$no_types <- ifelse(is.na(poke$type_2), 1, 2)\n## Error:\n## ! Assigned data `ifelse(is.na(poke$type_2), 1, 2)` must be compatible with existing data.\n## ✖ Existing data has 1526 rows.\n## ✖ Assigned data has 0 rows.\n## ℹ Only vectors of size 1 are recycled.\n\n# This checks number of types vs. value of type_2 (sanity)\nt(table(poke$type_2, poke$no_types, useNA = 'ifany'))\n## Error in table(poke$type_2, poke$no_types, useNA = \"ifany\"): all arguments must have the same length\n\nNotice that we had to type the name of the dataset at least 3 times to perform the operation we were looking for. I could reduce that to 2x with the ifelse function, but it’s still a lot of typing.\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke <- poke %>%\n  mutate(no_types = if_else(is.na(type_2), 1, 2))\n## Error in `mutate()`:\n## ! Problem while computing `no_types = if_else(is.na(type_2), 1, 2)`.\n## Caused by error in `if_else()`:\n## ! object 'type_2' not found\n\nselect(poke, type_2, no_types) %>% table(useNA = 'ifany') %>% t()\n## Error in `select()`:\n## ! Can't subset columns that don't exist.\n## ✖ Column `type_2` doesn't exist.\n\nThe last 2 rows are just to organize the output - we keep only the two variables we’re working with, and get a crosstab.\n\n\nIn python, this type of variable operation (replacing one value with another) can be most easily done with the replace function, which takes arguments (thing_to_replace, value_to_replace_with).\n\nimport pandas as pd\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke[\"no_types\"] = 1 # default value\npoke.loc[~poke.type_2.isna(), \"no_types\"] = 2 # change those with a defined type 2\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'DataFrame' object has no attribute 'type_2'\n\nAnother function that may be useful is the assign function, which can be used to create new variables if you don’t want to use the [\"new_col\"] notation. In some circumstances, .assign(var = ...) is a bit easier to work with because Python distinguishes between modifications to data and making a copy of the entire data frame (which is something I’d like to not get into right now for simplicity’s sake).\n\n\n\nThe learning curve here isn’t actually knowing how to assign new variables (though that’s important). The challenge comes when you want to do something new and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables.\n\n\n\n\n\n\nMutate and new challenges\n\n\n\n\n\nI’m not going to be able to teach you how to handle every mutate statement task you’ll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to read documentation, google things intelligently, and to understand what you’re reading enough to actually implement it. This is something that comes with practice (and lots of googling, stack overflow searches, etc.).\nGoogle and StackOverflow are very common and important programming skills!\n\n\nSource\n\n\n\n\nSource\n\n\nIn this textbook, the examples will expose you to solutions to common problems (or require that you do some basic reading yourself); unfortunately, there are too many common problems for us to work through line-by-line.\nPart of the goal of this textbook is to help you learn how to read through a package description and evaluate whether the package will do what you want. We’re going to try to build some of those skills starting now. It would be relatively easy to teach you how to do a set list of tasks, but you’ll be better statisticians and programmers if you learn the skills to solve niche problems on your own.\n\n\nApologies for the noninclusive language, but the sentiment is real. Source\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere is a quick table of places to look in R and python to solve some of the more common problems.\n\n\nProblem\nR\nPython\n\n\n\nDates and Times\n\nlubridate package (esp. ymd_hms() and variants, decimal_date(), and other convenience functions)\n\npandas has some date time support by default; see the datetime module for more functionality.\n\n\nString manipulation\n\nstringr package\nQuick Tips [5], Whirlwind Tour of Python chapter [6]"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#summarize",
    "href": "part-wrangling/03-data-cleaning.html#summarize",
    "title": "20  Data Cleaning",
    "section": "\n20.7 Summarize",
    "text": "20.7 Summarize\nThe next verb is one that we’ve already implicitly seen in action: summarize takes a data frame with potentially many rows of data and reduces it down to one row of data using some function. You have used it to get single-row summaries of vectorized data in R, and we’ve used e.g. group_by + count in Python to perform certain tasks as well.\nHere (in a trivial example), I compute the overall average HP of a Pokemon in each generation, as well as the average number of characters in their name. Admittedly, that last computation is a bit silly, but it’s mostly for demonstration purposes.\n\n\nR: dplyr\nPython\n\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\npoke %>%\n  mutate(name_chr = nchar(name)) %>%\n  summarize(n = max(pokedex_no), hp = mean(hp), name_chr = mean(name_chr))\n## # A tibble: 1 × 3\n##       n    hp name_chr\n##   <dbl> <dbl>    <dbl>\n## 1  1008  71.2     7.55\n\n\n\nIn python, instead of a summarize function, there are a number of shorthand functions that we often use to summarize things, such as mean. You can also build custom summary functions [7], or use the agg() function to define multiple summary variables. agg() will even let you use different summary functions for each variable, just like summarize.\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke = poke.assign(name_length = poke.name.str.len())\npoke[['hp', 'name_length']].mean()\n## hp             71.178244\n## name_length     7.545216\n## dtype: float64\npoke[['hp', 'name_length']].agg(['mean', 'min'])\n##              hp  name_length\n## mean  71.178244     7.545216\n## min    1.000000     3.000000\npoke[['pokedex_no', 'hp', 'name_length']].agg({'pokedex_no':'nunique', 'hp':'mean', 'name_length':'mean'})\n## pokedex_no     1008.000000\n## hp               71.178244\n## name_length       7.545216\n## dtype: float64\n\n\n\n\nThe real power of summarize, though, is in combination with Group By. We’ll see more summarize examples, but it’s easier to make good examples when you have all the tools - it’s hard to demonstrate how to use a hammer if you don’t also have a nail."
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#group-by-power",
    "href": "part-wrangling/03-data-cleaning.html#group-by-power",
    "title": "20  Data Cleaning",
    "section": "\n20.8 Group By + (?) = Power!",
    "text": "20.8 Group By + (?) = Power!\nFrequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to\n\nsplit my dataset into smaller datasets - one for each day\ncompute summary values for each smaller dataset\nput my summarized data back together into a single dataset\n\nThis is known as the split-apply-combine [9] or sometimes, map-reduce [10] strategy (though map-reduce is usually on specifically large datasets and performed in parallel).\nIn tidy parlance, group_by is the verb that accomplishes the first task. summarize accomplishes the second task and implicitly accomplishes the third as well.\n\n\n\n\nThe ungroup() command is just as important as the group_by() command! (by Allison Horst)\n\n\n\n\nR: dplyr\nPython\n\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\npoke %>%\n  mutate(name_chr = nchar(name)) %>%\n  group_by(gen) %>%\n  summarize(n = length(unique(pokedex_no)), hp = mean(hp), name_chr = mean(name_chr))\n## # A tibble: 9 × 4\n##     gen     n    hp name_chr\n##   <dbl> <int> <dbl>    <dbl>\n## 1     1   151  65.3     7.23\n## 2     2   100  71.0     7.36\n## 3     3   135  65.8     7.16\n## 4     4   107  69.4     6.85\n## 5     5   156  75.8     7.77\n## 6     6    72  72.9     7.47\n## 7     7    88  73.2     8.04\n## 8     8    96  77.9     8.01\n## 9     9   103  75.8     8.66\n\n\n\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke = poke.assign(name_length = poke.name.str.len())\npoke.groupby('gen')[['hp', 'name_length']].mean()\n##             hp  name_length\n## gen                        \n## 1    65.333333     7.231579\n## 2    71.024194     7.362903\n## 3    65.777202     7.160622\n## 4    69.382022     6.853933\n## 5    75.827004     7.767932\n## 6    72.882353     7.470588\n## 7    73.233083     8.037594\n## 8    77.940299     8.014925\n## 9    75.756098     8.658537\npoke.groupby('gen')[['hp', 'name_length']].agg(['mean', 'min'])\n##             hp     name_length    \n##           mean min        mean min\n## gen                               \n## 1    65.333333  10    7.231579   3\n## 2    71.024194  20    7.362903   4\n## 3    65.777202   1    7.160622   4\n## 4    69.382022  20    6.853933   4\n## 5    75.827004  30    7.767932   4\n## 6    72.882353  38    7.470588   5\n## 7    73.233083  25    8.037594   6\n## 8    77.940299  25    8.014925   4\n## 9    75.756098  10    8.658537   5\npoke.groupby('gen')[['pokedex_no', 'hp', 'name_length']].agg({'pokedex_no':'nunique', 'hp':'mean', 'name_length':'mean'})\n##      pokedex_no         hp  name_length\n## gen                                    \n## 1           151  65.333333     7.231579\n## 2           100  71.024194     7.362903\n## 3           135  65.777202     7.160622\n## 4           107  69.382022     6.853933\n## 5           156  75.827004     7.767932\n## 6            72  72.882353     7.470588\n## 7            88  73.233083     8.037594\n## 8            96  77.940299     8.014925\n## 9           103  75.756098     8.658537\n\n\n\n\nWhen you group_by a variable, your result carries this grouping with it. In R, summarize will remove one layer of grouping (by default), but if you ever want to return to a completely ungrouped data set, you should use the ungroup() command. In Python, you should consider using reset_index or grouped_thing.obj() to access the original information[11].\n\n\n\n\n\n\nStorms Example\n\n\n\nLet’s try a non-trivial example, using the storms dataset that is part of the dplyr package.\n\n\nR\nPython\n\n\n\n\nlibrary(dplyr)\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\nstorms\n## # A tibble: 11,859 × 13\n##    name   year month   day  hour   lat  long status        categ…¹  wind press…²\n##    <chr> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>         <ord>   <int>   <int>\n##  1 Amy    1975     6    27     0  27.5 -79   tropical dep… -1         25    1013\n##  2 Amy    1975     6    27     6  28.5 -79   tropical dep… -1         25    1013\n##  3 Amy    1975     6    27    12  29.5 -79   tropical dep… -1         25    1013\n##  4 Amy    1975     6    27    18  30.5 -79   tropical dep… -1         25    1013\n##  5 Amy    1975     6    28     0  31.5 -78.8 tropical dep… -1         25    1012\n##  6 Amy    1975     6    28     6  32.4 -78.7 tropical dep… -1         25    1012\n##  7 Amy    1975     6    28    12  33.3 -78   tropical dep… -1         25    1011\n##  8 Amy    1975     6    28    18  34   -77   tropical dep… -1         30    1006\n##  9 Amy    1975     6    29     0  34.4 -75.8 tropical sto… 0          35    1004\n## 10 Amy    1975     6    29     6  34   -74.8 tropical sto… 0          40    1002\n## # … with 11,849 more rows, 2 more variables:\n## #   tropicalstorm_force_diameter <int>, hurricane_force_diameter <int>, and\n## #   abbreviated variable names ¹​category, ²​pressure\n\nstorms <- storms %>%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n\n\n\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/storms.csv\", on_bad_lines='skip')\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove month/day/hour \n# (keep year for ID purposes, names are reused)\nstorms = storms.drop([\"month\", \"day\", \"hour\"], axis = 1)\n\n\n\n\nWe have named storms, observation time, storm location, status, wind, pressure, and diameter (for tropical storms and hurricanes).\nOne thing we might want to know is at what point each storm was the strongest. Let’s define strongest in the following way:\n\nThe points where the storm is at its lowest atmospheric pressure (generally, the lower the atmospheric pressure, the more trouble a tropical disturbance will cause).\nIf there’s a tie, we might want to know when the maximum wind speed occurred.\nIf that still doesn’t get us a single row for each observation, lets just pick out the status and category (these are determined by wind speed, so they should be the same if maximum wind speed is the same) and compute the average time where this occurred.\n\nLet’s start by translating these criteria into basic operations. I’ll use dplyr function names here, but I’ll also specify what I mean when there’s a conflict (e.g. filter in dplyr means something different than filter in python).\nInitial attempt:\n\n\nFor each storm (group_by),\nwe need the point where the storm has lowest atmospheric pressure. (filter - pick the row with the lowest pressure).\n\nThen we read the next part: “If there is a tie, pick the maximum wind speed.”\n\ngroup_by\n\narrange by ascending pressure and descending wind speed\n\nfilter - pick the row(s) which have the lowest pressure and highest wind speed\n\nThen, we read the final condition: if there is still a tie, pick the status and category and compute the average time.\n\ngroup_by\n\narrange by ascending pressure and descending wind speed (this is optional if we write our filter in a particular way)\n\nfilter - pick the row(s) which have the lowest pressure and highest wind speed\n\nsummarize - compute the average time and category (if there are multiple rows)\n\nLet’s write the code, now that we have the order of operations straight!\n\n\nR\nPython\n\n\n\n\nmax_power_storm <- storms %>%\n  # Storm names can be reused, so we need to have year to be sure it's the same instance\n  group_by(name, year) %>%\n  filter(pressure == min(pressure, na.rm = T)) %>%\n  filter(wind == max(wind, na.rm = T)) %>%\n  summarize(pressure = mean(pressure), \n            wind = mean(wind), \n            category = unique(category), \n            status = unique(status), \n            time = mean(time)) %>%\n  arrange(time) %>%\n  ungroup()\nmax_power_storm\n## # A tibble: 512 × 7\n##    name      year pressure  wind category status         time               \n##    <chr>    <dbl>    <dbl> <dbl> <ord>    <chr>          <dttm>             \n##  1 Amy       1975      981    60 0        tropical storm 1975-07-02 12:00:00\n##  2 Caroline  1975      963   100 3        hurricane      1975-08-31 06:00:00\n##  3 Doris     1975      965    95 2        hurricane      1975-09-02 21:00:00\n##  4 Belle     1976      957   105 3        hurricane      1976-08-09 00:00:00\n##  5 Gloria    1976      970    80 1        hurricane      1976-09-30 00:00:00\n##  6 Anita     1977      926   150 5        hurricane      1977-09-02 06:00:00\n##  7 Clara     1977      993    65 1        hurricane      1977-09-08 12:00:00\n##  8 Evelyn    1977      994    65 1        hurricane      1977-10-15 00:00:00\n##  9 Amelia    1978     1005    45 0        tropical storm 1978-07-31 00:00:00\n## 10 Bess      1978     1005    40 0        tropical storm 1978-08-07 12:00:00\n## # … with 502 more rows\n\n\n\n\ngrouped_storms = storms.groupby([\"name\", \"year\"])\n\ngrouped_storm_sum = grouped_storms.agg({\n  \"pressure\": lambda x: x.min()\n}).reindex()\n\n# This gets all the information from storms\n# corresponding to name/year/max pressure\nmax_power_storm = grouped_storm_sum.merge(storms, on = [\"name\", \"year\", \"pressure\"])\n\nmax_power_storm = max_power_storm.groupby([\"name\", \"year\"]).agg({\n  \"pressure\": \"min\",\n  \"wind\": \"max\",\n  \"category\": \"mean\",\n  \"status\": \"unique\",\n  \"time\": \"mean\"\n})\n\n\n\n\nIf we want to see a visual summary, we could plot a histogram of the minimum pressure of each storm.\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nggplot(max_power_storm, aes(x = pressure)) + geom_histogram()\n\n\n\n\n\n\n\nfrom plotnine import *\n\nggplot(max_power_storm, aes(x = \"pressure\")) + geom_histogram(bins=30)\n## <ggplot: (8739321200622)>\n\n\n\n\n\n\n\nWe could also look to see whether there has been any change over time in pressure.\n\n\nR\nPython\n\n\n\n\nggplot(max_power_storm, aes(x = time, y = pressure)) + geom_point()\n\n\n\n\n\n\n\nggplot(max_power_storm, aes(x = \"time\", y = \"pressure\")) + geom_point()\n## <ggplot: (8739321000135)>\n\n\n\n\n\n\n\nIt seems to me that there are fewer high-pressure storms before 1990 or so, which may be due to the fact that some weak storms may not have been observed or recorded prior to widespread radar coverage in the Atlantic.\n\nAnother interesting way to look at this data would be to examine the duration of time a storm existed, as a function of its maximum category. Do stronger storms exist for a longer period of time?\n\n\nR\nPython\n\n\n\n\nstorm_strength_duration <- storms %>%\n  group_by(name, year) %>%\n  summarize(duration = difftime(max(time), min(time), units = \"days\"), \n            max_strength = max(category)) %>%\n  ungroup() %>%\n  arrange(desc(max_strength))\n\nstorm_strength_duration %>%\n  ggplot(aes(x = max_strength, y = duration)) + geom_boxplot()\n\n\n\n\n\n\n\nstorm_strength_duration = storms.groupby([\"name\", \"year\"]).agg(duration = (\"time\", lambda x: max(x) - min(x)),max_strength = (\"category\", \"max\"))\n\nggplot(aes(x = \"factor(max_strength)\", y = \"duration\"), data = storm_strength_duration) + geom_boxplot()\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: ggplot.__init__() got multiple values for argument 'data'\n\n\n\n\nYou don’t need to know how to create these plots yet, but I find it much easier to look at the chart and answer the question I started out with.\nWe could also look to see how a storm’s diameter evolves over time, from when the storm is first identified (group_by + mutate)\nDiameter measurements don’t exist for all storms, and they appear to measure the diameter of the wind field - that is, the region where the winds are hurricane or tropical storm force. (?storms documents the dataset and its variables).\n\n\nR\nPython\n\n\n\nNote the use of as.numeric(as.character(max(category))) to get the maximum (ordinal categorical) strength and convert that into something numeric that can be plotted.\n\nstorm_evolution <- storms %>%\n  filter(!is.na(hurricane_force_diameter)) %>%\n  group_by(name, year) %>%\n  mutate(time_since_start = difftime(time, min(time), units = \"days\")) %>%\n  ungroup()\n\nggplot(storm_evolution, \n       aes(x = time_since_start, y = hurricane_force_diameter, \n           group = name)) + geom_line(alpha = .2) + \n  facet_wrap(~year, scales = \"free_y\")\n\n\n\n\n\n\n\nstorm_evolution = storms.loc[storms.hurricane_force_diameter.notnull(),:]\n\nstorm_evolution = storm_evolution.assign(age = storm_evolution.groupby([\"name\", \"year\"], group_keys = False).apply(lambda x: x.time - x.time.min()))\n\n(ggplot(storm_evolution, \n       aes(x = \"age\", y = \"hurricane_force_diameter\", \n           group = \"name\")) + geom_line(alpha = .2) + \n  facet_wrap(\"year\", scales = \"free_y\"))\n## <ggplot: (8739321000369)>\n## \n## /home/susan/.local/lib/python3.10/site-packages/plotnine/facets/facet.py:440: PlotnineWarning: If you need more space for the x-axis tick text use ... + theme(subplots_adjust={'wspace': 0.25}). Choose an appropriate value for 'wspace'.\n\n\n\n\n\n\n\nFor this plot, I’ve added facet_wrap(~year) to produce sub-plots for each year. This helps us to be able to see some individuality, because otherwise there are far too many storms.\nIt seems that the vast majority of storms have a single bout of hurricane force winds (which either decreases or just terminates near the peak, presumably when the storm hits land and rapidly disintegrates). However, there are a few interesting exceptions - my favorite is in 2008 - the longest-lasting storm seems to have several local peaks in wind field diameter. If we want, we can examine that further by plotting it separately.\n\n\nR\nPython\n\n\n\n\nstorm_evolution %>%\n  filter(year == 2008) %>%\n  arrange(desc(time_since_start))\n## # A tibble: 327 × 15\n##    name   year month   day  hour   lat  long status        categ…¹  wind press…²\n##    <chr> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>         <ord>   <int>   <int>\n##  1 Ike    2008     9    14     6  35.5 -93.7 tropical sto… 0          35     985\n##  2 Ike    2008     9    14     0  33.5 -94.9 tropical sto… 0          35     980\n##  3 Ike    2008     9    13    18  31.7 -95.3 tropical sto… 0          50     974\n##  4 Ike    2008     9    13    12  30.3 -95.2 hurricane     2          85     959\n##  5 Ike    2008     9    13     7  29.3 -94.7 hurricane     2          95     950\n##  6 Ike    2008     9    13     6  29.1 -94.6 hurricane     2          95     951\n##  7 Ike    2008     9    13     0  28.3 -94   hurricane     2          95     952\n##  8 Fay    2008     8    27     0  35   -85.8 tropical dep… -1         15    1005\n##  9 Ike    2008     9    12    18  27.5 -93.2 hurricane     2          95     954\n## 10 Fay    2008     8    26    18  34.6 -86.5 tropical dep… -1         20    1004\n## # … with 317 more rows, 4 more variables: tropicalstorm_force_diameter <int>,\n## #   hurricane_force_diameter <int>, time <dttm>, time_since_start <drtn>, and\n## #   abbreviated variable names ¹​category, ²​pressure\n\nstorm_evolution %>% filter(name == \"Ike\") %>%\n  ggplot(aes(x = time, y = hurricane_force_diameter, color = category)) + geom_point()\n\n\n\n\n\n\n\nstorm_evolution.query(\"year==2008\").sort_values(['age'], ascending = False).head()\n##      name  year  ...                time              age\n## 8000  Ike  2008  ... 2008-09-14 06:00:00 13 days 00:00:00\n## 7999  Ike  2008  ... 2008-09-14 00:00:00 12 days 18:00:00\n## 7998  Ike  2008  ... 2008-09-13 18:00:00 12 days 12:00:00\n## 7997  Ike  2008  ... 2008-09-13 12:00:00 12 days 06:00:00\n## 7996  Ike  2008  ... 2008-09-13 07:00:00 12 days 01:00:00\n## \n## [5 rows x 12 columns]\n(ggplot(\n  storm_evolution.query(\"year==2008 & name=='Ike'\"),\n  aes(x = \"time\", y = \"hurricane_force_diameter\", color = \"category\")) +\n  geom_point())\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: 0\n\n\n\n\n\n\n\n\n\nRadar coverage map from 1995, from [12]"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#summarizing-across-multiple-variables",
    "href": "part-wrangling/03-data-cleaning.html#summarizing-across-multiple-variables",
    "title": "20  Data Cleaning",
    "section": "\n20.9 Summarizing Across Multiple Variables",
    "text": "20.9 Summarizing Across Multiple Variables\nSuppose we want to summarize the numerical columns of any storm which was a hurricane (over the entire period it was a hurricane). We don’t want to write out all of the summarize statements individually, so we use across() instead (in dplyr).\n\n\nR\nPython\n\n\n\nThe dplyr package is filled with other handy functions for accomplishing common data-wrangling tasks. across() is particularly useful - it allows you to make a modification to several columns at the same time.\n\n\ndplyr’s across() function lets you apply a mutate or summarize statement to many columns (by Allison Horst)\n\n\n\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\n\nstorms <- storms %>%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n# Use across to get average of all numeric variables\navg_hurricane_intensity <- storms %>%\n  filter(status == \"hurricane\") %>%\n  group_by(name) %>%\n  summarize(across(where(is.numeric), mean, na.rm = T), .groups = \"drop\") \n\navg_hurricane_intensity %>%\n  select(name, year, month, wind, pressure, tropicalstorm_force_diameter, hurricane_force_diameter) %>%\n  arrange(desc(wind)) %>% \n  # get top 10\n  filter(row_number() <= 10) %>%\n  knitr::kable() # Make into a pretty table\n\n\n\n\n\n\n\n\n\n\n\n\nname\nyear\nmonth\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\nAndrew\n1992\n8.000000\n118.2609\n946.6522\nNaN\nNaN\n\n\nMitch\n1998\n10.000000\n115.9091\n945.3182\nNaN\nNaN\n\n\nRita\n2005\n9.000000\n114.7368\n931.6316\n265.2941\n97.05882\n\n\nIsabel\n2003\n9.000000\n112.1875\n946.5417\nNaN\nNaN\n\n\nGilbert\n1988\n9.000000\n110.8929\n945.4286\nNaN\nNaN\n\n\nLuis\n1995\n8.928571\n110.5952\n948.6190\nNaN\nNaN\n\n\nWilma\n2005\n10.000000\n110.3030\n939.4242\n349.8333\n118.33333\n\n\nMatthew\n2016\n9.880952\n109.5238\n952.1190\n263.5714\n62.02381\n\n\nHugo\n1989\n9.000000\n106.5789\n950.9211\nNaN\nNaN\n\n\nDavid\n1979\n8.457143\n105.1429\n956.1429\nNaN\nNaN\n\n\n\n\n\n\n\nStackoverflow reference\nWe can use python’s list comprehensions in combination with .agg to accomplish the same task as dplyr’s across function.\n\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/storms.csv\")\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove year/month/day/hour\nstorms = storms.drop([\"year\", \"month\", \"day\", \"hour\"], axis = 1)\n\n# Remove non-hurricane points\nstorms = storms.query(\"status == 'hurricane'\")\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: 'mean' for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]\n\nBy default, pandas skips NaN values. If we want to be more clear, or want to pass another argument into the function, we can use what is called a lambda function - basically, a “dummy” function that has some arguments but not all of the arguments. Here, our lambda function is a function of x, and we calculate x.mean(skipna=True) for each x passed in (so, for each column).\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: lambda x: x.mean(skipna=True) for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#try-it-out---data-cleaning",
    "href": "part-wrangling/03-data-cleaning.html#try-it-out---data-cleaning",
    "title": "20  Data Cleaning",
    "section": "\n20.10 Try it out - Data Cleaning",
    "text": "20.10 Try it out - Data Cleaning\nYou can read about the gapminder project here.\nThe gapminder data used for this set of problems contains data from 142 countries on 5 continents. The filtered data in gapminder (in R) contain data about every 5 year period between 1952 and 2007, the country’s life expectancy at birth, population, and per capita GDP (in US $, inflation adjusted). In the gapminder_unfiltered table, however, things are a bit different. Some countries have yearly data, observations are missing, and some countries don’t have complete data. The gapminder package in python (install with pip install gapminder) is a port of the R package, but doesn’t contain the unfiltered data, so we’ll instead use a CSV export.\n\n\n\n\n\n\nRead in the Data\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"gapminder\" %in% installed.packages()) install.packages(\"gapminder\")\nlibrary(gapminder)\ngapminder_unfiltered\n## # A tibble: 3,313 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 3,303 more rows\n\n\n\n\nimport pandas as pd\n\ngapminder_unfiltered = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/gapminder_unfiltered.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 1: How Bad is It?\n\n\n\n\n\n\n\nProblem\nR\nPython\n\n\n\nUsing your EDA skills, determine how bad the unfiltered data are. You may want to look for missing values, number of records, etc. Use query or filter to show any countries which have incomplete data. Describe, in words, what operations were necessary to get this information.\n\n\n\ngapminder_unfiltered %>% \n  group_by(country) %>% \n  summarize(n = n(), missinglifeExp = sum(is.na(lifeExp)), \n            missingpop = sum(is.na(pop)),\n            missingGDP = sum(is.na(gdpPercap))) %>%\n  filter(n != length(seq(1952, 2007, by = 5)))\n## # A tibble: 83 × 5\n##    country        n missinglifeExp missingpop missingGDP\n##    <fct>      <int>          <int>      <int>      <int>\n##  1 Armenia        4              0          0          0\n##  2 Aruba          8              0          0          0\n##  3 Australia     56              0          0          0\n##  4 Austria       57              0          0          0\n##  5 Azerbaijan     4              0          0          0\n##  6 Bahamas       10              0          0          0\n##  7 Barbados      10              0          0          0\n##  8 Belarus       18              0          0          0\n##  9 Belgium       57              0          0          0\n## 10 Belize        11              0          0          0\n## # … with 73 more rows\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data.\n\n\n\n(\n  gapminder_unfiltered.\n  set_index(\"country\").\n  filter([\"lifeExp\", \"pop\", \"gdpPercap\"]).\n  groupby(\"country\").\n  agg(lambda x: x.notnull().sum()).\n  query(\"lifeExp != 12 | pop != 12 | gdpPercap != 12\")\n  )\n##                       lifeExp  pop  gdpPercap\n## country                                      \n## Armenia                     4    4          4\n## Aruba                       8    8          8\n## Australia                  56   56         56\n## Austria                    57   57         57\n## Azerbaijan                  4    4          4\n## ...                       ...  ...        ...\n## United Arab Emirates        8    8          8\n## United Kingdom             13   13         13\n## United States              57   57         57\n## Uzbekistan                  4    4          4\n## Vanuatu                     7    7          7\n## \n## [83 rows x 3 columns]\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data.\n\n\n\n\n\n\n\n\n\n\n\n\nTask 2: Exclude any data which isn’t at 5-year increments\n\n\n\n\n\nStart in 1952 (so 1952, 1957, 1962, …, 2007).\n\n\nR\nPython\n\n\n\n\ngapminder_unfiltered %>%\n  filter(year %in% seq(1952, 2007, by = 5))\n## # A tibble: 2,013 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 2,003 more rows\n\n\n\nReminder about python list comprehensions\nExplanation of the query @ statement\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\ngapminder_unfiltered.query(\"year in @years_to_keep\")\n##           country continent  year  lifeExp       pop   gdpPercap\n## 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n## 1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n## 2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n## 3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n## 4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...           ...       ...   ...      ...       ...         ...\n## 3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n## 3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n## 3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n## 3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n## 3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [2013 rows x 6 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask 3: Exclude any countries that don’t have a full set of observations\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\ngapminder_unfiltered %>%\n  filter(year %in% seq(1952, 2007, by = 5)) %>%\n  group_by(country) %>%\n  mutate(nobs = n()) %>% # Use mutate instead of summarize so that all rows stay\n  filter(nobs == 12) %>%\n  select(-nobs)\n## # A tibble: 1,704 × 6\n## # Groups:   country [142]\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 1,694 more rows\n\n\n\n\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\n\n(\n  gapminder_unfiltered.\n  # Remove extra years\n  query(\"year in @years_to_keep\").\n  groupby(\"country\").\n  # Calculate number of observations (should be exactly 12)\n  # This is the equivalent of mutate on a grouped data set\n  apply(lambda grp: grp.assign(nobs = grp['lifeExp'].notnull().sum())).\n  # Keep rows with 12 observations\n  query(\"nobs == 12\").\n  # remove nobs column\n  drop(\"nobs\", axis = 1)\n  )\n##                       country continent  year  lifeExp       pop   gdpPercap\n## country                                                                     \n## Afghanistan 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n##             1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n##             2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n##             3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n##             4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...                       ...       ...   ...      ...       ...         ...\n## Zimbabwe    3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n##             3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n##             3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n##             3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n##             3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [1704 rows x 6 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nIntroduction to dplyr and Single Table dplyr functions\nR for Data Science: Data Transformations\nAdditional practice exercises: Intro to the tidyverse, group_by + summarize examples, group_by + mutate examples (from a similar class at Iowa State)\nBase R data manipulation\n\nVideos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of dplyr as well.\n\n\nTidyTuesday Python github repo - replicating Tidy Tuesday analyses in Python with Pandas"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#references",
    "href": "part-wrangling/03-data-cleaning.html#references",
    "title": "20  Data Cleaning",
    "section": "\n20.11 References",
    "text": "20.11 References\n\n\n\n\n[1] \nPandas, “Indexing and selecting data,” Pandas 1.4.3 Documentation. 2022 [Online]. Available: https://pandas.pydata.org/docs/user_guide/indexing.html#indexing. [Accessed: Jun. 30, 2022]\n\n\n[2] \npwwang, “Datar: A Grammar of Data Manipulation in python.” May 2022 [Online]. Available: https://pwwang.github.io/datar/. [Accessed: Jun. 30, 2022]\n\n\n[3] \nM. Chow, “nycflights13: A data package for nyc flights (the nycflights13 R package).” 2020 [Online]. Available: https://github.com/machow/nycflights13. [Accessed: Jun. 30, 2022]\n\n\n[4] \nPython Foundation, “Data Structures,” Python 3.10.5 documentation. Jun. 2022 [Online]. Available: https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions. [Accessed: Jun. 30, 2022]\n\n\n[5] \nC. Nguyen, “Tips for String Manipulation in Python,” Towards Data Science. Sep. 2021 [Online]. Available: https://towardsdatascience.com/tips-for-string-manipulation-in-python-92b1fc3f4d9f. [Accessed: Jul. 01, 2022]\n\n\n[6] \nJ. VanderPlas, “String Manipulation and Regular Expressions,” in A Whirlwind Tour of Python, O’Reilly Media, 2016 [Online]. Available: https://jakevdp.github.io/WhirlwindTourOfPython/14-strings-and-regular-expressions.html. [Accessed: Jul. 01, 2022]\n\n\n[7] \nC. Whorton, “Applying Custom Functions to Groups of Data in Pandas,” Medium. Jul. 2021 [Online]. Available: https://towardsdatascience.com/applying-custom-functions-to-groups-of-data-in-pandas-928d7eece0aa. [Accessed: Jul. 01, 2022]\n\n\n[8] \nH. Wickham, “The split-apply-combine strategy for data analysis,” Journal of statistical software, vol. 40, pp. 1–29, 2011. \n\n\n[9] \n\n“Group by: Split-apply-combine,” in Pandas 1.4.3 documentation, Python, 2022 [Online]. Available: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html. [Accessed: Jul. 01, 2022]\n\n\n[10] \nJ. Dean and S. Ghemawat, “MapReduce: Simplified data processing on large clusters,” Communications of the ACM, vol. 51, no. 1, pp. 107–113, Jan. 2008, doi: 10.1145/1327452.1327492. [Online]. Available: https://doi.org/10.1145/1327452.1327492. [Accessed: Jul. 01, 2022]\n\n\n[11] \nM. Dancho, “Answer to \"Is there an \"ungroup by\" operation opposite to .groupby in pandas?\",” Stack Overflow. Mar. 2021 [Online]. Available: https://stackoverflow.com/a/66879388/2859168. [Accessed: Jul. 01, 2022]\n\n\n[12] \nC. Mass, “The pacific northwest has the worst coastal weather radar coverage in the continental u.s.: Documentation of the problem and a call for action. Department of atmospheric sciences,” Jan. 12, 2006. [Online]. Available: https://www.atmos.washington.edu/~cliff/coastalradarold.html. [Accessed: Jan. 14, 2023]"
  },
  {
    "objectID": "part-wrangling/04-strings.html#fa-bullseye-objectives",
    "href": "part-wrangling/04-strings.html#fa-bullseye-objectives",
    "title": "21  Working with Strings",
    "section": "\n21.1  Objectives",
    "text": "21.1  Objectives\n\nUse functions to perform find-and-replace operations\nUse functions to split string data into multiple columns/variables\nUse functions to join string data from multiple columns/variables into a single column/variable\n\n\n\nPerhaps one day you’ll be able to put this knowledge to use in a practical setting!"
  },
  {
    "objectID": "part-wrangling/04-strings.html#basic-operations",
    "href": "part-wrangling/04-strings.html#basic-operations",
    "title": "21  Working with Strings",
    "section": "\n21.2 Basic Operations",
    "text": "21.2 Basic Operations\nNearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different “levels” of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated).\n\nSome people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski\n\n\n\nAlternately, the xkcd version of the above quote\n\n\nThe stringr cheatsheet by RStudio may be helpful as you complete tasks related to this section - it may even be useful in Python as the 2nd page has a nice summary of regular expressions.\n\n\nTable 21.1: Table of string functions in R and python. x is the string or vector of strings, pattern is a pattern to be found within the string, a and b are indexes, and encoding is a string encoding, such as UTF8 or ASCII.\n\n\n\n\n\n\nTask\nR\nPython\n\n\n\nReplace pattern with replacement\n\n\nbase: gsub(pattern, replacement, x)\nstringr: str_replace(x, pattern, replacement) and str_replace_all(x, pattern, replacement)\n\npandas: x.str.replace(pattern, replacement) (not vectorized over pattern or replacement)\n\n\nConvert case\n\nbase: tolower(x), toupper(x)\nstringr: str_to_lower(x), str_to_upper(x) , str_to_title(x)\n\npandas: x.str.lower(), x.str.upper()\n\n\n\nStrip whitespace from start/end\n\nbase: trimws(x)\nstringr: str_trim(x) , str_squish(x)\n\npandas: x.str.strip()\n\n\n\nPad strings to a specific length\n\nbase: sprintf(format, x)\nstringr: str_pad(x, …)\n\npandas: x.str.pad()\n\n\n\nTest if the string contains a pattern\n\nbase: grep(pattern, x) or grepl(pattern, x)\nstringr: str_detect(x, pattern)\n\npandas: x.str.contains(pattern)\n\n\n\nCount how many times a pattern appears in the string\n\nbase: gregexpr(pattern, x) + sapply to count length of the returned list\nstringi: stri_count(x, pattern)\nstringr: str_count(x, pattern)\n\npandas: x.str.count(pattern)\n\n\n\nFind the first appearance of the pattern within the string\n\nbase: regexpr(pattern, x)\nstringr: str_locate(x, pattern)\n\npandas: x.str.find(pattern)\n\n\n\nFind all appearances of the pattern within the string\n\nbase: gregexpr\nstringr: str_locate_all(x, pattern)\n\npandas: x.str.findall(pattern)\n\n\n\nDetect a match at the start/end of the string\n\nbase: use regular expr.\nstringr: str_starts(x, pattern) ,str_ends(x, pattern)\n\npandas: x.str.startswith(pattern) , x.str.endswith(pattern)\n\n\n\nSubset a string from index a to b\n\nbase: substr(x, a, b)\nstringr: str_sub(x, a, b)\n\npandas: x.str.slice(a, b, step)\n\n\n\nConvert string encoding\n\nbase: iconv(x, encoding)\nstringr: str_conv(x, encoding)\n\npandas: x.str.encode(encoding)\n\n\n\n\n\nIn Table 21.1, multiple functions are provided for e.g. common packages and situations. Pandas methods are specifically those which work in some sort of vectorized manner. Base methods (in R) do not require additional packages, where stringr methods require the stringr package, which is included in the tidyverse1."
  },
  {
    "objectID": "part-wrangling/04-strings.html#converting-strings-to-numbers",
    "href": "part-wrangling/04-strings.html#converting-strings-to-numbers",
    "title": "21  Working with Strings",
    "section": "\n21.3 Converting strings to numbers",
    "text": "21.3 Converting strings to numbers\nOne of the most common tasks when reading in and tidying messy data is that numeric-ish data can come in many forms that are read (by default) as strings. The data frame below provides an example of a few types of data which may be read in in unexpected ways. How do we tell R or Python that we want all of these columns to be treated as numbers?\n\n\n\nTable 21.2: Different “messy” number formats\n\n\nint_col\nfloat_col\nmix_col\nmissing_col\nmoney_col\neu_numbers\nboolean_col\ncustom\n\n\n\n0\n1\n1.1\na\n1\n£1,000.00\n1.000.000,00\nTrue\nY\n\n\n1\n2\n1.2\n2\n2\n£2,400.00\n2.000.342,00\nFalse\nY\n\n\n2\n3\n1.3\n3\n3\n£2,400.00\n3.141,59\nTrue\nN\n\n\n3\n4\n4.7\n4\nnan\n£2,400.00\n34,25\nTrue\nN\n\n\n\n\n\nNumbers, currencies, dates, and times are written differently based on what country you’re in [2]. In computer terms, this is the locale, and it affects everything from how your computer formats the date/time to what character set it will try to use to display things [3].\nLocales are something you may want to skip if you’re just starting out and you don’t work with code written by people in other countries. If you’re collaborating internationally, however, you may want to at least skim the section below to be aware of potential issues when locale-related problems crop up.\nIf you’ve never had to deal with the complexities of working on a laptop designed for one country using another country’s conventions, know that it isn’t necessarily the easiest thing to do.\n\n\n\n\n\n\nAdvanced: Locales\n\n\n\n\n\nFind your locale\n\n\n Type Get-WinSystemLocale into your CMD or powershell terminal.\n\n (10.4 and later) and  Type locale into your terminal\nGet set up to work with locales\nWhile this isn’t required, it may be useful and is definitely good practice if you’re planning to work with data generated internationally.\nThis article tells you how to set things up in linux . The biggest difference in other OS is going to be how to install new locales, so here are some instructions on that for other OS.\n\n\n Installing languages\n\n\n Change locales. Installing or creating new locales seems to be more complicated, and since I do not have a mac, I can’t test this out easily myself.\n\n\n\n\nWe’ll use Table 21.2 to explore different string operations focused specifically on converting strings to numbers.\n\n\nGet the data: Python\nR\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/number-formats.csv\")\n\n\n\n\ndf <- read.csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/number-formats.csv\", colClasses = \"character\")\n\nBy default, R tries to outsmart us and read the data in as numbers. I’ve disabled this behavior by setting colClasses='character' so that you can see how these functions work… but in general, R seems to be a bit more willing to try to guess what you want. This can be useful, but can also be frustrating when you don’t know how to disable it.\n\n\n\n\n\n\n\n\n\nConverting Columns Using Your Best Guess\n\n\n\nBoth R and Python have ways to “guess” what type a column is and read the data in as that type. When we initially read in the data above, I had to explicitly disable this behavior in R. If you’re working with data that is already read in, how do you get R and Python to guess what type something is?\n\n\nR\nPython\n\n\n\nHere, R gets everything “right” except the eu_numbers, money_col, and custom cols, which makes sense - these contain information that isn’t clearly numeric or doesn’t match the default numeric formatting on my machine (which is using en_US.UTF-8 for almost everything). If we additionally want R to handle mix_col, we would have to explicitly convert to numeric, causing the a to be converted to NA\n\nlibrary(dplyr)\nlibrary(readr)\ndf_guess <- type_convert(df)\nstr(df_guess)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : int  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : chr  \"a\" \"2\" \"3\" \"4\"\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n\nThe type_convert function has a locale argument; readr includes a locale() function that you can pass to type_convert that allows you to define your own locale. Because we have numeric types structured from at least two locales in this data frame, we would have to specifically read the data in specifying which columns we wanted read with each locale.\n\nlibrary(dplyr)\nlibrary(readr)\nfixed_df <- type_convert(df) \nfixed_df2 <- type_convert(df, locale = locale(decimal_mark = ',', grouping_mark = '.'))\n# Replace EU numbers col with the type_convert results specifying that locale\nfixed_df$eu_numbers = fixed_df$eu_numbers\nstr(fixed_df)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : int  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : chr  \"a\" \"2\" \"3\" \"4\"\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n\n\n\nSimilarly, Python does basically the same thing as R: mix_col, money_col, and custom are all left as strings, while floats, integers, and logical values are handled correctly.\n\nfixed_df = df.infer_objects()\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\n\nAs in R, we can set the locale in Python to change how things are read in.\n\nfrom babel.numbers import parse_decimal\n\n# Convert eu_numbers column specifically\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nConverting Columns Directly\n\n\n\nObviously, we can also convert some strings to numbers using type conversion functions that we discussed in Section 7.5. This is fairly easy in R, but a bit more complex in Python, because Python has several different types of ‘missing’ or NA variables that are not necessarily compatible.\n\n\nR\nPython\n\n\n\nHere, we use the across helper function from dplyr to convert all of the columns to numeric. Note that the last 3 columns don’t work here, because they contain characters R doesn’t recognize as numeric characters.\n\nlibrary(dplyr)\n\ndf_numeric <- mutate(df, across(everything(), as.numeric))\nstr(df_numeric)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : num  NA 2 3 4\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : num  NA NA NA NA\n##  $ eu_numbers : num  NA NA NA NA\n##  $ boolean_col: num  NA NA NA NA\n##  $ custom     : num  NA NA NA NA\n\n\n\n\ndf_numeric = df.apply(pd.to_numeric, errors='coerce')\ndf_numeric.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col        float64\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom         float64\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Converting Y/N data\n\n\n\nThe next thing we might want to do is convert our custom column so that it has 1 instead of Y and 0 instead of N. There are several ways we can handle this process:\n\nWe could use factors/categorical variables, which have numeric values “under the hood”, but show up as labeled.\nWe could (in this particular case) test for equality with “Y”, but this approach would not generalize well if we had more than 2 categories.\nWe could take a less nuanced approach and just find-replace and then convert to a number.\n\nSome of these solutions are more kludgy than others, but I’ve used all 3 approaches when dealing with categorical data in the past, depending on what I wanted to do with it afterwards.\n\n\nR\nPython\n\n\n\n\nlibrary(stringr) # work with strings easily\nfixed_df = fixed_df %>%\n  mutate(\n    # factor approach\n    custom1 = factor(custom, levels = c(\"N\", \"Y\"), labels = c(\"Y\", \"N\")),\n    # test for equality\n    custom2 = (custom == \"Y\"),\n    # string replacement\n    custom3 = str_replace_all(custom, c(\"Y\" = \"1\", \"N\" = \"0\")) %>%\n      as.numeric()\n  )\n\nstr(fixed_df)\n## 'data.frame':    4 obs. of  11 variables:\n##  $ int_col    : int  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : chr  \"a\" \"2\" \"3\" \"4\"\n##  $ missing_col: num  1 2 3 NA\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  $ custom1    : Factor w/ 2 levels \"Y\",\"N\": 2 2 1 1\n##  $ custom2    : logi  TRUE TRUE FALSE FALSE\n##  $ custom3    : num  1 1 0 0\n\n\n\nWe’ve already done a brief demonstration of string methods in Python when we trimmed off the £ character. In this situation, it’s better to use the pandas replace method, which allows you to pass in a list of values and a list of replacements.\n\n# Categorical (factor) approach\nfixed_df['custom1'] = fixed_df['custom'].astype(\"category\") # convert to categorical variable\n# Equality/boolean approach\nfixed_df['custom2'] = fixed_df['custom'] == \"Y\"\n# string replacement\nfixed_df['custom3'] = fixed_df['custom'].replace([\"Y\", \"N\"], [\"1\", \"0\"]).astype(\"int\")\n\nfixed_df.dtypes\n## int_col           int64\n## float_col       float64\n## mix_col          object\n## missing_col     float64\n## money_col        object\n## eu_numbers      float64\n## boolean_col        bool\n## custom           object\n## custom1        category\n## custom2            bool\n## custom3           int64\n## dtype: object"
  },
  {
    "objectID": "part-wrangling/04-strings.html#find-and-replace",
    "href": "part-wrangling/04-strings.html#find-and-replace",
    "title": "21  Working with Strings",
    "section": "\n21.4 Find and replace",
    "text": "21.4 Find and replace\nAnother way to fix some issues is to just find-and-replace the problematic characters. This is not always the best solution2, and may introduce bugs if you use the same code to analyze new data with characters you haven’t anticipated, but in so many cases it’s also the absolute easiest, fastest, simplest way forward and easily solves many different problems.\nI’ll show you how to correct all of the issues reading in the data using solutions shown above, but please do consider reading [4] so that you know why find-and-replace isn’t (necessarily) the best option for locale-specific formatting.\n\n\n\n\n\n\nExample: find and replace\n\n\n\nLet’s start with the money column.\n\n\nR\nPython\n\n\n\nIn R, parse_number() handles the money column just fine - the pound sign goes away and we get a numeric value. This didn’t work by default with type_convert, but as long as we mutate and tell R we expect a number, things work well. Then, as we did above, we can specify the locale settings so that decimal and grouping marks are handled correctly even for countries which use ‘,’ for decimal and ‘.’ for thousands separators.\n\nfixed_df = df %>%\n  type_convert() %>% # guess everything\n  mutate(money_col = parse_number(money_col),\n         eu_numbers = parse_number(eu_numbers, \n                                   locale = locale(decimal_mark = ',', \n                                                   grouping_mark = '.')))\n\n\n\nIn python, a similar approach doesn’t work out, because the pound sign is not handled correctly.\n\nfrom babel.numbers import parse_decimal\n\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'decimal.Decimal' object has no attribute 'replace'\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x, locale = 'en_GB'))\n## Error in py_call_impl(callable, dots$args, dots$keywords): babel.numbers.NumberFormatError: '£1,000.00' is not a valid decimal number\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n# Remove £ from string\nfixed_df['money_col'] = fixed_df['money_col'].str.removeprefix(\"£\")\n# Then parse the number\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'StringMethods' object has no attribute 'removeprefix'\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x))\n# Then convert to numeric\n## Error in py_call_impl(callable, dots$args, dots$keywords): babel.numbers.NumberFormatError: '£1,000.00' is not a valid decimal number\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: Unable to parse string \"£1,000.00\" at position 0\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Locale find-and-replace\n\n\n\nWe could also handle the locale issues using find-and-replace, if we wanted to…\n\n\nR\nPython\n\n\n\nNote that str_remove is shorthand for str_replace(x, pattern, \"\"). There is a little bit of additional complexity in switching “,” for “.” and vice versa - we have to change “,” to something else first, so that we can replace “.” with “,”. This is not elegant but it does work. It also doesn’t generalize - it will mess up numbers formatted using the US/UK convention, and it won’t handle numbers formatted using other conventions from other locales.\n\nfixed_df = df %>%\n  type_convert() %>% # guess everything\n  mutate(money_col = str_remove(money_col, \"£\") %>% parse_number(),\n         eu_numbers = str_replace_all(eu_numbers, \n                                      c(\",\" = \"_\", \n                                        \"\\\\.\" = \",\", \n                                        \"_\" = \".\")) %>%\n           parse_number())\n\n\n\n\nfrom babel.numbers import parse_decimal\n\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column: \n# Replace . with nothing (remove .), then\n# Replace , with .\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].\\\nstr.replace('\\.', '').\\\nstr.replace(',', '.')\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: Can only use .str accessor with string values!\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].\\\nstr.removeprefix(\"£\").\\\nstr.replace(',', '')\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'StringMethods' object has no attribute 'removeprefix'\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: Unable to parse string \"£1,000.00\" at position 0\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\nfixed_df\n##    int_col  float_col mix_col  ...  eu_numbers boolean_col  custom\n## 0        1        1.1       a  ...  1000000.00        True       Y\n## 1        2        1.2       2  ...  2000342.00       False       Y\n## 2        3        1.3       3  ...     3141.59        True       N\n## 3        4        4.7       4  ...       34.25        True       N\n## \n## [4 rows x 8 columns]"
  },
  {
    "objectID": "part-wrangling/04-strings.html#separating-multi-variable-columns",
    "href": "part-wrangling/04-strings.html#separating-multi-variable-columns",
    "title": "21  Working with Strings",
    "section": "\n21.5 Separating multi-variable columns",
    "text": "21.5 Separating multi-variable columns\nAnother common situation is to have multiple variables in one column. This can happen, for instance, when conducting a factorial experiment: Instead of having separate columns for each factor, researchers sometimes combine several different factors into a single label for a condition to simplify data entry.\nIn pandas, we use x.str.split() to split columns in a DataFrame, in R we use the tidyr package’s separate_wider_xxx() series of functions.\n\n\n\n\n\n\nExample: Separating columns\n\n\n\nWe’ll use the table3 object included in dplyr for this example. You can load it in R and then load the reticuate package to be able to access the object in python as r.table3.\n\n\nPicture the operation\nR\nPython\n\n\n\n\n\nWe want to separate the rate column into two new columns, cases and population.\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table3)\nseparate_wider_delim(table3, rate, delim = \"/\", names = c('cases', 'pop'), cols_remove = F)\n## # A tibble: 6 × 5\n##   country      year cases  pop        rate             \n##   <chr>       <dbl> <chr>  <chr>      <chr>            \n## 1 Afghanistan  1999 745    19987071   745/19987071     \n## 2 Afghanistan  2000 2666   20595360   2666/20595360    \n## 3 Brazil       1999 37737  172006362  37737/172006362  \n## 4 Brazil       2000 80488  174504898  80488/174504898  \n## 5 China        1999 212258 1272915272 212258/1272915272\n## 6 China        2000 213766 1280428583 213766/1280428583\n\n\n\n\ntable3 = r.table3\ntable3[['cases', 'pop']] = table3.rate.str.split(\"/\", expand = True)\ntable3\n##        country    year               rate   cases         pop\n## 0  Afghanistan  1999.0       745/19987071     745    19987071\n## 1  Afghanistan  2000.0      2666/20595360    2666    20595360\n## 2       Brazil  1999.0    37737/172006362   37737   172006362\n## 3       Brazil  2000.0    80488/174504898   80488   174504898\n## 4        China  1999.0  212258/1272915272  212258  1272915272\n## 5        China  2000.0  213766/1280428583  213766  1280428583\n\nThis uses python’s multiassign capability. Python can assign multiple things at once if those things are specified as a sequence (e.g. cases, pop). In this case, we split the rate column and assign two new columns, essentially adding two columns to our data frame and labeling them at the same time."
  },
  {
    "objectID": "part-wrangling/04-strings.html#joining-columns",
    "href": "part-wrangling/04-strings.html#joining-columns",
    "title": "21  Working with Strings",
    "section": "\n21.6 Joining columns",
    "text": "21.6 Joining columns\nIt’s also not uncommon to need to join information stored in two columns into one column. A good example of a situation in which you might need to do this is when we store first and last name separately and then need to have a ‘name’ column that has both pieces of information together.\n\n\n\n\n\n\nExample: Joining columns\n\n\n\nWe’ll use the table5 object included in dplyr for this example. You can load it in R and then load the reticuate package to be able to access the object in python as r.table5.\n\n\nPicture the operation\nR\nPython\n\n\n\n\n\nWe want to join the century and year columns into a new column, yyyy.\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table5)\nunite(table5, col = yyyy, c(century, year), sep = \"\", remove = F) %>%\n  # convert all columns to sensible types\n  readr::type_convert()\n## # A tibble: 6 × 5\n##   country      yyyy century year  rate             \n##   <chr>       <dbl>   <dbl> <chr> <chr>            \n## 1 Afghanistan  1999      19 99    745/19987071     \n## 2 Afghanistan  2000      20 00    2666/20595360    \n## 3 Brazil       1999      19 99    37737/172006362  \n## 4 Brazil       2000      20 00    80488/174504898  \n## 5 China        1999      19 99    212258/1272915272\n## 6 China        2000      20 00    213766/1280428583\n\n\n\n\nimport pandas as pd\n\ntable5 = r.table5\n# Concatenate the two columns with string addition\ntable5['yyyy'] = table5.century + table5.year\n# convert to number\ntable5['yyyy'] = pd.to_numeric(table5.yyyy)\ntable5\n##        country century year               rate  yyyy\n## 0  Afghanistan      19   99       745/19987071  1999\n## 1  Afghanistan      20   00      2666/20595360  2000\n## 2       Brazil      19   99    37737/172006362  1999\n## 3       Brazil      20   00    80488/174504898  2000\n## 4        China      19   99  212258/1272915272  1999\n## 5        China      20   00  213766/1280428583  2000"
  },
  {
    "objectID": "part-wrangling/04-strings.html#regular-expressions",
    "href": "part-wrangling/04-strings.html#regular-expressions",
    "title": "21  Working with Strings",
    "section": "\n21.7 Regular Expressions",
    "text": "21.7 Regular Expressions\nMatching exact strings is easy - it’s just like using find and replace.\n\nhuman_talk <- \"blah, blah, blah. Do you want to go for a walk?\"\ndog_hears <- str_extract(human_talk, \"walk\")\ndog_hears\n## [1] \"walk\"\n\n\n\n\n\nTo generate #1 albums, ‘jay –help’ recommends the -z flag. XKCD comics by Randall Munroe CC-A-NC 2.5.\n\n\nA regular expression is a sequence of characters that specify a match pattern to search for in a larger text [5]. Regular expressions may be used to specify find or find-and-replace operations on strings.\nRegular expressions can be extremely useful for cleaning and extracting data: they can replace misspellings, extract pieces of information from longer strings, and flexibly handle different ways people may input data. They may be incredibly powerful, but they can also be complicated to create and the expressions themselves may be cryptic and nearly impossible to decode.\nBut, if you can master even a small amount of regular expression notation, you’ll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you’re creative, but often they’re much simpler.\nHere are some useful regular expressions3:\n\nValidate a phone number [6]: ^\\(*\\d{3}\\)*( |-)*\\d{3}( |-)*\\d{4}$\n\nCheck for first and last names [7]: ^[\\w'\\-,.][^0-9_!¡?÷?¿/\\\\+=@#$%ˆ&*(){}|~<>;:[\\]]{2,}$\n(This is a tricky proposition and this regular expression does make some assumptions about what characters are valid for names.)\nMatch a 5 or 9 digit zip code: (^\\d{5}$)|(^\\d{9}$)|(^\\d{5}-\\d{4}$)\n\n\nThese tasks are all well-suited for regular expressions. More complicated tasks, such as validating an email address, are less suited for regular expressions, though there are regular expressions that exist [8] for that task.\n\n\nI’ve assembled a YouTube playlist of different explanations of regular expressions, if you prefer that type of tutorial.\n\n\n\n\nThe following demonstrations are intended for advanced students: if you are just learning how to program, you may want to come back to these when you need them.\n\n\n\n\n\n\nRegular Expression Basics\n\n\n\n\n\nYou may find it helpful to follow along with this section using this web app built to test R regular expressions for R. A similar application for Perl compatible regular expressions (used by SAS and Python) can be found here. The subset of regular expression syntax we’re going to cover here is fairly limited (and common to SAS, Python, and R, with a few adjustments), but you can find regular expressions to do just about anything string-related. As with any tool, there are situations where it’s useful, and situations where you should not use a regular expression, no matter how much you want to.\nHere are the basics of regular expressions:\n\n\n[] enclose sets of characters\nEx: [abc] will match any single character a, b, c\n\n\n- specifies a range of characters (A-z matches all upper and lower case letters)\nto match - exactly, precede with a backslash (outside of []) or put the - last (inside [])\n\n\n\n. matches any character (except a newline)\nTo match special characters, escape them using \\ (in most languages) or \\\\ (in R). So \\. or \\\\. will match a literal ., \\$ or \\\\$ will match a literal $.\n\n\n\nR\nPython\n\n\n\n\nnum_string <- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn <- str_extract(num_string, \"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\")\nssn\n## [1] \"123-45-6789\"\n\n\n\nIn python, a regular expression is indicated by putting the character ‘r’ right before the quoted expression. This tells python that any backslashes in the string should be left alone – if R had that feature, we wouldn’t have to escape all the backslashes!\n\nimport re\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn = re.search(r\"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\", num_string)\nssn\n## <re.Match object; span=(42, 53), match='123-45-6789'>\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecifying repetition\n\n\n\n\n\nListing out all of those numbers can get repetitive, though. How do we specify repetition?\n\n\n* means repeat between 0 and inf times\n\n+ means 1 or more times\n\n? means 0 or 1 times – most useful when you’re looking for something optional\n\n{a, b} means repeat between a and b times, where a and b are integers. b can be blank. So [abc]{3,} will match abc, aaaa, cbbaa, but not ab, bb, or a. For a single number of repeated characters, you can use {a}. So {3, } means “3 or more times” and {3} means “exactly 3 times”\n\n\n\nR\nPython\n\n\n\n\nlibrary(stringr)\nstr_extract(\"banana\", \"[a-z]{1,}\") # match any sequence of lowercase characters\n## [1] \"banana\"\nstr_extract(\"banana\", \"[ab]{1,}\") # Match any sequence of a and b characters\n## [1] \"ba\"\nstr_extract_all(\"banana\", \"(..)\") # Match any two characters\n## [[1]]\n## [1] \"ba\" \"na\" \"na\"\nstr_extract(\"banana\", \"(..)\\\\1\") # Match a repeated thing\n## [1] \"anan\"\n\n\nnum_string <- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn <- str_extract(num_string, \"[0-9]{3}-[0-9]{2}-[0-9]{4}\")\nssn\n## [1] \"123-45-6789\"\nphone <- str_extract(num_string, \"[0-9]{3}.[0-9]{3}.[0-9]{4}\")\nphone\n## [1] \"123-456-7890\"\nnuid <- str_extract(num_string, \"[0-9]{8}\")\nnuid\n## [1] \"12345678\"\nbank_balance <- str_extract(num_string, \"\\\\$[0-9,]+\\\\.[0-9]{2}\")\nbank_balance\n## [1] \"$50,000,000.23\"\n\n\n\n\nimport re\nre.search(r\"[a-z]{1,}\", \"banana\") # match any sequence of lowercase characters\n## <re.Match object; span=(0, 6), match='banana'>\nre.search(r\"[ab]{1,}\", \"banana\") # Match any sequence of a and b characters\n## <re.Match object; span=(0, 2), match='ba'>\nre.findall(r\"(..)\", \"banana\") # Match any two characters\n## ['ba', 'na', 'na']\nre.search(r\"(..)\\1\", \"banana\") # Match a repeated thing\n## <re.Match object; span=(1, 5), match='anan'>\n\n\nimport re\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn = re.search(r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\", num_string)\nssn\n## <re.Match object; span=(42, 53), match='123-45-6789'>\nphone = re.search(r\"[0-9]{3}.[0-9]{3}.[0-9]{4}\", num_string)\nphone\n## <re.Match object; span=(7, 19), match='123-456-7890'>\nnuid = re.search(r\"[0-9]{8}\", num_string)\nnuid\n## <re.Match object; span=(27, 35), match='12345678'>\nbank_balance = re.search(r\"\\$[0-9,]+\\.[0-9]{2}\", num_string)\nbank_balance\n## <re.Match object; span=(77, 91), match='$50,000,000.23'>\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatching Locations\n\n\n\n\n\nThere are also ways to “anchor” a pattern to a part of the string (e.g. the beginning or the end)\n\n\n^ has multiple meanings:\n\nif it’s the first character in a pattern, ^ matches the beginning of a string\nif it follows [, e.g. [^abc], ^ means “not” - for instance, “the collection of all characters that aren’t a, b, or c”.\n\n\n\n$ means the end of a string\n\nCombined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses.\n\n\nR\nPython\n\n\n\n\naddress <- \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num <- str_extract(address, \"^[0-9]{1,}\")\n\n # Match everything alphanumeric up to the comma\nstreet <- str_extract(address, \"[A-z0-9 ]{1,}\")\nstreet <- str_remove(street, house_num) %>% str_trim() # remove house number\n\ncity <- str_extract(address, \",.*,\") %>% str_remove_all(\",\") %>% str_trim()\n\nzip <- str_extract(address, \"[0-9-]{5,10}$\") # match 5 and 9 digit zip codes\n\n\n\nPython match objects contain 3 things: .span(), which has the start and end positions of the match, .string, which contains the original string passed into the function, and .group(), which contains the actual matching portion of the string.\n\nimport re\n\naddress = \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num = re.search(r\"^[0-9]{1,}\", address).group()\n\n# Match everything alphanumeric up to the comma\nstreet = re.search(r\"[A-z0-9 ]{1,}\", address).group()\nstreet = street.replace(house_num, \"\").strip() # remove house number\n\ncity = re.search(\",.*,\", address).group().replace(\",\", \"\").strip()\n\nzip = re.search(r\"[0-9-]{5,10}$\", address).group() # match 5 and 9 digit zip codes\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing Information\n\n\n\n\n\n\n\n() are used to capture information. So ([0-9]{4}) captures any 4-digit number\n\na|b will select a or b.\n\nIf you’ve captured information using (), you can reference that information using backreferences.\nIn most languages, backreferences look like this: \\1 for the first reference, \\9 for the ninth. In R, backreferences are \\\\1 through \\\\9.\n\n\nR\nPython\n\n\n\nIn R, the \\ character is special, so you have to escape it. So in R, \\\\1 is the first reference, and \\\\2 is the second, and so on.\n\nphone_num_variants <- c(\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\")\nphone_regex <- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).?([0-9]{4})\"\n# \\\\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\\\( and \\\\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nstr_extract(phone_num_variants, phone_regex)\n## [1] \"(123) 456-7980\"  \"123.456.7890\"    \"+1 123-456-7890\"\nstr_replace(phone_num_variants, phone_regex, \"\\\\1\\\\2\\\\3\")\n## [1] \"1234567980\" \"1234567890\" \"1234567890\"\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk <- \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears <- str_extract_all(human_talk, \"walk|treat\")\ndog_hears\n## [[1]]\n## [1] \"walk\"  \"treat\"\n\n\n\n\nimport pandas as pd\nimport re\n\nphone_num_variants = pd.Series([\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\"])\nphone_regex = re.compile(\"\\+?[0-9]{0,3}? ?\\(?([0-9]{3})?\\)?.?([0-9]{3}).?([0-9]{4})\")\n# \\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\( and \\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nres = phone_num_variants.str.findall(phone_regex)\nres2 = phone_num_variants.str.replace(phone_regex, \"\\\\1\\\\2\\\\3\")\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk = \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears = re.findall(r\"walk|treat\", human_talk)\ndog_hears\n## ['walk', 'treat']\n\n\n\n\n\n\n\n\n\n\n\n\n\nPutting it all Together\n\n\n\n\n\nWe can test our regular expressions to ensure that they are specific enough to pull out what we want, while not pulling out other similar information:\n\n\nR\nPython\n\n\n\n\nstrings <- c(\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\")\n\nphone_regex <- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})\"\ndog_regex <- \"(walk|treat)\"\naddr_regex <- \"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\"\nabab_regex <- \"(..)\\\\1\"\n\ntibble(\n  text = strings,\n  phone = str_detect(strings, phone_regex),\n  dog = str_detect(strings, dog_regex),\n  addr = str_detect(strings, addr_regex),\n  abab = str_detect(strings, abab_regex))\n## # A tibble: 6 × 5\n##   text                                                   phone dog   addr  abab \n##   <chr>                                                  <lgl> <lgl> <lgl> <lgl>\n## 1 abcdefghijklmnopqrstuvwxyzABAB                         FALSE FALSE FALSE TRUE \n## 2 banana orange strawberry apple                         FALSE FALSE FALSE TRUE \n## 3 ana went to montana to eat a banana                    FALSE FALSE FALSE TRUE \n## 4 call me at 432-394-2873. Do you want to go for a walk… TRUE  TRUE  FALSE FALSE\n## 5 phone: (123) 456-7890, nuid: 12345678, bank account b… TRUE  FALSE FALSE FALSE\n## 6 1600 Pennsylvania Ave NW, Washington D.C., 20500       FALSE FALSE TRUE  FALSE\n\n\n\n\nimport pandas as pd\nimport re\n\nstrings = pd.Series([\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\"])\n\nphone_regex = re.compile(r\"\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})\")\ndog_regex = re.compile(r\"(walk|treat)\")\naddr_regex = re.compile(r\"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\")\nabab_regex = re.compile(r\"(..)\\1\")\n\npd.DataFrame({\n  \"text\": strings,\n  \"phone\": strings.str.contains(phone_regex),\n  \"dog\": strings.str.contains(dog_regex),\n  \"addr\": strings.str.contains(addr_regex),\n  \"abab\": strings.str.contains(abab_regex)})\n##                                                 text  phone  ...   addr   abab\n## 0                     abcdefghijklmnopqrstuvwxyzABAB  False  ...  False   True\n## 1                     banana orange strawberry apple  False  ...  False   True\n## 2                ana went to montana to eat a banana  False  ...  False   True\n## 3  call me at 432-394-2873. Do you want to go for...   True  ...  False  False\n## 4  phone: (123) 456-7890, nuid: 12345678, bank ac...   True  ...  False  False\n## 5   1600 Pennsylvania Ave NW, Washington D.C., 20500  False  ...   True  False\n## \n## [6 rows x 5 columns]\n## \n## <string>:3: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n## <string>:4: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n## <string>:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n## <string>:6: UserWarning: This pattern has match groups. To actually get the groups, use str.extract."
  },
  {
    "objectID": "part-wrangling/04-strings.html#references",
    "href": "part-wrangling/04-strings.html#references",
    "title": "21  Working with Strings",
    "section": "\n21.8 References",
    "text": "21.8 References\n\n\n\n\n[1] \nP. Norvig, “How to write a spelling corrector. Norvig.com,” Feb. 01, 2007. [Online]. Available: http://norvig.com/spell-correct.html. [Accessed: Mar. 08, 2023]\n\n\n[2] \nM. Ashour, “A Concise Guide to Number Localization,” Phrase. Feb. 2022 [Online]. Available: https://phrase.com/blog/posts/number-localization/. [Accessed: Jul. 25, 2022]\n\n\n[3] \nWikipedia Contributors, “Locale (computer software),” Wikipedia. Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Locale_(computer_software)&oldid=1082900932. [Accessed: Jul. 25, 2022]\n\n\n[4] \nA. Herrmann, “How to deal with international data formats in Python,” herrmann.tech. Feb. 2021 [Online]. Available: https://herrmann.tech/en/blog/2021/02/05/how-to-deal-with-international-data-formats-in-python.html. [Accessed: Jul. 25, 2022]\n\n\n[5] \n\n“Regular expression,” Wikipedia. Feb. 28, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Regular_expression&oldid=1142135804. [Accessed: Mar. 09, 2023]\n\n\n[6] \nJ. Atwood, “Regex use vs. Regex abuse. Coding horror,” Feb. 16, 2005. [Online]. Available: https://blog.codinghorror.com/regex-use-vs-regex-abuse/. [Accessed: Mar. 09, 2023]\n\n\n[7] \nCristianGuerrero, “Answer to \"regular expression for first and last name\". Stack overflow,” Aug. 24, 2017. [Online]. Available: https://stackoverflow.com/a/45871742. [Accessed: Mar. 09, 2023]\n\n\n[8] \nL. Ristic, “Validate email addresses with regular expressions in JavaScript. Stack abuse,” Oct. 14, 2021. [Online]. Available: https://stackabuse.com/validate-email-addresses-with-regular-expressions-in-javascript/. [Accessed: Mar. 09, 2023]"
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#fa-bullseye-objectives",
    "href": "part-wrangling/05-data-reshape.html#fa-bullseye-objectives",
    "title": "22  Reshaping Data",
    "section": "\n22.1  Objectives",
    "text": "22.1  Objectives\nBroadly, your objective while reading this chapter is to be able to identify datasets which have “messy” formats and determine a sequence of operations to transition the data into “tidy” format. To do this, you should be master the following concepts:\n\nDetermine what data format is necessary to generate a desired plot or statistical model\nUnderstand the differences between “wide” and “long” format data and how to transition between the two structures"
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#tidy-and-messy-data",
    "href": "part-wrangling/05-data-reshape.html#tidy-and-messy-data",
    "title": "22  Reshaping Data",
    "section": "\n22.2 Tidy and Messy Data",
    "text": "22.2 Tidy and Messy Data\n\n22.2.1 Motivating Example\nConsider the spreadsheet screenshot in Figure 22.1.\n\n\nFigure 22.1: Spreadsheet intended for human consumption, from [1] (Chapter 3)\n\n\nThis spreadsheet shows New Zealand High School certificate achievement levels for a boys-only school. Typically, students would get level 1 in year 11, level 2 in year 12, and level 3 in year 13, but it is possible for students to gain multiple levels in a single year. This data is organized to show the number of students gaining each type of certification (broken out by gender) across each of the 3 years. There are many blank cells that provide ample space to see the data, and all of the necessary variables are represented: there are essentially three 2x3 tables showing the number of students attaining each NCEA level in each year of school. If all of the information is present in this table, is there really a problem? Perhaps not if the goal is just to display the data, but analyzing this data effectively, or plotting it in a way that is useful, requires some restructuring. Figure 22.2 shows a restructured version of this data in a more compact rectangular format.\n\n\nFigure 22.2: Spreadsheet reorganized for data analysis\n\n\nIn Figure 22.2, each column contains one variable: Year, gender, level, and total number of students. Each row contains one observation. We still have 18 data points, but this format is optimized for statistical analysis, rather than to display for (human) visual consumption. We will refer to this restructured data as “tidy” data: it has a single column for each variable and a single row for each observation.\n\n22.2.2 Defining Tidy data\nThe illustrations below are lifted from an excellent blog post [2] about tidy data; they’re reproduced here because\n\nthey’re beautiful and licensed as CCA-4.0-by, and\nthey might be more memorable than the equivalent paragraphs of text without illustration.\n\nMost of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data entry or viewing, rather than for statistical analysis:\n\nTables may be laid out for easy data entry, so that there are multiple observations in a single row\nIt may be visually preferable to arrange columns of data to show multiple times or categories on the same row for easy comparison\n\nWhen we analyze data, however, we care much more about the fundamental structure of observations: discrete units of data collection. Each observation may have several corresponding variables that may be measured simultaneously, but fundamentally each discrete data point is what we are interested in analyzing.\nThe structure of tidy data reflects this preference for keeping the data in a fundamental form: each observation is in its own row, any observed variables are in single columns. This format is inherently rectangular, which is also important for statistical analysis - our methods are typically designed to work with matrices of data.\n\n\nFigure 22.3: Tidy data format, illustrated.\n\n\n\n\nAn illustration of the principle that every messy dataset is messy in its own way.\n\n\nThe preference for tidy data has several practical implications: it is easier to reuse code on tidy data, allowing for analysis using a standardized set of tools (rather than having to build a custom tool for each data analysis job).\n\n\nTidy data is easier to manage because the same tools and approaches apply to multiple datasets.\n\n\nIn addition, standardized tools for data analysis means that it is easier to collaborate with others: if everyone starts with the same set of assumptions about the data set, you can borrow methods and tools from a collaborator’s analysis and easily apply them to your own data set.\n\n\n\n\n\nCollaboration with tidy data.\n\n\n\n\n\n\nTidy data enables standardized workflows.\n\n\n\n\nFigure 22.4: Tidy data makes it easier to collaborate with others and analyze new data using standardized workflows.\n\n\n\n\n\n\n\n\nExamples: Messy Data\n\n\n\nThese datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.\n\n\nTable 1\n2\n3\n4\n5\n\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nHere, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column).\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nHere, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nThis form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can’t do anything with this format as it stands, because we can’t do math on data stored as characters. However, this form might be easier to read and record for a human being.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nIn this form, we have two tables - one for population, and one for cases. Each year’s observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we’ll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nTable 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don’t always handle dates the way you’d hope they would.\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Classifying Messy Data\n\n\n\n\n\nProblem\nTable 1\n2\n3\n4\n5\n\n\n\nFor each of the datasets in the previous example, determine whether each table is tidy. If it is not, identify which rule or rules it violates.\nWhat would you have to do in order to compute a standardized TB infection rate per 100,000 people?\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nThis is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population*100,000.\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nEach variable does not have its own column (so a single year’s observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nEach value does not have its own cell (and each variable does not have its own column). In Table 3, you’d have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nThere are multiple observations in each row because there is not a column for year. To compute the rate, you’d need to “stack” the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nEach variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren’t actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you’d need to combine the two year columns together first).\n\n\n\n\n\nIt is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be:\n\nEach data set goes into its own table (or tibble, if you are using R)\nEach variable gets its own column\n\n\n\n\n\n\n\nAdditional reading\n\n\n\n[3] - IBM SPSS ad that talks about the perils of spreadsheets\n[4] - assembled news stories involving spreadsheet mishaps\n\n\nYou have learned some of the skills to tidy data in Chapter 21, and you’ll learn more in Chapter 23, but by the end of this chapter you will have many of the skills needed to wrangle the most common “messy” data sets into “tidy” form."
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#pivot-operations",
    "href": "part-wrangling/05-data-reshape.html#pivot-operations",
    "title": "22  Reshaping Data",
    "section": "\n22.3 Pivot operations",
    "text": "22.3 Pivot operations\nIt’s fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren’t necessarily the most friendly for analysis.\n\n\nWide and Long format data. Source\n\n\nThe two operations we’ll learn here are wide -> long and long -> wide.\n\n\nPivoting from wide to long (and back) Source\n\n\nThis animation uses the R functions pivot_wider() and pivot_longer() Animation source, but the concept is the same in both R and python.\n\n22.3.1 Longer\nIn many cases, the data come in what we might call “wide” form - some of the column names are not names of variables, but instead, are themselves values of another variable.\n\n\nPicture the Operation\nR\nPython\n\n\n\nTables 4a and 4b are good examples of data which is in “wide” form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively.\n\ntable4a\n## # A tibble: 3 × 3\n##   country     `1999` `2000`\n##   <chr>        <dbl>  <dbl>\n## 1 Afghanistan    745   2666\n## 2 Brazil       37737  80488\n## 3 China       212258 213766\ntable4b\n## # A tibble: 3 × 3\n##   country         `1999`     `2000`\n##   <chr>            <dbl>      <dbl>\n## 1 Afghanistan   19987071   20595360\n## 2 Brazil       172006362  174504898\n## 3 China       1272915272 1280428583\n\nThe solution to this is to rearrange the data into “long form”: to take the columns which contain values and “stack” them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn’t being stacked (e.g. country, in both the example above and the image below).\n\n\nA visual representation of what the pivot_longer operation looks like in practice.\n\n\nOnce our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we’ll have tidy(er) data.\n\n\n\ntba <- table4a %>% \n  pivot_longer(-country, names_to = \"year\", values_to = \"cases\")\ntbb <- table4b %>% \n  pivot_longer(-country, names_to = \"year\", values_to = \"population\")\n\n# To get the tidy data, we join the two together (see Table joins below)\nleft_join(tba, tbb, by = c(\"country\", \"year\")) %>%\n  # make year numeric b/c it's dumb not to\n  mutate(year = as.numeric(year))\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <dbl>  <dbl>      <dbl>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\nThe columns are moved to a variable with the name passed to the argument “names_to” (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument “values_to” (again, hopefully easy to remember).\nWe identify ID variables (variables which we don’t want to pivot) by not including them in the pivot statement. We can do this in one of two ways:\n\nselect only variables we want to pivot: pivot_longer(table4a, cols =1999:2000, names_to = \"year\", values_to = \"cases\")\n\nselect variables we don’t want to pivot, using - to remove them. (see above, where -country excludes country from the pivot operation)\n\nWhich option is easier depends how many things you’re pivoting (and how the columns are structured).\nIf we wanted to avoid the table join, we could do this process another way: first, we would add a column to each tibble called id with values “cases” and “population” respectively. Then, we could bind the two tables together by row (so stack them on top of each other). We could then do a wide-to-long pivot, followed by a long-to-wide pivot to get our data into tidy form.\n\n# Create ID columns\ntable4a.x <- table4a %>% mutate(id = \"cases\")\ntable4b.x <- table4b %>% mutate(id = \"population\")\n# Create one table\ntable4 <- bind_rows(table4a.x, table4b.x)\n\ntable4_long <- table4 %>%\n  # rearrange columns\n  select(country, id, `1999`, `2000`) %>%\n  # Don't pivot country or id\n  pivot_longer(-c(country:id), names_to = \"year\", values_to = \"count\")\n\n# Intermediate fully-long form\ntable4_long\n## # A tibble: 12 × 4\n##    country     id         year       count\n##    <chr>       <chr>      <chr>      <dbl>\n##  1 Afghanistan cases      1999         745\n##  2 Afghanistan cases      2000        2666\n##  3 Brazil      cases      1999       37737\n##  4 Brazil      cases      2000       80488\n##  5 China       cases      1999      212258\n##  6 China       cases      2000      213766\n##  7 Afghanistan population 1999    19987071\n##  8 Afghanistan population 2000    20595360\n##  9 Brazil      population 1999   172006362\n## 10 Brazil      population 2000   174504898\n## 11 China       population 1999  1272915272\n## 12 China       population 2000  1280428583\n\n# make wider, with case and population columns\ntable4_tidy <- table4_long %>%\n  pivot_wider(names_from = id, values_from = count)\n\ntable4_tidy\n## # A tibble: 6 × 4\n##   country     year   cases population\n##   <chr>       <chr>  <dbl>      <dbl>\n## 1 Afghanistan 1999     745   19987071\n## 2 Afghanistan 2000    2666   20595360\n## 3 Brazil      1999   37737  172006362\n## 4 Brazil      2000   80488  174504898\n## 5 China       1999  212258 1272915272\n## 6 China       2000  213766 1280428583\n\n\n\nIn Pandas, pandas.melt(...) takes id_vars, value_vars, var_name, and value_name. Otherwise, it functions nearly exactly the same as pivot_longer; the biggest difference is that column selection works differently in python than it does in the tidyverse.\nAs in R, we can choose to either do a melt/pivot_longer operation on each table and then join the tables together, or we can concatenate the rows and do a melt/pivot_longer operation followed by a pivot/pivot_wider operation.\n\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntba = pd.melt(table4a, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'cases')\ntbb = pd.melt(table4b, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'population')\n\n# To get the tidy data, we join the two together (see Table joins below)\ntable4_tidy = pd.merge(tba, tbb, on = [\"country\", \"year\"], how = 'left')\n\nHere’s the melt/pivot_longer + pivot/pivot_wider version:\n\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntable4a['id'] = \"cases\"\ntable4b['id'] = \"population\"\n\ntable4 = pd.concat([table4a, table4b])\n\n# Fully long form\ntable4_long = pd.melt(table4, id_vars = ['country', 'id'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'count')\n\n# Tidy form - case and population columns\ntable4_tidy2 = pd.pivot(table4_long, index = ['country', 'year'], columns = ['id'], values = 'count')\n# reset_index() gets rid of the grouped index\ntable4_tidy2.reset_index()\n## id      country  year     cases    population\n## 0   Afghanistan  1999     745.0  1.998707e+07\n## 1   Afghanistan  2000    2666.0  2.059536e+07\n## 2        Brazil  1999   37737.0  1.720064e+08\n## 3        Brazil  2000   80488.0  1.745049e+08\n## 4         China  1999  212258.0  1.272915e+09\n## 5         China  2000  213766.0  1.280429e+09\n\n\n\n\n\n22.3.2 Wider\nWhile it’s very common to need to transform data into a longer format, it’s not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again.\n\n\nPicture the Operation\nR\nPython\n\n\n\nTable 2 is an example of a table that is in long format but needs to be converted to a wider layout to be “tidy” - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows.\n\n\nA visual representation of what the pivot_wider operation looks like in practice.\n\n\n\n\n\ntable2 %>%\n  pivot_wider(names_from = type, values_from = count)\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <dbl>  <dbl>      <dbl>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\n\n\n\ntable2 = r.table2\n\npd.pivot(table2, index = ['country', 'year'], columns = ['type'], values = 'count').reset_index()\n## type      country    year     cases    population\n## 0     Afghanistan  1999.0     745.0  1.998707e+07\n## 1     Afghanistan  2000.0    2666.0  2.059536e+07\n## 2          Brazil  1999.0   37737.0  1.720064e+08\n## 3          Brazil  2000.0   80488.0  1.745049e+08\n## 4           China  1999.0  212258.0  1.272915e+09\n## 5           China  2000.0  213766.0  1.280429e+09\n\n\n\n\n\n\n\n\n\n\nTry it Out!\n\n\n\nIn the next section, we’ll be using the WHO surveillance of disease incidence data (link). I originally wrote this using data from 2020, but the WHO has since migrated to a new system and now provides their data in a much tidier long form (Excel link). For demonstration purposes, I’ll continue using the messier 2020 data, but the link is no longer available on the WHO’s site.\nIt will require some preprocessing before it’s suitable for a demonstration. I’ll do some of it, but in this section, you’re going to do the rest.\n\n\nPreprocessing\nProblem\nR solution\nPython solution\n\n\n\nYou don’t have to understand what this code is doing just yet.\n\nlibrary(readxl)\nlibrary(purrr) # This uses the map() function as a replacement for for loops. \n# It's pretty sweet\nlibrary(tibble)\nlibrary(dplyr)\n\ndownload.file(\"https://github.com/srvanderplas/datasets/raw/main/raw/2020_WHO_incidence_series.xls\", \"../data/2020_WHO_incidence_series.xls\")\nsheets <- excel_sheets(\"../data/2020_WHO_incidence_series.xls\")\nsheets <- sheets[-c(1, length(sheets))] # get rid of 1st and last sheet name\n\n# This command says \"for each sheet, read in the excel file with that sheet name\"\n# map_df means paste them all together into a single data frame\ndisease_incidence <- map_df(sheets, ~read_xls(path =\"../data/2020_WHO_incidence_series.xls\", sheet = .))\n\n# Alternately, we could write a loop:\ndisease_incidence2 <- tibble() # Blank data frame\nfor (i in 1:length(sheets)) {\n  disease_incidence2 <- bind_rows(\n    disease_incidence2, \n    read_xls(path = \"../data/2020_WHO_incidence_series.xls\", sheet = sheets[i])\n  )\n}\n\n# export for Python (and R, if you want)\nreadr::write_csv(disease_incidence, file = \"../data/2020_who_disease_incidence.csv\")\n\n\n\nDownload the exported data here and import it into Python and R. Transform it into long format, so that there is a year column. You should end up with a table that has dimensions of approximately 6 columns and 83,000 rows (or something close to that).\nCan you make a line plot of cases of measles in Bangladesh over time?\n\nhead(disease_incidence)\n## # A tibble: 6 × 43\n##   WHO_R…¹ ISO_c…² Cname Disease `2018` `2017` `2016` `2015` `2014` `2013` `2012`\n##   <chr>   <chr>   <chr> <chr>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n## 1 EMR     AFG     Afgh… CRS         NA     NA     NA      0      0      0     NA\n## 2 EUR     ALB     Alba… CRS          0      0     NA     NA     NA      0      0\n## 3 AFR     DZA     Alge… CRS         NA     NA      0      0     NA     NA      0\n## 4 EUR     AND     Ando… CRS          0      0      0     NA     NA      0      0\n## 5 AFR     AGO     Ango… CRS         NA     NA     NA     NA     NA     NA     NA\n## 6 AMR     ATG     Anti… CRS          0      0      0      0      0      0      0\n## # … with 32 more variables: `2011` <dbl>, `2010` <dbl>, `2009` <dbl>,\n## #   `2008` <dbl>, `2007` <dbl>, `2006` <dbl>, `2005` <dbl>, `2004` <dbl>,\n## #   `2003` <dbl>, `2002` <dbl>, `2001` <dbl>, `2000` <dbl>, `1999` <dbl>,\n## #   `1998` <dbl>, `1997` <dbl>, `1996` <dbl>, `1995` <dbl>, `1994` <dbl>,\n## #   `1993` <dbl>, `1992` <dbl>, `1991` <dbl>, `1990` <dbl>, `1989` <dbl>,\n## #   `1988` <dbl>, `1987` <dbl>, `1986` <dbl>, `1985` <dbl>, `1984` <dbl>,\n## #   `1983` <dbl>, `1982` <dbl>, `1981` <dbl>, `1980` <dbl>, and abbreviated …\n\n\n\n\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(stringr)\nwho_disease <- read_csv(\"../data/2020_who_disease_incidence.csv\", na = \".\")\n\nwho_disease_long <- who_disease %>%\n  pivot_longer(matches(\"\\\\d{4}\"), names_to = \"year\", values_to = \"cases\") %>%\n  rename(Country = Cname) %>%\n  mutate(Disease = str_replace(Disease, \"CRS\", \"Congenital Rubella\"),\n         year = as.numeric(year),\n         cases = as.numeric(cases))\n\nfilter(who_disease_long, Country == \"Bangladesh\", Disease == \"measles\") %>%\n  ggplot(aes(x = year, y = cases)) + geom_line()\n\n\n\n\n\n\n\nimport pandas as pd\nfrom plotnine import *\n\nwho_disease = pd.read_csv(\"../data/2020_who_disease_incidence.csv\", na_values = ['NA', 'NaN'])\nwho_disease_long = pd.melt(who_disease, id_vars = ['WHO_REGION', 'ISO_code', 'Cname', 'Disease'], var_name = 'year', value_name = 'cases')\n# Rename cname to country\nwho_disease_long = who_disease_long.rename(columns={\"Cname\": \"Country\"})\nwho_disease_long.replace(\"CRS\", \"Congenital Rubella\")\nwho_disease_long['year'] = pd.to_numeric(who_disease_long['year'])\n\ntmp = who_disease_long.query(\"Country=='Bangladesh' & Disease == 'measles'\")\nggplot(tmp, aes(x = \"year\", y = \"cases\")) + geom_line()"
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#sec-gas-price-ex",
    "href": "part-wrangling/05-data-reshape.html#sec-gas-price-ex",
    "title": "22  Reshaping Data",
    "section": "\n22.4 Example: Gas Prices Data",
    "text": "22.4 Example: Gas Prices Data\nThe US Energy Information Administration tracks gasoline prices, with data available on a weekly level since late 1994. You can go to this site to see a nice graph of gas prices, along with a corresponding table. \n\n\nGas prices at US EIA site\n\n\nThe data in the table is structured in a fairly easy to read form: each row is a month; each week in the month is a set of two columns: one for the date, one for the average gas price. While this data is definitely not tidy, it is readable.\nBut looking at the chart at the top of the page, it’s not clear how we might get that chart from the data in the format it’s presented here: to get a chart like that, we would need a table where each row was a single date, and there were columns for date and price. That would be tidy form data, and so we have to get from the wide, human-readable form into the long, tidier form that we can graph.\n\n\n\n\n\n\nSetup: Gas Price Data Cleaning\n\n\n\nFor the next example, we’ll read the data in from the HTML table online and work to make it something we could e.g. plot. Before we can start cleaning, we have to read in the data:\n\n\nR\nPython\n\n\n\n\nlibrary(rvest) # scrape data from the web\nlibrary(xml2) # parse xml data\nurl <- \"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\"\n\nhtmldoc <- read_html(url)\ngas_prices_html <- html_table(htmldoc, fill = T, trim = T)[[5]][,1:11]\n\n\n\n\nFirst 6 rows of gas prices data as read into R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear-Month\nWeek 1\nWeek 1\nWeek 2\nWeek 2\nWeek 3\nWeek 3\nWeek 4\nWeek 4\nWeek 5\nWeek 5\n\n\n\nYear-Month\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\n\n\n1994-Nov\n\n\n\n\n\n\n11/28\n1.175\n\n\n\n\n1994-Dec\n12/05\n1.143\n12/12\n1.118\n12/19\n1.099\n12/26\n1.088\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1995-Jan\n01/02\n1.104\n01/09\n1.111\n01/16\n1.102\n01/23\n1.110\n01/30\n1.109\n\n\n1995-Feb\n02/06\n1.103\n02/13\n1.099\n02/20\n1.093\n02/27\n1.101\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\ngas_prices_html = pd.read_html(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\")[4]\n## Error in py_call_impl(callable, dots$args, dots$keywords): ImportError: lxml not found, please install it\n\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_html' is not defined\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Manual Formatting in Excel\n\n\n\n\n\nProblem\nSolution\nVideo\n\n\n\nAn excel spreadsheet of the data as downloaded in January 2023 is available here. Can you manually format the data (or even just the first year or two of data) into a long, skinny format?\nWhat steps are involved?\n\n\n\nCopy the year-month column, creating one vertical copy for every set of columns\nMove each block of two columns down to the corresponding vertical copy\nDelete empty rows\nFormat dates\nDelete empty columns\n\n\n\n\n\n\nFigure 22.5: Here is a video of me doing most of the cleaning steps - I skipped out on cleaning up the dates because Excel is miserable for working with dates.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Formatting with Pivot Operations\n\n\n\n\n\nProblem\nSketch\nR solution\nPython solution\n\n\n\nCan you format the data in a long-skinny format for plotting using pivot operations without any database merges?\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\nHow do your steps compare to the steps you used for the manual approach?\n\n\n\n\nSteps to work through the gas prices data cleaning process\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(magrittr) # pipe friendly operations\n\n# Function to clean up column names\n# Written as an extra function because it makes the code a lot cleaner\nfix_gas_names <- function(x) {\n  # Add extra header row information\n  paste(x, c(\"\", rep(c(\"Date\", \"Value\"), times = 5))) %>%\n    # trim leading/trailing spaces\n    str_trim() %>%\n    # replace characters in names that aren't ok for variables in R\n    make.names()\n}\n\n# Clean up the table a bit\ngas_prices_raw <- gas_prices_html %>%\n  set_names(fix_gas_names(names(.))) %>%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %>%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n\nhead(gas_prices_raw)\n## # A tibble: 6 × 11\n##   Year.Month Week.1.Date Week.…¹ Week.…² Week.…³ Week.…⁴ Week.…⁵ Week.…⁶ Week.…⁷\n##   <chr>      <chr>       <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n## 1 1994-Nov   \"\"          \"\"      \"\"      \"\"      \"\"      \"\"      11/28   1.175  \n## 2 1994-Dec   \"12/05\"     \"1.143\" \"12/12\" \"1.118\" \"12/19\" \"1.099\" 12/26   1.088  \n## 3 1995-Jan   \"01/02\"     \"1.104\" \"01/09\" \"1.111\" \"01/16\" \"1.102\" 01/23   1.110  \n## 4 1995-Feb   \"02/06\"     \"1.103\" \"02/13\" \"1.099\" \"02/20\" \"1.093\" 02/27   1.101  \n## 5 1995-Mar   \"03/06\"     \"1.103\" \"03/13\" \"1.096\" \"03/20\" \"1.095\" 03/27   1.102  \n## 6 1995-Apr   \"04/03\"     \"1.116\" \"04/10\" \"1.134\" \"04/17\" \"1.149\" 04/24   1.173  \n## # … with 2 more variables: Week.5.Date <chr>, Week.5.Value <chr>, and\n## #   abbreviated variable names ¹​Week.1.Value, ²​Week.2.Date, ³​Week.2.Value,\n## #   ⁴​Week.3.Date, ⁵​Week.3.Value, ⁶​Week.4.Date, ⁷​Week.4.Value\n\n\n# gas_prices_raw <- select(gas_prices_raw, -c(X, Date))\ngas_prices_long <- pivot_longer(gas_prices_raw, -Year.Month,\n                                names_to = \"variable\", values_to = \"value\")\n\nhead(gas_prices_long)\n## # A tibble: 6 × 3\n##   Year.Month variable     value\n##   <chr>      <chr>        <chr>\n## 1 1994-Nov   Week.1.Date  \"\"   \n## 2 1994-Nov   Week.1.Value \"\"   \n## 3 1994-Nov   Week.2.Date  \"\"   \n## 4 1994-Nov   Week.2.Value \"\"   \n## 5 1994-Nov   Week.3.Date  \"\"   \n## 6 1994-Nov   Week.3.Value \"\"\n\n\ngas_prices_sep <- separate(gas_prices_long, variable, into = c(\"extra\", \"week\", \"variable\"), sep = \"\\\\.\") %>%\n  select(-extra)\nhead(gas_prices_sep)\n## # A tibble: 6 × 4\n##   Year.Month week  variable value\n##   <chr>      <chr> <chr>    <chr>\n## 1 1994-Nov   1     Date     \"\"   \n## 2 1994-Nov   1     Value    \"\"   \n## 3 1994-Nov   2     Date     \"\"   \n## 4 1994-Nov   2     Value    \"\"   \n## 5 1994-Nov   3     Date     \"\"   \n## 6 1994-Nov   3     Value    \"\"\n\n\ngas_prices_wide <- pivot_wider(gas_prices_sep, id_cols = c(\"Year.Month\", \"week\"), names_from = variable, values_from = value)\nhead(gas_prices_wide)\n## # A tibble: 6 × 4\n##   Year.Month week  Date    Value  \n##   <chr>      <chr> <chr>   <chr>  \n## 1 1994-Nov   1     \"\"      \"\"     \n## 2 1994-Nov   2     \"\"      \"\"     \n## 3 1994-Nov   3     \"\"      \"\"     \n## 4 1994-Nov   4     \"11/28\" \"1.175\"\n## 5 1994-Nov   5     \"\"      \"\"     \n## 6 1994-Dec   1     \"12/05\" \"1.143\"\n\n\ngas_prices_date <- gas_prices_wide %>%\n  filter(nchar(Value) > 0) %>%\n  separate(Year.Month, into = c(\"Year\", \"Month\"), sep = \"-\") %>%\n  mutate(Date = paste(Year, Date, sep = \"/\")) %>%\n  select(-c(1:3))\n  \nhead(gas_prices_date)\n## # A tibble: 6 × 2\n##   Date       Value\n##   <chr>      <chr>\n## 1 1994/11/28 1.175\n## 2 1994/12/05 1.143\n## 3 1994/12/12 1.118\n## 4 1994/12/19 1.099\n## 5 1994/12/26 1.088\n## 6 1995/01/02 1.104\n\n\nlibrary(lubridate)\ngas_prices <- gas_prices_date %>%\n  mutate(Date = ymd(Date),\n         Price.per.gallon = as.numeric(Value)) %>%\n  select(-Value)\n  \nhead(gas_prices)\n## # A tibble: 6 × 2\n##   Date       Price.per.gallon\n##   <date>                <dbl>\n## 1 1994-11-28             1.18\n## 2 1994-12-05             1.14\n## 3 1994-12-12             1.12\n## 4 1994-12-19             1.10\n## 5 1994-12-26             1.09\n## 6 1995-01-02             1.10\n\n\n\n\nimport numpy as np\n\ndef fix_gas_names(x):\n  xx = pd.Series(x)\n  # add extra stuff to x\n  y = [\"Date\", \"Value\"]*5\n  y = [\"\", *y, \"\", \"\"]\n  names = xx + ' ' + y\n  names = names.str.strip()\n  names = names.str.replace(\" \", \".\")\n  return list(names)\n\n\ngas_prices_raw = gas_prices_html.copy()\n\n# What do column names look like?\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_html' is not defined\ngas_prices_raw.columns # Multi-Index \n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\ncolnames\n\n# Set new column names\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'colnames' is not defined\ngas_prices_raw.columns = colnames\n\n# Drop any rows with NaN in Year-Month\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'colnames' is not defined\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n\n# Drop extra columns on the end\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\ngas_prices_raw = gas_prices_raw.iloc[:,0:11]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\ngas_prices_raw.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\n\n\ngas_prices_long = pd.melt(gas_prices_raw, id_vars = 'Year-Month', var_name = 'variable')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\ngas_prices_long.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_long' is not defined\n\n\ngas_prices_sep = gas_prices_long\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_long' is not defined\ngas_prices_sep[[\"extra\", \"week\", \"variable\"]] = gas_prices_sep.variable.str.split(r'\\.', expand = True)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_sep' is not defined\ngas_prices_sep = gas_prices_sep.drop('extra', axis = 1)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_sep' is not defined\ngas_prices_sep.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_sep' is not defined\n\n\ngas_prices_wide = pd.pivot(gas_prices_sep, index=['Year-Month', 'week'], columns = 'variable', values = 'value')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_sep' is not defined\ngas_prices_wide.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_wide' is not defined\n\n\ngas_prices_date = gas_prices_wide.dropna(axis = 0, subset = ['Date', 'Value']).reset_index()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_wide' is not defined\ngas_prices_date[['Year', 'Month']] = gas_prices_date['Year-Month'].str.split(r'-', expand = True)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_date' is not defined\ngas_prices_date['Date'] = gas_prices_date.Year + '/' + gas_prices_date.Date\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_date' is not defined\ngas_prices_date['Date'] = pd.to_datetime(gas_prices_date.Date)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_date' is not defined\ngas_prices_date.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_date' is not defined\n\n\n\ngas_prices = gas_prices_date.drop([\"Year-Month\", \"Year\", \"Month\", \"week\"], axis = 1)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_date' is not defined\ngas_prices['Price_per_gallon'] = gas_prices.Value\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices' is not defined\ngas_prices = gas_prices.drop(\"Value\", axis = 1)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices' is not defined\ngas_prices.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices' is not defined\n\n\n\n\n\n\nWe’ll return to this example in Section 23.5 to demonstrate how you can use pivot operations and database merges together to complete this operation in a slightly different way.\n\n\n\n\n\n\nOther resources\n\n\n\n[5] - very nice task-oriented chapter that’s below the level addressed in this course but still useful"
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#references",
    "href": "part-wrangling/05-data-reshape.html#references",
    "title": "22  Reshaping Data",
    "section": "\n22.5 References",
    "text": "22.5 References\n\n\n\n\n[1] \nQ. E. McCallum, Bad data handbook: Mapping the world of data problems, 1. ed. Beijing, Köln: O’Reilly, 2013. \n\n\n[2] \nJ. Lowndes and A. Horst, “Tidy data for efficiency, reproducibility, and collaboration,” Openscapes. Oct. 2020 [Online]. Available: https://www.openscapes.org/blog/2020/10/12/tidy-data//. [Accessed: Jul. 21, 2022]\n\n\n[3] \nInternational Business Machines, “The risks of using spreadsheets for statistical analysis,” The risks of using spreadsheets for statistical analysis. Nov. 2018 [Online]. Available: https://www.ibm.com/downloads/cas/7YEX9BKK. [Accessed: Jul. 21, 2022]\n\n\n[4] \nP. O’Beirne, F. Hermans, T. Cheng, and M. P. Campbell, “Horror Stories,” European Spreadsheet Risks Interest Group Horror Stories. Oct. 2020 [Online]. Available: http://www.eusprig.org/horror-stories.htm. [Accessed: Jul. 21, 2022]\n\n\n[5] \nJ. Dougherty and I. Ilyankou, “Clean Up Messy Data,” in Hands-On Data Visualization, 1st ed., O’Reilly Media, 2021, p. 480 [Online]. Available: http://handsondataviz.github.io/. [Accessed: Jul. 21, 2022]"
  },
  {
    "objectID": "part-wrangling/06-data-join.html#fa-bullseye-objectives",
    "href": "part-wrangling/06-data-join.html#fa-bullseye-objectives",
    "title": "23  Joining Data",
    "section": "\n23.1  Objectives",
    "text": "23.1  Objectives\n\nIdentify columns (keys) which can be used to join separate but related tables\nSketch/plan out join operations based on matching keys and given objectives\nImplement planned join operations in R or python\nIdentify when join operations have not completed successfully by looking for duplicated rows, number of rows/columns in the finished object, and missing value counts.\n\n\n\n\n\n\n\nRelational Data Example: Primary School Records\n\n\n\n\n\nEach individual has certain characteristics:\n\nfull_name\ngender\nbirth date\nID number\n\nEach student has specific characteristics:\n\nID number\nparent name\nparent phone number\nmedical information\nClass ID\n\nTeachers may also have additional information:\n\nID number\nClass ID\nemployment start date\neducation level\ncompensation level\n\nThere are also fields like grades, which occur for each student in each class, but multiple times a year.\n\nID number\nStudent ID\nClass ID\nyear\nterm number\nsubject\ngrade\ncomment\n\nAnd for teachers, there are employment records on a yearly basis\n\nID number\nEmployee ID\nyear\nrating\ncomment\n\nBut each class also has characteristics that describe the whole class as a unit:\n\nlocation ID\nclass ID\nmeeting time\ngrade level\n\nEach location might also have some logistical information attached:\n\nlocation ID\nroom number\nbuilding\nnumber of seats\nAV equipment\n\n\nWe could go on, but you can see that this data is hierarchical, but also relational:\n\neach class has both a teacher and a set of students\neach class is held in a specific location that has certain equipment\n\nIt would be silly to store this information in a single table (though it can be done) because all of the teacher information would be duplicated for each student in each class; all of the student’s individual info would be duplicated for each grade. There would be a lot of wasted storage space and the tables would be much more confusing as well.\nBut, relational data also means we have to put in some work when we have a question that requires information from multiple tables. Suppose we want a list of all of the birthdays in a certain class. We would need to take the following steps:\n\nget the Class ID\nget any teachers that are assigned that Class ID - specifically, get their ID number\nget any students that are assigned that Class ID - specifically, get their ID number\nappend the results from teachers and students so that there is a list of all individuals in the class\nlook through the “individual data” table to find any individuals with matching ID numbers, and keep those individuals’ birth days.\n\nIt is helpful to develop the ability to lay out a set of tables in a schema (because often, database schemas aren’t well documented) and mentally map out the steps that you need to combine tables to get the information you want from the information you have."
  },
  {
    "objectID": "part-wrangling/06-data-join.html#vocabulary",
    "href": "part-wrangling/06-data-join.html#vocabulary",
    "title": "23  Joining Data",
    "section": "\n23.2 Vocabulary",
    "text": "23.2 Vocabulary\nTable joins allow us to combine information stored in different tables, keeping certain information (the stuff we need) while discarding extraneous information.\nKeys are values that are found in multiple tables that can be used to connect the tables. A key (or set of keys) uniquely identify an observation. A primary key identifies an observation in its own table. A foreign key identifies an observation in another table.\nThere are 3 main types of table joins:\n\nMutating joins, which add columns from one table to matching rows in another table\nEx: adding birthday to the table of all individuals in a class\nFiltering joins, which remove rows from a table based on whether or not there is a matching row in another table (but the columns in the original table don’t change)\nEx: finding all teachers or students who have class ClassID\nSet operations, which treat observations as set elements (e.g. union, intersection, etc.)\nEx: taking the union of all student and teacher IDs to get a list of individual IDs"
  },
  {
    "objectID": "part-wrangling/06-data-join.html#illustrating-joins",
    "href": "part-wrangling/06-data-join.html#illustrating-joins",
    "title": "23  Joining Data",
    "section": "\n23.3 Illustrating Joins",
    "text": "23.3 Illustrating Joins\nNote: all of these animations are stolen from https://github.com/gadenbuie/tidyexplain.\nIf we start with two tables, x and y,\n\nThe next several sections will show animations demonstrating the different types of joins.\n\n23.3.1 Mutating Joins\nWe’re primarily going to focus on mutating joins, as filtering joins can be accomplished by … filtering … rather than by table joins.\n\n\nInner Join\nLeft Join\nRight Join\nFull Join\n\n\n\nWe can do a filtering inner_join to keep only rows which are in both tables (but we keep all columns)\n\n\n\nBut what if we want to keep all of the rows in x? We would do a left_join\n\nIf there are multiple matches in the y table, though, we might have to duplicate rows in x. This is still a left join, just a more complicated one.\n\n\n\nIf we wanted to keep all of the rows in y, we would do a right_join:\n\n(or, we could do a left join with y and x, but… either way is fine).\n\n\nAnd finally, if we want to keep all of the rows, we’d do a full_join:\n\nYou can find other animations corresponding to filtering joins and set operations here\n\n\n\nEvery join has a “left side” and a “right side” - so in some_join(A, B), A is the left side, B is the right side.\nJoins are differentiated based on how they treat the rows and columns of each side. In mutating joins, the columns from both sides are always kept.\n\n\n\n\n\n\n\n\n\n\nLeft Side\nRight Side\n\n\n\n\nJoin Type\nRows\nCols\n\n\ninner\nmatching\nall\nmatching\n\n\nleft\nall\nall\nmatching\n\n\nright\nmatching\nall\nall\n\n\nouter\nall\nall\nall\n\n\n\n\n23.3.1.1 Code\n\n\nR (base)\nR (tidy)\nPandas\n\n\n\nJoins in base R are accomplished with the merge command.\nSpecify the keys to join by using by (or by.x and by.y if the column names are different in the two tables). Specify the rows to keep using all or all.x and all.y. By default, R will merge on any variables that have the same names in each table.\n\nmerge(x, y, by = \"v1\", all = F) # inner join\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\nmerge(x, y, by = \"v1\", all = T) # full join\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 <NA>\n## 4  4 <NA>   y4\nmerge(x, y, by = \"v1\", all.x = T) # left join\n##   v1 v2   v3\n## 1  1 x1   y1\n## 2  2 x2   y2\n## 3  3 x3 <NA>\nmerge(x, y, by = \"v1\", all.y = T) # right join\n##   v1   v2 v3\n## 1  1   x1 y1\n## 2  2   x2 y2\n## 3  4 <NA> y4\n\n\n\ndplyr contains functions that specifically implement mutating joins separately, primarily for code readability.\n\nlibrary(dplyr)\ninner_join(x, y)\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\nleft_join(x, y)\n##   v1 v2   v3\n## 1  1 x1   y1\n## 2  2 x2   y2\n## 3  3 x3 <NA>\nright_join(x, y)\n##   v1   v2 v3\n## 1  1   x1 y1\n## 2  2   x2 y2\n## 3  4 <NA> y4\nfull_join(x, y)\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 <NA>\n## 4  4 <NA>   y4\n\n\n\nMutating joins in pandas are accomplished with the merge command. The join type can be specified using the how parameter (left, right, outer, inner, cross). Specify the keys to join by using on (or left_on and right_on if the column names are different in the two tables).\n\nimport pandas as pd\npd.merge(x, y) # inner join (how = 'inner' is default)\n##    v1  v2  v3\n## 0   1  x1  y1\n## 1   2  x2  y2\npd.merge(x, y, how = 'left')\n##    v1  v2   v3\n## 0   1  x1   y1\n## 1   2  x2   y2\n## 2   3  x3  NaN\npd.merge(x, y, how = 'right')\n##     v1   v2  v3\n## 0  1.0   x1  y1\n## 1  2.0   x2  y2\n## 2  4.0  NaN  y4\npd.merge(x, y, how = 'outer') # full join\n##     v1   v2   v3\n## 0  1.0   x1   y1\n## 1  2.0   x2   y2\n## 2  3.0   x3  NaN\n## 3  4.0  NaN   y4\n\n\n\n\n\n\n\n\n\n\nDemo: Mutating Joins\n\n\n\n\n\nR\nPython\n\n\n\n\nlibrary(tibble)\nlibrary(dplyr)\nt1 <- tibble(x = c(\"A\", \"B\", \"D\"), y = c(1, 2, 3))\nt2 <- tibble(x = c(\"B\", \"C\", \"D\"), z = c(2, 4, 5))\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\ninner_join(t1, t2)\n## # A tibble: 2 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 D         3     5\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs.\n\nleft_join(t1, t2)\n## # A tibble: 3 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\nleft_join(t2, t1)\n## # A tibble: 3 × 3\n##   x         z     y\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 C         4    NA\n## 3 D         5     3\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\nright_join(t1, t2)\n## # A tibble: 3 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 D         3     5\n## 3 C        NA     4\nright_join(t2, t1)\n## # A tibble: 3 × 3\n##   x         z     y\n##   <chr> <dbl> <dbl>\n## 1 B         2     2\n## 2 D         5     3\n## 3 A        NA     1\n\nAn outer join keeps everything - all rows, all columns. In dplyr, it’s known as a full_join.\n\nfull_join(t1, t2)\n## # A tibble: 4 × 3\n##   x         y     z\n##   <chr> <dbl> <dbl>\n## 1 A         1    NA\n## 2 B         2     2\n## 3 D         3     5\n## 4 C        NA     4\n\n\n\n\n# This works because I already created the objects in R\n# and have the reticulate package loaded\nt1 = r.t1\nt2 = r.t2\n\nAn inner join keeps only rows that exist on both sides, but keeps all columns.\n\nimport pandas as pd\npd.merge(t1, t2, on = ['x']) # inner is default\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  D  3.0  5.0\n\nA left join keeps all of the rows in the left side, and adds any columns from the right side that match rows on the left. Rows on the left that don’t match get filled in with NAs.\n\npd.merge(t1, t2, on  = 'x', how = 'left')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'left')\n##    x    z    y\n## 0  B  2.0  2.0\n## 1  C  4.0  NaN\n## 2  D  5.0  3.0\n\nThere is a similar construct called a right join that is equivalent to flipping the arguments in a left join. The row and column ordering may be different, but all of the same values will be there\n\npd.merge(t1, t2, on  = 'x', how = 'right')\n##    x    y    z\n## 0  B  2.0  2.0\n## 1  C  NaN  4.0\n## 2  D  3.0  5.0\npd.merge(t2, t1, on = 'x', how = 'right')\n##    x    z    y\n## 0  A  NaN  1.0\n## 1  B  2.0  2.0\n## 2  D  5.0  3.0\n\nAn outer join keeps everything - all rows, all columns.\n\npd.merge(t1, t2, on  = 'x', how = 'outer')\n##    x    y    z\n## 0  A  1.0  NaN\n## 1  B  2.0  2.0\n## 2  D  3.0  5.0\n## 3  C  NaN  4.0\n\n\n\n\n\n\nI’ve included the other types of joins as animations because the animations are so useful for understanding the concept, but feel free to read through more information on these types of joins here [1].\n\n23.3.2 Filtering Joins\n\n\nSemi Join\nAnti Join\n\n\n\nA semi join keeps matching rows from x and y, discarding all other rows and keeping only the columns from x.\n\n\n\nAn anti-join keeps rows in x that do not have a match in y, and only keeps columns in x.\n\n\n\n\n\n23.3.2.1 Code\n\n\nR (base)\nR (tidy)\nPandas\n\n\n\nSemi and anti joins aren’t available by default in base R. You have to do multiple stages of operations to get either one to work.\n\n## Semi-join\n# First, do an inner join\ninnerxy = merge(x, y, all = F)\ninnerxy\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\n# Then, only keep cols in x\nsemixy = innerxy[,names(innerxy)%in% names(x)]\nsemixy\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\n\n## Anti-join\n# First, do an outer join\nouterxy = merge(x, y, all = T)\nouterxy\n##   v1   v2   v3\n## 1  1   x1   y1\n## 2  2   x2   y2\n## 3  3   x3 <NA>\n## 4  4 <NA>   y4\n# Then, drop any rows with NAs\nantixy = na.omit(outerxy)\nantixy\n##   v1 v2 v3\n## 1  1 x1 y1\n## 2  2 x2 y2\n# Then, only keep cols in x\nantixy = antixy[,names(antixy) %in% names(x)]\nantixy\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\n\n\n\n\nlibrary(dplyr)\nsemi_join(x, y)\n##   v1 v2\n## 1  1 x1\n## 2  2 x2\nanti_join(x, y)\n##   v1 v2\n## 1  3 x3\n\n\n\nIn pandas, we have to be a bit tricky to get semi and anti joins.\n\nimport pandas as pd\n# First, we merge the two data frames (inner by default)\nsemixy = pd.merge(x, y) # Semi join\nsemixy\n\n# Then, we drop the extra columns\n##    v1  v2  v3\n## 0   1  x1  y1\n## 1   2  x2  y2\nsemixy = semixy[semixy.columns.intersection(x.columns)]\nsemixy\n##    v1  v2\n## 0   1  x1\n## 1   2  x2\n\n\n# This syntax keeps track of which rows are from which table\nouter = x.merge(y, how='outer', indicator=True)\nouter\n# Then we drop any rows that aren't 'left_only'\n##     v1   v2   v3      _merge\n## 0  1.0   x1   y1        both\n## 1  2.0   x2   y2        both\n## 2  3.0   x3  NaN   left_only\n## 3  4.0  NaN   y4  right_only\nantixy = outer[(outer._merge=='left_only')].drop('_merge', axis=1)\nantixy\n# Then we drop any cols that aren't in x\n##     v1  v2   v3\n## 2  3.0  x3  NaN\nantixy = antixy[antixy.columns.intersection(x.columns)]\nantixy\n##     v1  v2\n## 2  3.0  x3\n\n\n\n\n\n23.3.3 Set Operations\nWhen talking about set operations, we start with two different data frames than those used above:\n\n\n\nUnion\nUnion All\nIntersection\nSet Difference\n\n\n\nAll unique rows from x and y\n\nOr, all unique rows from y and x.\n\n\n\nAll rows from x and y, keeping duplicate rows.\n\nThis is fundamentally the same as an rbind or bind_rows operation.\n\n\nCommon rows in x and y, keeping only unique rows.\n\n\n\nAll rows from x which are not also rows in y, keeping unique rows.\n\n\n\n\n\n\n23.3.3.1 Code\n\n\nR (base)\nR (tidy)\nPandas\n\n\n\n\nunionxy = unique(rbind(x, y))\nunionxy\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 5  2  b\n\nunionallxy = rbind(x, y)\nunionallxy\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 4  1  a\n## 5  2  b\n\nintersectxy = merge(x, y, all = F)\nintersectxy\n##   v1 v2\n## 1  1  a\n\nIt is possible to get set difference and intersection for data frames by applying the base methods setdiff and intersect, but dplyr does this by overriding those defaults, so it’s easier to just use that.\n\n\n\nlibrary(dplyr)\nunion(x, y)\n##   v1 v2\n## 1  1  a\n## 2  1  b\n## 3  2  a\n## 4  2  b\nunionall(x, y)\n## Error in unionall(x, y): could not find function \"unionall\"\nsetdiff(x, y)\n##   v1 v2\n## 1  1  b\n## 2  2  a\nsetdiff(y, x)\n##   v1 v2\n## 1  2  b\nintersect(x, y)\n##   v1 v2\n## 1  1  a\n\n\n\n\nimport pandas as pd\n\n# Union\npd.concat([x, y]).drop_duplicates(keep = False)\n\n# Union all\n##     v1 v2\n## 1  1.0  b\n## 2  2.0  a\n## 1  2.0  b\npd.concat([x, y])\n\n# Intersection\n##     v1 v2\n## 0  1.0  a\n## 1  1.0  b\n## 2  2.0  a\n## 0  1.0  a\n## 1  2.0  b\nintersect = x.merge(y, how='inner')\nintersect\n\n# Set Difference\n##     v1 v2\n## 0  1.0  a\nsetdiffxy = x.merge(y, how='outer', indicator=True)\nsetdiffxy = setdiffxy[(setdiffxy._merge=='left_only')].drop('_merge', axis = 1)\nsetdiffxy\n##     v1 v2\n## 1  1.0  b\n## 2  2.0  a\nsetdiffyx = x.merge(y, how='outer', indicator=True)\nsetdiffyx = setdiffyx[(setdiffyx._merge=='right_only')].drop('_merge', axis = 1)\nsetdiffyx\n##     v1 v2\n## 3  2.0  b"
  },
  {
    "objectID": "part-wrangling/06-data-join.html#example-nyc-flights",
    "href": "part-wrangling/06-data-join.html#example-nyc-flights",
    "title": "23  Joining Data",
    "section": "\n23.4 Example: NYC Flights",
    "text": "23.4 Example: NYC Flights\nWe’ll use the nycflights13 package in R. Unfortunately, the data in this package are too big for me to reasonably store on github (you’ll recall, I had to use a small sample the last time we played with this data…). So before we can work with this data, we have to load the tables into Python.\n\n\n\n\n\n\nLoading Data\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"nycflights13\" %in% installed.packages()) install.packages(\"nycflights13\")\nif (!\"dbplyr\" %in% installed.packages()) install.packages(\"dbplyr\")\nlibrary(nycflights13)\nlibrary(dbplyr)\nlibrary(reticulate)\n# This saves the database to a sqlite db file.\n# You will want to specify your own path\nnycflights13_sqlite(path = \"../data/\")\n## <SQLiteConnection>\n##   Path: /home/susan/Projects/Class/stat-computing-r-python/data/nycflights13.sqlite\n##   Extensions: TRUE\n\n\n\n\nimport sqlite3\ncon = sqlite3.connect(\"../data/nycflights13.sqlite\")\ncur = con.cursor()\n\n\n\n\n\n\nI am not going to cover SQLITE commands here - I’m just going to use the bare minimum, but you can find a very nice introduction to python and SQLITE at datacarpentry [2], and an introduction to the dbplyr package for a nice R-SQLITE interface.\n\n\n\n\n\n\nTry it out: Understanding Relational Data\n\n\n\n\n\nProblem\nSolution\n\n\n\nSketch a diagram of which fields in each table match fields in other tables. Use the data documentation to help you with your sketch.\n\n\nhere (scroll down a bit).\n\n\n\n\n\n\n\n\n\n\n\nExample: Mutating Joins\n\n\n\nThese functions may become a bit more interesting once we try them out on real-world data. Using the flights data, let’s determine whether there’s a relationship between the age of a plane and its delays.\n\n\nR\nPython\n\n\n\n\nlibrary(nycflights13)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nplane_age <- planes %>%\n  mutate(age = 2013 - year) %>% # This gets us away from having to deal with 2 different year columns\n  select(tailnum, age, manufacturer)\n\ndelays_by_plane <- flights %>%\n  select(dep_delay, arr_delay, carrier, flight, tailnum)\n\n# We only need to keep delays that have a plane age, so use inner join\nres <- inner_join(delays_by_plane, plane_age, by = \"tailnum\")\n\nggplot(res, aes(x = age, y = dep_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-20, 50))\n\nggplot(res, aes(x = age, y = arr_delay, group = cut_width(age, 1, center = 0))) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = c(-30, 60))\n\n\n\n\n\n\n\n\n\n\n\nIt doesn’t look like there’s much of a relationship to me. If anything, older planes are more likely to be early, but I suspect there aren’t enough of them to make that conclusion (3.54% are over 25 years old, and 0.28% are over 40 years old).\n\n\n\nimport pandas as pd\nimport sqlite3\nfrom plotnine import *\ncon = sqlite3.connect(\"../data/nycflights13.sqlite\")\n\nplanes = pd.read_sql_query(\"SELECT * FROM planes\", con)\nflights = pd.read_sql_query(\"SELECT * FROM flights\", con)\n\ncon.close() # close connection\n\nplane_age = planes.assign(age = lambda df: 2013 - df.year).loc[:,[\"tailnum\", \"age\", \"manufacturer\"]]\n\ndelays_by_plane = flights.loc[:, [\"dep_delay\", \"arr_delay\", \"carrier\", \"flight\", \"tailnum\"]]\n\nres = pd.merge(plane_age, delays_by_plane, on = \"tailnum\", how = \"inner\")\n\n# cut_width isn't in plotnine, so we have to create the bins ourselves first\nage_bins = [i for i in range(2 + int(max(res.age)))] \nres = res.assign(agebin = pd.cut(res.age, age_bins))\n# res.agebin.value_counts(dropna=False)\n\n(\nggplot(res, aes(x = \"age\", y = \"dep_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Departure Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = [-20, 50])\n)\n## <ggplot: (8771636962735)>\n## \n## /home/susan/.local/lib/python3.8/site-packages/plotnine/layer.py:334: PlotnineWarning: stat_boxplot : Removed 9374 rows containing non-finite values.\n(\nggplot(res, aes(x = \"age\", y = \"arr_delay\", group = \"agebin\")) + \n  geom_boxplot() + \n  ylab(\"Arrival Delay (min)\") + \n  xlab(\"Plane age\") + \n  coord_cartesian(ylim = (-30, 60))\n)\n## <ggplot: (8771631893447)>\n## \n## /home/susan/.local/lib/python3.8/site-packages/plotnine/layer.py:334: PlotnineWarning: stat_boxplot : Removed 10317 rows containing non-finite values."
  },
  {
    "objectID": "part-wrangling/06-data-join.html#sec-gas-price-ex2",
    "href": "part-wrangling/06-data-join.html#sec-gas-price-ex2",
    "title": "23  Joining Data",
    "section": "\n23.5 Example: Gas Prices Data",
    "text": "23.5 Example: Gas Prices Data\nLet’s return to the gas price data introduced in Section 22.4. I’ve repeated the setup chunks here for you to read in the data appropriately.\n\n\n\n\n\n\nSetup: Gas Price Data Cleaning\n\n\n\nFor the next example, we’ll read the data in from the HTML table online and work to make it something we could e.g. plot. Before we can start cleaning, we have to read in the data:\n\n\nR\nPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Formatting using merge + pivot\n\n\n\n\n\nProblem\nSketch\nR solution\nPython solution\n\n\n\nCan you format the data in a long-skinny format for plotting using pivot operations using wide-to-long pivot operation(s) and a database merge?\nYou can start with the gas_prices_raw\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\nHow do your steps compare to the steps you used for the manual approach?\n\n\n\n\n\nWe’ll use the same data cleaning function as before:\n\n# Clean up the table a bit\ngas_prices_raw <- gas_prices_html %>%\n  set_names(fix_gas_names(names(.))) %>%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %>%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n## Error in set_names(., fix_gas_names(names(.))): could not find function \"set_names\"\n\nhead(gas_prices_raw)\n## Error in head(gas_prices_raw): object 'gas_prices_raw' not found\n\n\ngas_prices_dates <- select(gas_prices_raw, 1, matches(\"Week.[1-5].Date\"))\n## Error in select(gas_prices_raw, 1, matches(\"Week.[1-5].Date\")): object 'gas_prices_raw' not found\ngas_prices_values <- select(gas_prices_raw, 1, matches(\"Week.[1-5].Value\"))\n## Error in select(gas_prices_raw, 1, matches(\"Week.[1-5].Value\")): object 'gas_prices_raw' not found\n\nhead(gas_prices_dates)\n## Error in head(gas_prices_dates): object 'gas_prices_dates' not found\nhead(gas_prices_values)\n## Error in head(gas_prices_values): object 'gas_prices_values' not found\n\n\ngas_prices_dates_long <- pivot_longer(gas_prices_dates, -Year.Month, names_to = \"week\", values_to = \"month_day\")\n## Error in pivot_longer(gas_prices_dates, -Year.Month, names_to = \"week\", : object 'gas_prices_dates' not found\ngas_prices_values_long <- pivot_longer(gas_prices_values, -Year.Month, names_to = \"week\", values_to = \"price_per_gallon\")\n## Error in pivot_longer(gas_prices_values, -Year.Month, names_to = \"week\", : object 'gas_prices_values' not found\n\nhead(gas_prices_dates_long)\n## Error in head(gas_prices_dates_long): object 'gas_prices_dates_long' not found\nhead(gas_prices_values_long)\n## Error in head(gas_prices_values_long): object 'gas_prices_values_long' not found\n\n\nlibrary(lubridate) # ymd function\ngas_prices_dates_long_clean <- gas_prices_dates_long %>%\n  filter(month_day != \"\") %>%\n  mutate(week = str_extract(week, \"\\\\d\") %>% as.numeric()) %>%\n  mutate(year = str_extract(Year.Month, \"\\\\d{4}\"), \n         Date = paste(year, month_day, sep = \"/\") %>% \n           ymd())\n## Error in filter(., month_day != \"\"): object 'gas_prices_dates_long' not found\n\ngas_prices_values_long_clean <- gas_prices_values_long %>%\n  filter(price_per_gallon != \"\") %>%\n  mutate(week = str_extract(week, \"\\\\d\") %>% as.numeric()) %>%\n  mutate(price_per_gallon = as.numeric(price_per_gallon))\n## Error in filter(., price_per_gallon != \"\"): object 'gas_prices_values_long' not found\n\nhead(gas_prices_dates_long_clean)\n## Error in head(gas_prices_dates_long_clean): object 'gas_prices_dates_long_clean' not found\nhead(gas_prices_values_long_clean)\n## Error in head(gas_prices_values_long_clean): object 'gas_prices_values_long_clean' not found\n\n\ngas_prices <- left_join(gas_prices_dates_long_clean, gas_prices_values_long_clean, by = c(\"Year.Month\", \"week\")) %>%\n  select(Date, price_per_gallon)\n## Error in left_join(gas_prices_dates_long_clean, gas_prices_values_long_clean, : object 'gas_prices_dates_long_clean' not found\nhead(gas_prices)\n## Error in head(gas_prices): object 'gas_prices' not found\n\n\n\n\ngas_prices_raw = gas_prices_html.copy()\n\n# What do column names look like?\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_html' is not defined\ngas_prices_raw.columns # Multi-Index \n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'fix_gas_names' is not defined\ncolnames\n\n# Set new column names\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'colnames' is not defined\ngas_prices_raw.columns = colnames\n\n# Drop any rows with NaN in Year-Month\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'colnames' is not defined\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\ngas_prices_raw.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\n\n\ngas_prices_dates = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Date', axis = 1)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\ngas_prices_values = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Value', axis = 1)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_raw' is not defined\ngas_prices_dates.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_dates' is not defined\ngas_prices_values.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_values' is not defined\n\n\ngas_prices_dates_long = pd.melt(gas_prices_dates, id_vars = 'Year-Month', var_name = \"week\", value_name = \"month_day\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_dates' is not defined\ngas_prices_values_long = pd.melt(gas_prices_values, id_vars = 'Year-Month', var_name = \"week\", value_name = \"price_per_gallon\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_values' is not defined\ngas_prices_dates_long.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_dates_long' is not defined\ngas_prices_values_long.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_values_long' is not defined\n\n\ngas_prices_dates_long_clean = gas_prices_dates_long.dropna().copy()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_dates_long' is not defined\ngas_prices_dates_long_clean[\"week\"] = gas_prices_dates_long_clean.week.str.extract(r\"Week.(\\d).Date\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"year\"] = gas_prices_dates_long_clean[\"Year-Month\"].str.extract(r\"(\\d{4})-[A-z]{3}\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"Date\"] = gas_prices_dates_long_clean.year + \"/\" + gas_prices_dates_long_clean.month_day\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_dates_long_clean[\"Date\"] = pd.to_datetime(gas_prices_dates_long_clean.Date)\n\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_values_long_clean = gas_prices_values_long.dropna().copy()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_values_long' is not defined\ngas_prices_values_long_clean[\"week\"] = gas_prices_values_long_clean.week.str.extract(r\"Week.(\\d).Value\")\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_values_long_clean' is not defined\ngas_prices_values_long_clean[\"price_per_gallon\"] = pd.to_numeric(gas_prices_values_long_clean[\"price_per_gallon\"])\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_values_long_clean' is not defined\ngas_prices_dates_long_clean.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices_values_long_clean.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_values_long_clean' is not defined\n\n\ngas_prices = pd.merge(gas_prices_dates_long_clean, gas_prices_values_long_clean, on = (\"Year-Month\", \"week\")).loc[:,[\"Date\", \"price_per_gallon\"]]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices_dates_long_clean' is not defined\ngas_prices.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'gas_prices' is not defined"
  },
  {
    "objectID": "part-wrangling/06-data-join.html#references",
    "href": "part-wrangling/06-data-join.html#references",
    "title": "23  Joining Data",
    "section": "\n23.6 References",
    "text": "23.6 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nThe Carpentries, “Accessing SQLite Databases Using Python and Pandas,” Data Analysis and Visualization in Python for Ecologists. 2022 [Online]. Available: https://datacarpentry.org/python-ecology-lesson/09-working-with-sql/index.html. [Accessed: Jul. 26, 2022]"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#fa-bullseye-objectives",
    "href": "part-wrangling/07-datetime.html#fa-bullseye-objectives",
    "title": "24  Dates and Times",
    "section": "\n24.1  Objectives",
    "text": "24.1  Objectives\n\nUnderstand the complexities of working with datetime data\nCreate datetime formatted data from character and numeric encodings\nFormat/print datetime data in the desired format"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#why-dates-and-times-are-hard",
    "href": "part-wrangling/07-datetime.html#why-dates-and-times-are-hard",
    "title": "24  Dates and Times",
    "section": "\n24.2 Why Dates and Times are hard",
    "text": "24.2 Why Dates and Times are hard\nI’m going to let Tom Scott deliver this portion of the material for me, as his times and timezones video is excellent and entertaining.\n\n\n\n\nThere is also an excellent StackOverflow question [1] and answers [3] demonstrating exactly how times and time zones can get very confusing even in relatively simple circumstances.\nLong story short, we will be using libraries in R and python which handle some of these complexities for us, because dates, times, and timezones are hard and we really don’t want to know exactly how hard they are. The libraries I’ve chosen for this are datetime in Python (used by Pandas), and lubridate in R.\n\n\n\n\n\n\nTry It Out - Getting Set up\n\n\n\n\n\nR\nPython\n\n\n\n\n## install.packages(\"lubridate\")\nlibrary(lubridate)\n\n# get current date/time\ntoday()\n## [1] \"2023-04-11\"\nnow()\n## [1] \"2023-04-11 09:12:14 CDT\"\n\nLubridate cheat sheet Lubridate documentation\n\n\n\npip install datetime\n\n\nimport datetime\n\ntoday = datetime.date.today()\ntoday\n## datetime.date(2023, 4, 11)\nprint(today)\n## 2023-04-11\nnow = datetime.datetime.now()\nnow\n## datetime.datetime(2023, 4, 11, 9, 12, 46, 269723)\nprint(now)\n## 2023-04-11 09:12:46.269723\n\npandas datetime documentation"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#getting-started",
    "href": "part-wrangling/07-datetime.html#getting-started",
    "title": "24  Dates and Times",
    "section": "\n24.3 Getting Started",
    "text": "24.3 Getting Started\nLet’s differentiate between three types of data which refer to a point in time:\n\na date\n\na time within a day\na date-time - a specific time on a specific date\n\nNow, let’s think about all of the different ways we can specify dates. The table below has examples along with strptime formats that are used in both R and python for telling the computer which date format is used.\n\n\nTable 24.1: Different ways to specify dates and times.\n\n\n\n\n\n\n\n\n\nExample\nType\nNotes\n\nstrptime format\n\n\n\n1\nJanuary 12, 2023\ndate\nCommon in US/N. America\n%B %d, %Y\n\n\n2\n12 January 2023\ndate\nCommon in Europe\n%d %B %Y\n\n\n3\n01/12/2023\ndate\nCommon in US\n%m/%d/%Y\n\n\n4\n1/12/23\ndate\nCommon in US\n%m/%d/%y\n\n\n5\n12/01/2023\ndate\nCommon in Europe/Asia\n%d/%m/%Y\n\n\n6\n2023-01-12\ndate\nISO 8601 standard\n(automatically sorts chronologically)\n%Y-%m-%d\nor %F\n\n\n7\n12 2023\ndate\nday of year + year\n%j %Y\n\n\n8\n9:23 PM\ntime\n12h time\n%I:%M %p\n\n\n9\n21:23\ntime\n24h time (military time)\n%H:%M\nor %R\n\n\n10\n21:23:05\ntime\n24h time (with seconds)\n%H:%M:%S\nor %T\n\n\n11\n2023-01-12T21:23:05\ndatetime\nISO 8601 international standard\n%FT%T\n\n\n\n\nNote that rows 4 and 5 of Table 24.1 are ambiguous if you don’t know what location your data comes from - the dates could refer to December 1, 2023 or January 12, 2023. This only gets worse if you use 2-digit years.\nThere are three main ways that you might want to create a date/time [4]:\n\nFrom a string\nFrom individual date/time components\nFrom an existing date/time object"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#creating-dates-and-times",
    "href": "part-wrangling/07-datetime.html#creating-dates-and-times",
    "title": "24  Dates and Times",
    "section": "\n24.4 Creating Dates and Times",
    "text": "24.4 Creating Dates and Times\n\n24.4.1 Creation from Strings\nDates and times are often stored in tabular formats as strings. In some cases, these are read in and automatically formatted as date-times, but in other situations, you have to specify the format yourself.\n\n\n\n\n\n\nDemo: Datetimes from Strings\n\n\n\nLet’s use some data from the US Geological Service with records of earthquakes with magnitude greater than 6 on the Richter scale that occurred between January 1, 2000 and January 1, 2023. You can pull this data yourself using https://earthquake.usgs.gov/earthquakes/map/, but you can also access a CSV of the data here.\n\n\nR + lubridate\nBase R\nPandas\n\n\n\n\nlibrary(lubridate)\nquake <- read.csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake)\n## 'data.frame':    3484 obs. of  13 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\n\nBy default, read.csv reads the time information in as a character variable.\n\nlibrary(readr)\nquake2 <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake2)\n## spc_tbl_ [3,484 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ #EventID         : chr [1:3484] \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : POSIXct[1:3484], format: \"2022-12-28 16:34:20\" \"2022-12-20 10:34:24\" ...\n##  $ Latitude         : num [1:3484] -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num [1:3484] 171 -124 179 -101 -173 ...\n##  $ Depth/km         : num [1:3484] 10 17.9 73 18 38 ...\n##  $ Author           : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr [1:3484] \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr [1:3484] \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num [1:3484] 6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr [1:3484] \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   `#EventID` = col_character(),\n##   ..   Time = col_datetime(format = \"\"),\n##   ..   Latitude = col_double(),\n##   ..   Longitude = col_double(),\n##   ..   `Depth/km` = col_double(),\n##   ..   Author = col_character(),\n##   ..   Catalog = col_character(),\n##   ..   Contributor = col_character(),\n##   ..   ContributorID = col_character(),\n##   ..   MagType = col_character(),\n##   ..   Magnitude = col_double(),\n##   ..   MagAuthor = col_character(),\n##   ..   EventLocationName = col_character()\n##   .. )\n##  - attr(*, \"problems\")=<externalptr>\n\nHowever, if we use readr::read_csv, the data is correctly read in as a POSIXct format, which is how R indicates that something is a datetime object.\nIf we want to directly convert the Time column in quake to a datetime, we can use the lubridate package, which has helper functions ymd_hms, ymd, and more. Our data is formatted in ISO 8601 standard format, which means we can easily read it in with ymd_hms() .\n\nlibrary(lubridate)\nlibrary(dplyr)\nquake <- quake %>% \n  mutate(dateTime = ymd_hms(Time))\nstr(quake)\n## 'data.frame':    3484 obs. of  14 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\n##  $ dateTime         : POSIXct, format: \"2022-12-28 16:34:20\" \"2022-12-20 10:34:24\" ...\n\nWe can then test whether quake$dateTime is the same as quake2$Time :\n\nall.equal(quake2$Time, quake$dateTime)\n## [1] TRUE\n\nSo in the case that your data is not automatically read in as a date-time, you can use the helper functions from lubridate (ymd_hms, ymd, mdy, …) to convert strings to date-time data.\n\n\nAs lovely as the lubridate package is, there are some situations where using the tidyverse may not be desirable or even allowed. It is helpful to know how to solve this problem in base R, even if 99% of the time we can use the much easier-to-remember lubridate package.\nIn this case, we would use the as.POSIXct function, and we probably want to have the reference page up (run ?strptime in the R console to pull up the help page).\nWe’ll need to get the codes that tell R what format our datetimes use - you can use Table 24.1, if you like, or read the as.POSIXct help page to see all possible format codes.\n\nquake <- read.csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake)\n## 'data.frame':    3484 obs. of  13 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\nquake$dateTime2 <- as.POSIXct(quake$Time, \"%Y-%m-%dT%H:%M:%S\")\nall.equal(quake$dateTime, quake$dateTime2)\n## [1] TRUE\n\nSo using as.POSIXct we do not get the convenient handling of time zones that we got using ymd_hms, but we can set the time zone explicitly if we want to do so.\n\nquake$dateTime2 <- as.POSIXct(quake$Time, tz = \"UTC\", \"%Y-%m-%dT%H:%M:%S\")\nall.equal(quake$dateTime, quake$dateTime2)\n## [1] TRUE\n\n\n\nIn pandas, we can use the to_datetime method. If the format is not specified, pandas will try to guess the date-time format; in this case, the guess works, but if not, you can provide a format = … argument that works the same way as R.\n\nimport pandas as pd\n## Error: ModuleNotFoundError: No module named 'pandas'\nquake = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\n## Error: NameError: name 'pd' is not defined\nquake.dtypes\n## Error: NameError: name 'quake' is not defined\nquake.Time[0:10]\n\n# Convert to datetime\n## Error: NameError: name 'quake' is not defined\nquake['dateTime'] = pd.to_datetime(quake.Time)\n## Error: NameError: name 'pd' is not defined\nquake.dtypes\n## Error: NameError: name 'quake' is not defined\nquake.dateTime[0:10]\n\n# Convert to datetime\n## Error: NameError: name 'quake' is not defined\nquake['dateTime2'] = pd.to_datetime(quake.Time, format = \"%Y-%m-%dT%H:%M:%S\")\n## Error: NameError: name 'pd' is not defined\nquake.dtypes\n## Error: NameError: name 'quake' is not defined\nquake.dateTime2[0:10]\n## Error: NameError: name 'quake' is not defined\n\n\n\n\n\n\n\n\n\n\n\n\nTry it Out - Datetimes from Strings\n\n\n\nIt’s usually important for new parents to keep a log of the new baby’s feeds, to ensure that the baby is getting enough liquids and isn’t getting dehydrated. I used an app to keep track of my daughter’s feeds from birth (though here, we’ll only work with the first 3 months of data), and it used a reasonable, if not standard way to store dates and times.\n\n\nProblem\nR solution\nPython solution\n\n\n\nTake a look at the first month of feeds. Note that these data are from August 7, 2021 to November 4, 2021 – roughly baby’s first 90 days.\n\nConvert Start and End to datetime variables\nCan you plot the feeds somehow?\nCan you do arithmetic with datetimes to see if there are any user entry errors?\nThis data was created by a highly unreliable and error prone couple of individuals – specifically, sleep-deprived new parents.\n\nTo do this, you may need to figure out how to specify a non-standard date format in R and/or python. The parse_date_time function is useful in R, and pd.to_datetime() takes a format argument in python.\n\n\nFirst, let’s read the data in and explore a bit.\n\nlibrary(lubridate)\nlibrary(readr)\nfeeds <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/feeds_initial.csv\")\nhead(feeds)\n## # A tibble: 6 × 6\n##      id Start               End       Type  `Quantity (oz)` `Quantity (ml or g)`\n##   <dbl> <chr>               <chr>     <chr>           <dbl>                <dbl>\n## 1  1368 20:03:30 11-04-2021 20:45:21… Brea…              NA                   NA\n## 2  1366 18:00:29 11-04-2021 18:18:29… Brea…              NA                   NA\n## 3  1365 16:27:29 11-04-2021 17:03:26… Brea…              NA                   NA\n## 4  1364 14:30:01 11-04-2021 14:42:05… Brea…              NA                   NA\n## 5  1367 12:48:29 11-04-2021 13:50:29… Bott…               3                   88\n## 6  1363 10:59:18 11-04-2021 11:15:18… Bott…               3                   88\n\n# Looks like %H:%M:%S %m-%d-%Y format.\n\nIt looks like the data is stored in a format where the time (%H:%M:%S) is first and the date (%m-%d-%Y) is second. We can use the parse_date_time function in lubridate\n\nfeeds <- feeds %>%\n  mutate(Start = parse_date_time(Start, orders = c(\"%H:%M:%S %m-%d-%Y\")),\n         End = parse_date_time(End, orders = c(\"%H:%M:%S %m-%d-%Y\")))\n\nLet’s then explore how we might plot this data:\n\nlibrary(ggplot2)\nggplot(feeds, aes(xmin = Start, xmax = End, fill = Type)) + \n  geom_rect(aes(ymin = 1, ymax = 2)) + # Specify default aes\n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  theme_bw() + theme(legend.position = \"bottom\") + \n  scale_y_continuous(breaks = NULL)\n\n\n\n\n\nlibrary(ggplot2)\nfeeds %>%\n  mutate(day = floor_date(Start, \"day\"),\n         hour_start = Start - day,\n         hour_end = End - day) %>%\n  mutate(across(starts_with(\"hour\"), ~as.numeric(., units = \"hours\"))) %>%\n  mutate(doy = yday(day)) %>%\nggplot(aes(ymin = day, ymax = day+days(1), xmin = hour_start, xmax = hour_end, fill = Type)) + \n  geom_rect() + # Specify default aes\n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  scale_x_continuous(\"Hour of the day\") + \n  theme_bw() + theme(legend.position = \"bottom\")\n\n\n\n\nWe can also calculate the duration of each feed and look at the distributions for each type of feed.\n\nfeeds <- feeds %>%\n  mutate(duration = End - Start)\n\nggplot(feeds, aes(x = duration, fill = Type)) + geom_histogram(color = \"black\") + \n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  theme_bw() + theme(legend.position = \"none\") + \n  xlab(\"Feed duration, in seconds\") + facet_wrap(~Type)\n\n\n\n\nWe can see a few suspiciously long feeds - 9000 seconds is 2.5 hours, which is not unheard of for a baby to breastfeed, but would be an exceptionally long bottle feed (unless a parent fell asleep before hitting “stop” on the feed, which is much more likely).\n\n\nFirst, let’s read the data in and explore a bit.\n\nimport pandas as pd\n## Error: ModuleNotFoundError: No module named 'pandas'\nfeeds = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/feeds_initial.csv\")\n## Error: NameError: name 'pd' is not defined\nfeeds.head()\n\n# Looks like %H:%M:%S %m-%d-%Y format.\n## Error: NameError: name 'feeds' is not defined\n\nIt looks like the data is stored in a format where the time (%H:%M:%S) is first and the date (%m-%d-%Y) is second. We can use the format argument to pd.to_datetime to specify this:\n\nfeeds[\"Start\"] = pd.to_datetime(feeds.Start, format = \"%H:%M:%S %m-%d-%Y\")\n## Error: NameError: name 'pd' is not defined\nfeeds[\"End\"] = pd.to_datetime(feeds.End, format = \"%H:%M:%S %m-%d-%Y\")\n## Error: NameError: name 'pd' is not defined\nfeeds.head()\n## Error: NameError: name 'feeds' is not defined\n\nIn Python, it is helpful to do a bit of transformation first - this is partly because I’m not as good with Python plotting systems.\n\nimport datetime as dt\nfeeds[\"day\"] = feeds.Start.dt.strftime(\"%Y-%m-%d\")\n## Error: NameError: name 'feeds' is not defined\nfeeds[\"day\"] = pd.to_datetime(feeds.day, format = \"%Y-%m-%d\")\n## Error: NameError: name 'pd' is not defined\nfeeds[\"day_end\"] = feeds.day + dt.timedelta(days = 1)\n## Error: NameError: name 'feeds' is not defined\nfeeds[\"time_start\"] = feeds.Start - feeds.day\n## Error: NameError: name 'feeds' is not defined\nfeeds[\"time_end\"] = feeds.End - feeds.day\n## Error: NameError: name 'feeds' is not defined\nfeeds[\"duration\"] = feeds.time_end - feeds.time_start\n## Error: NameError: name 'feeds' is not defined\n\nNote that as of January 2023, RStudio does not correctly display timedelta data types in python. They show up as NAs in the table, but are printed fine in the console. Don’t spend hours trying to figure out why it isn’t working – it’s bad enough that I did.\n\nfrom plotnine import *\n## Error: ModuleNotFoundError: No module named 'plotnine'\n(\n  ggplot(feeds, aes(xmin = \"Start\", xmax = \"End\", fill = \"Type\")) + \n  geom_rect(aes(ymin = 1, ymax = 2)) + \n  scale_fill_manual(values = [\"cornflowerblue\", \"pink\"]) + \n  theme_bw() + scale_y_continuous(breaks = [])\n)\n## Error: NameError: name 'ggplot' is not defined\n\n\nfrom plotnine import *\n## Error: ModuleNotFoundError: No module named 'plotnine'\n(\n  ggplot(feeds, aes(xmin = \"time_start\", xmax = \"time_end\", ymin = \"day\", ymax = \"day_end\", fill = \"Type\")) + \n  geom_rect() + \n  scale_fill_manual(values = [\"cornflowerblue\", \"pink\"]) + \n  theme_bw()\n)\n## Error: NameError: name 'ggplot' is not defined\n\n\n\n\n\n\n\n24.4.2 Creation from Components\nSometimes, instead of a single string, you’ll have the individual components of the date-time spread across columns. The nycflights13 data is a good example of this.\n\n\n\n\n\n\nDemo: Datetimes from Components\n\n\n\n\n\nR + lubridate\nBase R\nPython\n\n\n\nIn lubridate, the make_date() and make_datetime() functions can be used to create date-times from component pieces.\n\nlibrary(nycflights13)\n\nflights %>%\n  select(year, month, day, hour, minute) %>% \n  head()\n## # A tibble: 6 × 5\n##    year month   day  hour minute\n##   <int> <int> <int> <dbl>  <dbl>\n## 1  2013     1     1     5     15\n## 2  2013     1     1     5     29\n## 3  2013     1     1     5     40\n## 4  2013     1     1     5     45\n## 5  2013     1     1     6      0\n## 6  2013     1     1     5     58\n\nflights <- flights %>%\n  mutate(date = make_date(year, month, day),\n         datetime = make_datetime(year, month, day, hour, minute))\n\nflights %>% select(date, datetime, year, month, day, hour, minute)\n## # A tibble: 336,776 × 7\n##    date       datetime             year month   day  hour minute\n##    <date>     <dttm>              <int> <int> <int> <dbl>  <dbl>\n##  1 2013-01-01 2013-01-01 05:15:00  2013     1     1     5     15\n##  2 2013-01-01 2013-01-01 05:29:00  2013     1     1     5     29\n##  3 2013-01-01 2013-01-01 05:40:00  2013     1     1     5     40\n##  4 2013-01-01 2013-01-01 05:45:00  2013     1     1     5     45\n##  5 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  6 2013-01-01 2013-01-01 05:58:00  2013     1     1     5     58\n##  7 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  8 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  9 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n## 10 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n## # ℹ 336,766 more rows\n\n\n\nIn base R, we can use the ISOdate function to create date times.\n\nflights$datetime_base = with(flights, ISOdatetime(year, month, day, hour, minute, sec= 0, tz=\"UTC\"))\nall.equal(flights$datetime, flights$datetime_base)\n## [1] TRUE\n\n\n\nIn pandas, we can pass multiple columns to pd.to_datetime() and as long as they are named reasonably, pandas will handle the conversion. If we want to have the date but not the time for some reason, we just pass fewer columns to pandas.\n\nfrom nycflights13 import flights\n## Error: ModuleNotFoundError: No module named 'nycflights13'\nflights[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n## Error: NameError: name 'flights' is not defined\nflights[\"date\"] = pd.to_datetime(flights[[\"year\", \"month\", \"day\"]])\n## Error: NameError: name 'pd' is not defined\nflights[\"datetime\"] = pd.to_datetime(flights[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]])\n\n## Error: NameError: name 'pd' is not defined\nflights[[\"date\", \"datetime\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n## Error: NameError: name 'flights' is not defined\n\n\n\n\n\n\n\n\n24.4.3 Creation from Other Objects\nSometimes, you may have information in one type of variable (e.g. a datetime) and want to split it into a date and a time, separately.\nSome systems store datetimes as the number of seconds from a specific point (commonly, the Unix Epoch, midnight on 1970-01-01). You may have to convert from seconds since this epoch (or some other epoch [5]) to an actual date-time that is human readable.\nIf you ever have to convert dates and times that were stored in Microsoft Excel, it can be helpful to know that Microsoft stores dates as the number of days since January 1, 1900 [6] (or if the spreadsheet was created on a Mac, January 1, 1904) [7]. Yes, this is as confusing as it sounds. Don’t use MS Excel for handling dates [8], [9] (or really, at all, now that you know better tools). Geneticists have actually renamed genes because Microsoft won’t fix Excel to handle dates properly [10].\n\n\n\n\n\n\nDemo: Creation from Other Objects\n\n\n\n\n\nR + lubridate\nBase R\nPython\n\n\n\nIn lubridate, the as_date() and as_datetime() functions can be used to create date-times from other objects.\n\ntmp <- flights %>%\n  mutate(date2 = as_date(datetime))\n\n# Check that date and date2 are the same\nall.equal(flights$date, flights$date2)\n## [1] \"Modes: numeric, NULL\"                                \n## [2] \"Lengths: 336776, 0\"                                  \n## [3] \"Attributes: < Modes: list, NULL >\"                   \n## [4] \"Attributes: < Lengths: 1, 0 >\"                       \n## [5] \"Attributes: < names for target but not for current >\"\n## [6] \"Attributes: < current is not list-like >\"            \n## [7] \"target is Date, current is NULL\"\n\nHere’s a demonstration of epoch timekeeping.\n\ncurrent_time <- now(tzone = \"UTC\")\n# This converts to the number of seconds since the Unix epoch\nseconds_since_epoch <- current_time %>% seconds()\n# Now let's convert back to a datetime\n(current_time2 <- as_datetime(seconds_since_epoch))\n## [1] \"2023-04-11 14:12:53 UTC\"\n# Check to see that they're equal\nall.equal(current_time, current_time2)\n## [1] TRUE\n\n\n\nIn base R, we can use the as.Date function to create dates from datetimes.\n\nflights$date2 = as.Date(flights$date)\nall.equal(flights$date, flights$date2)\n## [1] TRUE\n\nWe can handle epochs as well:\n\n# Let's see what was 10000 days after the UNIX epoch\nas.Date(1e4, origin = \"1970-01-01\")\n## [1] \"1997-05-19\"\n\n# If we use as.POSIXct, we are counting in seconds from midnight\nas.POSIXct(1e4, origin = as.POSIXct(\"1970-01-01 00:00:00\"))\n## [1] \"1970-01-01 02:46:40 CST\"\n\nBy default, as.POSIXct will use the system’s time zone, which may not be desirable; you can always set the time zone yourself if you would like to do so.\n\n\nIn pandas, we can pass multiple columns to pd.to_datetime() and as long as they are named reasonably, pandas will handle the conversion. If we want to have the date but not the time for some reason, we just pass fewer columns to pandas.\n\nfrom nycflights13 import flights\n## Error: ModuleNotFoundError: No module named 'nycflights13'\nflights[\"date2\"] = flights.date.dt.date # Convert datetime to date\n\n# They look the same\n## Error: NameError: name 'flights' is not defined\nflights[[\"date\", \"date2\"]]\n## Error: NameError: name 'flights' is not defined\nflights.dtypes\n# date2 is an object, date is a datetime64.\n## Error: NameError: name 'flights' is not defined\n\nWe created flights.date using pd.to_datetime(). Given this comparison, it may be better to use to.datetime() and append .dt.date on the end if you do not want to keep the time information that is provided by default."
  },
  {
    "objectID": "part-wrangling/07-datetime.html#working-with-dates-and-times",
    "href": "part-wrangling/07-datetime.html#working-with-dates-and-times",
    "title": "24  Dates and Times",
    "section": "\n24.5 Working with Dates and Times",
    "text": "24.5 Working with Dates and Times\nIn this section, we’ll work with comments by famous Reddit artist Shitty_Watercolour, who responds to people’s comments with a quickly created watercolor-style painting.\n\n\nHere’s one of Shitty Watercolour’s works: \n\n\n\n\n\n\nGetting the Data\n\n\n\n\n\nR\nPython\n\n\n\nNote: The textbook caches data, so your results may differ from those shown here because RedditExtractoR only acquires the last ~1000 comments from a user.\n\n# remotes::install_github(\"ivan-rivera/RedditExtractor\")\nlibrary(RedditExtractoR)\n\ncomment_list <- get_user_content(\"Shitty_Watercolour\")\nwatercolour <- comment_list$Shitty_Watercolour$comments\n\n\n\n\n# Get data from R directly, since redditExtractor package is in R\nwatercolour = r.watercolour\n\n\n\n\n\n\n\n24.5.1 Time Zones\nWe often store data in UTC1, but we may want to represent the data in a more familiar time zone for interpretation’s purposes.\n\nWorking with Time Zones. Here, time is a placeholder for whatever variable is being converted.\n\n\n\n\n\n\nTask\nLanguage\nFunction\n\n\n\nSet the time zone\nR\nas_datetime(time, tz = \"GMT\")\n\n\n\nPython\ntime.apply(lambda x: pd.Timestamp(x).tz_localize(\"GMT\"))\n\n\nDisplay the time in a diff TZ\nR\nwith_tz(time, tz = \"America/Chicago\")\n\n\n\nPython\ntime.apply(lambda x: pd.Timestamp(x).tz_convert(\"America/Chicago\"))\n\n\n\n\n\n\n\n\n\nTry it Out - Formatting Dates\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe watercolour dataset above contains 959 comments from Shitty_Watercolour, with the UTC date and timestamp of the comment.\nFigure out how to format these data into a proper date type and timestamp that is user-readable. Make sure you inform R or python that the timestamp is provided in UTC.\n Compare a couple of your timestamps to the timestamps provided by Reddit when you mouse over a comment.\nCan you get R or Python to output the date in your timezone?\n\n\n\nwatercolour <- watercolour %>%\n  mutate(date = ymd(date_utc, tz = \"UTC\"),\n         time = as_datetime(timestamp),\n         time_cst = with_tz(time, tzone_out = \"CST\"))\n\nwatercolour[,c(\"date\", \"time\", \"time_cst\")]\n## # A tibble: 959 × 3\n##    date                time                time_cst           \n##    <dttm>              <dttm>              <dttm>             \n##  1 2017-12-19 00:00:00 2017-12-19 22:08:19 2017-12-19 16:08:19\n##  2 2017-12-19 00:00:00 2017-12-19 22:07:03 2017-12-19 16:07:03\n##  3 2017-12-19 00:00:00 2017-12-19 20:47:55 2017-12-19 14:47:55\n##  4 2017-12-19 00:00:00 2017-12-19 20:01:19 2017-12-19 14:01:19\n##  5 2017-12-17 00:00:00 2017-12-17 19:58:51 2017-12-17 13:58:51\n##  6 2017-12-15 00:00:00 2017-12-15 02:53:23 2017-12-14 20:53:23\n##  7 2017-12-12 00:00:00 2017-12-12 16:56:23 2017-12-12 10:56:23\n##  8 2017-12-08 00:00:00 2017-12-08 17:48:05 2017-12-08 11:48:05\n##  9 2017-12-08 00:00:00 2017-12-08 03:50:17 2017-12-07 21:50:17\n## 10 2017-12-07 00:00:00 2017-12-07 18:00:58 2017-12-07 12:00:58\n## # ℹ 949 more rows\n\n\n\n\nfrom datetime import datetime\n\nwatercolour[\"date\"] = pd.to_datetime(watercolour.date_utc).dt.date\n## Error: NameError: name 'pd' is not defined\nwatercolour[\"time\"] = pd.to_datetime(watercolour.timestamp, unit = 's')\n\n# Tell Python the time is in UTC \n## Error: NameError: name 'pd' is not defined\nwatercolour[\"time\"] = watercolour.time.apply(lambda x: pd.Timestamp(x).tz_localize(\"UTC\"))\n## Error: AttributeError: 'dict' object has no attribute 'time'\nwatercolour[\"time_cst\"] = watercolour.time.apply(lambda x: pd.Timestamp(x).tz_convert(\"US/Central\"))\n## Error: AttributeError: 'dict' object has no attribute 'time'\nwatercolour[[\"date\", \"time\", \"time_cst\"]]\n## Error: TypeError: unhashable type: 'list'\n\n\n\n\n\n\n\n24.5.2 Time Spans\nDates and times can be added and subtracted - after all, underneath, they’re usually implemented as a number of XXX from the reference time point, where XXX is usually seconds for datetimes and days for dates.\nIn R, the difference between two timestamps is called a duration and is implemented in the duration class See [11, Ch. 16.4.1] for more info. In Python, a similar class exists and is called a timedelta [12].\n\n\n\n\n\n\nTry it Out - Datetime Math\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nUse the watercolour data and plot the interval between successive Shitty_Watercolour posts in minutes. What can you conclude?\n\n\n\nwatercolour <- watercolour %>%\n  arrange(time) %>%\n  mutate(diff = as.duration(time - lag(time, 1)),\n         diffmin = as.numeric(diff, \"minutes\"))\n\nlibrary(ggplot2)\nggplot(watercolour, aes(x = diffmin)) + \n  geom_histogram() + \n  xlab(\"Time between posts (minutes)\") + \n  ylab(\"# Posts\") + \n  scale_x_log10(breaks = c(1, 15, 30, 60, 1440, 10080))\n\n\n\n\nMost of the time, Shitty_Watercolour takes at least 15 minutes to generate a new comment. There is also a noticable peak just before 1440 minutes, indicating that as with most users, Shitty_Watercolour is active at approximately the same time each day for a few hours. The final break shown, 10080, is the number of minutes in a week, indicating that occasionally, Shitty_Watercolour goes more than a week between posts.\n\n\n\nfrom datetime import datetime\n\nwatercolour = watercolour.sort_values(by = 'time')\n## Error: AttributeError: 'dict' object has no attribute 'sort_values'\nwatercolour[\"diff\"] = watercolour.time.diff().astype('timedelta64')\n# This formats in minutes\n## Error: AttributeError: 'dict' object has no attribute 'time'\nwatercolour[\"diffmin\"] = watercolour.time.diff().astype('timedelta64[m]')\n# Remove negative minutes - something funky there?\n## Error: AttributeError: 'dict' object has no attribute 'time'\nwatercolour = watercolour.query(\"diffmin > 0\")\n## Error: AttributeError: 'dict' object has no attribute 'query'\nimport seaborn.objects as so\n## Error: ModuleNotFoundError: No module named 'seaborn'\np = (\n  so.\n  Plot(watercolour, watercolour[\"diffmin\"]).\n  add(so.Bars(width=.95), so.Hist(bins = 30)).\n  scale(x = so.Continuous(trans = \"log\").\n    tick(at = [1, 15, 30, 60, 1440, 10080]).\n    label(like=\"{x:d}\")).\n  label(x = \"Time between posts (minutes)\", y = \"# Posts\")\n)\n## Error: NameError: name 'so' is not defined\np.show()\n## Error: NameError: name 'p' is not defined"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#references",
    "href": "part-wrangling/07-datetime.html#references",
    "title": "24  Dates and Times",
    "section": "\n24.6 References",
    "text": "24.6 References\n\n\n\n\n[1] \nFreewind, “Why is subtracting these two times (in 1927) giving a strange result? Stack overflow,” May 22, 2021. [Online]. Available: https://stackoverflow.com/q/6841333/2859168. [Accessed: Jan. 21, 2023]\n\n\n[2] \nJ. Skeet, “Answer to \"why is subtracting these two times (in 1927) giving a strange result?\". Stack overflow,” Jul. 27, 2011. [Online]. Available: https://stackoverflow.com/a/6841479/2859168. [Accessed: Jan. 21, 2023]\n\n\n[3] \nM. Borgwardt, “Answer to \"why is subtracting these two times (in 1927) giving a strange result?\". Stack overflow,” Jul. 27, 2011. [Online]. Available: https://stackoverflow.com/a/6841572/2859168. [Accessed: Jan. 21, 2023]\n\n\n[4] \nH. W. {and}. G. Grolemund, “Dates and times,” in R for data science, 1st ed., O’Reilly Media, p. 518 [Online]. Available: https://r4ds.had.co.nz/dates-and-times.html. [Accessed: Jan. 23, 2023]\n\n\n[5] \nWikipedia contributors, “Epoch (computing),” Wikipedia. Mar. 23, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Epoch_(computing)&oldid=1146251518. [Accessed: Apr. 04, 2023]\n\n\n[6] \nMicrosoft Support, “DATEVALUE function - Microsoft Support,” 2023. [Online]. Available: https://support.microsoft.com/en-us/office/datevalue-function-df8b07d4-7761-4a93-bc33-b7471bbff252. [Accessed: Apr. 04, 2023]\n\n\n[7] \nElizabeth Mott, “Why Do Dates Come in Different in Excel From a Mac to a PC?” Jun. 12, 2013. [Online]. Available: https://smallbusiness.chron.com/dates-come-different-excel-mac-pc-68917.html. [Accessed: Apr. 04, 2023]\n\n\n[8] \nH. Caudill, “Excel Hell: A cautionary tale,” Dec. 19, 2018. [Online]. Available: https://medium.com/all-the-things/a-single-infinitely-customizable-app-for-everything-else-9abed7c5b5e7. [Accessed: Apr. 04, 2023]\n\n\n[9] \nChris88888888, “Excel Still Sucks at Recognizing Dates.” Apr. 15, 2020. [Online]. Available: https://answers.microsoft.com/en-us/msoffice/forum/all/excel-still-sucks-at-recognizing-dates/5305f6db-8211-49d5-932d-c4871df27fc7. [Accessed: Apr. 04, 2023]\n\n\n[10] \nJ. Vincent, “Scientists rename human genes to stop Microsoft Excel from misreading them as dates,” Aug. 06, 2020. [Online]. Available: https://www.theverge.com/2020/8/6/21355674/human-genes-rename-microsoft-excel-misreading-dates. [Accessed: Apr. 04, 2023]\n\n\n[11] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[12] \nPython Foundation, “Datetime Basic date and time types,” Apr. 05, 2023. [Online]. Available: https://docs.python.org/3/library/datetime.html. [Accessed: Apr. 05, 2023]"
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#fa-bullseye-objectives",
    "href": "part-wrangling/08-functional-prog.html#fa-bullseye-objectives",
    "title": "25  Functional Programming",
    "section": "\n25.1  Objectives",
    "text": "25.1  Objectives\n\nUse functional programming to replace for loops\nArticulate why functional programming can be preferable to using for loops\nUse functional programming to clean data, model data subsets, and assemble hierarchical data."
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#programming-philosophies",
    "href": "part-wrangling/08-functional-prog.html#programming-philosophies",
    "title": "25  Functional Programming",
    "section": "\n25.2 Programming Philosophies",
    "text": "25.2 Programming Philosophies\n\n\n\n\n\n\nAdvanced\n\n\n\nThis section is intended for everyone, but I do not expect that people who are just learning to program for the first time will fully absorb everything in this section. Get what you can out of this, use it to improve how you write code, and come back to it later if it’s too confusing.\n\n\nJust as spoken languages fall into families, like Indo-European or Sino-Tibetan, programming languages also have broad classifications. Here are a few “families” or classifications of programming languages [1]:\n\nMany languages are procedural: a program provides a list of instructions that tell the computer what to do with provided input. C, Pascal, Fortran, and UNIX shells are naturally procedural. JavaScript is also a fairly natural procedural language. Many R analysis scripts are also naturally written in a procedural style; SAS code is almost always procedural.\n\nDeclarative languages use code to describe the problem that needs to be solved, and the language figures out how to solve it. SQL is the most common declarative language you’ll encounter for data-related tasks.\n\nObject oriented languages (sometimes abbreviated OOP, for object-oriented programming) manipulate collections of objects or classes. Data is stored in classes that have associated functions, which are often called methods. Java is explicitly object-oriented; C++ and Python support object-oriented programming but don’t force you to use those features.\n\nFunctional programming languages describe a problem using a set of functions, which only take inputs and produce outputs. Functions don’t have any internal tracking of state - purely functional languages move from input to output without storing variables or even printing output to the command line, but it is common to adopt a functional approach to programming without requiring strict adherence to all principles of a fully functional approach. Haskell and Rust are fairly standard functional programming languages.\n\n\n\nHadley’s talk on The Joy of Functional Programming for Data Science\n\n\nFunctional programming languages have a goal of writing pure functions - functions that do not change the global state (stuff stored in objects, memory, parameters, or files) of the program and thus have no side effects. The return value of a pure function is based solely on the inputs to the function. Not all functions can be pure functions - for instance, there’s no pure way to do file IO operations. But it is a nice goal to be able to move parameters into functions and have the correct object returned from that function, so that you can pipe multiple operations together into a pipeline.\nMost general-purpose languages like C++ and Python and even some domain languages like R support multiple different programming paradigms. While preparing to write this chapter, I saw functional programming books with examples in Java [2], JavaScript [3], and C# [4] - all languages that I would associate with OOP or procedural styles. I also found books teaching object oriented programming using Fortran 90-95 [5], which is something I wouldn’t have considered possible.\nAll of this is to say that while certain languages are built around principles like OOP or functional programming, almost every language has users who rely more heavily on one approach than the other. There are very few “pure” programming languages, which reminds me of one of my favorite quotes about English:\n\n“The problem with defending the purity of the English language is that English is about as pure as a cribhouse whore. We don’t just borrow words; on occasion, English has pursued other languages down alleyways to beat them unconscious and rifle their pockets for new vocabulary.” ― James D. Nicoll\n\n\n\n\n\n\n\nObject Oriented Philosophy in R and Python\n\n\n\n\n\nPython\nR\n\n\n\nWhen you call df.size() in Python, you are calling the size method that is part of the df object, which is a DataFrame. This suggests that Pandas, at least, is programmed using an object-oriented paradigm.\n\n\n\n\n\nAn easy example of R’s object oriented nature is that when you fit different models or perform different tests, the default output is different.\n\ndata(mtcars)\n\nr1 <- t.test(mtcars$mpg~mtcars$vs)\nprint(r1)\n## \n##  Welch Two Sample t-test\n## \n## data:  mtcars$mpg by mtcars$vs\n## t = -4.6671, df = 22.716, p-value = 0.0001098\n## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n## 95 percent confidence interval:\n##  -11.462508  -4.418445\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        16.61667        24.55714\n\nr2 <- lm(mtcars$mpg ~ mtcars$vs)\nprint(r2)\n## \n## Call:\n## lm(formula = mtcars$mpg ~ mtcars$vs)\n## \n## Coefficients:\n## (Intercept)    mtcars$vs  \n##       16.62         7.94\n\nThe output is different because each test/regression model object has a different print method, which allows R to create different output for each type of object.\n\n\n\n\n\nFunctional programming allows us to write programs that are more modular, and thus, are easier to test and debug. In addition, functional programming encourages you to think about data pipelines: a sequence of steps that are reproducible and reusable for different data sets and analyses. Functional programming is convenient for another (more esoteric, but important) reason - it allows you to prove that a function or series of functions is actually correct (rather than just testing input/output combinations).\n\n\nA pipeline of functional data analysis. Each function (station) modifies the data in some way and the returned result is passed into the next function (station) as input. This allows a sequence of functions to format data, visualize it, model it, and then package the results. While this paradigm doesn’t require a functional approach, functional programming does make it simpler. Modified from Allison Horst’s work\n\n\nIf you have been using the R pipe (|> or %>%), you didn’t realize it, but you were already using functional programming. Piping results from one function to another in a chain is a prime example of the “pure function” idea - it allows us to chain each step of a sequence together to create a sequence that is modular and testable.\n\n\n\n\n\n\nA simple Functional Example\n\n\n\nA functional is a function that takes another function as input and returns a vector as output.\nOne simple example of a functional that is found in both R and Python is the apply function (or variants in R like lapply, sapply, tapply). In Python, .apply is a method in Pandas, but we can find an even more low-level equivalent in the ideas of list comprehensions and map functions.\nOne additional concept that is helpful before we start is the idea of a lambda function - a small anonymous function (that is, a function that is not named or stored in a variable). Lambda functions are great for filling in default arguments, but they have many other uses as well.\nCan you identify the lambda functions in each of the following examples?\n\n\nR\nPython\n\n\n\nThis code generates 5 draws from a normal random variable with the specified mean and standard deviation 1.\n\nlapply(1:5, function(x) rnorm(5, mean = x, sd = 1))\n## [[1]]\n## [1] 0.4387285 1.1330657 1.0441055 1.7244030 0.6735600\n## \n## [[2]]\n## [1] 1.356978 4.717603 1.814021 1.735615 2.203974\n## \n## [[3]]\n## [1] 5.159385 4.439005 1.514152 4.067009 1.644368\n## \n## [[4]]\n## [1] 3.655217 5.049472 3.739394 3.329780 3.617121\n## \n## [[5]]\n## [1] 4.981520 5.046483 3.755596 4.185624 4.286423\n\nOr, if you have R 4.1.0 or above, you can use a shorthand version:\n\nlapply(1:5, \\(x) rnorm(5, mean = x))\n## [[1]]\n## [1] 0.3410169 2.3214890 3.1804891 1.4336504 1.0650393\n## \n## [[2]]\n## [1] 1.9890210 1.5627826 0.4912011 2.5291908 2.4457862\n## \n## [[3]]\n## [1] 4.370364 2.180753 2.376926 2.602497 3.282503\n## \n## [[4]]\n## [1] 2.937465 4.203590 4.327174 3.050180 4.626548\n## \n## [[5]]\n## [1] 4.643520 4.538882 4.008337 4.768433 3.945161\n\nThe \\(x) is shorthand for function(x) and allows you to quickly and easily define anonymous functions in R.\n\n\nThis code generates 5 draws from a normal random variable with the specified mean and standard deviation 1.\n\nimport numpy as np\n\n# List comprehension approach\nr1 = [np.random.normal(i, size = 5) for i in range(1, 6)]\nprint(r1) \n\n# Functional approach\n# Defining a lambda function allows us to fill in non-default options\n## [array([-0.77906066,  2.05214355,  1.72554001,  0.7584667 ,  2.24633474]), array([2.40111491, 2.31918225, 1.36232223, 0.79807613, 1.47834854]), array([4.16725694, 2.19048384, 3.11457723, 2.54691536, 2.09751842]), array([3.00100303, 5.0984273 , 3.06988761, 2.99330161, 4.31783032]), array([3.34069165, 4.36551468, 5.97572347, 5.42477504, 5.76606932])]\nr2 = map(lambda a: np.random.normal(a, size = 5), range(1, 6))\n\n# This is what map spits out by default\nprint(r2)\n# get your results back out with list()\n## <map object at 0x7f07681d3730>\nr2b = list(r2) \nprint(r2b)\n## [array([1.06923657, 2.12554275, 1.61925869, 0.04934962, 0.9048769 ]), array([1.71822287, 2.58916875, 1.70476669, 2.72649437, 1.85903448]), array([2.1156096 , 2.07148658, 3.61407381, 4.19072349, 2.58930252]), array([5.78777524, 3.67496362, 3.42908602, 4.23364318, 2.33627933]), array([4.02028475, 4.74887834, 3.5279838 , 3.92797907, 4.24446515])]"
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#replacing-loops-with-functional-programming",
    "href": "part-wrangling/08-functional-prog.html#replacing-loops-with-functional-programming",
    "title": "25  Functional Programming",
    "section": "\n25.3 Replacing Loops with Functional Programming",
    "text": "25.3 Replacing Loops with Functional Programming\nOne really convenient application of functional programming is to replace loops. As Hadley Wickham says in [6],\n\nthe real downside of for loops is that they’re very flexible: a loop conveys that you’re iterating, but not what should be done with the results\n\nThat is, in many cases when programming with data, what we want is to iterate over a vector and return a vector of results. This is a perfect use case for functional programming, since we’re specifying both that we’re iterating AND more explicitly collecting the results into a form that makes sense.\nIf we work with this definition of functional programming, then python list comprehensions are also a functional approach: they specify how the results are collected (usually by putting [] around the statement) and how the iteration will occur [7].\n\n\nThere is an excellent vignette comparing Base R functional programming approaches to the purrr package that is worth a look if you’ve used one and want to try the other [8].\nLet’s look at a few examples.\n\n\n\n\n\n\nDemo\n\n\n\nSuppose we want to look at the Lego data and create a decade variable that describes the decade a set was first released.\n\n\nbase R\nR: purrr\nPython\n\n\n\n\nlego <- read.csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv\")\n\nlego$decade <- sapply(lego$year, \\(x) floor(x/10)*10)\nhead(lego[,c(\"set_num\", \"name\", \"year\", \"decade\")])\n##   set_num                       name year decade\n## 1   001-1                      Gears 1965   1960\n## 2  0011-2          Town Mini-Figures 1979   1970\n## 3  0011-3 Castle 2 for 1 Bonus Offer 1987   1980\n## 4  0012-1         Space Mini-Figures 1979   1970\n## 5  0013-1         Space Mini-Figures 1979   1970\n## 6  0014-1         Space Mini-Figures 1979   1970\n\nStrictly speaking, this use of sapply isn’t necessary - because R is vectorized by default, we could also have used lego$decade <- floor(lego$year/10)*10. However, there are functions in R that are not fully vectorized, and it is useful to know this approach for those use-cases as well, and it’s easier to demonstrate this approach with a relatively simple use case.\n\n\nIn purrr, you can create anonymous functions using ~ with . as a placeholder. If you need more parameters, you can use .x, .y and map2 (for now) or .1, .2, .3, ... with pmap.\n\nlibrary(purrr)\nlibrary(readr)\nlibrary(dplyr)\n\nlego <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv\")\n\nlego <- lego |> # Either pipe will work here\n  mutate(decade = purrr::map_int(year, ~floor(./10)*10))\n\nlego |> \n  select(set_num, name, year, decade) |> \n  head()\n## # A tibble: 6 × 4\n##   set_num name                        year decade\n##   <chr>   <chr>                      <dbl>  <int>\n## 1 001-1   Gears                       1965   1960\n## 2 0011-2  Town Mini-Figures           1979   1970\n## 3 0011-3  Castle 2 for 1 Bonus Offer  1987   1980\n## 4 0012-1  Space Mini-Figures          1979   1970\n## 5 0013-1  Space Mini-Figures          1979   1970\n## 6 0014-1  Space Mini-Figures          1979   1970\n\n\n\n\nimport pandas as pd\nimport math\n\nlego = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/lego_sets.csv\")\nlego['decade'] = [math.floor(i/10)*10 for i in lego.year]\nlego[['set_num', 'name', 'year', 'decade']].head()\n##   set_num                        name  year  decade\n## 0   001-1                       Gears  1965    1960\n## 1  0011-2           Town Mini-Figures  1979    1970\n## 2  0011-3  Castle 2 for 1 Bonus Offer  1987    1980\n## 3  0012-1          Space Mini-Figures  1979    1970\n## 4  0013-1          Space Mini-Figures  1979    1970\n\n\n\n\n\n\nFor a more interesting example, though, let’s consider fitting a different linear regression for each generation of Pokemon, describing the relationship between HP (hit points) and CP (combat power, aka total in this dataset).\n\n\nI am sure that the python code I’ve written here is a bit kludgy, so if you are more fluent in python/pandas than I am, please feel free to submit a pull request if you know a better or more “pretty” way to do this.\n\n\n\n\n\n\nExample: Pokemon modeling\n\n\n\n\n\nbase R\nTidy R\nPython\n\n\n\n\npoke <- read.csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\n# Get rid of mega pokemon - they're a different thing\npoke <- subset(poke, !grepl(\"Mega\", poke$variant)) # step 1\n\n# Split into a list of data frames from each gen\npoke_gens <- split(poke, poke$gen) # step 2\n\n# Fit linear regressions for each generation of pokemon\nmodels <- lapply(poke_gens, \\(df) lm(total ~ hp, data = df)) # step 3\n\n# Pull out coefficients and r-squared values\nresults <- lapply(models, \\(res) data.frame(coef1 = coef(res)[1], coef2 = coef(res)[2], rsq = summary(res)$r.squared))  # step 4\n\n# Join the results back into a data.frame\nresults <- do.call(\"rbind\", results) # step 5\n\nresults\n##      coef1    coef2       rsq\n## 1 262.4730 2.394032 0.3895255\n## 2 258.5815 2.133868 0.3165292\n## 3 245.3375 2.753912 0.2686807\n## 4 268.5792 2.792350 0.3500187\n## 5 189.1339 3.506458 0.5578802\n## 6 252.7434 2.798942 0.5278159\n## 7 234.9329 3.293730 0.4382885\n## 8 205.2726 3.500811 0.6042219\n## 9 236.8656 2.757408 0.4757028\n\n\nData in data frame\nData split into a list of data frames\nModels in a list corresponding to data in step 2\nResults in a list of data frames corresponding to models in step 3\nBind results in step 4 back into a data frame\n\nIn each step, we specify not only what the iterative action should be, but also what form the results will take.\n\n\nIn the tidyverse, we use tidyr::nest() to accomplish a similar thing to split in base R.\nThis approach is designed to work entirely within a single data frame, which keeps the environment relatively clean and ensures that each step’s results are stored in a convenient, easy-to-find place.\n\nlibrary(purrr)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\n\nres <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\") %>%\n  # str_detect doesn't play nice with NAs, so replace NA with \"\"\n  mutate(variant = replace_na(variant, \"\")) %>%\n  # Remove mega pokemon\n  filter(str_detect(variant, \"Mega\", negate = T)) %>% # step 1\n  # Sub-data-frames\n  nest(.by = gen) %>% # step 2\n  # Fit model\n  mutate(model = map(data, ~lm(total ~ hp, data = .))) %>% # step 3\n  # Extract coefficients\n  mutate(res = map(model, ~data.frame(coef1 = coef(.)[1], \n                                      coef2 = coef(.)[2], \n                                      rsq = summary(.)$r.squared))) %>% # step 4\n  # Bind together\n  unnest(c(res)) # step 5\nres\n## # A tibble: 9 × 6\n##     gen data                model  coef1 coef2   rsq\n##   <dbl> <list>              <list> <dbl> <dbl> <dbl>\n## 1     1 <tibble [269 × 15]> <lm>    262.  2.39 0.390\n## 2     2 <tibble [118 × 15]> <lm>    259.  2.13 0.317\n## 3     3 <tibble [173 × 15]> <lm>    245.  2.75 0.269\n## 4     4 <tibble [173 × 15]> <lm>    269.  2.79 0.350\n## 5     5 <tibble [236 × 15]> <lm>    189.  3.51 0.558\n## 6     6 <tibble [118 × 15]> <lm>    253.  2.80 0.528\n## 7     7 <tibble [133 × 15]> <lm>    235.  3.29 0.438\n## 8     8 <tibble [134 × 15]> <lm>    205.  3.50 0.604\n## 9     9 <tibble [123 × 15]> <lm>    237.  2.76 0.476\n\nOur data takes the form:\n\nAn ungrouped data frame\nA data frame with 9 rows, one for each generation, with a list-column data that contains the full data for each generation\nWe fit our model and store the model results into another list-column named model that contains the fitted model object\nWe define some summary information and store it into a list-column containing each 1-row data frame\nWe “unnest” the summary information, which is equivalent to bringing the columns we defined up to the primary level and binding the rows together.\n\nAt each step, we’re specifying the form of the results along with the contents.\n\n\nThis construct of storing everything inside a single data frame isn’t as common in Python, but we can make it work with only a little extra effort.\nYou will need to pip install statsmodels to get the statsmodels [9] package that implements many basic statistical models. The scikit-learn package [10] is another commonly used package [11], but it does not have the easy accessor functions to pull out e.g. coefficients and r-squared values, so we’ll use statsmodels here.\n\nimport pandas as pd\nfrom statsmodels.formula.api import ols\n\n# Create a function to fit a linear regression\n# There is probably a better way to do this flexibly,\n# but this approach is simple and useful for illustrative purposes\ndef pokereg(data):\n  x = data[[\"hp\"]].values\n  y = data[[\"total\"]].values\n  model = ols('total ~ hp', data)\n  results = model.fit()\n  return results\n\nres = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\n\n# Replace NAs with \"\"\nres[\"variant\"] = [\"\" if pd.isna(i) else i for i in res.variant]\n# Remove mega pokemon\nres = res.query('~(variant.str.contains(\"(?:^[mM]ega)\"))')\n\n# Group data frames and apply regression function to each group\nres_reg = (\n  res # step 1\n    .groupby(\"gen\") # step 2\n    .apply(pokereg) # step 3\n)\n\n# Make results into a dataframe and rename the column as 'results'\nres_reg = pd.DataFrame(res_reg).rename(columns = {0:'results'}) # step 4\n\n# Get values of interest and store in new columns # step 5\nres_reg = res_reg.reset_index() # store gen in its own column\nres_reg['coef1'] = res_reg.results.map(lambda x: x.params[0])\nres_reg['coef2'] = res_reg.results.map(lambda x: x.params[1])\nres_reg['rsq'] = res_reg.results.map(lambda x: x.rsquared)\n\nres_reg[['gen', 'coef1', 'coef2', 'rsq']]\n##    gen       coef1     coef2       rsq\n## 0    1  262.472956  2.394032  0.389526\n## 1    2  258.581538  2.133868  0.316529\n## 2    3  245.337496  2.753912  0.268681\n## 3    4  268.579237  2.792350  0.350019\n## 4    5  189.133898  3.506458  0.557880\n## 5    6  252.743437  2.798942  0.527816\n## 6    7  234.932868  3.293730  0.438289\n## 7    8  205.272637  3.500811  0.604222\n## 8    9  236.865627  2.757408  0.475703\n\nWhile this doesn’t store our data in the same DataFrame as the model results, we do have a key that links the two: the gen variable is present in both res and res_reg and can be used to join the data to the regression results, if necessary.\n\nData in an ungrouped data frame\nWe group by gen (generation)\nWe apply the function pokereg to fit a linear regression, and the results are stored in an indexed Series where the index corresponds to gen.\nWe make the results into a DataFrame so that we can add extra columns, and rename the automatically created Series to results to be more descriptive\nWe create summary information and store the summaries in columns in the data frame.\n\nWhile the grouping and binding operations are in a different order in Python than in R, the basic specification of the structure of the output each time we iterate is similar.\n\n\n\n\n\n\n\nHere’s another demonstration of the use of the tidymodels package and purrr to fit multiple regression models to data subsets."
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#complex-data-structures",
    "href": "part-wrangling/08-functional-prog.html#complex-data-structures",
    "title": "25  Functional Programming",
    "section": "\n25.4 Complex Data Structures",
    "text": "25.4 Complex Data Structures\nNot all datasets are strictly tabular. One of the most common situations where we get data that can’t be made into a completely tabular structure is when we’re dealing with hierarchical data: tree structures, (network) graph structures, and even most webpages contain data that isn’t strictly tabular in nature. Sometimes, we can get that data into a tabular structure, but it generally depends on the data itself.\nOne of the most common structures for storing data on the web is JSON: JavaScript Object Notation[12].\n\n\n(JSON is pronounced “Jason”, like the person’s name).\n In this section we’ll work with some data gathered from TMDB (the movie database). I submitted a query for all movies that Patrick Stewart was involved with, and you can find the resulting JSON file here.\n\n\n\n\n\n\nJSON File Parsing\n\n\n\n\n\nR\nPython\n\n\n\nWe’ll use the jsonlite package to read the data in, but invariably this package still requires us to do some post-processing ourselves.\n\nlibrary(jsonlite)\ndata_url <- \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Patrick_Stewart.json\"\n\nps_json <- fromJSON(data_url)\n\n\nExploring the output structure\n\n# head(ps_json) # This output is too long\nmap(ps_json, head) # show the first 6 rows of each element in the list\n## $cast\n##   adult                    backdrop_path       genre_ids    id\n## 1 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg 878, 28, 12, 53   193\n## 2 FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg 878, 28, 12, 53   199\n## 3 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg 878, 28, 12, 53   200\n## 4 FALSE /6z9w8eidKWDDXwZNSVNaRolAYEP.jpg 878, 28, 12, 53   201\n## 5 FALSE /4ADZ2iiATjoKxZwjJRiEo1x6Fk0.jpg              99 10946\n## 6 FALSE                             <NA>              99 21746\n##   original_language              original_title\n## 1                en      Star Trek: Generations\n## 2                en    Star Trek: First Contact\n## 3                en     Star Trek: Insurrection\n## 4                en          Star Trek: Nemesis\n## 5                en                       Earth\n## 6                en The Secret of Life on Earth\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                overview\n## 1                                                                                                                                                                                                                                                                                                              Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 2                                                                                                                                                                                                                                                    The Borg, a relentless race of cyborgs, are on a direct course for Earth. Violating orders to stay away from the battle, Captain Picard and the crew of the newly-commissioned USS Enterprise E pursue the Borg back in time to prevent the invaders from changing Federation history and assimilating the galaxy.\n## 3                                                                                                                                                                                                                                                                                 When an alien race and factions within Starfleet attempt to take over a planet that has \"regenerative\" properties, it falls upon Captain Picard and the crew of the Enterprise to defend the planet's people as well as the very ideals upon which the Federation itself was founded.\n## 4 En route to the honeymoon of William Riker to Deanna Troi on her home planet of Betazed, Captain Jean-Luc Picard and the crew of the U.S.S. Enterprise receives word from Starfleet that a coup has resulted in the installation of a new Romulan political leader, Shinzon, who claims to seek peace with the human-backed United Federation of Planets. Once in enemy territory, the captain and his crew make a startling discovery: Shinzon is human, a slave from the Romulan sister planet of Remus, and has a secret, shocking relationship to Picard himself.\n## 5                                                                                                                                                                                                                                                                                                 From the acclaimed team that brought you BBC's visual feast \"Planet Earth,\" this feature length film incorporates some of the same footage from the series with all new scenes following three remarkable, yet sadly endangered, families of animal across the globe.\n## 6                                                                                                                                                                                                                                                                          A breathtaking adventure across five continents and through time to reveal nature's most vital secret. Watch a flying fox gorge itself on a midnight snack of figs. Climb into the prickly jaws of insect-eating plants. Witness a mantis disguised as a flower petal lure its prey to doom.\n##   popularity                      poster_path release_date\n## 1     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 2     29.750 /vrC1lkTktFQ4AqBfqf4PXoDDLcw.jpg   1996-11-22\n## 3     28.786 /xQCMAHeg5M9HpDIqanYbWdr4brB.jpg   1998-12-11\n## 4     33.614 /cldAwhvBmOv9jrd3bXWuqRHoXyq.jpg   2002-12-13\n## 5      9.188 /xybnXW6E28W9agiwUeGLbTYS454.jpg   2007-04-22\n## 6      1.831 /baa6T6noxiFUZcb6Jz8TjjlOoCH.jpg   1993-10-14\n##                         title video vote_average vote_count\n## 1      Star Trek: Generations FALSE        6.526       1126\n## 2    Star Trek: First Contact FALSE        7.305       1519\n## 3     Star Trek: Insurrection FALSE        6.425       1025\n## 4          Star Trek: Nemesis FALSE        6.293       1218\n## 5                       Earth FALSE        7.600        311\n## 6 The Secret of Life on Earth FALSE        6.000          1\n##                 character                credit_id order\n## 1 Captain Jean-Luc Picard 52fe4225c3a36847f80076d9     0\n## 2 Captain Jean-Luc Picard 52fe4226c3a36847f8007ba3     0\n## 3 Captain Jean-Luc Picard 52fe4226c3a36847f8007c27     0\n## 4 Captain Jean-Luc Picard 52fe4226c3a36847f8007cf1     0\n## 5                Narrator 52fe43d79251416c75020267     0\n## 6        Narrator (voice) 52fe4425c3a368484e01220f     0\n## \n## $crew\n##   adult                    backdrop_path            genre_ids      id\n## 1 FALSE                             <NA>               99, 35 1093380\n## 2 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg      878, 28, 12, 53     200\n## 3 FALSE /ccsLztuF4cKlfnriitwdxs0coBa.jpg 10770, 14, 18, 10751   48358\n## 4 FALSE /eMxx1QohCBbhFEiB9SYIGFo2oK3.jpg                   37   47913\n## 5 FALSE /li27iYcGbSp89YTlVRmwujteykw.jpg 18, 36, 10770, 10749   37945\n## 6 FALSE /g5CMQPz5cqUHro9pNLBRW7cT8cY.jpg               18, 14   16716\n##   original_language          original_title\n## 1                en           Red Dwarf A-Z\n## 2                en Star Trek: Insurrection\n## 3                en   The Canterville Ghost\n## 4                en           King of Texas\n## 5                en      The Lion in Winter\n## 6                en       A Christmas Carol\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 overview\n## 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     A compilation of clips and interviews, originally broadcast on BBC2's Red Dwarf Night in 1998, and subsequently included on the DVD release of Red Dwarf series 2.\n## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  When an alien race and factions within Starfleet attempt to take over a planet that has \"regenerative\" properties, it falls upon Captain Picard and the crew of the Enterprise to defend the planet's people as well as the very ideals upon which the Federation itself was founded.\n## 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                       When a teenaged girl moves to England, with her brothers and parents into the ancient Canterville Hall, she's not at all happy. Especially as there's a ghost and a mysterious re-appearing bloodstain on the hearth. She campaigns to go back home, and her dad, believing the ghost's pranks are Ginny's, is ready to send her back. But then Ginny actually meets the elusive 17th-century Sir Simon de Canterville (not to mention the cute teenaged duke next door), and she sets her hand to the task of freeing Sir Simon from his curse.\n## 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In this re-imagining of Shakespear's King Lear, Patrick Stewart stars as John Lear, a Texas cattle baron, who, after dividing his wealth among his three daughters, is rejected by them.\n## 5 King Henry II (Patrick Stewart) keeps his wife, Eleanor (Glenn Close) locked away in the towers because of her frequent attempts to overthrow him. With Eleanor out of the way he can have his dalliances with his young mistress (Yuliya Vysotskaya). Needless to say the queen is not pleased, although she still has affection for the king. Working through her sons, she plots the king's demise and the rise of her second and preferred son, Richard (Andrew Howard), to the throne. The youngest son, John (Rafe Spall), an overweight buffoon and the only son holding his father's affection is the king's choice after the death of his first son, young Henry. But John is also overly eager for power and is willing to plot his father's demise with middle brother, Geoffrey (John Light) and the young king of France, Phillip (Jonathan Rhys Meyers). Geoffrey, of course sees his younger brother's weakness and sees that route as his path to power. Obviously political and court intrigue ensues\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Scrooge is a miserly old businessman in 1840s London. One Christmas Eve he is visited by the ghost of Marley, his dead business partner. Marley foretells that Scrooge will be visited by three spirits, each of whom will attempt to show Scrooge the error of his ways. Will Scrooge reform his ways in time to celebrate Christmas?\n##   popularity                      poster_path release_date\n## 1      0.841                             <NA>   2003-02-10\n## 2     28.786 /xQCMAHeg5M9HpDIqanYbWdr4brB.jpg   1998-12-11\n## 3      9.267 /m71l7oGGKLxdQaUceVTndg2qjJJ.jpg   1996-01-27\n## 4      3.951 /jFvDJsgnLRVuRpsbm3YHIn0dHxI.jpg   2002-06-02\n## 5      7.097 /f6yEfCBBMNp6jdny9AD4ZTG9tgi.jpg   2003-12-26\n## 6     13.427 /oi1NcVDXlFEsdpLp37BJmFbVlg9.jpg   1999-12-05\n##                     title video vote_average vote_count\n## 1           Red Dwarf A-Z FALSE        0.000          0\n## 2 Star Trek: Insurrection FALSE        6.425       1025\n## 3   The Canterville Ghost FALSE        6.042         48\n## 4           King of Texas FALSE        5.100         11\n## 5      The Lion in Winter FALSE        6.100         17\n## 6       A Christmas Carol FALSE        6.800        161\n##                  credit_id department                job\n## 1 63fead85699fb70096ff260e       Crew             Thanks\n## 2 52fe4226c3a36847f8007c1d Production           Producer\n## 3 5962f6d292514122510c57a0 Production        Co-Producer\n## 4 59807f88925141491d0113a0 Production Executive Producer\n## 5 5f72da29e4b5760039f36206 Production Executive Producer\n## 6 63c31ac8d46537007dbd999a Production Executive Producer\n## \n## $id\n## [1] 2387\n\nBy default, fromJSON does a LOT of heavy lifting for us:\n\nIdentifying the structure of the top-level data: cast, crew, and id information\nParses cast information into a data frame with list-columns\nParses crew information into a data frame with list-columns\n\nIt’s hard to explain how nice this is to someone who hasn’t had to parse this type of information by hand before… so let’s briefly explore that process.\n\nlibrary(jsonlite)\n\nps_messy <- fromJSON(data_url, simplifyVector = T, simplifyDataFrame = F)\n\n\nExploring the output structure (long version)\n\n# Top-level objects (show the first object in the list)\nps_messy$cast[[1]]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] \"/mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg\"\n## \n## $genre_ids\n## [1] 878  28  12  53\n## \n## $id\n## [1] 193\n## \n## $original_language\n## [1] \"en\"\n## \n## $original_title\n## [1] \"Star Trek: Generations\"\n## \n## $overview\n## [1] \"Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\"\n## \n## $popularity\n## [1] 19.783\n## \n## $poster_path\n## [1] \"/rHsCYDGHFUarGh5k987b0EFU6kC.jpg\"\n## \n## $release_date\n## [1] \"1994-11-18\"\n## \n## $title\n## [1] \"Star Trek: Generations\"\n## \n## $video\n## [1] FALSE\n## \n## $vote_average\n## [1] 6.526\n## \n## $vote_count\n## [1] 1126\n## \n## $character\n## [1] \"Captain Jean-Luc Picard\"\n## \n## $credit_id\n## [1] \"52fe4225c3a36847f80076d9\"\n## \n## $order\n## [1] 0\nps_messy$crew[[1]]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## NULL\n## \n## $genre_ids\n## [1] 99 35\n## \n## $id\n## [1] 1093380\n## \n## $original_language\n## [1] \"en\"\n## \n## $original_title\n## [1] \"Red Dwarf A-Z\"\n## \n## $overview\n## [1] \"A compilation of clips and interviews, originally broadcast on BBC2's Red Dwarf Night in 1998, and subsequently included on the DVD release of Red Dwarf series 2.\"\n## \n## $popularity\n## [1] 0.841\n## \n## $poster_path\n## NULL\n## \n## $release_date\n## [1] \"2003-02-10\"\n## \n## $title\n## [1] \"Red Dwarf A-Z\"\n## \n## $video\n## [1] FALSE\n## \n## $vote_average\n## [1] 0\n## \n## $vote_count\n## [1] 0\n## \n## $credit_id\n## [1] \"63fead85699fb70096ff260e\"\n## \n## $department\n## [1] \"Crew\"\n## \n## $job\n## [1] \"Thanks\"\nps_messy$id\n## [1] 2387\n\nLet’s start with the cast list. Most objects seem to be single entries; the only thing that isn’t is the genre_ids field. So let’s see whether we can just convert each list entry to a data frame, and then deal with the genre_ids column afterwards.\n\ncast_list <- ps_messy$cast\n\n\nData frame conversion\n\nas.data.frame(cast_list[[1]])\n##   adult                    backdrop_path genre_ids  id original_language\n## 1 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg       878 193                en\n## 2 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        28 193                en\n## 3 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        12 193                en\n## 4 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        53 193                en\n##           original_title\n## 1 Star Trek: Generations\n## 2 Star Trek: Generations\n## 3 Star Trek: Generations\n## 4 Star Trek: Generations\n##                                                                                                                                                                                                                                                   overview\n## 1 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 2 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 3 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n## 4 Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\n##   popularity                      poster_path release_date\n## 1     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 2     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 3     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n## 4     19.783 /rHsCYDGHFUarGh5k987b0EFU6kC.jpg   1994-11-18\n##                    title video vote_average vote_count               character\n## 1 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n## 2 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n## 3 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n## 4 Star Trek: Generations FALSE        6.526       1126 Captain Jean-Luc Picard\n##                  credit_id order\n## 1 52fe4225c3a36847f80076d9     0\n## 2 52fe4225c3a36847f80076d9     0\n## 3 52fe4225c3a36847f80076d9     0\n## 4 52fe4225c3a36847f80076d9     0\n\n\nmap(cast_list, as.data.frame)\n## Error in `map()`:\n## ℹ In index: 6.\n## Caused by error:\n## ! arguments imply differing number of rows: 1, 0\n\nWell, that didn’t work, but the error message at least tells us what index is causing the problem: 6. Let’s look at that data:\n\nData frame conversion errors\n\ncast_list[[6]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## NULL\n## \n## $genre_ids\n## [1] 99\n## \n## $id\n## [1] 21746\n## \n## $original_language\n## [1] \"en\"\n\nOk, so backdrop_path is NULL, and as.data.frame can’t handle the fact that some fields are defined (length 1) and others are NULL (length 0). We could possibly replace the NULL with NA first?\n\nfix_nulls <- function(x) {\n  lapply(x, \\(y) if (is.null(y)) NA else y)\n}\n\ncast_list_fix <- map(cast_list, fix_nulls)\n\ncast_list_fix[[6]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] NA\n## \n## $genre_ids\n## [1] 99\n## \n## $id\n## [1] 21746\n## \n## $original_language\n## [1] \"en\"\n\nmap(cast_list_fix, as.data.frame)\n## Error in `map()`:\n## ℹ In index: 8.\n## Caused by error:\n## ! arguments imply differing number of rows: 1, 0\n\ncast_list_fix[[8]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] NA\n## \n## $genre_ids\n## list()\n## \n## $id\n## [1] 33335\n## \n## $original_language\n## [1] \"en\"\n\nOk, well, this time, we have an issue with position 8, and we have an empty list of genre_ids.\nAn empty list and NULL both have length 0, so let’s alter our fix_nulls function to test for things of length 0 instead of testing for nulls. That should fix both problems using the same code, and we’re trying to directly test for the issue which was causing problems, which is perhaps a better approach anyways.\n\nfix_nulls <- function(x) {\n  lapply(x, \\(y) if (length(y) == 0) NA else y)\n}\n\ncast_list_fix <- map(cast_list, fix_nulls)\ncast_list_fix[[8]][1:5]\n## $adult\n## [1] FALSE\n## \n## $backdrop_path\n## [1] NA\n## \n## $genre_ids\n## [1] NA\n## \n## $id\n## [1] 33335\n## \n## $original_language\n## [1] \"en\"\n\ncast_list_df <- map_df(cast_list_fix, as.data.frame)\ncast_list_df[1:10, 1:5]\n##    adult                    backdrop_path genre_ids  id original_language\n## 1  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg       878 193                en\n## 2  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        28 193                en\n## 3  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        12 193                en\n## 4  FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg        53 193                en\n## 5  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg       878 199                en\n## 6  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg        28 199                en\n## 7  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg        12 199                en\n## 8  FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg        53 199                en\n## 9  FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg       878 200                en\n## 10 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg        28 200                en\n\nWe still have too many rows for each entry because of the multiple genre_ids. But we can fix that with the nest command.\n\ncast_list <- nest(cast_list_df, genre_ids = genre_ids )\ncast_list[1:10,c(1:4, 17)]\n## # A tibble: 10 × 5\n##    adult backdrop_path                       id original_language genre_ids\n##    <lgl> <chr>                            <int> <chr>             <list>   \n##  1 FALSE /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg   193 en                <tibble> \n##  2 FALSE /wygUDDRNpeKUnkekRGeLCZM93tA.jpg   199 en                <tibble> \n##  3 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg   200 en                <tibble> \n##  4 FALSE /6z9w8eidKWDDXwZNSVNaRolAYEP.jpg   201 en                <tibble> \n##  5 FALSE /4ADZ2iiATjoKxZwjJRiEo1x6Fk0.jpg 10946 en                <tibble> \n##  6 FALSE <NA>                             21746 en                <tibble> \n##  7 FALSE /cN4qq4B8JR4ekuKAIKGVa4bBssl.jpg 25224 en                <tibble> \n##  8 FALSE <NA>                             33335 en                <tibble> \n##  9 FALSE /89hVgLIH55PVE7wwLCVZF1j3ZGL.jpg 26950 en                <tibble> \n## 10 FALSE /x6f4Axjvr5Ybi2mfdpVSWvASdxX.jpg 28123 en                <tibble>\n\nThen, we’d have to apply this whole process to the crew list as well. Let’s see how robust our process actually is!\n\ncrew_list <- ps_messy$crew\ncrew_list_fix <- map(crew_list, fix_nulls)\ncrew_list_df <- map_df(crew_list_fix, as.data.frame)\ncrew_list <- nest(crew_list_df, genre_ids = genre_ids )\ncrew_list[1:5,c(1:4, 17)]\n## # A tibble: 5 × 5\n##   adult backdrop_path                         id original_language genre_ids\n##   <lgl> <chr>                              <int> <chr>             <list>   \n## 1 FALSE <NA>                             1093380 en                <tibble> \n## 2 FALSE /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg     200 en                <tibble> \n## 3 FALSE /ccsLztuF4cKlfnriitwdxs0coBa.jpg   48358 en                <tibble> \n## 4 FALSE /eMxx1QohCBbhFEiB9SYIGFo2oK3.jpg   47913 en                <tibble> \n## 5 FALSE /li27iYcGbSp89YTlVRmwujteykw.jpg   37945 en                <tibble>\n\nOk, so that actually worked, but only because the structure of the crew data is the same as the structure of the cast data.\nIt’s good to see what we’d have to do manually if fromJSON() failed on us. It’s also an excellent example of functional programming in a practical setting.\nLet’s finish this up by converting our cast and crew data frames into a single data frame with a variable indicating which source DF is relevant.\n\npatrick_stewart_movies <- bind_rows(\n  mutate(cast_list, role = \"cast\"),\n  mutate(crew_list, role = \"crew\")\n)\npatrick_stewart_movies %>%\n  arrange(id)\n## # A tibble: 156 × 20\n##    adult backdrop_path              id original_language original_title overview\n##    <lgl> <chr>                   <int> <chr>             <chr>          <chr>   \n##  1 FALSE /mNdsbVuRdsyo8eitW2IBW…   193 en                Star Trek: Ge… \"Captai…\n##  2 FALSE /wygUDDRNpeKUnkekRGeLC…   199 en                Star Trek: Fi… \"The Bo…\n##  3 FALSE /vsjuHP9RQZJgYUvvSlO3m…   200 en                Star Trek: In… \"When a…\n##  4 FALSE /vsjuHP9RQZJgYUvvSlO3m…   200 en                Star Trek: In… \"When a…\n##  5 FALSE /6z9w8eidKWDDXwZNSVNaR…   201 en                Star Trek: Ne… \"En rou…\n##  6 FALSE /2mEXtIjgsoe5uqH70CLps…   815 en                Animal Farm    \"An ani…\n##  7 FALSE /5wJ2tckpvwcxGCAgZicco…   841 en                Dune           \"In the…\n##  8 FALSE /92mpNNg6v2PN2HN2C2Z4g…  1273 en                TMNT           \"After …\n##  9 FALSE /wvqdJLVh0mSblly7UnYFP…  2080 en                X-Men Origins… \"After …\n## 10 FALSE /hPDv0O8tvbEvcVVphIieS…  2107 en                L.A. Story     \"With t…\n## # ℹ 146 more rows\n## # ℹ 14 more variables: popularity <dbl>, poster_path <chr>, release_date <chr>,\n## #   title <chr>, video <lgl>, vote_average <dbl>, vote_count <int>,\n## #   character <chr>, credit_id <chr>, order <int>, genre_ids <list>,\n## #   role <chr>, department <chr>, job <chr>\n\nWe could theoretically clean this up so that movies where Patrick Stewart was in both the cast and crew are on a single row, but I think this is “good enough” for now.\n\n\nPandas includes a read_json function, so let’s try that and see if it works as well as fromJSON() did in R:\n\nimport pandas as pd\n\ndata_url = \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Patrick_Stewart.json\"\n\npd.read_json(data_url)\n## Error: ValueError: All arrays must be of the same length\n\nIf we read the documentation for read_json, we can see that we have a few different options - maybe playing around with some of those options will help? Our top-level structure is a list with 3 values: cast, crew, and id. So let’s see if we can read things in as a series instead of a DataFrame first, and hopefully we can use that to get some traction on the situation.\n\npatrick_stewart = pd.read_json(data_url, typ='series', orient = 'records')\n\n# List the objects\npatrick_stewart.index\n\n# First item in the cast list\n## Index(['cast', 'crew', 'id'], dtype='object')\npatrick_stewart.cast[0]\n## {'adult': False, 'backdrop_path': '/mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg', 'genre_ids': [878, 28, 12, 53], 'id': 193, 'original_language': 'en', 'original_title': 'Star Trek: Generations', 'overview': \"Captain Jean-Luc Picard and the crew of the Enterprise-D find themselves at odds with the renegade scientist Soran who is destroying entire star systems. Only one man can help Picard stop Soran's scheme...and he's been dead for seventy-eight years.\", 'popularity': 19.783, 'poster_path': '/rHsCYDGHFUarGh5k987b0EFU6kC.jpg', 'release_date': '1994-11-18', 'title': 'Star Trek: Generations', 'video': False, 'vote_average': 6.526, 'vote_count': 1126, 'character': 'Captain Jean-Luc Picard', 'credit_id': '52fe4225c3a36847f80076d9', 'order': 0}\n\nSo now how do we get our data into a proper form? If we read the documentation a bit further, we can see a “See also” section that has a json_normalize function which promises to “Normalize semi-structured JSON data into a flat table”. That sounds pretty good, let’s try it!\n\nps_cast = pd.json_normalize(patrick_stewart.cast)\nps_cast.head()\n##    adult                     backdrop_path  ...                 credit_id  order\n## 0  False  /mNdsbVuRdsyo8eitW2IBW2BWRkU.jpg  ...  52fe4225c3a36847f80076d9      0\n## 1  False  /wygUDDRNpeKUnkekRGeLCZM93tA.jpg  ...  52fe4226c3a36847f8007ba3      0\n## 2  False  /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg  ...  52fe4226c3a36847f8007c27      0\n## 3  False  /6z9w8eidKWDDXwZNSVNaRolAYEP.jpg  ...  52fe4226c3a36847f8007cf1      0\n## 4  False  /4ADZ2iiATjoKxZwjJRiEo1x6Fk0.jpg  ...  52fe43d79251416c75020267      0\n## \n## [5 rows x 17 columns]\n\nHuh, that actually worked! (I’m not used to this type of thing working on the first try).\n\nps_crew = pd.json_normalize(patrick_stewart.crew)\nps_crew.head()\n##    adult                     backdrop_path  ...  department                 job\n## 0  False                              None  ...        Crew              Thanks\n## 1  False  /vsjuHP9RQZJgYUvvSlO3mjJpXkq.jpg  ...  Production            Producer\n## 2  False  /ccsLztuF4cKlfnriitwdxs0coBa.jpg  ...  Production         Co-Producer\n## 3  False  /eMxx1QohCBbhFEiB9SYIGFo2oK3.jpg  ...  Production  Executive Producer\n## 4  False  /li27iYcGbSp89YTlVRmwujteykw.jpg  ...  Production  Executive Producer\n## \n## [5 rows x 17 columns]\n\nWe can combine these as we did in R into a single data frame, and sort by movie ID to simplify the list.\n\nps_cast['role'] = 'cast'\nps_crew['role'] = 'crew'\nps_movies = pd.concat([ps_cast, ps_crew])\nps_movies[['id', 'original_title', 'character', 'job']].sort_values(['id'])\n##           id               original_title                character       job\n## 0        193       Star Trek: Generations  Captain Jean-Luc Picard       NaN\n## 1        199     Star Trek: First Contact  Captain Jean-Luc Picard       NaN\n## 2        200      Star Trek: Insurrection  Captain Jean-Luc Picard       NaN\n## 1        200      Star Trek: Insurrection                      NaN  Producer\n## 3        201           Star Trek: Nemesis  Captain Jean-Luc Picard       NaN\n## ..       ...                          ...                      ...       ...\n## 46   1088162  The Elves and the Shoemaker         Narrator (voice)       NaN\n## 0    1093380                Red Dwarf A-Z                      NaN    Thanks\n## 140  1093380                Red Dwarf A-Z                     Self       NaN\n## 126  1095754           John Clare: \"I Am\"            Cyrus Redding       NaN\n## 47   1104829     In the Company of Whales                 Narrator       NaN\n## \n## [156 rows x 4 columns]\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: JSON File Parsing\n\n\n\n\n\nThe Movie Database\n\n\nI used TMDB to find all movies resulting from the query “Star Trek” and stored the resulting JSON file here.\n\n\nProblem\nR solution\nPython solution\n\n\n\nCreate a data frame using the Star Trek query results. Because there were 6 pages of query results, the JSON file looks a bit different than the format used in the example above. Can you create a plot of the release date and rating of each movie?\n\n\n\nlibrary(jsonlite)\nlibrary(tidyr)\nlibrary(dplyr)\n\nfile_loc <- \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Star_Trek.json\"\n\nstartrek <- fromJSON(file_loc) |>\n  unnest(results)\n\nstartrek |>\n  select(title, release_date, popularity, vote_average, vote_count) |>\n  head()\n## # A tibble: 6 × 5\n##   title                    release_date popularity vote_average vote_count\n##   <chr>                    <chr>             <dbl>        <dbl>      <int>\n## 1 Star Trek                2009-05-06         51.2         7.43       9075\n## 2 Star Trek: Nemesis       2002-12-13         33.6         6.29       1218\n## 3 Star Trek Beyond         2016-07-07         40.3         6.78       6037\n## 4 Star Trek: Insurrection  1998-12-11         28.8         6.42       1025\n## 5 Star Trek Into Darkness  2013-05-05         41.3         7.33       8370\n## 6 Star Trek: First Contact 1996-11-22         29.8         7.30       1519\n\n# convert release_date to datetime\nlibrary(lubridate)\nstartrek <- startrek |>\n  mutate(rel_date = ymd(release_date))\n\nstartrek |>\n  arrange(rel_date) |>\n  select(title, rel_date, popularity, vote_average, vote_count) |>\n  head()\n## # A tibble: 6 × 5\n##   title                            rel_date   popularity vote_average vote_count\n##   <chr>                            <date>          <dbl>        <dbl>      <int>\n## 1 Jr. Star Trek                    1969-01-01       0.6          0             0\n## 2 Ömer the Tourist in Star Trek    1973-01-01       1.90         6.36         38\n## 3 Star Trek: The Motion Picture    1979-12-07      26.5          6.50       1480\n## 4 Star Trek II: The Wrath of Khan  1982-06-04      21.6          7.47       1655\n## 5 Leonard Nimoy: Star Trek Memori… 1983-01-01       0.6          7             1\n## 6 Star Trek III: The Search for S… 1984-06-01      17.0          6.62       1159\n\nlibrary(ggplot2)\nggplot(startrek, aes(x = rel_date, y = popularity)) + geom_point() + \n  xlab(\"Release Date\")\n\n\n\n\nggplot(startrek, aes(x = rel_date, y = vote_average)) + geom_point() + \n  xlab(\"Release Date\")\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nfile_loc = \"https://raw.githubusercontent.com/srvanderplas/stat-computing-r-python/main/data/Star_Trek.json\"\ntrek = pd.read_json(file_loc)\n\ntrek['results2'] = trek.results.map(pd.json_normalize)\n\n# This doesn't actually keep the page info but I don't think we need that\ntrek_tidy = pd.concat(trek.results2.to_list())\ntrek_tidy['rel_date'] = pd.to_datetime(trek_tidy.release_date)\n\nimport matplotlib.pyplot as plt\np1 = trek_tidy.plot.scatter('rel_date', 'popularity')\nplt.show()\n\n\n\np2 = trek_tidy.plot.scatter('rel_date', 'vote_average')\nplt.show()"
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#assembling-hierarchical-data",
    "href": "part-wrangling/08-functional-prog.html#assembling-hierarchical-data",
    "title": "25  Functional Programming",
    "section": "\n25.5 Assembling Hierarchical Data",
    "text": "25.5 Assembling Hierarchical Data\nAnother common situation we find ourselves in as analysts is to have multiple levels of data.\nLet’s start with a totally absurd hypothetical situation: Suppose I watched the documentary “Chicken People” and became interested in the different breeds of chicken. As a data scientist, I want to assemble a dataset on chicken breeds that I might use to decide what breed(s) to raise.\nA site such as Cackle Hatchery has an overall summary table as well as pages for each individual breed. I’m not going to show you how to web scrape here - it’s not relevant to this chapter - but we can at least outline the process:\n\nAcquire the overall table\nUse the links to each breed in the overall table to get more specific information for each breed\n\nThis will require a function to scrape that individual data\nWe can use map to apply that function to acquire individual data from each breed\n\n\n\nI’ve used this approach to generate two files:\n\n\nchicken-breeds.csv - the original table of breed information\n\nchicken-breed-details.json, which is a JSON file assembled by scraping information off of each breed’s individual page.\n\n\n\n\n\n\n\nTry It Out: Chicken Breed Data Assembly\n\n\n\n\n\nProblem\nR solution\nPython\n\n\n\nCan you create a nested data frame that has all of the information from both the CSV and JSON file in a single tabular structure?\n\n\n\nlibrary(readr)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(jsonlite)\nlibrary(stringr)\n\noverall <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breeds.csv\")\ndetails <- fromJSON(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breed-details.json\")\n\nhead(overall)\n## # A tibble: 6 × 9\n##   `Chicken Breed Name`    `Egg Production` `Egg Color` `Cold Hardy` `Heat Hardy`\n##   <chr>                   <chr>            <chr>       <chr>        <chr>       \n## 1 Austra White            220-280 eggs pe… Cream       Good         Good        \n## 2 Ayam Cemani             80-100 per year  White       Good         <NA>        \n## 3 Barnevelder             150-200 eggs pe… Dark Brown  Poor         <NA>        \n## 4 Barred Cochin Bantam    Fair             Brown       Very         <NA>        \n## 5 Barred Cochin Standard  110-160 eggs pe… Brown       Very         <NA>        \n## 6 Barred Old English Ban… 120 eggs per ye… Cream       Good         <NA>        \n## # ℹ 4 more variables: Purpose <chr>, Broody <chr>, `Mating Ratio` <chr>,\n## #   `Roost Height` <chr>\nhead(details)\n##                             name\n## 1           Austra White Chicken\n## 2            Ayam Cemani Chicken\n## 3            Barnevelder Chicken\n## 4           Barred Cochin Bantam\n## 5 Barred Cochin Standard Chicken\n## 6 Barred Old English Game Bantam\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              description\n## 1 Our Austra Whites are a cross between one of our best Cackle HatcheryÂ® production/bloodlines of Black Australorp rooster and one of our best Cackle HatcheryÂ® production/bloodlines of White Leghorn hens (parent stock). They were first developed in the early 1900â\\u0080\\u0099s. This cross produces offspring that are very good layers throughout the year and lay a very light brown to off white egg shell color egg. The Austra White pullet will be white with little black specks in some feathers. This cross is considered to be a heavier breed and their disposition is calmer than the pure Leghorn chicken breed. There are many benefits to raising baby chickens from this cross. These hens will lay a lot of large eggs; have good feed to egg production ratios and good for chickens for backyards. Raising chickens for eggs at home can be fun and relaxing. Free range chicken eggs are great tasting eggs and this hybrid chicken breed has good reflexes for predator avoidance, however, they are white and easily seen.\\nWe also offer at limited times of the year Austra White Fertile Hatching Eggs\n## 2                                    The breed originated from the island of Java, Indonesia and has probably been used for centuries in religious and mystical purposes. The breed was first described by Dutch colonial settlers and first imported to Europe in 1998 by Dutch breeder Jan Steverink. Their beak and tongue, comb and wattles, even their meat bones and organs appear black. The blood of the Ayam Cemani is normally colored. The birdâ\\u0080\\u0099s black color occurs as a result of excess pigmentation of the tissues, caused by a genetic condition known as fibro melanosis. This gene is also found in some other black fowl breeds. Roosters can get some mulberry upon maturity due to testosterone and other influences. The hens lay cream-colored eggs, although they are poor setters and rarely hatch their own brood. Our Ayam Cemaniâ\\u0080\\u0099s bloodline includes Raven and some Greenfire.\\n30% will have white color leakages in tongue, mouth and toes.\\nWe cannot guarantee the distribution of black pigment on chicks.\\nWe now have Ayam Cemani Fertile Hatching Eggs for sale click here!\n## 3                                                                                                                                                                                                                                                                                                                                                                          The Barnevelder chicken originates from the Barneveld region of Holland and known for laying a dark brown egg. This beautiful bird has a single comb, is hardy and quiet and doesnâ\\u0080\\u0099t mind being confined. The breed was first recognized by the American Standard of Perfection in 1991. Cackle HatcheryÂ®â\\u0080\\u0099s Barnevelders breeding stock will produce feathering of partridge single laced andÂ  double laced feather pattern. Each year breeding season our objective is to breed more for the double laced pattern. The Barnevelder chicken is rare to find in the USA but becoming more popular each year. To buy Barnevelder chickens, please select your quantity under 50 above.\\nAlso may likeÂ Dark Brown Egg Female Surplus.\n## 4                                                                                                                                                                                                                                                                                                                                                                                          At Cackle HatcheryÂ®, we offer several different types of the Cochin Bantam, including the Barred Cochin Bantam. If youâ\\u0080\\u0099re not familiar with this chicken breed, it is a miniature version of the Standard Cochin. For more than a century these chickens were admitted to the American Poultry Standard of Perfection, and they make great pets and mothers for chicks. The standard version of the breed, the Barred Cochin Bantam, is an excellent choice, so place your order today. For more details about the Barred Cochin Chicken, please contact us!\\nMany people who like this breed of chicken also like the standard version of the breed, theÂ Barred Cochin Chicken.\\nAlso may likeÂ Cochin Bantam Special Surplus.\n## 5                                                                                                                                          When it comes to rare breed chickens, the professionals at Cackle HatcheryÂ® have a lot to offer. We have more than 200 breeds to choose from, including the Barred Cochin Standard. This bird is one of the many color types of Cochins that we have available and it is notable for several reasons. This is a very large chicken with a lot of unique feathering and feathered legs. This chicken is also great around children, making it a perfect pet for the family farm. This is a very hard color to find of the standard cochin with very few breeders in the USA. We further improved our flock by adding some of Roland Doerr bloodline into our flock in 2009. Make a great show and exhibition type chicken. You can place your order today or you can call us for more information.\\nMany people who like this breed of chicken also like the miniature version of the breed (bantam), theÂ Barred Cochin Bantam Chicken.\\nAlso, may likeÂ Cochin Standard Surplus Special.\\nÂ\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                             Cackle HatcheryÂ® offers several varieties of the Old English Bantam, a miniature version of the Standard Old English Game chicken. The Barred Old English Game Bantam is just one of many high-quality chicken breeds we have available, and this variety is notable for its black and white spotted coloring. Because these chickens require little space and feed they make perfect pets, and they are generally well behaved. In fact, some Barred Old English Game Bantams can even become so tame that they will sit on your arm. Get started today by placing your order for baby chicks, and contact us if you have questions!\\nAlso may likeÂ Old English Bantam Surplus Special.\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    breed_facts\n## 1                                                                                                                                                                                                                                                                                                                                                                : Not applicable, Â Henâ\\u0080\\u0094â\\u0080\\u00935 lbs\\nRoosterâ\\u0080\\u0094â\\u0080\\u00946 1/2 lbs\\nPulletâ\\u0080\\u00944 1/2 lbs\\nCockerelâ\\u0080\\u0094-5 1/2 lbs, Â Primary production, Egg Laying & Pet/Secondary meat source, Very light brown to off white egg shell color, Â Â 220-280 eggs per year (estimates only, seeÂ FAQ), Â Large, Â Active, Â 80-85%, Fertility Percentage:Â 65-80%, Non Setter, Â 12 Females to 1 Males, Â 4 to 8 feet, Â Hybrid, Â No, Â No, Â Cackle HatcheryÂ® Poultry Breeding Farm has been developing our bloodline or strain of Austra White since 1939.\n## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                         Â Not applicable, Â Hen â\\u0080\\u0094â\\u0080\\u0094-4.1/2 lbs\\nRoosterâ\\u0080\\u0094-7 lb\\nPulletâ\\u0080\\u0094â\\u0080\\u00944Â lbs\\nCockerelâ\\u0080\\u00945 lbs, Â Ornamental/ Meat and Egg, Â Cream, Light tan, Â 80-120 per yearÂ (estimates only, seeÂ FAQ), Docile, Â 8 Females to 1 Male, Â 4+ feet, Yes sometimes, Â Java, Indonesia, Â No, Â Not listed, Breeder Farm Source:Â Cackle HatcheryÂ® Poultry Breeding Farm has been developing our bloodline or strain of pure Ayam Cemani since 2018\n## 3                                                                                                                                                                                                                                                                                                Â Continental Class, Weights:Â Henâ\\u0080\\u0094â\\u0080\\u00936 lbs\\nRoosterâ\\u0080\\u0094â\\u0080\\u00947 lbs\\nPulletâ\\u0080\\u0094-5 lbs\\nCockerelâ\\u0080\\u0094â\\u0080\\u00936 lbs, Â Egg Laying; Exhibition, Â Dark Brown, Â 150-200 eggs per year (estimates only, see FAQ), Â Large, Â Active, Â 80-85%, Fertility Percentage:Â 65-80%, Broody:Â Non Setter, Â 7 Females to 1 Male, Â 2 to 4 feet, Country of Origin:Â Holland, APA:Â Yes, Recognized by the American Standard of Perfection, TLC:Â Not Listed, BREEDER FARM SOURCE:Â Cackle HatcheryÂ® Poultry Breeding Farm has been developing our bloodline/strain of pure Barnevelder chickens since 2008.\n## 4                                                                                                                                                                                                                                                                                                                                                                           Feather Legged Bantams, Hen â\\u0080\\u0094â\\u0080\\u0094-26 oz Â Â Â Â Â Â Roosterâ\\u0080\\u0094â\\u0080\\u009330 oz, Purpose and Type:Â Pets,Very Broody, Ornamental; Exhibition, Egg Shell Color:Â Brown Bantam Sized Egg, Egg Production:Â Fair, Egg Size:Â Small, Â Docile, 75-80%, Â 40-55%, Broody:Â Setters, Â 6 Females to 1 Male, Â 0 to 2 feet, Country of Origin:Â Asia, Yes, Recognized by the Standard of Perfection in 1965, No, â\\u0080\\u009c Cackle HatcheryÂ® Poultry Breeding Farmâ\\u0080\\u009d developing our bloodline or strain of pure Cochin Bantams since 1971.\n## 5 Â Asiatic Class, Weights â\\u0080\\u0093Â Henâ\\u0080\\u0094â\\u0080\\u00938 1/2 lbs\\nRoosterâ\\u0080\\u0094â\\u0080\\u009411 lbs\\nPulletâ\\u0080\\u00947 lbs\\nCockerelâ\\u0080\\u0094-9 lbs, Purpose and Type â\\u0080\\u0093Â OrnamentalÂ and meat; Exhibition, Â Brown, Egg ProductionÂ â\\u0080\\u0093 110-160 eggs per year (*estimates only, seeÂ FAQ), Egg Size:Â Medium-Large, Â Docile, Â 40-55%, Broody:Â Setter, Â 6 Females to 1 Male, Â 0 to 2 feet, Asia, Yes, Recognized by the American Standard of Perfection in 1982., Recovering Status, Considered a sustainable heritage chicken breed, Â â\\u0080\\u009cCackle HatcheryÂ®Â Poultry Breeding Farmâ\\u0080\\u009d developing our bloodline or strain of pure color varieties of standard size Cochin chickens since 1975., Breeder Farm Source:Â Cackle HatcheryÂ®Â Poultry Breeding Farmâ\\u0080\\u009d developing our bloodline or strain of pure color varieties of standard size Cochin chickens since 1975.\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Game Bantams, â\\u0080\\u0093Â Hen â\\u0080\\u0094â\\u0080\\u0094-22 ozÂ Â Â Â Â Â Â Â Roosterâ\\u0080\\u0094-24 oz\\nPulletâ\\u0080\\u0094â\\u0080\\u009420 ozÂ Â Â Â Â Â Â Â Cockerelâ\\u0080\\u009422 oz, Purpose and TypeÂ â\\u0080\\u0093 Ornamental; Exhibition, Egg Shell ColorÂ â\\u0080\\u0093 Cream or Tinted Bantam Sized Egg, Â â\\u0080\\u0093Â Poor, Egg Size:Â Small, Â Active, Â 40-55%, Broody:Â Setters, Â 9 Females to 1 Male, Â 3+ feet, â\\u0080\\u0093Â Europe, : No, No\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    availability\n## 1                                                    04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, unavailable, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable\n## 2 04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , unavailable, low-availability, unavailable, available, low-availability, available, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, low-availability, low-availability, unavailable, low-availability, low-availability, low-availability, low-availability\n## 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NULL\n## 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NULL\n## 5                     04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , low-availability, low-availability, unavailable, low-availability, low-availability, available, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, available, unavailable, unavailable, unavailable, low-availability, low-availability, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable\n## 6                                                    04/12/2023 , 04/17/2023 , 04/19/2023 , 04/24/2023 , 04/26/2023 , 05/01/2023 , 05/03/2023 , 05/08/2023 , 05/10/2023 , 05/15/2023 , 05/17/2023 , 05/22/2023 , 05/24/2023 , 05/29/2023 , 05/31/2023 , 06/05/2023 , 06/07/2023 , 06/12/2023 , 06/14/2023 , 06/19/2023 , 06/21/2023 , 06/26/2023 , 06/28/2023 , 07/03/2023 , 07/05/2023 , 07/10/2023 , 07/12/2023 , 07/17/2023 , 07/19/2023 , 07/24/2023 , 07/26/2023 , 07/31/2023 , 08/02/2023 , 08/07/2023 , 08/09/2023 , 08/14/2023 , 08/16/2023 , 08/23/2023 , 08/30/2023 , 09/06/2023 , 09/13/2023 , 09/20/2023 , 09/27/2023 , 10/02/2023 , unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, low-availability, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable, unavailable\n##                                                                                                                            videos\n## 1                                                                                       https://www.youtube.com/embed/fnKRESJcpuY\n## 2                                            https://www.youtube.com/embed/AapH24ImQBo, https://www.youtube.com/embed/Y00jzCR7b-s\n## 3 https://www.youtube.com/embed/J7hIxdTgOPc, https://www.youtube.com/embed/sqhz7Rdc_0U, https://www.youtube.com/embed/T7UeVZe3j10\n## 4 https://www.youtube.com/embed/kr7huXt_-Fk, https://www.youtube.com/embed/dwsRHSr5scc, https://www.youtube.com/embed/k82eEH913Y8\n## 5                                            https://www.youtube.com/embed/iNwIbI3xbHA, https://www.youtube.com/embed/HbdlI_AEVQM\n## 6                                            https://www.youtube.com/embed/3tN2xqkoxLc, https://www.youtube.com/embed/dpLJK0Sc2xk\n##                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            reviews\n## 1 364845, 326510, 326419, 172038, 171864, 171778, 171750, 171713, 171601, 171490, 171424, 171417, 171391, 171259, 171219, 170835, 170494, 170468, 170254, 170205, 170136, 5, 5, 5, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 5, NA, NA, NA, NA, NA, NA, Andrea , Millermcnutt , Vinny , Sharon , Chad , Alexa , Lori , Mike , mmeyer , Kasen , Raymond , rhodyray , Sonia  , Chickens R Us , Darrell , Dorothy , Jill, Indiana April 2013 , Michelle, New York May 2013 , Beth Oklahoma February 2015 , Michael Georgia February 2014 , Janie, Missouri May 2014 , 2020-07-17T16:18:08-05:00, 2020-04-18T01:05:55-05:00, 2020-03-13T03:22:27-05:00, 2018-08-08T21:29:59-05:00, 2018-03-08T20:42:45-05:00, 2017-10-30T22:18:53-05:00, 2017-09-08T01:32:20-05:00, 2017-07-29T13:21:09-05:00, 2017-05-15T06:48:03-05:00, 2017-03-08T17:41:26-05:00, 2016-12-06T03:08:46-05:00, 2016-12-03T23:16:45-05:00, 2016-11-06T05:53:01-05:00, 2016-06-16T20:08:04-05:00, 2016-05-19T10:45:33-05:00, 2015-12-25T06:23:42-05:00, 2015-11-30T14:06:01-05:00, 2015-11-27T21:42:12-05:00, 2015-11-12T15:56:18-05:00, 2015-11-10T21:37:58-05:00, 2015-11-09T15:29:51-05:00, Very good, LOOOOVE my Austra White Hen, Good, Thank You, Austra Duds, Got my Chicks!, Nice birds!, Wonderful Chicks, Motherâ\\u0080\\u0099s day surprise , Good, Like this bird, Good Hens, Adventurous birds! , Not your ordinary personalities, BEST\\n, Yum! We used to get , Cackle Hatchery, Cackle Hatchery, Testimonal, Cackle Hatchery, Cackle Hatchery, Very happy to see my 25 hens get here on time. All of them seem, so far happy and healthy., Purchased Austra White pullet last year. I LOOOVE her, she went broody late summer, hatched 2 eggs (which I had ordered) such a good Mommy bird.  Henny Penny is very friendly, she lets me pick her up and she comes running to me when I walk out the door.  She is a precious bird!, Want to say two years later they are great birds hardy and great free rangers. 5 eggs a week from my birds. Will get this breed again. , Got my chicks, all alive and doing well. I had 2 setters and they adopted them as their own. So precious. Thank you, a very pleased customer., I ordered a handful of these last spring and have to admit I am disappointed with them. When they do lay the eggs are super large and a beautiful cream color but mine do not lay often. Two of them are broody every couple months and never seem to lay and the other two only give me eggs once in awhile. My Easter Eggers, Ameraucanas, and Welsummers are by far better layers., Got my baby chicks this morning! All healthy, all doing fantastic! The tracking for my package was spot on with much appreciated detail. I got my chicks shipped to Hawaii which has additional fees and you have to have a higher minimum of birds and it was totally 100% worth it. I will be using your service in the future and will be recommending it to others. Thank you so very much!, I have 3 of these pullets and at about 21 weeks they are just starting to lay.  They are large, handsome and very friendly birds. As youngsters they were always flying up on my shoulders and are very adventurous.  Their eggs are still smallish and are light cream in color.  Iâ\\u0080\\u0099m very happy with them., I received 4 Austra White pullets as part of an order of 16 chicks on March 1st and they have made beautiful hens.  One began laying at 16 weeks of age which is the earliest I have ever heard of.  I would highly recommend this breed and Cackle Hatchery as this flock of hens that also contains 4 Buff Orpingtons, 3 Turkens, 3 Cuckoo Marans and 2 Black Golden Laced Wyandottes is the nicest flock I have ever had in my 50+ years of raising chickens., I ordered 15 chicks at the end of November 2016.  10 EEâ\\u0080\\u0099s and 5\\n Austra Whites.  All 5 of the Whites survived, but I lost 5 EE in January.  The white chicks were more aggrisive then my Easter eggers and separated them for a month until bigger.  I kept them in a large brooder box in an enclosed porch area with an electric chicken heater until April. The whites were very hungry chicks.  Iâ\\u0080\\u0099ve never had a leg horn breed type before.  They are certainly more assertive than my Easter egger, RIR,  and Cinnamon Queens (they lay large to jumbo eggs, but are kind of dumb birds).  Today, Motherâ\\u0080\\u0099s Day, one of my Austra Whites layed a small pretty cream egg!  Looking forward to seeing how big  and how many they  can lay., Good, Love this bird.  Not as skittish or as flighty as many suggest.  One was bullied by a Black Sex Link however I solved the issue by removing the Black Sex Link for three days and now everything is fine.  Lay some of the biggest eggs Iâ\\u0080\\u0099ve ever seen.  Two within the last two weeks measuring 3â\\u0080³ X 2â\\u0080³ (thatâ\\u0080\\u0099s big my friend).  Friendly at times and will eat from my hand., Received chicks in the middle of March and as they age have become more friendly.  Guess it takes time for them to trust humans.  Anyhow they have been consistent layers and one bird in particular has layed two eggs over the last month that measured 3â\\u0080³ X 2â\\u0080³.  Now they are humongeous eggs.  The Austra Whites share a coop with Red and Black Sex link hens are all are doing fine., We ordered 6 Austra Whites and received 7 on time and in excellent, healthy condition. We gave them water with electrolytes right away, and they were so much fun to handle as tiny chicks. They are now about 5 months old and we still have all 7. They havenâ\\u0080\\u0099t started laying yet, but they are very adventurous and have been from the time they were baby chicks. They were the first of all 5 breeds we have to fly out of the trough we kept the babies in, they were the first to scale our fence, they were the first to find their way onto the roof of our house, and they are the only breed we have that will wander far from the house (which kinda worries us because we have had hawks get a couple other breeds, and Austra Whites areâ\\u0080¦wellâ\\u0080¦white.) They definitely keep us on our toes! , Out of 4:  One was very mean at one month, pecking all her siblings so I had to get rid of her.  Another one escaped three weeks ago and never was seen again.  Another is an escape artists and ALWAYS climbs up over the chicken wire.  I have one ordinary one.  Phew!  This breed keeps me hopping., I am now on week number 2 with the 22 Chicks Cackle has sent me. They are all very healthy and beautiful birds. They are growing very nice and canâ\\u0080\\u0099t wait for them to be able to move to the Big coop so they can roam around freely. More orders to come for sure. Friendly FAST Service for sure. Already made a pre-order for some Buff Orpingtons. Can not wait to get them., Yum! We used to get green eggs all the time but my green egg layers have been on strkie lately so all we have now is rose and brown ð\\u009f\\u0099\\u0082 I Love green eggs!, Just wanted to tell you that we received our chicks midday on the 17th..and all were perfectly perfect and adorable! Thanks soooooo much for working with us on a quicker ship date:), They all arrived yesterday in great health and lots of vigor. By far the best order we have ever received from any hatchery. Thanks, Last year I purchased a dozen chickens six barred rocks and six Austra Whites. i am pleased with them. All of these girls are healthy and ornery at a year old. They are just now at nearly a year old starting to lay eggs. All of the eggs i have found have been quite large, surprising for pullets. They are all loaded with personality. With the exception of a few chickens I obtained from neighbors and friends, at least 3/4 of my flock came from Cackle Hatchery. Even the mixed breeds that I hatched out indirectly came from your hatchery since the parent birds were hatched in your facility. We have 23 chickens in all and I am always getting compliments on how good they look, as well as how friendly they are. I just wanted to say thank you for these fine quality birds that I get to enjoy having around. Thank you, Just a quick thank you. Once again your company delivered a box full of live healthy and vibrant chickens. All arrived doing fine. We ordered 60 and weâ\\u0080\\u0099re not disappointed with the chicks in the least. Thanks to Cackle sending a few more chicks for warmth than we ordered.\\nWe ordered Egyptian for the first time. They are the most active chicks I have ever seen. The special heavy assorted was a great bargain. The chicks were even marked as requested!!!. You have earned my business once again. I have shared your catalog with friends and relatives and will continue to do so. Thanks again and keep up the excellent service., I received my chicks today and found all but 1 made it alive not bad odds for ordering 70 chicks. Thank you for the chicks and I am looking forward to ordering more. \n## 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       607315, 607316, Cackle Hatchery , Cackle Hatchery , 2022-10-05T11:35:27-05:00, 2022-10-05T11:36:06-05:00, Oh no so sorry to hear that. As stated on website possible white on wings, though rare,  is normal and always go black by first molt and 30% will have white color leakages in tongue, mouth and toes.\\nWe cannot guarantee the distribution of black pigment on chicks., Oh no so sorry to hear that. As stated on website possible white on wings, though rare, is normal and always go black by first molt and 30% will have white color leakages in tongue, mouth and toes. We cannot guarantee the distribution of black pigment on chicks.\n## 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  172155, 171588, 171517, 170545, 170528, 5, NA, NA, NA, NA, Mtviewranch , AnaMaria , T Rathjen , Jerry, Arizona March 2012 , Bill, Missouri February 2013 , 2019-01-07T02:35:08-05:00, 2017-05-05T20:35:00-05:00, 2017-03-20T19:00:11-05:00, 2015-12-02T21:11:34-05:00, 2015-11-30T17:33:10-05:00, Love them!!, Beautiful Birds, Beautiful Birds, Chicken order, Thank You, Iâ\\u0080\\u0099ve ordered from you guys twice now and I have not lost one chick. In the two orders you sent extra girls for warmth I think 7 all together bonus! Beautiful birds my barnevelders just started laying large dark speckled eggs. They have great personalities I highly recommend this breed., My 4 Barnevelders are 3 weeks old. Theyâ\\u0080\\u0099ve arrived in 24 hours to Atlanta, in great health. They seem to grow by the second, but weâ\\u0080\\u0099ve had no problems since their arrival. They are beutiful, full of personality, friendly and well behaved. Im very happy. Thank you. I will try to post a follow ip when they are older., I purchased my chicks in Sept of 2016.  Everyone of them were healthy and lived.  I purchased a combination of Sussex and Wyandotteâ\\u0080\\u0099s.  They were all hand raised and are very friendly and love to â\\u0080\\u009ctalkâ\\u0080\\u009d to you.  We had no issues with health or poop and they have grown into beautiful, large birds, and produce plenty of eggs.  We will definitely buy from you again when we are ready to expand our flock., I just wanted to send you some feedback on an order you sent me. They arrived all healthy and are doing great!!! My wife and I wanted to thank you for the way you handled the order and the quality of the chicks you sent us., Iâ\\u0080\\u0099m just wanting to express my thanks to you for the five different breeds of chicks that I got from your hatchery Feb 12th. They are doing quite well, one or two of the chicks has a soft poop but still seem to be full of energy. Being I never experienced chicks before, I was really surprised at how fast they grow. On the fifth day I had to extend my wall of the brooding area because they would jump and or fly over my twelve inch high pen. I did lose one of the barred rocks around the third or fourth day. I actually expected to lose two or three but I am blessed to have lost only one. Thanks for good service, information and kindly taking the time to answer my many questions.\n## 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     325390, 325352, 325322, 325320, 325106, 170959, 170836, 5, 5, 3, 5, 2, NA, NA, farmr john , Susannah C , mudman , Crystal , Grace , Sylvia June 2008 , Fares , 2019-10-31T16:58:50-05:00, 2019-10-30T16:33:57-05:00, 2019-10-29T21:24:13-05:00, 2019-10-29T21:00:55-05:00, 2019-07-01T14:24:36-05:00, 2016-01-11T16:37:32-05:00, 2015-12-25T06:46:45-05:00, Awesome birds, Beautiful, sweet birds / consistent layers, review, Beautiful chickens and fun pets !, Not my favs, Cackle Hatchery, We had a great year , These birds are awesome . They are lively from the start unlike to golden laced cochins. The show type bred into these birds is incredible. I have 6 pullets and 3 cockerels that could stand lots of competition. They are also heavy egg layers. So very impressed with them. They should be priced at double the cost!, We brought in barred cochin bantams last spring, and with thoughtful, gentle interaction, they have grown up to be beautiful, friendly, personable birds with very sweet personalities. I have many breeds in my flock, all with their own charm, but I do think these two Barred Cochins are perhaps the sweetest, cuddliest birds Iâ\\u0080\\u0099ve ever had.  Highly recommend for a gentle flock. (We have Polish, Silkies, Seramas, Brahmas, Wyandottes, Ameraucanas.) These birds arenâ\\u0080\\u0099t bullies and would not do well with super aggressive flockmates., nice breed doing well, We absolutely love our Barred Cochin Bantams!  I bought them for my children to raise as beloved pets.  They are so beautiful and sweet tempered.  They love to be held and fed worms.  , We bought a few different kinds of bantams and these were definitely my least favorite. One died in transit, one died two days after we got them, one was a rooster, and one is a nice little pullet. We bought sexed females. All-in-all I like the one that we ended up with, but did not like how sickly the rooster and the one that died were. Our other bantams that we received, golden laced Cochin, Rhode island red, and barred Rock, have all been healthy and seem much stronger., Just wanted you to know how pleased I am with my chickens from your company. They arrived in great health and have grown into these beauties. Many Thanks!, We had a great year at county fair.  This was our fifth year doing 4-H, but our first year being on the hoetaemsd with 4-H.  We have done static (the cooking, rockets, sewing, etc) exhibits before and last year we took 3 meat goats, but we had to keep them a t a friends house for 2 months prior to fair.  Last year se moved to a rental house 1 week before fair and didnâ\\u0080\\u0099t get our statics turned in.  So this year, now that we have a few acres we got a little carried away and took 38 statics   between 3 kids, and we took 3 dairy goats, 5 meat goats, 3 sheep, 6 chickens and 1 rabbit. Whoo!  that was probably a bit much!  We had a great time and the kids received 100 ribbons, plaques and awards in 4-H and our family got another 15 in open class.  We tried a lot of new things and the kids were pleased that for our first year in poultry they got either champion or reserve in showmanship.  We found out we donâ\\u0080\\u0099t like sheep, and we like the chickens more than we thought we would.  We love 4-H and will be incorporating 4-H projects into our homeschool so that most of the projects are done by June and we donâ\\u0080\\u0099t have the summer scramble to finish and then we can just concentrate on the livestock.\n## 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      326461, 326272, 326143, 171438, 170950, 5, 5, 5, NA, NA, Garcia  , PonyGirl , Ellen , Debbie North Carolina Jan 2017 , Larry December 2015 , 2020-04-02T06:12:50-05:00, 2020-01-11T22:50:46-05:00, 2019-12-10T23:38:08-05:00, 2017-01-04T15:23:10-05:00, 2016-01-11T15:51:18-05:00, Great Birds , Love my cacklehatchery chickens, Beautiful Birds, Cust Response, Cackle Hatchery, Great, beautiful, and wonderful mothers. My hen was only about 7 months old and she became broody and hatched out 7 chicks. They are great hens and letâ\\u0080\\u0099s not forget the roosters! He is very protective of his hens but at the same time, he is very nice and gentle and likes to be held by humans. Great bred for first time chicken owners or if you live in cold areas like colorado , Our chickens arrived in excellent condition and have all been incredibly healthy.  The are now 6 months old and absolutely beautiful. I get compliments on how pretty my cochins are all the time.  I highly recommend buying all your chickens from Cacklehatchery.com!!! , Ordered Barred Cochins. I love these birds. Big, sturdy birds that are beautiful, fully feathered down to their feet, and so sweet. Hands down my favorite breed!, Just wanted to say that the Standard Golden Laced Cochins and the Standard Barred Cochins I ordered and received back in April are looking just beautiful! Very beautiful chickens, with great feathering! I am pleased with both breeds, and am especially excited about the standard sized Barred Cochins because it was only when I saw them on your list of cochins did I realize that the Barred even existed in the standard size! Thank you again, I would and do recommended your hatchery to my friends and acquaintances., Liked your youtube videos!\n## 6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          170939, Emily Ohio April 2008 , 2016-01-11T15:27:50-05:00, Thanks You Cackle Hatchery, I received my order this morning and could not be more pleased! I ordered 30 chicks and received 57. They are beautiful! Also I want to thank you for offering a discount to 4-Hers! I really appreciate it. I will be showing my birds at the fair this year. You folks also have AWESOME customer service, so keep up the good work! You have won my business!\n\nbreed_details <- details |>\nunnest(c(\"name\", \"description\")) |>\n  mutate(name = str_remove_all(name, \" Chicken\") |>\n           str_remove_all(\"[^\\\\x00-\\\\x7F]+\") |> \n           str_remove_all(\"Standard|Game\") |>\n           str_replace_all(\"D’\", \"d\") |>\n           str_to_title() |>\n           str_squish())\n\noverall <- overall |>\n  mutate(name = `Chicken Breed Name` |>\n           str_remove_all(\"[^\\\\x00-\\\\x7F]+\") |>\n           str_remove_all(\"Standard|Game\") |>\n           str_replace_all(\"D’\", \"d\") |>\n           str_to_title() |>\n           str_squish())\n\n# Names aren't exactly the same, but close enough after some minor string manipulation\nanti_join(overall, breed_details)\n## # A tibble: 7 × 10\n##   `Chicken Breed Name`    `Egg Production` `Egg Color` `Cold Hardy` `Heat Hardy`\n##   <chr>                   <chr>            <chr>       <chr>        <chr>       \n## 1 Buff Ameraucanas        180-200 eggs pe… Blue        Very         <NA>        \n## 2 Red Broiler             Poor             Brown       Good         Good        \n## 3 French Cuckoo Marans    180+ eggs per y… Dark Brown  Moderate     Moderate    \n## 4 Saipan Jungle Fowl      Poor             Light Brown Poor         <NA>        \n## 5 Splash Old English Ban… Poor             Cream       Good         <NA>        \n## 6 White Crested Blue Pol… Good             White       Poor         <NA>        \n## 7 White Rock              200 – 280 eggs … Brown       Very         <NA>        \n## # ℹ 5 more variables: Purpose <chr>, Broody <chr>, `Mating Ratio` <chr>,\n## #   `Roost Height` <chr>, name <chr>\nanti_join(breed_details, overall)\n## # A tibble: 5 × 6\n##   name                description        breed_facts availability videos reviews\n##   <chr>               <chr>              <list>      <list>       <list> <list> \n## 1 Blue Polish         \"When it comes to… <df>        <df>         <chr>  <df>   \n## 2 Buff Ameraucana     \"Buff Ameraucana … <df>        <named list> <chr>  <df>   \n## 3 Cackles Red Broiler \"A great alternat… <df>        <named list> <chr>  <df>   \n## 4 Saipan              \"Saipan chickens … <df>        <df>         <chr>  <df>   \n## 5 White Plymouth Rock \"The White Plymou… <df>        <df>         <chr>  <df>\n\nchickens <- full_join(overall, breed_details)\nhead(chickens)\n## # A tibble: 6 × 15\n##   `Chicken Breed Name`    `Egg Production` `Egg Color` `Cold Hardy` `Heat Hardy`\n##   <chr>                   <chr>            <chr>       <chr>        <chr>       \n## 1 Austra White            220-280 eggs pe… Cream       Good         Good        \n## 2 Ayam Cemani             80-100 per year  White       Good         <NA>        \n## 3 Barnevelder             150-200 eggs pe… Dark Brown  Poor         <NA>        \n## 4 Barred Cochin Bantam    Fair             Brown       Very         <NA>        \n## 5 Barred Cochin Standard  110-160 eggs pe… Brown       Very         <NA>        \n## 6 Barred Old English Ban… 120 eggs per ye… Cream       Good         <NA>        \n## # ℹ 10 more variables: Purpose <chr>, Broody <chr>, `Mating Ratio` <chr>,\n## #   `Roost Height` <chr>, name <chr>, description <chr>, breed_facts <list>,\n## #   availability <list>, videos <list>, reviews <list>\n\n\n\n\nimport pandas as pd\noverall = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breeds.csv\")\ndetails = pd.read_json(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/chicken-breed-details.json\")\n\noverall['name'] = overall['Chicken Breed Name'].str.replace(\"[^\\x00-\\x7F]|Standard|Game|Chicken\", '', regex=True).str.replace(\"D’\", \"d\").str.replace(\"\\s{1,}\", \" \", regex = True).str.title().str.strip()\n\ndetails = details.explode(['name', 'description'])\n\ndetails['name'] = details['name'].str.replace(\"[^\\x00-\\x7F]|Standard|Game|Chicken\", '', regex=True).str.replace(\"D’\", \"d\").str.replace(\"\\s{1,}\", \" \", regex = True).str.title().str.strip()\n  \nchickens = overall.merge(details, how='outer', on = 'name',  indicator=True)\nchickens[(chickens._merge!='both')][['name', '_merge']]\n##                           name      _merge\n## 52            Buff Ameraucanas   left_only\n## 61                 Red Broiler   left_only\n## 90        French Cuckoo Marans   left_only\n## 135         Saipan Jungle Fowl   left_only\n## 157  Splash Old English Bantam   left_only\n## 172  White Crested Blue Polish   left_only\n## 181                 White Rock   left_only\n## 186                Blue Polish  right_only\n## 187            Buff Ameraucana  right_only\n## 188        Cackles Red Broiler  right_only\n## 189                     Saipan  right_only\n## 190        White Plymouth Rock  right_only\nchickens.head()\n##        Chicken Breed Name  ... _merge\n## 0            Austra White  ...   both\n## 1             Ayam Cemani  ...   both\n## 2             Barnevelder  ...   both\n## 3    Barred Cochin Bantam  ...   both\n## 4  Barred Cochin Standard  ...   both\n## \n## [5 rows x 16 columns]\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: Cleaning Chicken Data\n\n\n\n\n\nProblem\nR solution\nPython\n\n\n\nUnnest the chicken breed facts data, cleaning the responses. Which jobs are most suitable for a functional programming approach?\n\n\n\n# Column names in breed_facts are too different\n# chickens_exp <- chickens |> unnest('breed_facts', names_sep='facts')\n\nfix_names <- function(df) {\n  if (!is.null(df)) {\n    names(df) <- names(df) |>\n      str_to_title() |>\n      str_remove_all(\"[^A-z]\") |> # Remove anything that isn't A-z, including spaces.\n      str_replace_all(c(\"CountryOfOrigin?\" = \"Origin\", \"Weights\" = \"Weight\", \"Tlc\" = \"TLC\", \"Albc\" = \"ALBC\", \"Apa\" = \"APA\", \"BroodyS\" = \"Broody\", \"Temperment\" = \"Temperament\", \"Broody\" = \"Broody_facts\", \"Purpose\" = \"Purpose_facts\")) |>\n      str_remove_all(\"Shell|FarmSource|SourceFarm|Small|PoultryShow\") |>\n      str_replace_all(\"^$\", \"xxx\") # replace blank names with xxx\n    df\n  } else {\n    return(NULL) \n  }\n}\nchickens_fix <- chickens |> \n  mutate(breed_facts = map(breed_facts, fix_names))\n\n# Test names\nchickens_fix$breed_facts %>% map(names) |> unlist() |> unique()\n##  [1] \"Class\"                \"Weight\"               \"Purpose_factsAndType\"\n##  [4] \"EggColor\"             \"EggProduction\"        \"EggSize\"             \n##  [7] \"Temperament\"          \"GenderAccuracy\"       \"FertilityPercentage\" \n## [10] \"Broody_facts\"         \"MatingRatio\"          \"RoostHeight\"         \n## [13] \"Origin\"               \"APA\"                  \"TLC\"                 \n## [16] \"Breeder\"              \"Purpose_facts\"        \"xxx\"                 \n## [19] \"ALBC\"                 \"Rooster\"              \"Pullet\"              \n## [22] \"Cockerel\"             \"Exhibition\"\n\nWe’ve fixed some of the misspellings and duplications. Rooster, Pullet, and Cockerel are all likely to be parsing issues stemming from Weight, but that’s the reality of working with data that is gathered from the internet.\n\nchickens_exp <- chickens_fix |> unnest(\"breed_facts\")\n\nhead(chickens_exp[,c(1, 16:37)])\n## # A tibble: 6 × 23\n##   `Chicken Breed Name`      EggProduction     EggSize Temperament GenderAccuracy\n##   <chr>                     <chr>             <chr>   <chr>       <chr>         \n## 1 Austra White              \"Â Â 220-280 egg… Â Large Â Active    Â 80-85%      \n## 2 Ayam Cemani               \"Â 80-120 per ye… <NA>    Docile      <NA>          \n## 3 Barnevelder               \"Â 150-200 eggs … Â Large Â Active    Â 80-85%      \n## 4 Barred Cochin Bantam      \"Egg Production:… Egg Si… Â Docile    75-80%        \n## 5 Barred Cochin Standard    \"Egg ProductionÂ… Egg Si… Â Docile    <NA>          \n## 6 Barred Old English Bantam \"Â â\\u0080\\u0093… Egg Si… Â Active    <NA>          \n## # ℹ 18 more variables: FertilityPercentage <chr>, Broody_facts <chr>,\n## #   MatingRatio <chr>, RoostHeight <chr>, Origin <chr>, APA <chr>, TLC <chr>,\n## #   Breeder <chr>, Purpose_facts <chr>, xxx <chr>, ALBC <chr>, Rooster <chr>,\n## #   Pullet <chr>, Cockerel <chr>, Exhibition <chr>, availability <list>,\n## #   videos <list>, reviews <list>\n\nThere’s still quite a bit of cleaning left to do to get this data to be “pretty”.\n\ntidy_col <- function(x, text = \"(?:\\\\(estimates only, see FAQ\\\\))|(?:^APA)|(?:^TLC)|EggSize|(?:Fertility Percentage)|(?:Purpose and Type)\") {\n  str_remove_all(x, \"[\\u0600-\\u06FF]\") |> # Remove non-ascii characters\n    str_remove_all(\"[Â®â¢Ââ]\") |>\n    str_remove_all(text) |>\n    str_remove_all(\"[:\\\\.\\\\?!\\\\*]\") |>\n    str_replace_all(\"\\u0094\", \"-\") |>\n    str_replace_all(\"-{1,}\", \"-\") |>\n    str_squish()\n}\n\ntmp <- mutate(chickens_exp, across(Class:Purpose_facts, tidy_col))\n\nhead(select(tmp, 1, Class:Purpose_facts))\n## # A tibble: 6 × 18\n##   `Chicken Breed Name`  Class Weight Purpose_factsAndType EggColor EggProduction\n##   <chr>                 <chr> <chr>  <chr>                <chr>    <chr>        \n## 1 Austra White          Not … Hen-5… Primary production,… Very li… 220-280 eggs…\n## 2 Ayam Cemani           Not … Hen -… <NA>                 Cream, … 80-120 per y…\n## 3 Barnevelder           Cont… Weigh… Egg Laying; Exhibit… Dark Br… 150-200 eggs…\n## 4 Barred Cochin Bantam  Feat… Hen -… Pets,Very Broody, O… Egg She… Egg Producti…\n## 5 Barred Cochin Standa… Asia… Weigh… Ornamental and meat… Brown    Egg Producti…\n## 6 Barred Old English B… Game… Hen -… Ornamental; Exhibit… Egg She… Poor         \n## # ℹ 12 more variables: EggSize <chr>, Temperament <chr>, GenderAccuracy <chr>,\n## #   FertilityPercentage <chr>, Broody_facts <chr>, MatingRatio <chr>,\n## #   RoostHeight <chr>, Origin <chr>, APA <chr>, TLC <chr>, Breeder <chr>,\n## #   Purpose_facts <chr>\n\nIf we consider the use of across() as a functional programming technique (which it is), then it is much easier to create a generic tidy_col function than to tidy each column individually. There are probably a few things we’ve missed, but the data looks decent for the amount of time we put in.\n\n\n\nimport pandas as pd\n\nXXX TODO"
  },
  {
    "objectID": "part-wrangling/08-functional-prog.html#references",
    "href": "part-wrangling/08-functional-prog.html#references",
    "title": "25  Functional Programming",
    "section": "\n25.6 References",
    "text": "25.6 References\n\n\n\n\n[1] \nA. M. Kuchling, “Functional programming HOWTO. cPython documentation,” Oct. 31, 2022. [Online]. Available: https://docs.python.org/3/howto/functional.html. [Accessed: Mar. 20, 2023]\n\n\n[2] \nR.-G. Urma, M. Fusco, and A. Mycroft, Modern java in action: Lambdas, streams, functional and reactive programming, 2nd edition. Shelter Island: Manning, 2018. \n\n\n[3] \nM. Fogus, Functional JavaScript: Introducing functional programming with underscore.js, 1st edition. Sebastopol, CA: O’Reilly Media, 2013. \n\n\n[4] \nE. Buonanno, Functional programming in c#: How to write better c# code, 1st edition. Shelter Island, NY: Manning, 2017. \n\n\n[5] \nE. Akin, Object-oriented programming via fortran 90-95. Cambridge University Press, 2003 [Online]. Available: http://gen.lib.rus.ec/book/index.php?md5=2d996a202fdd840e79a791a4eb854432\n\n\n\n[6] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[7] \nrituraj_jain, “Nested list comprehensions in python. GeeksforGeeks,” Nov. 07, 2018. [Online]. Available: https://www.geeksforgeeks.org/nested-list-comprehensions-in-python/. [Accessed: Mar. 20, 2023]\n\n\n[8] \nHadley Wickham, Lionel Henry, and RStudio, “Purrr <-> base r. Purrr base r,” Oct. 10, 2022. [Online]. Available: https://purrr.tidyverse.org/articles/base.html. [Accessed: Apr. 10, 2023]\n\n\n[9] \nS. Seabold and J. Perktold, “Statsmodels: Econometric and statistical modeling with python,” in 9th python in science conference, 2010 [Online]. Available: https://conference.scipy.org/proceedings/scipy2010/pdfs/seabold.pdf\n\n\n\n[10] \nF. Pedregosa et al., “Scikit-learn: Machine learning in python,” Journal of Machine Learning Research, vol. 12, pp. 2825–2830, 2011. \n\n\n[11] \nA. Menon, “Linear regression in 6 lines of python. Medium,” Oct. 25, 2018. [Online]. Available: https://towardsdatascience.com/linear-regression-in-6-lines-of-python-5e1d0cd05b8d. [Accessed: Oct. 17, 2022]\n\n\n[12] \nWikipedia contributors, “JSON,” Wikipedia. Apr. 05, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=JSON&oldid=1148380721. [Accessed: Apr. 10, 2023]"
  },
  {
    "objectID": "part-wrangling/09-spatial-formats.html#references",
    "href": "part-wrangling/09-spatial-formats.html#references",
    "title": "26  Spatial data",
    "section": "26.1 References",
    "text": "26.1 References"
  },
  {
    "objectID": "graveyard.html#sec-shell-commands",
    "href": "graveyard.html#sec-shell-commands",
    "title": "27  Other Topics",
    "section": "\n27.1 Shell Commands",
    "text": "27.1 Shell Commands\nWhen talking to computers, sometimes it is convenient to cut through the graphical interfaces, menus, and so on, and just tell the computer what to do directly, using the system shell (aka terminal, command line prompt, console).\nMost system shells are fully functioning programming languages in their own right. This section isn’t going to attempt to teach you those skills - we’ll focus instead on the basics - how to change directories, list files, and run programs.\n\n27.1.1 Launching the system terminal\nIn RStudio, you can access a system terminal in the lower left corner by clicking on the tab labeled Terminal. If the tab does not exist, then go to Tools -> Terminal -> New Terminal in the main application toolbar.\nSometimes, it is preferable to launch a terminal separate from RStudio. Here’s how to do that:\n\n\n Windows\n Mac\n Linux\n\n\n\nOption 1: Default Windows terminal (cmd.exe)\n\nGo to the search bar/start menu\nType in cmd.exe\nA black window should appear.\n\nOption 2: Git bash (if you have git installed)\n\nGo to the search bar/start menu\nType in bash\nClick on the Git Bash application\n\nIf you choose option 2, use the commands for Bash/Linux below. Bash tends to be a bit less clunky than the standard windows terminal.\n\n\nOption 1: Dock\n\nClick the launchpad icon\nType Terminal in the search field\nClick Terminal\n\nOption 2: Finder\n\nOpen the Applications/Utilities folder\nDouble-click on Terminal\n\n\n\nOn most systems, pressing Ctrl-Alt-T or Super-T (Windows-T) will launch a terminal.\nOtherwise, launch your system menu (usually with the Super/Windows key) and type Terminal. You may have multiple options here; I prefer Konsole but I’m usually using KDE as my desktop environment. Other decent options include Gnome-terminal and xterm, and these are usually associated with Gnome and XFCE desktop environments, respectively.\n\n\n\n\n27.1.2 File Path Structure\nOn Windows, file paths are constructed as follows: C:\\Folder 1\\Folder_2\\file.R. Paths are generally not case sensitive, so you can reference the same file path as c:\\folder 1\\folder_2\\file.R. Usually, paths are encased in \"\" because spaces make interpreting file paths complicated and Windows paths have lots of spaces.\nOn Unix systems, file paths are constructed as follows: /home/user/folder1/folder2/file.R. Paths are case sensitive, so you cannot reach /home/user/folder1/folder2/file.R if you use /home/user/folder1/folder2/file.r. On Unix systems, spaces in file paths must be escaped with \\, so any space character in a terminal should be typed \\ instead.\nThis quickly gets complicated and annoying when working on code that is meant for multiple operating systems. These complexities are why when you’re constructing a file path in R or python, you should use commands like file.path(\"folder1\", \"folder2\", \"file.r\") or os.path.join(\"folder1\", \"folder2\", \"file.py\"), so that your code will work on Windows, Mac, and Linux by default.\n\n27.1.3 Basic Terminal Commands\nI have listed commands here for the most common languages used in each operating system. If you are using Git Bash on Windows, follow the commands for Linux/Bash. If you are using Windows PowerShell, google the commands.\nIn most cases, Mac/Zsh is similar to Linux/Bash, but there are a few differences1.\n\n\n\n\n\n\n\n\n\nTask\n\n Windows/CMD\n\n Mac/Zsh\n\n Linux/Bash\n\n\n\nList your current working directory\ncd\npwd\npwd\n\n\nChange directory\ncd <path to new dir>\ncd <path to new dir>\ncd <path to new dir>\n\n\nList files and folders in current directory\ndir\nls\nls\n\n\nCopy file\nxcopy <source> <destination> <arguments>\ncp <arguments> <source> <destination>\ncp <arguments> <source> <destination>\n\n\nCreate directory\nmkdir <foldername>\nmkdir <foldername>\nmkdir <foldername>\n\n\nDisplay file contents\ntype <filename>\ncat <filename>\ncat <filename>"
  },
  {
    "objectID": "graveyard.html#sec-math-logic",
    "href": "graveyard.html#sec-math-logic",
    "title": "27  Other Topics",
    "section": "\n27.2 Mathematical Logic",
    "text": "27.2 Mathematical Logic\nIn Chapter 9 and Chapter 11 we talk about more complicated data structures and control structures (for loops, if statements). I’ve included this section because it may be useful to review some concepts from mathematical logic.\nUnfortunately, to best demonstrate mathematical logic, I’m going to need you to know that a vector is like a list of the same type of thing. In R, vectors are defined using c(), so c(1, 2, 3) produces a vector with entries 1, 2, 3. In Python, we’ll primarily use numpy arrays, which we create using np.array([1, 2, 3]). Technically, this is creating a list, and then converting that list to a numpy array.\n\n27.2.1 And, Or, and Not\nWe can combine logical statements using and, or, and not.\n\n(X AND Y) requires that both X and Y are true.\n(X OR Y) requires that one of X or Y is true.\n(NOT X) is true if X is false, and false if X is true. Sometimes called negation.\n\nIn R, we use ! to symbolize NOT, in Python, we use ~ for vector-wise negation (NOT).\nOrder of operations dictates that NOT is applied before other operations. So NOT X AND Y is read as (NOT X) AND (Y). You must use parentheses to change the way this is interpreted.\n\n\nR\nPython\n\n\n\n\nx <- c(TRUE, FALSE, TRUE, FALSE)\ny <- c(TRUE, TRUE, FALSE, FALSE)\n\nx & y # AND\n## [1]  TRUE FALSE FALSE FALSE\nx | y # OR\n## [1]  TRUE  TRUE  TRUE FALSE\n!x & y # NOT X AND Y\n## [1] FALSE  TRUE FALSE FALSE\nx & !y # X AND NOT Y\n## [1] FALSE FALSE  TRUE FALSE\n\n\n\n\nimport numpy as np\nx = np.array([True, False, True, False])\ny = np.array([True, True, False, False])\n\nx & y\n## array([ True, False, False, False])\nx | y\n## array([ True,  True,  True, False])\n~x & y\n## array([False,  True, False, False])\nx & ~y\n## array([False, False,  True, False])\n\n\n\n\n\n27.2.2 De Morgan’s Laws\nDe Morgan’s Laws are a set of rules for how to combine logical statements. You can represent them in a number of ways:\n\nNOT(A or B) is equivalent to NOT(A) and NOT(B)\nNOT(A and B) is equivalent to NOT(A) or NOT(B)\n\n\n\nDefinitions\nDeMorgan’s First Law\nDeMorgan’s Second Law\n\n\n\n\n\nVenn Diagram of Set A and Set B\n\n\nSuppose that we set the convention that .\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A OR B (aka NOT (A OR B)) is the same as the region that is outside of (NOT A) and (NOT B)\n\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A AND B (aka NOT (A AND B)) is the same as the region that is outside of (NOT A) OR (NOT B)"
  },
  {
    "objectID": "graveyard.html#sec-controlling-loops",
    "href": "graveyard.html#sec-controlling-loops",
    "title": "27  Other Topics",
    "section": "\n27.3 Controlling Loops with Break, Next, Continue",
    "text": "27.3 Controlling Loops with Break, Next, Continue\n\n\nSometimes it is useful to control the statements in a loop with a bit more precision. You may want to skip over code and proceed directly to the next iteration, or, as demonstrated in the previous section with the break statement, it may be useful to exit the loop prematurely.\n\n27.3.1 Break Statement\n\n\nA break statement is used to exit a loop prematurely\n\n\n\n27.3.2 Next/Continue Statement\n\n\nA next (or continue) statement is used to skip the body of the loop and continue to the next iteration\n\n\n\n\n\n\n\n\nExample: Next/continue and Break statements\n\n\n\nLet’s demonstrate the details of next/continue and break statements.\nWe can do different things based on whether i is evenly divisible by 3, 5, or both 3 and 5 (thus divisible by 15)\n\n\nR\nPython\n\n\n\n\nfor (i in 1:20) {\n  if (i %% 15 == 0) {\n    print(\"Exiting now\")\n    break\n  } else if (i %% 3 == 0) {    \n    print(\"Divisible by 3\")\n    next\n    print(\"After the next statement\") # this should never execute\n  } else if (i %% 5 == 0) {\n    print(\"Divisible by 5\")\n  } else {\n    print(i)\n  }\n}\n## [1] 1\n## [1] 2\n## [1] \"Divisible by 3\"\n## [1] 4\n## [1] \"Divisible by 5\"\n## [1] \"Divisible by 3\"\n## [1] 7\n## [1] 8\n## [1] \"Divisible by 3\"\n## [1] \"Divisible by 5\"\n## [1] 11\n## [1] \"Divisible by 3\"\n## [1] 13\n## [1] 14\n## [1] \"Exiting now\"\n\n\n\n\nfor i in range(1, 20):\n  if i%15 == 0:\n    print(\"Exiting now\")\n    break\n  elif i%3 == 0:\n    print(\"Divisible by 3\")\n    continue\n    print(\"After the next statement\") # this should never execute\n  elif i%5 == 0:\n    print(\"Divisible by 5\")\n  else: \n    print(i)\n## 1\n## 2\n## Divisible by 3\n## 4\n## Divisible by 5\n## Divisible by 3\n## 7\n## 8\n## Divisible by 3\n## Divisible by 5\n## 11\n## Divisible by 3\n## 13\n## 14\n## Exiting now\n\n\n\n\n\n\nTo be quite honest, I haven’t really ever needed to use next/continue statements when I’m programming, and I rarely use break statements. However, it’s useful to know they exist just in case you come across a problem where you could put either one to use."
  },
  {
    "objectID": "graveyard.html#sec-recursion",
    "href": "graveyard.html#sec-recursion",
    "title": "27  Other Topics",
    "section": "\n27.4 Recursion",
    "text": "27.4 Recursion\nUnder construction.\nIn the meantime, check out [1] (R) and [2] (Python) for decent coverage of the basic idea of recursive functions."
  },
  {
    "objectID": "graveyard.html#sec-text-encoding",
    "href": "graveyard.html#sec-text-encoding",
    "title": "27  Other Topics",
    "section": "\n27.5 Text Encoding",
    "text": "27.5 Text Encoding\nI’ve left this section in because it’s a useful set of tricks, even though it does primarily deal with SAS.\nDon’t know what UTF-8 is? Watch this excellent YouTube video explaining the history of file encoding!\nSAS also has procs to accommodate CSV and other delimited files. PROC IMPORT may be the simplest way to do this, but of course a DATA step will work as well. We do have to tell SAS to treat the data file as a UTF-8 file (because of the japanese characters).\nWhile writing this code, I got an error of “Invalid logical name” because originally the filename was pokemonloc. Let this be a friendly reminder that your dataset names in SAS are limited to 8 characters in SAS.\n/* x \"curl https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv > ../data/pokemon_gen_1-8.csv\";\nonly run this once to download the file... */\nfilename pokeloc '../data/pokemon_gen_1-8.csv' encoding=\"utf-8\";\n\n\nproc import datafile = pokeloc out=poke\n  DBMS = csv; /* comma delimited file */\n  GETNAMES = YES\n  ;\nproc print data=poke (obs=10); /* print the first 10 observations */\n  run;\nAlternately (because UTF-8 is finicky depending on your OS and the OS the data file was created under), you can convert the UTF-8 file to ASCII or some other safer encoding before trying to read it in.\nIf I fix the file in R (because I know how to fix it there… another option is to fix it manually),\n\nlibrary(readr)\nlibrary(dplyr)\ntmp <- read_csv(\"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\")[,-1]\nwrite_csv(tmp, \"../data/pokemon_gen_1-8.csv\")\n\ntmp <- select(tmp, -japanese_name) %>%\n  # iconv converts strings from UTF8 to ASCII by transliteration - \n  # changing the characters to their closest A-Z equivalents.\n  # mutate_all applies the function to every column\n  mutate_all(iconv, from=\"UTF-8\", to = \"ASCII//TRANSLIT\")\n\nwrite_csv(tmp, \"../data/pokemon_gen_1-8_ascii.csv\", na='.')\n\nThen, reading in the new file allows us to actually see the output.\nlibname classdat \"sas/\";\n/* Create a library of class data */\n\nfilename pokeloc  \"../data/pokemon_gen_1-8_ascii.csv\";\n\nproc import datafile = pokeloc out=classdat.poke\n  DBMS = csv /* comma delimited file */\n  replace;\n  GETNAMES = YES;\n  GUESSINGROWS = 1028 /* use all data for guessing the variable type */\n  ;\nproc print data=classdat.poke (obs=10); /* print the first 10 observations */\n  run; \nThis trick works in so many different situations. It’s very common to read and do initial processing in one language, then do the modeling in another language, and even move to a different language for visualization. Each programming language has its strengths and weaknesses; if you know enough of each of them, you can use each tool where it is most appropriate."
  },
  {
    "objectID": "graveyard.html#sec-other-topics-refs",
    "href": "graveyard.html#sec-other-topics-refs",
    "title": "27  Other Topics",
    "section": "\n27.6 References",
    "text": "27.6 References\n\n\n\n\n[1] \nDataMentor, “R recursion. DataMentor,” Nov. 24, 2017. [Online]. Available: https://www.datamentor.io/r-programming/recursion/. [Accessed: Jan. 10, 2023]\n\n\n[2] \nParewa Labs Pvt. Ltd., “Python recursion. Learn python interactively,” 2020. [Online]. Available: https://www.programiz.com/python-programming/recursion. [Accessed: Jan. 10, 2023]"
  }
]