[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Computing using R and Python",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Statistical Computing using R and Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe cover of this book is an amalgam of different images by the lovely @allison_horst, which are released under the cc-by 4.0 license. I have modified them to remove most of the R package references and arrange them to represent the topics covered in this book.\nLaptop icon used in the tab/logo created by Good Ware - Flaticon\nThroughout this book, I have borrowed liberally from other online tutorials, published books, and blog posts. I have tried to ensure that I link to the source material throughout the book and provide appropriate credit to anyone whose examples I have used, modified, or repurposed. Special thanks to the tutorials provided by Posit/RStudio and the tidyverse project."
  },
  {
    "objectID": "how-to-use.html#content-overload",
    "href": "how-to-use.html#content-overload",
    "title": "How to Use This Book",
    "section": "Content Overload!",
    "text": "Content Overload!\nThis book is designed to demonstrate introductory statistical programming concepts and techniques. It is intended as a substitute for hours and hours of video lectures - watching someone code and talk about code is not usually the best way to learn how to code. It’s far better to learn how to code by … coding.\nI hope that you will work through this book week by week over the semester. I have included comics, snark, gifs, YouTube videos, extra resources, and more: my goal is to make this a collection of the best information I can find on statistical programming.\nIn most cases, this book includes way more information than you need. Everyone comes into this class with a different level of computing experience, so I’ve attempted to make this book comprehensive. Unfortunately, that means some people will be bored and some will be overwhelmed. Use this book in the way that works best for you - skip over the stuff you know already, ignore the stuff that seems too complex until you understand the basics. Come back to the scary stuff later and see if it makes more sense to you."
  },
  {
    "objectID": "how-to-use.html#book-format-guide",
    "href": "how-to-use.html#book-format-guide",
    "title": "How to Use This Book",
    "section": "Book Format Guide",
    "text": "Book Format Guide\nI’ve made an effort to use some specific formatting and enable certain features that make this book a useful tool for this class.\n\nButtons/Links\nThe book contains a number of features which should help you navigate, use, improve, and respond to the textbook.\n\n\n\nTextbook features, menus, and interactive options\n\n\n\n\nSpecial Sections\nSome instructions depend on your operating system. Where it’s shorter, I will use tabs to provide you with OS specific instructions. Here are the icons I will use:\n\n Windows Mac Linux\n\n\nWindows-specific instructions\n\n\nMac specific instructions\n\n\nLinux specific instructions. I will usually try to make this generic, but if it’s gui based, my instructions will usually be for KDE.\n\n\n\n\n\n\n\n\n\nWarnings\n\n\n\nThese sections contain things you may want to look out for: common errors, mistakes, and unfortunate situations that may arise when programming.\n\n\n\n\n\n\n\n\nDemonstrations\n\n\n\nThese sections demonstrate how the code being discussed is used (in a simple way).\n\n\n\n\n\n\n\n\nExamples\n\n\n\nThese sections contain illustrations of the concepts discussed in the chapter. Don’t skip them, even though they may be long!\n\n\n\n\n\n\n\n\nTry it out\n\n\n\nThese sections contain activities you should do to reinforce the things you’ve just read. You will be much more successful if you read the material, review the example, and then try to write your own code. Most of the time, these sections will have a specific format:\n\nProblemR SolutionPython Solution\n\n\nThe problem will be in the first tab for you to start with\n\n\nA solution will be provided in R, potentially with an explanation.\n\n\nA solution will be provided in Python as well.\n\n\n\nIn some cases, the problem will be more open-ended and may not adhere to this format, but most try it out sections in this book will have solutions provided. I highly recommend that you attempt to solve the problem yourself before you look at the solutions - this is the best way to learn. Passively reading code does not result in information retention.\n\n\n\n\n\n\n\n\nEssential Reading\n\n\n\nThese sections may direct you to additional reading material that is essential for understanding the topic. For instance, I will sometimes link to other online textbooks rather than try to rehash the content myself when someone else has done it better.\n\n\n\n\n\n\n\n\nLearn More\n\n\n\nThese sections will direct you to additional resources that may be helpful to consult as you learn about a topic. You do not have to use these sections unless you are 1) bored, or 2) hopelessly lost. They’re provided to help but are not expected reading (Unlike the essential reading sections in red).\n\n\n\n\n\n\n\n\nNotes\n\n\n\nThese generic sections contain information I may want to call attention to, but that isn’t necessarily urgent or a common error trap.\n\n\n\n\n\n\n\n\nAdvanced\n\n\n\nThese sections are intended to apply to more advanced courses. If you are taking an introductory course, feel free to skip that content for now.\n\n\n\nExpandable Sections\n\n\nThese are expandable sections, with additional information when you click on the line\n\nThis additional information may be information that is helpful but not essential, or it may be that an example just takes a LOT of space and I want to make sure you can skim the book without having to scroll through a ton of output.\n\n\n\n\n\n\n\nAnother type of expandable note\n\n\n\n\n\nAnswers or punchlines may be hidden in this type of expandable section as well."
  },
  {
    "objectID": "part-tools/00-tools-intro.html",
    "href": "part-tools/00-tools-intro.html",
    "title": "Part I: Tools",
    "section": "",
    "text": "This part of the textbook provides an overview of the different tools we will be using: R, python, quarto, markdown, pandoc, consoles, and so on. It can be a bit confusing at first, especially if you’re not familiar with how your computer works, where files are stored, and different ways to tell your computer what to do.\nChapter 1 gives you some important background material about how a computer functions.\nChapter 2 tells you exactly what software you need to install for the rest of this textbook.\nChapter 3 discusses the different ways we can talk to R and python, and the pros and cons of each."
  },
  {
    "objectID": "part-tools/01-computer-basics.html#fa-bullseye-objectives",
    "href": "part-tools/01-computer-basics.html#fa-bullseye-objectives",
    "title": "1  Computer Basics",
    "section": "1.1  Objectives",
    "text": "1.1  Objectives\n\nKnow the meaning of computer hardware and operating system terms such as hard drive, memory, CPU, OS/operating system, file system, directory, and system paths\nUnderstand the basics of how the above concepts relate to each other and contribute to how a computer works\nUnderstand the file system mental model for computers"
  },
  {
    "objectID": "part-tools/01-computer-basics.html#hardware",
    "href": "part-tools/01-computer-basics.html#hardware",
    "title": "1  Computer Basics",
    "section": "1.2 Hardware",
    "text": "1.2 Hardware\nHere is a short 3-minute video on the basic hardware that makes up your computer. It is focused on desktops, but the same components (with the exception of the optical drive) are commonly found in cell phones, smart watches, and laptops.\n\n\n\n\nWhen programming, it is usually helpful to understand the distinction between RAM and disk storage (hard drives). We also need to know at least a little bit about processors (so that we know when we’ve asked our processor to do too much). Most of the other details aren’t necessary (for now).\n\n\nChapter 1 of Python for Everybody - Computer hardware architecture"
  },
  {
    "objectID": "part-tools/01-computer-basics.html#operating-systems",
    "href": "part-tools/01-computer-basics.html#operating-systems",
    "title": "1  Computer Basics",
    "section": "1.3 Operating Systems",
    "text": "1.3 Operating Systems\nOperating systems, such as Windows, MacOS, or Linux, are a sophisticated program that allows CPUs to keep track of multiple programs and tasks and execute them at the same time."
  },
  {
    "objectID": "part-tools/01-computer-basics.html#file-systems",
    "href": "part-tools/01-computer-basics.html#file-systems",
    "title": "1  Computer Basics",
    "section": "1.4 File Systems",
    "text": "1.4 File Systems\n\nFor this class, it will probably be important to distinguish between local file storage (C:/ drive , /user/your-name/ , or /home/your-name/ ) and network/virtual file systems, such as OneDrive and iCloud. Over time, it has become harder to ensure that you are working on a local machine, but working “in the cloud” can cause odd errors when programming and in particular when working with version control systems1.\nYou want to save your files in this class to your physical hard drive. This will save you a lot of troubleshooting time.\n\nEvidently, there has been a bit of generational shift as computers have evolved: the “file system” metaphor itself is outdated because no one uses physical files anymore. This article [1] is an interesting discussion of the problem: it makes the argument that with modern search capabilities, most people use their computers as a laundry hamper instead of as a nice, organized filing cabinet.\nRegardless of how you tend to organize your personal files, it is probably helpful to understand the basics of what is meant by a computer file system – a way to organize data stored on a hard drive. Since data is always stored as 0’s and 1’s, it’s important to have some way to figure out what type of data is stored in a specific location, and how to interpret it.\n\n\n\n\nThat’s not enough, though - we also need to know how computers remember the location of what is stored where. Specifically, we need to understand file paths.\n\n\n\n\nWhen you write a program, you may have to reference external files - data stored in a .csv file, for instance, or a picture. Best practice is to create a file structure that contains everything you need to run your entire project in a single file folder (you can, and sometimes should, have sub-folders).\nFor now, it is enough to know how to find files using file paths, and how to refer to a file using a relative file path from your base folder. In this situation, your “base folder” is known as your working directory - the place your program thinks of as home."
  },
  {
    "objectID": "part-tools/01-computer-basics.html#system-paths",
    "href": "part-tools/01-computer-basics.html#system-paths",
    "title": "1  Computer Basics",
    "section": "1.5 System Paths",
    "text": "1.5 System Paths\nWhen you install software, it is saved in a specific location on your computer, like C:/Program Files/ on , /Applications/ on , or /usr/local/bin/ on . For the most part, you don’t need to keep track of where programs are installed, because the install process (usually) automatically creates icons on your desktop or in your start menu, and you find your programs there.\nUnfortunately, that isn’t sufficient when you’re programming, because you may need to know where a program is in order to reference that program – for instance, if you need to pop open a browser window as part of your program, you’re (most likely) going to have to tell your computer where that browser executable file lives.\nTo simplify this process, operating systems have what’s known as a “system path” or “user path” - a list of folders containing important places to look for executable and other important files. You may, at some point, have to edit your system path to add a new folder to it, making the executable files within that folder more easily available.\n\n\n\n\n\n\nHow To Modify System Paths\n\n\n\n\n\nHow to set system paths (general)\nOperating-system specific instructions cobbled together from a variety of different sources:\n\n On Windows\n On Mac\n On Linux\n\n\n\n\nIf you run across an error that says something along the lines of\n\ncould not locate xxx.exe\nThe system cannot find the path specified\nCommand Not Found\n\nyou might start thinking about whether your system path is set correctly for what you’re trying to do.\nIf you want to locate where an executable is found (in this example, we’ll use git), you can run where git on windows, or which git on OSX/Linux.\nSome programs, like RStudio, have places where you can set the locations of common dependencies. If you go to Tools > Global Options > Git/SVN, you can set the path to git."
  },
  {
    "objectID": "part-tools/01-computer-basics.html#part-tools-01-refs",
    "href": "part-tools/01-computer-basics.html#part-tools-01-refs",
    "title": "1  Computer Basics",
    "section": "1.6 References",
    "text": "1.6 References\n\n\n\n\n[1] D. Robitzski, “Gen z kids apparently don’t understand how file systems work. Futurism,” Sep. 24, 2021. [Online]. Available: https://futurism.com/the-byte/gen-z-kids-file-systems. [Accessed: Jan. 09, 2023]"
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#fa-bullseye-objectives",
    "href": "part-tools/02-setting-up-computer.html#fa-bullseye-objectives",
    "title": "2  Setting Up Your Computer",
    "section": "\n2.1  Objectives",
    "text": "2.1  Objectives\n\nSet up RStudio, R, Quarto, and python\nBe able to run demo code in R and python"
  },
  {
    "objectID": "part-tools/02-setting-up-computer.html#installation-process",
    "href": "part-tools/02-setting-up-computer.html#installation-process",
    "title": "2  Setting Up Your Computer",
    "section": "\n2.2 Installation Process",
    "text": "2.2 Installation Process\nIn this section, I will provide you with links to set up various programs on your own machine. If you have trouble with these instructions or encounter an error, post on the class message board or contact me for help.\n\n\nDownload and run the R installer for your operating system from CRAN:\n\nWindows: https://cran.rstudio.com/bin/windows/base/\nMac: https://cran.rstudio.com/bin/macosx/\nLinux: https://cran.rstudio.com/bin/linux/ (pick your distribution)\n\nIf you are on Windows, you should also install the Rtools4 package; this will ensure you get fewer warnings later when installing packages.\nMore detailed instructions for Windows are available here\n\n\nDownload and install the latest version of python 3\n\nThen, install Jupyter using the instructions here\nAdditional instructions for installing Python 3 from Python for Everybody if you have trouble.\n\n\nDownload and install the latest version of RStudio for your operating system. RStudio is a integrated development environment (IDE) for R - it contains a set of tools designed to make writing R code easier.\nDownload and install the latest version of Quarto for your operating system. Quarto is a command-line tool released by RStudio that allows Rstudio to work with python and other R specific tools in a unified way.\n\nThe following steps may be necessary depending on which class you’re in. If you want to be safe, go ahead and complete these steps as well.\n\nInstall git using the instructions here. Consult the troubleshooting guide if you have issues. If that fails, then seek help in office hours.\n\nInstall LaTeX and rmarkdown:\n\nLaunch R, and type the following commands into the console:\n\n\n\n\ninstall.packages(c(\"tinytex\", \"knitr\", \"rmarkdown\", \"quarto\"))\nlibrary(tinytex)\ninstall_tinytex()\n\n\n\n\n\n\n\nYour turn\n\n\n\nOpen RStudio on your computer and explore a bit.\n\nCan you find the R console? Type in 2+2 to make sure the result is 4.\nRun the following code in the R console:\n\ninstall.packages(\n  c(\"tidyverse\", \"rmarkdown\", \"knitr\", \"quarto\")\n)\n\nCan you find the text editor?\n\nCreate a new quarto document (File -> New File -> Quarto Document).\nPaste in the contents of this document.\nCompile the document (Ctrl/Cmd + Shift + K) and use the Viewer pane to see the result.\nIf this all worked, you have RStudio, Quarto, R, and Python set up correctly on your machine."
  },
  {
    "objectID": "part-tools/03-scripts-notebooks.html#fa-bullseye-objectives",
    "href": "part-tools/03-scripts-notebooks.html#fa-bullseye-objectives",
    "title": "3  Scripts and Notebooks",
    "section": "\n3.1  Objectives",
    "text": "3.1  Objectives\n\nUnderstand the different modes that can be used to interact with a programming language\nIdentify which mode and language is being used given a screenshot or other contextual information\nSelect the appropriate mode (interactive, script, notebook) for a given task given considerations such as target audience, human intervention, and need to repeat the analysis."
  },
  {
    "objectID": "part-tools/03-scripts-notebooks.html#a-short-history-of-talking-to-computers",
    "href": "part-tools/03-scripts-notebooks.html#a-short-history-of-talking-to-computers",
    "title": "3  Scripts and Notebooks",
    "section": "\n3.2 A Short History of Talking to Computers",
    "text": "3.2 A Short History of Talking to Computers\nThe fundamental goal of this chapter is to learn how to talk to R and Python. In the very beginning, people told computers what to do using punch cards [1]. This required that you have every step of your program and data planned out in advance - you’d submit your punch cards to the computer, and then come back 24-72 hours later to find out you’d gotten two cards out of order. Dropping a tray of punch cards was … problematic.\nThankfully, we’re mostly free of the days where being a bit clumsy could erase a semester of hard work. As things grew more evolved and we got actual monitors and (eventually) graphical interfaces, we started using interactive terminals (interactive mode) to boss computers around.\n\n\n\n\n\n\nYour Turn - Interactive Mode\n\n\n\n\n\nR\nPython\n\n\n\nOpen RStudio and navigate to the Console tab. You can issue commands directly to R by typing something in at the > prompt.\nTry typing in 2+2 and hit enter.\n\n\nOpen RStudio and navigate to the Terminal tab. This is your computer’s ‘terminal’ - where you tell the computer what to do.\nFirst, we have to tell it what language we’d like to work in - by default, it’s going to work in  Batch (Windows),  Zsh (Mac), or  Bash (Linux). Luckily, we can avoid these and tell the computer we want to work in python by typing in python3 or python (depending on how your computer is set up). This will launch an interactive python session (ipython).\nYou should get a prompt that looks like this: >>>\nType in 2+2 and hit enter.\n\n\n\n\n\nInteractive mode is useful for quick, one-off analyses, but if you need to repeat an analysis (or remember what you did), interactive mode is just awful. Once you close the program, the commands (and results) are gone. This is particularly inconvenient when you need to run the same task multiple times. For example, each day I may want to pull the weather forecast and observed weather values from the national weather service using the same commands. I don’t want to manually re-type them each day!\nTo somewhat address this issue, most computing languages allow you to provide a sequence of commands in a text file known as a script. Scripts are typically meant to run on their own - they may perform computations, format data and save it, scrape data from the web… the possibilities are endless, but they are typically meant to run without the person running the script having to read all of the commands.\n\n\n\n\n\n\nYour Turn - Terminal Mode\n\n\n\n\nDownload scripts.zip and unzip the file.\nOpen a system terminal in the directory where you unzipped the files.\n\n\n\n Windows\n Mac\n Linux\n\n\n\nOpen the folder. Type cmd into the location bar at the top of the window and hit enter. The command prompt will open in the desired location.\n\n\nOpen a finder window and navigate to the folder you want to use. If you don’t have a path bar at the bottom of the finder window, choose View > Show Path Bar. Control-click the folder in the path bar and choose Open in Terminal.\n\n\nOpen the folder in your file browser. Select the path to the folder in the path bar and copy it to the clipboard. Launch a terminal and type cd, and then paste the copied path. Hit enter. (There may be more efficient ways to do this, but these instructions work for most window managers).\n\n\n\n\nNow, let’s try out script mode in R and Python!\n\n\n\nR\nPython\n\n\n\nThis assumes that the R binary has been added to your system path. If these instructions don’t work, please ask for help or visit office hours.\nIn the terminal, type Rscript words.R dickens-oliver-twist.txt\nYou should get some output that looks like this:\nuser@computer:~/scripts$ Rscript words.R dickens-oliver-twist.txt \ntext\n the  and        to   of    a  his   in   he  was \n8854 4902 4558 3767 3763 3569 2272 2224 1931 1684\n\n\nThis assumes that the python binary has been added to your system path. If these instructions don’t work, please ask for help or visit office hours.\nIn the terminal, type python3 words.py and hit Enter. You will be prompted for the file name. Enter dickens-oliver-twist.txt and hit Enter again.\nYou should get some output that looks like this:\nuser@computer:~/scripts$ python3 words.py \nEnter file:dickens-oliver-twist.txt\nthe 8854\n\n\n\n\n\nScripts, and compiled programs generated from scripts, are responsible for much of what you interact with on a computer or cell phone day-to-day. When the goal is to process a file or complete a task in exactly the same way each time, a script is the right choice for the job.\nHowever, when working with data, we sometimes prefer to combine scripts with interactive mode - that is, we use a script file to keep track of which commands we run, but we run the script interactively. About 60% of my day-to-day computing is done using R or python scripts that are run interactively.\n\n\n\n\n\n\nYour Turn - Script Mode\n\n\n\nIf you haven’t already, download scripts.zip and unzip the file.\nOpen RStudio and use RStudio to complete the following tasks.\n\n\nR\nPython\n\n\n\n\nUse RStudio to open the words-noinput.R file in the scripts folder you downloaded and unzipped.\nWhat do you notice about the appearance of the file? Is there an icon in the tab to tell you what type of file it is? Are some words in the file highlighted?\nCopy the path to the scripts folder.\nOS Specific Instructions:  Windows,  Mac,  Linux\nIn the Console, type in setwd(\"<paste path here>\"), where you paste your file path from step 3 between the quotes. Hit enter.\nIn the words-noinput.R file, hit the “source” button in the top right. Do you get the same output that you got from running the file as a script from the terminal? Why do you think that is?\nClick on the last line of the file and hit Run (or Ctrl/Cmd + Enter). Do you get the output now?\nClick on the first line of the file and hit Run (or Ctrl/Cmd + Enter). This runs a single line of the file. Use this to run each line of the file in turn. What could you learn from doing this?\n\n\n\n\nUse RStudio or your preferred python editor to open the words-noinput.py file in the scripts folder you downloaded and unzipped.\nWhat do you notice about the appearance of the file? Is there an icon in the tab to tell you what type of file it is? Are some words in the file highlighted?\nCopy the path to the scripts folder.\nOS Specific Instructions:  Windows,  Mac,  Linux\nIn the Console, type in setwd(\"<paste path here>\"), where you paste your file path from step 3 between the quotes. Hit enter.\nIn the words-noinput.py file, hit the “source” button in the top right. Do you get the same output that you got from running the file as a script from the terminal? What changes?\nClick on the first line of the file and hit Run (or Ctrl/Cmd + Enter). This runs a single line of the file. Use this to run each line of the file in turn. What do you learn from doing this?\n\n\n\n\n\n\nUsing scripts interactively allows us to see what is happening in the script step-by-step, and to examine the results during the program’s evaluation. This can be beneficial when applying a script to a new dataset, because it allows us to change things on the fly while still keeping the same basic order of operations."
  },
  {
    "objectID": "part-tools/03-scripts-notebooks.html#writing-code-for-people",
    "href": "part-tools/03-scripts-notebooks.html#writing-code-for-people",
    "title": "3  Scripts and Notebooks",
    "section": "\n3.3 Writing Code for People",
    "text": "3.3 Writing Code for People\nOne problem with scripts and interactive modes of using programming languages is that we’re spending most of our time writing code for computers to read – which doesn’t necessarily imply that our code is easy for humans to read.\nThere are two solutions to this problem, and I encourage you to make liberal use of both of them (together).\n\n3.3.1 Code Comments\nA comment is a part of computer code which is intended only for people to read. It is not evaluated or run by the computing language.\nTo “comment out” a single line of code in R or python, put a # (pound sign/hashtag) just before the part of the code you do not want to be evaluated.\n\n\n\n\n\n\nAdding Comments to Code\n\n\n\n\n\nR\nPython\n\n\n\n\n2 + 2 + 3\n\n[1] 7\n\n2 + 2 # + 3\n\n[1] 4\n\n# This line is entirely commented out\n\n\n\n\n2 + 2 + 3\n\n7\n\n2 + 2 # + 3\n# This line is entirely commented out\n\n4\n\n\n\n\n\n\n\nMany computing languages, such as Java, C/C++, and JavaScript have mechanisms to comment out an entire paragraph. Neither R nor Python has so-called “block comments” - instead, you can use keyboard shortcuts in RStudio to comment out an entire chunk of code (or text) using Ctrl/Cmd-Shift-C.\n\n3.3.2 Literate Programming - Notebooks and more!\nWhile code comments add human-readable text to code, scripts with comments are still primarily formatted for the computer’s convenience. However, most of the time spent on any given document is spent by people, not by computers. We often write parallel documents - user manuals, academic papers, tutorials, etc. which explain the purpose of our code and how to use it, but this can get clumsy over time, and requires updating multiple documents (sometimes in multiple places), which often leads to the documentation getting out-of-sync from the code.\nTo solve this problem, Donald Knuth invented the concept of literate programming: interspersing text and code in the same document using structured text to indicate which lines are code and which lines are intended for human consumption.\nThis textbook is written using a literate format - quarto markdown - which allows me to include code chunks in R, python, and other languages, alongside the text, pictures, and other formatting necessary to create a textbook.\n\n3.3.2.1 Quarto\nOne type of literate programming document is a quarto markdown document.\nWe will use quarto markdown documents for most of the components of this class because they allow you to answer assignment questions, write reports with figures and tables generated from data, and provide code all in the same file.\nWhile literate documents aren’t ideal for jobs where a computer is doing things unobserved (such as pulling data from a web page every hour), they are extremely useful in situations where it is desireable to have both code and an explanation of what the code is doing and what the results of that code are in the same document.\n\n\n\n\n\n\nYour turn: Quarto Markdown\n\n\n\nIn RStudio, create a new quarto markdown document: File > New File > Quarto Document. Give your document a title and an author, and select HTML as the output.\nCopy the following text into your document and hit the “Render” button at the top of the file.\nThis defines an R code chunk. The results will be included in the compiled HTML file.\n\n```{r}\n2 + 2 \n```\n\nThis defines a python code chunk. The results will be included in the compiled HTML file.\n\n```{python}\n2 + 2\n```\n\n# This is a header\n\n## This is a subheader\n\nI can add paragraphs of text, as well as other structured text such as lists:\n\n1. First thing\n2. Second thing\n  - nested list\n  - nested list item 2\n3. Third thing\n\nI can even include images and [links](https://www.oldest.org/entertainment/memes/)\n\n![Goodwin's law is almost as old as the internet itself.](https://www.oldest.org/wp-content/uploads/2017/10/Godwins-Law.jpg)\n\n\nMarkdown is a format designed to be readable and to allow document creators to focus on content rather than style.\n\nA Markdown-formatted document should be publishable as-is, as plain text, without looking like it’s been marked up with tags or formatting instructions. – John Gruber\n\nYou can read more about pandoc markdown (and quarto markdown, which is a specific type of pandoc markdown) here [2].\nMarkdown documents are compiled into their final form (usually, HTML, PDF, Docx) in multiple stages:\n\nAll code chunks are run and the results are saved and inserted into the markdown document.\nRmd/qmd -> md\nThe markdown document is converted into its final format using pandoc, a program that is designed to ensure you can generate almost any document format. This may involve conversion to an intermediate file (e.g. .tex files for PDF documents).\n\nAn error in your code will likely cause a failure at stage 1 of the process. An error in the formatting of your document, or missing pictures, and miscellaneous other problems may cause errors in stage 2.\n\n\n\n\n\n\nHistory\n\n\n\nQuarto markdown is the newest version of a long history of literate document writing in R. A previous version, Rmarkdown, had to be compiled using R; quarto can be compiled using R or python or the terminal directly.\nPrior to Rmarkdown, the R community used knitr and Sweave to integrate R code with LaTeX documents (another type of markup document that has a steep learning curve and is harder to read).\n\n\n\n3.3.2.2 Jupyter\nWhere quarto comes primarily out of the R community and those who are agnostic whether R or Python is preferable for data science related computing, Jupyter is essentially an equivalent notebook technology that comes from the python side of the world.\nQuarto supports using the jupyter engine for chunk compilation, but jupyter notebooks have some (rather technical) features that make them less desirable for an introductory computing class [3].\n\n\n\n\n\n\nLearn More about Notebooks\n\n\n\nThere are some excellent opinions surrounding the use of notebooks in data analysis:\n\n\nWhy I Don’t Like Notebooks” by Joel Grus at JupyterCon 2018\n\nThe First Notebook War by Yihui Xie (response to Joel’s talk).\n\nYihui Xie is the person responsible for knitr and Rmarkdown and was involved in the development of quarto."
  },
  {
    "objectID": "part-tools/03-scripts-notebooks.html#part-tools-03-refs",
    "href": "part-tools/03-scripts-notebooks.html#part-tools-03-refs",
    "title": "3  Scripts and Notebooks",
    "section": "\n3.4 References",
    "text": "3.4 References\n\n\n\n\n[1] \n\n“Punched card input/output.” Jan. 08, 2023 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Punched_card_input/output&oldid=1132250858\n\n\n\n[2] \nPosit PBC, “Quarto - markdown basics,” 2023. [Online]. Available: https://quarto.org/docs/authoring/markdown-basics.html. [Accessed: Jan. 09, 2023]\n\n\n[3] \nY. Xie, “The first notebook war,” Sep. 10, 2018. [Online]. Available: https://yihui.org/en/2018/09/notebook-war/. [Accessed: Jan. 09, 2023]"
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#fa-bullseye-objectives",
    "href": "part-tools/04-Rstudio-interface.html#fa-bullseye-objectives",
    "title": "4  RStudio’s Interface",
    "section": "4.1  Objectives",
    "text": "4.1  Objectives\n\nLocate different panes of RStudio\nUse cues such as buttons and icons to identify what type of file is open and what language is being interpreted"
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#overview",
    "href": "part-tools/04-Rstudio-interface.html#overview",
    "title": "4  RStudio’s Interface",
    "section": "4.2 Overview",
    "text": "4.2 Overview\n An RStudio window is by default divided into 4 panes, each of which may contain several tabs. You can reconfigure the locations of these tabs based on your preferences by selecting the toolbar button with 4 squares (just left of the Addins dropdown menu).\nIn the default configuration, - The top left is the editor pane, where you will write code and other content. - The bottom left is the console pane, which contains your R/python interactive consoles as well as a system terminal and location for checking the status of background jobs. - The top right contains the environment and history tabs (among others) - The top left contains the files and help tabs (among others)\nYou do not need to know what all of these tabs do right now. For the moment, it’s enough to get a sense of the basics - where to write code (top left), where to look for results (bottom left), where to get help (bottom right), and where to monitor what R/python are doing (top right)."
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#the-editorfile-pane-top-left",
    "href": "part-tools/04-Rstudio-interface.html#the-editorfile-pane-top-left",
    "title": "4  RStudio’s Interface",
    "section": "4.3 The Editor/File Pane (Top Left)",
    "text": "4.3 The Editor/File Pane (Top Left)\nThe buttons and layout within this pane change based on the type of file you have open.\n\nR scriptPython scriptQuarto markdownText file\n\n\n\n\n\nThe logo on the script file indicates the file type. When an R file is open, there are Run and Source buttons on the top which allow you to run selected lines of code (Run) or source (run) the entire file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the script file indicates the file type. When a python file is open, there are Run and Source buttons on the top which allow you to run selected lines of code (Run) or source (run) the entire file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the script file indicates the file type. When a quarto markdown file is open, there is a render button at the top which allows you to compile the file to see its “pretty”, non-markup form. In the same toolbar, there are buttons to add a code chunk as well as to run a selcted line of code or chunk of code. You can toggle between source (shown) and visual mode to see a more word-like rendering of the quarto markdown file. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is.\n\n\n\n\n\n\n\nThe logo on the text file indicates the file type. When a text file (or other unknown file extension) is open, there are very few buttons in the editor window. Code line numbers are provided on the left (this is a handy way to see where in the code the errors occur), and you can see line:character numbers at the bottom left. At the bottom right, there is another indicator of what type of file Rstudio thinks this is."
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#the-console-pane-bottom-left",
    "href": "part-tools/04-Rstudio-interface.html#the-console-pane-bottom-left",
    "title": "4  RStudio’s Interface",
    "section": "4.4 The Console Pane (Bottom Left)",
    "text": "4.4 The Console Pane (Bottom Left)\nLet’s compare what the console pane looks like when we run a line of R code compared to a line of python code. The differences will help you figure out whether you need to exit out of Python to run R code and may help you debug some errors.\n\nPythonR\n\n\n\n\n\nWhen running python code from a script file, the console will show you that you are running in python by the logo at the top of the console pane. You will initially see lines indicating that you’re running R, and then you’ll see the lines highlighted in red which show R running the code in python – this is what converts the console from R to python. The command you ran will appear after >>>, and the results will appear immediately below. A >>> waits for a new command - to get back to R, you will need to type exit (as instructed by the red text). In the environment pane, you cna see another indicator that you’re viewing the python environment, with an object named ‘r’ that will allow you to move data back and forth between the two languages if you want to do so.\n\n\n\n\n\n\n\nWhen running R code from a script file, the console will show you that you are running in R by the logo at the top of the console pane. You will initially see lines indicating that you’re running R (they’re missing here because this isn’t the first command I ran in this session). The command you ran will appear after >, and the results will appear immediately below, with boxed numbers in front of each sequential line. A > waits for a new command . In the environment pane, you may see a new value pop up named .Last.value - this is part of user settings and you can stop it from appearing if you want to."
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#the-top-right-pane",
    "href": "part-tools/04-Rstudio-interface.html#the-top-right-pane",
    "title": "4  RStudio’s Interface",
    "section": "4.5 The Top Right Pane",
    "text": "4.5 The Top Right Pane\nThis pane contains a set of tabs that change based on your project and what you have enabled. If you’re using git with an Rstudio project, then this tab will show your git repository. If you’re working with an Rstudio project that has multiple files, such as a book or a website, then the pane will also have a Build tab that will build all of your project files.\nFor now, though, let’s assume you’re not in an Rstudio project and you just want to know what the heck an Environment pane (or any of the other tabs in here by default) is. We’re going to focus on two of the tabs that are the most relevant to you right now: Environment, and History.\n\n4.5.1 Environment tab\nThe Environment tab shows you any objects which are defined in memory in whatever language you’re currently using (as long as it’s R or python). You’ll see headers like “Data”, “Values”, and “Functions” within this table, and two columns - the name of the thing, and the value of the thing (if it’s a complicated object, you’ll see what type of object it is and possibly how long it is).\n\n\n\nThe environment tab shows you all of the objects in memory that the language you’re working in knows about.\n\n\nIf you’re working in both R and python, you can toggle which language’s environment you’re looking at using the language drop down button on the far left side.\n\n\n4.5.2 History tab\nAnother useful tab in this pane is the History tab, which shows you a running list of every command you’ve ever run. While I strongly encourage you to write your code in a text file in the editor pane, sometimes you deleted a line of code accidentally and want to get it back… and the history tab has you covered (unless you’ve cleared the history out).\n\n\n\nThe history tab shows you a list of all commands you’ve run and allows you to send them to the console or to source (the text editor)."
  },
  {
    "objectID": "part-tools/04-Rstudio-interface.html#the-bottom-right-pane",
    "href": "part-tools/04-Rstudio-interface.html#the-bottom-right-pane",
    "title": "4  RStudio’s Interface",
    "section": "4.6 The Bottom Right Pane",
    "text": "4.6 The Bottom Right Pane\nThis pane also contains a mishmash of tabs that have various uses. Here, we’ll focus on 3: Files, Packages, and Help. But first, to quickly summarize the remaining tabs, the Plots tab shows any plots you’ve generated (which we haven’t done yet), and the Viewer/Presentation tabs show you compiled documents (markdown), interactive graphics, and presentations.\n\n4.6.1 Files tab\n\n\n\nThe files tab shows you the files in your current working directory (by default), though you can navigate through it and find other files as necessary. If you want to return to your working directory, there’s a button for that in the “More” menu. One of the most important pieces of information in this pane is your path - you can construct the file path by using ~/ for home, and then for each folder, adding a slash between. The path to the folder we’re looking at here is thus ~/Projects/Class/stat-computing-r-python/.\n\n\n\n\n4.6.2 Packages tab\nThe packages tab isn’t quite relevant yet, but it will be soon. R and python both work off of packages - extensions to the default language that make it easier to accomplish certain tasks, like reading data from Excel files or drawing pretty charts. This tab shows all of the R packages you have installed on your machine, and which ones are currently loaded.\n\n\n\nYou can get important information from the packages tab, like what packages are loaded, easy access to documentation for each package, and what version of the package is installed.\n\n\nUnfortunately, the packages tab doesn’t cover python packages yet.\n\n\n4.6.3 Help tab\nThe help tab is a wonderful way to get help with how to use an R or python function.\n\n\n\nThe help tab makes it easy to get access to function documentation within Rstudio, so you don’t have to switch windows.\n\n\nBy default, you can search for an R function name in the search window, and documentation for matching functions will appear in the main part of the pane. To get help with python functions, you need to (in the python console) use ?<function name, so I would type in at the >>> prompt ?print to get the equivalent python help file."
  },
  {
    "objectID": "part-tools/05-git-and-github.html#fa-bullseye-objectives",
    "href": "part-tools/05-git-and-github.html#fa-bullseye-objectives",
    "title": "5  Version Control with Git",
    "section": "\n5.1  Objectives",
    "text": "5.1  Objectives\n\nInstall git\nCreate a github account\nUnderstand why version control is useful and what problems it can solve\nUnderstand the distinction between git and github, and what each is used for\nUse version control to track changes to a document (git add, commit, push, pull)"
  },
  {
    "objectID": "part-tools/05-git-and-github.html#installation",
    "href": "part-tools/05-git-and-github.html#installation",
    "title": "5  Version Control with Git",
    "section": "\n5.2 Installation",
    "text": "5.2 Installation\n\nInstall git using the instructions here.\n\nConsult the troubleshooting guide if you have issues.\nIf 1-2 fail, seek help in office hours.\n\n\n\n\n\n\n\n Mac Warning\n\n\n\nWith each version upgrade, you may find that git breaks. To fix it, you will have to reinstall Mac command line tools. Once you do this, git will start working again. See [2] for more information.\n\n\n\n5.2.1 Optional: Install a git client\nInstructions\nI don’t personally use a git client other than RStudio, but you may prefer to have a client that allows you to use a point-and-click interface. It’s up to you."
  },
  {
    "objectID": "part-tools/05-git-and-github.html#what-is-version-control",
    "href": "part-tools/05-git-and-github.html#what-is-version-control",
    "title": "5  Version Control with Git",
    "section": "\n5.3 What is Version Control ?",
    "text": "5.3 What is Version Control ?\n\n\n\n\n\n\nNote\n\n\n\nMost of this section is either heavily inspired by Happy Git and Github for the UseR [1] or directly links to that book.\n\n\n\n\n\nGit is a version control system - a structured way for tracking changes to files over the course of a project that may also make it easy to have multiple people working on the same files at the same time.\n\n\nVersion control is the answer to this file naming problem. Image Source “Piled Higher and Deeper” by Jorge Cham www.phdcomics.com\n\n\nGit manages a collection of files in a structured way - rather like “track changes” in Microsoft Word or version history in Dropbox, but much more powerful.\nIf you are working alone, you will benefit from adopting version control because it will remove the need to add _final.R to the end of your file names. However, most of us work in collaboration with other people (or will have to work with others eventually), so one of the goals of this program is to teach you how to use git because it is a useful tool that will make you a better collaborator.\nIn data science programming, we use git for a similar, but slightly different purpose. We use it to keep track of changes not only to code files, but to data files, figures, reports, and other essential bits of information.\nGit itself is nice enough, but where git really becomes amazing is when you combine it with GitHub - an online service that makes it easy to use git across many computers, share information with collaborators, publish to the web, and more. Git is great, but GitHub is … essential. In this class, we’ll be using both git and github, and your homework will be managed with GitHub Classroom.\n\n5.3.1 Git Basics\n\n\nIf that doesn’t fix it, git.txt contains the phone number of a friend of mine who understands git. Just wait through a few minutes of ‘It’s really pretty simple, just think of branches as…’ and eventually you’ll learn the commands that will fix everything. Image by Randall Munroe (XKCD) CC-A-NC-2.5.\n\n\nGit tracks changes to each file that it is told to monitor, and as the files change, you provide short labels describing what the changes were and why they exist (called “commits”). The log of these changes (along with the file history) is called your git commit history.\nWhen writing papers, this means you can cut material out freely, so long as the paper is being tracked by git - you can always go back and get that paragraph you cut out if you need to. You also don’t have to rename files - you can confidently save over your old files, so long as you remember to commit frequently.\n\n\n\n\n\n\nEssential Reading: Git\n\n\n\nThe git material in this chapter is just going to link directly to the book “Happy Git with R” by Jenny Bryan. It’s amazing, amusing, and generally well written. I’m not going to try to do better.\nGo read Chapter 1, if you haven’t already.\n\n\nNow that you have a general idea of how git works and why we might use it, let’s talk a bit about GitHub.\n\n5.3.2 GitHub: Git on the Web\n\n\n\n\n\n\nSet up a GitHub Account Now\n\n\n\nInstructions for setting up a GitHub account.\nBe sure you remember your signup email, username, and password - you will need them later.\n\n\nGit is a program that runs on your machine and keeps track of changes to files that you tell it to monitor. GitHub is a website that hosts people’s git repositories. You can use git without GitHub, but you can’t use GitHub without git.\n\n\n\n\n\n\nGit and Github: Slightly crude (but memorable) analogy\n\n\n\n\n\nGit is to GitHub what Porn is to PornHub. Specifically, GitHub hosts git repositories publicly, while PornHub hosts porn publicly. But it would be silly to equate porn and PornHub, and it’s similarly silly to think of GitHub as the only place you can use git repositories.\n\n\n\nIf you want, you can hook Git up to GitHub, and make a copy of your local git repository that lives in the cloud. Then, if you configure things correctly, your local repository will talk to GitHub without too much trouble. Using Github with Git allows you to easily make a cloud backup of your important code, so that even if your computer suddenly catches on fire, all of your important code files exist somewhere else.\nRemember: any data you don’t have in 3 different places is data you don’t care about.1"
  },
  {
    "objectID": "part-tools/05-git-and-github.html#using-version-control-with-rstudio",
    "href": "part-tools/05-git-and-github.html#using-version-control-with-rstudio",
    "title": "5  Version Control with Git",
    "section": "\n5.4 Using Version Control (with RStudio)",
    "text": "5.4 Using Version Control (with RStudio)\nThe first skill you need to actually practice in this class is using version control. By using version control from the very beginning, you will learn better habits for programming, but you’ll also get access to a platform for collaboration, hosting your work online, keeping track of features and necessary changes, and more.\n\n\n\n\n\n\n\n\n\n\n\nSo, what does your typical git/GitHub workflow look like? I’ll go through this in (roughly) chronological order. This is based off of a relatively high-level understanding of git - I do not have any idea how it works under the hood, but I’m pretty comfortable with the clone/push/pull/commit/add workflows, and I’ve used a few of the more complicated features (branches, pull requests) on occasion.\n\n5.4.1 Introduce yourself to git and set up SSH authentication\nYou need to tell git what your name and email address are, because every “commit” you make will be signed. This needs to be done once on each computer you’re using.\nFollow the instructions here, or run the lines below:\n\n\n\n\n\n\nNote\n\n\n\nThe lines of code below use interactive prompts. Click the copy button in the upper right corner of the box below, and then paste the whole thing into the R console. You will see a line that says “Your full name:” - type your name into the console. Similarly, the next line will ask you for an email address.)\n\n\n\n\nuser_name <- readline(prompt = \"Your full name: \")\nuser_email <- readline(prompt = \"The address associated w your github account: \")\n\ninstall.packages(\"usethis\")\nlibrary(usethis)\n\nuse_git_config(user.name = user_name, user.email = user_email, scope = \"user\")\n\n# Tell git to ignore all files that are OS-dependent and don't have useful data.\ngit_vaccinate() \n\n# Create a ssh key if one doesn't already exist\nif (!file.exists(git2r::ssh_path(\"id_rsa.pub\"))) {\n  # Create an ssh key (with no password - less secure, but simpler)\n  system(\"ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -q -N ''\") \n  # Find the ssh-agent that will keep track of the password\n  system(\"eval $(ssh-agent -s)\")\n  # Add the key\n  system(\"ssh-add ~/.ssh/id_rsa\")\n} \n\nThen, in RStudio, go to Tools > Global Options > Git/SVN. View your public key, and copy it to the clipboard.\nThen, proceed to github. Make sure you’re signed into GitHub. Click on your profile pic in upper right corner and go Settings, then SSH and GPG keys. Click “New SSH key”. Paste your public key in the “Key” box. Give it an informative title. For example, you might use 2022-laptop to record the year and computer. Click “Add SSH key”.\n\n5.4.2 Create a Repository\nRepositories are single-project containers. You may have code, documentation, data, TODO lists, and more associated with a project. If you combine a git repository with an RStudio project, you get a very powerful combination that will make your life much easier, allowing you to focus on writing code instead of figuring out where all of your files are for each different project you start.\nTo create a repository, you can start with your local computer first, or you can start with the online repository first.\n\n\n\n\n\n\nImportant\n\n\n\nBoth methods are relatively simple, but the options you choose depend on which method you’re using, so be careful not to get them confused.\n\n\n\n5.4.2.1 Local repository first\nLet’s suppose you already have a folder on your machine named hello-world-1 (you may want to create this folder now). You’ve created a starter document, say, a text file named README with “hello world” written in it.\nIf you want, you can use the following R code to set this up:\n\ndir <- \"./hello-world-1\"\nif (!dir.exists(dir)) {\n  dir.create(dir)\n}\nfile <- file.path(dir, \"README\")\nif (!file.exists(file)) {\n  writeLines(\"hello world\", con = file)\n}\n\nTo create a local git repository, we can go to the terminal (in Mac/Linux) or the git bash shell (in Windows), navigate to our repository folder (not shown, will be different on each computer), and type in\ngit init\nAlternately, if you prefer a GUI (graphical user interface) approach, that will work too:\n\nOpen Rstudio\nProject (upper right corner) -> New Project -> Existing Directory. Navigate to the directory.\n(In your new project) Tools -> Project options -> Git/SVN -> select git from the dropdown, initialize new repository. RStudio will need to restart.\nNavigate to your new Git tab on the top right.\n\n\n\n\n\nThe next step is to add our file to the repository.\nUsing the command line, you can type in git add README (this tells git to track the file) and then commit your changes (enter them into the record) using git commit -m \"Add readme file\".\nUsing the GUI, you navigate to the git pane, check the box next to the README file, click the Commit button, write a message (“Add readme file”), and click the commit button.\n\n\n\n\nThe final step is to create a corresponding repository on GitHub. Navigate to your GitHub profile and make sure you’re logged in. Create a new repository using the “New” button. Name your repository whatever you want, fill in the description if you want (this can help you later, if you forget what exactly a certain repo was for), and DO NOT add a README, license file, or anything else (if you do, you will have a bad time).\nYou’ll be taken to your empty repository, and git will provide you the lines to paste into your git shell (or terminal) – you can access this within RStudio, as shown below. Paste those lines in, and you’ll be good to go.\n\n\n\n\n\n5.4.2.2 GitHub repository first\nIn the GitHub-first method, you’ll create a repository in GitHub and then clone it to your local machine (clone = create an exact copy locally).\nGUI method:\n\nLog into GitHub and create a new repository\nInitialize your repository with a README\nCopy the repository location by clicking on the “Code” button on the repo homepage\nOpen RStudio -> Project -> New Project -> From version control. Paste your repository URL into the box. Hit enter.\nMake a change to the README file\nClick commit, then push your changes\nCheck that the remote repository (Github) updated\n\n\n\n\n\nCommand line method:\n\nLog into GitHub and create a new repository\nInitialize your repository with a README\nCopy the repository location by clicking on the “Code” button on the repo homepage\nNavigate to the location you want your repository to live on your machine.\nClone the repository by using the git shell or terminal: git clone <your repo url here>. In my case, this looks like git clone git@github.com:stat850-unl/hello-world-2.git\n\nMake a change to your README file and save the change\nCommit your changes: git commit -a -m \"change readme\" (-a = all, that is, any changed file git is already tracking).\nPush your changes to the remote (GitHub) repository and check that the repo has updated: git push\n\n\n\n\n\n\n\n5.4.3 Adding files\ngit add tells git that you want it to track a particular file.\n\n\ngit add diagram: add tells git to add the file to the index of files git monitors.\n\n\nYou don’t need to understand exactly what git is doing on the backend, but it is important to know that the actual contents of the file aren’t logged by git add - you have to commit your changes for the contents to change. git add deals solely with the index of files that git “knows about”, and what it thinks belongs in each commit.\nIf you use the RStudio GUI for your git interface, you generally won’t have to do much with git add; it’s (sort-of, kind-of) equivalent to clicking the check box.\n\n5.4.3.1 What files should I add to git?\nGit is built for tracking text files. It will (begrudgingly) deal with small binary files (e.g. images, PDFs) without complaining too much, but it is NOT meant for storing large files, and GitHub will not allow you to push anything that has a file larger than 100MB2. Larger files can be handled with git-lfs (large file storage), but storing large files online is not something you can get for free.\nIn general, you should only add a file to git if you created it by hand. If you compiled the result, that should not be in the git repository under normal conditions (there are exceptions to this rule – this book is hosted on GitHub, which means I’ve pushed the compiled book to the GitHub repository).\nYou should also be cautious about adding files like .Rprog, .directory, .DS_Store, etc. These files are used by your operating system or by RStudio, and pushing them may cause problems for your collaborators (if you’re collaborating). Tracking changes to these files also doesn’t really do much good.\nI highly recommend that you make a point to only add and commit files which you consciously want to track.\n\n5.4.4 Staging your changes\nIn RStudio, when you check a box next to the file name in the git tab, you are effectively adding the file (if it is not already added) AND staging all of the changes you’ve made to the file. In practice, git add will both add and stage all of the changes to any given file, but it is also useful in some cases to stage only certain lines from a file.\nMore formally, staging is saying “I’d like these changes to be added to the current version, I think”. Before you commit your changes, you have to first stage them. You can think of this like going to the grocery store: you have items in your cart, but you can put them back at any point before checkout. Staging changes is like adding items to your cart; committing those changes is like checking out.\nIndividually staging lines of a file is most useful in situations where you’ve made changes which should be part of multiple commits. To stage individual lines of a file, you can use git add -i at the command line, or you can attempt to use RStudio’s “stage selection” interface. Both will work, though git can’t always separate changes quite as finely as you might want (and as a result, RStudio’s interface sometimes seems unresponsive, even though the underlying issue is with what git can do).\n\n5.4.5 Committing your changes\nA git commit is the equivalent of a log entry - it tells git to record the state of the file, along with a message about what that state means. On the back end, git will save a copy of the file in its current state to its cache.\n\n\nHere, we commit the red line as a change to our file.\n\n\nIn general, you want your commit message to be relatively short, but also informative. The best way to do this is to commit small blocks of changes. Work to commit every time you’ve accomplished a small task. This will do two things:\n\nYou’ll have small, bite-sized changes that are briefly described to serve as a record of what you’ve done (and what still needs doing)\nWhen you mess up (or end up in a merge conflict) you will have a much easier time pinpointing the spot where things went bad, what code was there before, and (because you have nice, descriptive commit messages) how the error occurred.\n\n5.4.6 Pushing and Pulling\nWhen you’re working alone, you generally won’t need to worry about having to update your local copy of the repository (unless you’re using multiple machines). However, statistics is collaborative, and one of the most powerful parts of git is that you can use it to keep track of changes when multiple people are working on the same document.\n\nIf you are working collaboratively and you and your collaborator are working on the same file, git will be able to resolve the change you make SO LONG AS YOU’RE NOT EDITING THE SAME LINE. Git works based on lines of text - it detects when there is a change in any line of a text document.\nFor this reason, I find it makes my life easier to put each sentence on a separate line, so that I can tweak things with fewer merge conflicts. Merge conflicts aren’t a huge deal, but they slow the workflow down, and are best avoided where possible.\n\nPulling describes the process of updating your local copy of the repository (the copy on your computer) with the files that are “in the cloud” (on GitHub). git pull (or using the Pull button in RStudio) will perform this update for you. If you are working with collaborators in real time, it is good practice to pull, commit, and push often, because this vastly reduces the merge conflict potential (and the scope of any conflicts that do pop up).\nPushing describes the process of updating the copy of the repository on another machine (e.g. on GitHub) so that it has the most recent changes you’ve made to your machine.\n\n\n\n\n\ngit push copies the version of the project on your computer to GitHub\n\n\n\n\n\n\ngit pull copies the version of the project on GitHub to your computer\n\n\n\n\nFigure 5.1: Git push and git pull are used to sync your computer with the remote repository (usually hosted on GitHub)\n\n\nIn general, your workflow will be\n\nClone the project or create a new repository\nMake some changes\nStage the changes with git add\nCommit the changes with git commit\nPull any changes from the remote repository\nResolve any merge conflicts\nPush the changes (and merged files) with git push\n\nIf you’re working alone, steps 5 and 6 are not likely to be necessary, but it is good practice to just pull before you push anyways."
  },
  {
    "objectID": "part-tools/05-git-and-github.html#references",
    "href": "part-tools/05-git-and-github.html#references",
    "title": "5  Version Control with Git",
    "section": "\n5.5 References",
    "text": "5.5 References\n\n\n\n\n[1] \nJ. Bryan, J. Hester, and {The Stat 545 TAs}, Happy git and GitHub for the useR. 2021 [Online]. Available: https://happygitwithr.com/. [Accessed: May 09, 2022]\n\n\n[2] \ndustbuster, “Answer to \"git is not working after macOS update (xcrun: Error: Invalid active developer path (/library/developer/CommandLineTools)\". Stack overflow,” Sep. 26, 2018. [Online]. Available: https://stackoverflow.com/a/52522566/2859168. [Accessed: Jan. 13, 2023]"
  },
  {
    "objectID": "part-gen-prog/00-gen-prog.html",
    "href": "part-gen-prog/00-gen-prog.html",
    "title": "Part II: General Programming",
    "section": "",
    "text": "In this portion of the textbook, we’ll talk about the basics of programming in a general sense (that is, we’re not yet focusing on programming with data).\nBefore we start in on the hard stuff, we’ll quickly go through what programming is and what the vocabulary of a programming language looks like in Chapter 6.\nChapter 7 will discuss the basics: variable types, how to assign variables, and how to convert between simple variable types.\nChapter 8 will discuss how to use built-in and package functions to make R and python more powerful. After this section, you should be able to use R or Python as a calculator.\nChapter 9 will discuss the use of vectors and matrices in R and Python. Along the way, you’ll get a quick refresher in mathematical logic - the use of And, Or, and Not.\nIf you’ve had linear algebra, Chapter 10 will tell you how to use R and python to perform matrix calculations. If you haven’t had linear algebra yet, skip this section and move on to Chapter 11.\nChapter 11 will discuss control structures - ways to change the flow of a program based on variable values and operating condition. This will include discussions of if-statements and different types of loops.\nChapter 12 will discuss writing your own functions.\nOnce we’ve covered these topics, we should be ready to focus on programming with, for, and on data."
  },
  {
    "objectID": "part-gen-prog/00-intro.html#what-is-programming",
    "href": "part-gen-prog/00-intro.html#what-is-programming",
    "title": "6  Introduction to Programming",
    "section": "\n6.1 What is Programming?",
    "text": "6.1 What is Programming?\n\nProgramming today is a race between software engineers striving to build bigger and better idiot-proof programs, and the universe trying to produce bigger and better idiots. So far, the universe is winning. - Rick Cook\n\nProgramming is the art of solving a problem by developing a sequence of steps that make up a solution, and then very carefully communicating those steps to the computer. To program, you need to know how to\n\nbreak a problem down into smaller, easily solvable problems\nsolve the small problems\ncommunicate the solution to a computer using a programming language\n\nIn this book, we’ll be using both R and Python, and we’ll be using these languages to solve problems that are related to working with data. At first, we’ll start with smaller, simpler problems that don’t involve data, but by the end, you will hopefully be able to solve some statistical problems using one or both languages.\nIt will be hard at first - you have to learn the vocabulary in both languages in order to be able to put commands into logical “sentences”. The problem solving skills are the same for all programming languages, though, and while those are harder to learn, they’ll last you a lifetime.\nJust as you wouldn’t expect to learn French or Mandarin fluently after taking a single class, you cannot expect to be fluent in R or python once you’ve worked through this book. Fluency takes years of work and practice, and lots of mistakes along the way. You cannot learn a language (programming or otherwise) if you’re worried about making mistakes. Take a minute and put those concerns away somewhere, take a deep breath, and remember the Magic School Bus Motto:\n\n\nFor those who don’t know, the Magic School Bus is a PBS series that aired in the 1990s and was brought back by Netflix in 2017. It taught kids about different principles of science and the natural world."
  },
  {
    "objectID": "part-gen-prog/00-intro.html#programming-vocabulary-hello-world",
    "href": "part-gen-prog/00-intro.html#programming-vocabulary-hello-world",
    "title": "6  Introduction to Programming",
    "section": "\n6.2 Programming Vocabulary: Hello World",
    "text": "6.2 Programming Vocabulary: Hello World\nI particularly like the way that Python for Everybody [1] explains vocabulary:\n\nUnlike human languages, the Python vocabulary is actually pretty small. We call this “vocabulary” the “reserved words”. These are words that have very special meaning to Python. When Python sees these words in a Python program, they have one and only one meaning to Python. Later as you write programs you will make up your own words that have meaning to you called variables. You will have great latitude in choosing your names for your variables, but you cannot use any of Python’s reserved words as a name for a variable.\n\n\nWhen we train a dog, we use special words like “sit”, “stay”, and “fetch”. When you talk to a dog and don’t use any of the reserved words, they just look at you with a quizzical look on their face until you say a reserved word. For example, if you say, “I wish more people would walk to improve their overall health”, what most dogs likely hear is, “blah blah blah walk blah blah blah blah.” That is because “walk” is a reserved word in dog language. Many might suggest that the language between humans and cats has no reserved words.\n\n\nThe reserved words in the language where humans talk to Python include the following:\n\nand       del       global      not       with\nas        elif      if          or        yield\nassert    else      import      pass\nbreak     except    in          raise\nclass     finally   is          return\ncontinue  for       lambda      try\ndef       from      nonlocal    while\n\nThat is it, and unlike a dog, Python is already completely trained. When you say ‘try’, Python will try every time you say it without fail.\n\n\nWe will learn these reserved words and how they are used in good time, but for now we will focus on the Python equivalent of “speak” (in human-to-dog language). The nice thing about telling Python to speak is that we can even tell it what to say by giving it a message in quotes:\n\n\nprint('Hello world!')\n## Hello world!\n\n\nAnd we have even written our first syntactically correct Python sentence. Our sentence starts with the function print followed by a string of text of our choosing enclosed in single quotes. The strings in the print statements are enclosed in quotes. Single quotes and double quotes do the same thing; most people use single quotes except in cases like this where a single quote (which is also an apostrophe) appears in the string.\n\nR has a slightly smaller set of reserved words:\nif          else     repeat      while\nfor         in       next        break\nTRUE        FALSE    NULL        Inf\nNA_integer_ NA_real_ NA_complex_ NA_character_\nNaN         NA       function    ...\nIn R, the “Hello World” program looks exactly the same as it does in python.\n\nprint('Hello world!')\n## [1] \"Hello world!\"\n\nIn many situations, R and python will be similar because both languages are based on C. R has a more complicated history [2], because it is also similar to Lisp, but both languages are still very similar to C and run C or C++ code in the background."
  },
  {
    "objectID": "part-gen-prog/00-intro.html#getting-help",
    "href": "part-gen-prog/00-intro.html#getting-help",
    "title": "6  Introduction to Programming",
    "section": "\n6.3 Getting help",
    "text": "6.3 Getting help\nIn both R and python, you can access help with a ? - the order is just slightly different.\nSuppose we want to get help on a for loop in either language.\nIn R, we can run this line of code to get help on for loops.\n\n?`for`\n\nBecause for is a reserved word in R, we have to use backticks (the key above the TAB key) to surround the word for so that R knows we’re talking about the function itself. Most other function help can be accessed using ?function_name. The backtick trick also works for functions that don’t start with letters, like +.\nIn python, we use for? to access the same information.\n\nfor? # help printed in the terminal\n?for # help printed in the help pane\n\n(You will have to run this in interactive mode for it to work in either language)\nw3schools has an excellent python help page that may be useful as well. Searching for help using google also works well, particularly if you know what sites are likely to be helpful, like w3schools and stackoverflow. A similar set of pages exists for R help on basic functions\n\n\n\n\n\n\nLearn More\n\n\n\nA nice explanation of the difference between an interpreter and a compiler. Both Python and R are interpreted languages that are compiled from lower-level languages like C."
  },
  {
    "objectID": "part-gen-prog/00-intro.html#sec-gen-prog-refs",
    "href": "part-gen-prog/00-intro.html#sec-gen-prog-refs",
    "title": "6  Introduction to Programming",
    "section": "\n6.4 References",
    "text": "6.4 References\n\n\n\n\n[1] \nD. C. R. Severance, Python for Everybody: Exploring Data in Python 3. Ann Arbor, MI: CreateSpace Independent Publishing Platform, 2016 [Online]. Available: https://www.py4e.com/html3/\n\n\n\n[2] \nR. Ihaka, “R : Past and future history,” 1998 [Online]. Available: https://www.stat.auckland.ac.nz/~ihaka/downloads/Interface98.pdf"
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#fa-bullseye-objectives",
    "href": "part-gen-prog/01-basic-var-types.html#fa-bullseye-objectives",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.1  Objectives",
    "text": "7.1  Objectives\n\nKnow the basic data types and what their restrictions are\nKnow how to test to see if a variable is a given data type\nUnderstand the basics of implicit and explicit type conversion\nWrite code that assigns values to variables"
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#basic-definitions",
    "href": "part-gen-prog/01-basic-var-types.html#basic-definitions",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.2 Basic Definitions",
    "text": "7.2 Basic Definitions\nFor a general overview, [1] is an excellent introduction to data types:\n\n\n\n\nLet’s start this section with some basic vocabulary.\n\na value is a basic unit of stuff that a program works with, like 1, 2, \"Hello, World\", and so on.\nvalues have types - 2 is an integer, \"Hello, World\" is a string (it contains a “string” of letters). Strings are in quotation marks to let us know that they are not variable names.\n\nIn most programming languages (including R and python), there are some very basic data types:\n\nlogical or boolean - FALSE/TRUE or 0/1 values. Sometimes, boolean is shortened to bool\ninteger - whole numbers (positive or negative)\n\ndouble or float or numeric- decimal numbers.\n\n\nfloat is short for floating-point value.\n\ndouble is a floating-point value with more precision (“double precision”).1\n\nR uses the name numeric to indicate a decimal value, regardless of precision.\n\n\ncharacter or string - holds text, usually enclosed in quotes.\n\n\n\n\n\n\n\nCapitalization matters!\n\n\n\nIn R, boolean values are TRUE and FALSE, but in Python they are True and False. Capitalization matters a LOT.\nOther things matter too: if we try to write a million, we would write it 1000000 instead of 1,000,000 (in both languages). Commas are used for separating numbers, not for proper spacing and punctuation of numbers. This is a hard thing to get used to but very important – especially when we start reading in data."
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#variables",
    "href": "part-gen-prog/01-basic-var-types.html#variables",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.3 Variables",
    "text": "7.3 Variables\nProgramming languages use variables - names that refer to values. Think of a variable as a container that holds something - instead of referring to the value, you can refer to the container and you will get whatever is stored inside.\n\n7.3.1 Assignment\nWe assign variables values using the syntax object_name <- value (R) or object_name = value (python). You can read this as “object name gets value” in your head.\nIn R, <- is used for assigning a value to a variable. So x <- \"R is awesome\" is read “x gets ‘R is awesome’” or “x is assigned the value ‘R is awesome’”. Technically, you can also use = to assign things to variables in R, but most style guides consider this to be poor programming practice, so seriously consider defaulting to <-.\nIn Python, = is used for assigning a value to a variable. This tends to be much easier to say out loud, but lacks any indication of directionality.\n\n\n\n\n\n\nDemo: Assignment\n\n\n\n\n\nR\nPython\n\n\n\n\nmessage <- \"So long and thanks for all the fish\"\nyear <- 2025\nthe_answer <- 42L\nearth_demolished <- FALSE\n\n\n\n\nmessage = \"So long and thanks for all the fish\"\nyear = 2025\nthe_answer = 42\nearth_demolished = False\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that in R, we assign variables values using the <- operator, where in Python, we assign variables values using the = operator. Technically, = will work for assignment in both languages, but <- is more common than = in R by convention.\n\n\nWe can then use the variables - do numerical computations, evaluate whether a proposition is true or false, and even manipulate the content of strings, all by referencing the variable by name.\n\n7.3.2 Naming Variables\n\nThere are only two hard things in Computer Science: cache invalidation and naming things.\n– Phil Karlton\n\nObject names must start with a letter and can only contain letters, numbers, _, and . in R. In Python, object names must start with a letter and can consist of letters, numbers, and _ (that is, . is not a valid character in a Python variable name). While it is technically fine to use uppercase variable names in Python, it’s recommended that you use lowercase names for variables (you’ll see why later).\nWhat happens if we try to create a variable name that isn’t valid?\nIn both languages, starting a variable name with a number will get you an error message that lets you know that something isn’t right - “unexpected symbol” in R and “invalid syntax” in python.\n\n\n\n\n\n\nInvalid Names\n\n\n\n\n\nR\nPython\n\n\n\n\n1st_thing <- \"check your variable names!\"\n## Error: <text>:1:2: unexpected symbol\n## 1: 1st_thing\n##      ^\n\n\n\n\n1st_thing <- \"check your variable names!\"\n\nNote: Run the above chunk in your python window - the book won’t compile if I set it to evaluate 😥. It generates an error of SyntaxError: invalid syntax (<string>, line 1)\n\nsecond.thing <- \"this isn't valid\"\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'second' is not defined\n\nIn python, trying to have a . in a variable name gets a more interesting error: “ is not defined”. This is because in python, some objects have components and methods that can be accessed with .. We’ll get into this more later, but there is a good reason for python’s restriction about not using . in variable names.\n\n\n\n\n\nNaming things is difficult! When you name variables, try to make the names descriptive - what does the variable hold? What are you going to do with it? The more (concise) information you can pack into your variable names, the more readable your code will be.\n\n\n\n\n\n\nLearn More\n\n\n\nWhy is naming things hard? - Blog post by Neil Kakkar\n\n\nThere are a few different conventions for naming things that may be useful:\n\n\nsome_people_use_snake_case, where words are separated by underscores\n\nsomePeopleUseCamelCase, where words are appended but anything after the first word is capitalized (leading to words with humps like a camel).\n\nsome.people.use.periods (in R, obviously this doesn’t work in python)\nA few people mix conventions with variables_thatLookLike.this and they are almost universally hated 👿\n\nAs long as you pick ONE naming convention and don’t mix-and-match, you’ll be fine. It will be easier to remember what you named your variables (or at least guess) and you’ll have fewer moments where you have to go scrolling through your script file looking for a variable you named."
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#types",
    "href": "part-gen-prog/01-basic-var-types.html#types",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.4 Types",
    "text": "7.4 Types\n\n\n\n\n\n\nTesting Types\n\n\n\nYou can use different functions to test whether a variable has a specific type.\n\n\nR\nPython\n\n\n\n\nis.logical(FALSE)\nis.integer(2L) # by default, R treats all numbers as numeric/decimal values. \n          # The L indicates that we're talking about an integer. \nis.integer(2)\nis.numeric(2)\nis.character(\"Hello, programmer!\")\nis.function(print)\n## [1] TRUE\n## [1] TRUE\n## [1] FALSE\n## [1] TRUE\n## [1] TRUE\n## [1] TRUE\n\nIn R, you use is.xxx functions, where xxx is the name of the type in question.\n\n\n\nisinstance(False, bool)\nisinstance(2, int)\nisinstance(2, (int, float)) # Test for one of multiple types\nisinstance(3.1415, float)\nisinstance(\"This is python code\", str)\n## True\n## True\n## True\n## True\n## True\n\nIn python, test for types using the isinstance function with an argument containing one or more data types in a tuple ((int, float) is an example of a tuple - a static set of multiple values).\nIf we want to test for whether something is callable (can be used like a function), we have to get slightly more complicated:\n\ncallable(print)\n## True\n\nThis is glossing over some much more technical information about differences between functions and classes (that we haven’t covered) [2].\n\n\n\n\n\n\n\n\n\n\n\nExample: Assignment and Testing Types\n\n\n\n\n\nCharacter\nLogical\nInteger\nDouble\nNumeric\n\n\n\n\nx <- \"R is awesome\"\ntypeof(x)\n## [1] \"character\"\nis.character(x)\n## [1] TRUE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] FALSE\n\n\nx = \"python is awesome\"\ntype(x)\n## <class 'str'>\nisinstance(x, str)\n## True\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## False\nisinstance(x, float)\n## False\n\n\n\n\nx <- FALSE\ntypeof(x)\n## [1] \"logical\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] TRUE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] FALSE\n\nIn R, is possible to use the shorthand F and T, but be careful with this, because F and T are not reserved, and other information can be stored within them. See this discussion for pros and cons of using F and T as variables vs. shorthand for true and false. 2\n\nx = False\ntype(x)\n## <class 'bool'>\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## True\nisinstance(x, int)\n## True\nisinstance(x, float)\n## False\n\nNote that in python, boolean variables are also integers. If your goal is to test whether something is a T/F value, you may want to e.g. test whether its value is one of 0 or 1, rather than testing whether it is a boolean variable directly, since integers can also function directly as bools in Python.\n\n\n\nx <- 2\ntypeof(x)\n## [1] \"double\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] TRUE\n\nWait, 2 is an integer, right?\n2 is an integer, but in R, values are assumed to be doubles unless specified. So if we want R to treat 2 as an integer, we need to specify that it is an integer specifically.\n\nx <- 2L # The L immediately after the 2 indicates that it is an integer.\ntypeof(x)\n## [1] \"integer\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] TRUE\nis.double(x)\n## [1] FALSE\nis.numeric(x)\n## [1] TRUE\n\n\nx = 2\ntype(x)\n## <class 'int'>\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## True\nisinstance(x, float)\n## False\n\n\n\n\nx <- 2.45\ntypeof(x)\n## [1] \"double\"\nis.character(x)\n## [1] FALSE\nis.logical(x)\n## [1] FALSE\nis.integer(x)\n## [1] FALSE\nis.double(x)\n## [1] TRUE\nis.numeric(x)\n## [1] TRUE\n\n\nx = 2.45\ntype(x)\n## <class 'float'>\nisinstance(x, str)\n## False\nisinstance(x, bool)\n## False\nisinstance(x, int)\n## False\nisinstance(x, float)\n## True\n\n\n\nA fifth common “type”3, numeric is really the union of two types: integer and double, and you may come across it when using str() or mode(), which are similar to typeof() but do not quite do the same thing.\nThe numeric category exists because when doing math, we can add an integer and a double, but adding an integer and a string is … trickier. Testing for numeric variables guarantees that we’ll be able to do math with those variables. is.numeric() and as.numeric() work as you would expect them to work.\nThe general case of this property of a language is called implicit type conversion - that is, R will implicitly (behind the scenes) convert your integer to a double and then add the other double, so that the result is unambiguously a double."
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#sec-type-conversions",
    "href": "part-gen-prog/01-basic-var-types.html#sec-type-conversions",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.5 Type Conversions",
    "text": "7.5 Type Conversions\nProgramming languages will generally work hard to seamlessly convert variables to different types. This is called implicit type casting - the computer implicitly changes the variable type to avoid a conflict.\n\n\n\n\n\n\nImplicit Type Conversion\n\n\n\n\n\nR\nPython\n\n\n\n\nTRUE + 2\n## [1] 3\n\n2L + 3.1415\n## [1] 5.1415\n\n\"abcd\" + 3\n## Error in \"abcd\" + 3: non-numeric argument to binary operator\n\n\n\n\nTrue + 2\n## 3\nint(2) + 3.1415\n## 5.141500000000001\n\"abcd\" + 3\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: can only concatenate str (not \"int\") to str\n\n\n\n\nThis conversion doesn’t always work - there’s no clear way to make “abcd” into a number we could use in addition. So instead, R or python will issue an error. This error pops up frequently when something went wrong with data import and all of a sudden you just tried to take the mean of a set of string/character variables. Whoops.\n\n\nWhen you want to, you can also use as.xxx() to make the type conversion explicit. So, the analogue of the code above, with explicit conversions would be:\n\n\n\n\n\n\nExplicit Type Conversion\n\n\n\n\n\nR\nPython\n\n\n\n\nas.double(TRUE) + 2\n## [1] 3\n\nas.double(2L) + 3.1415\n## [1] 5.1415\n\nas.numeric(\"abcd\") + 3\n## [1] NA\n\n\n\n\nint(True) + 2\n## 3\nfloat(2) + 3.1415\n## 5.141500000000001\nfloat(\"abcd\") + 3\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: could not convert string to float: 'abcd'\nimport pandas as pd # Load pandas library\npd.to_numeric(\"abcd\", errors = 'coerce') + 3\n## nan\n\n\n\n\nWhen we make our intent explicit (convert “abcd” to a numeric variable) we get an NA - a missing value - in R. In Python, we get a more descriptive error by default, but we can use the pandas library (which adds some statistical functionality) to get a similar result to the result we get in R.\nThere’s still no easy way to figure out where “abcd” is on a number line, but our math will still have a result - NA + 3 is NA."
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#what-type-is-it",
    "href": "part-gen-prog/01-basic-var-types.html#what-type-is-it",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.6 What Type is it?",
    "text": "7.6 What Type is it?\nIf you don’t know what type a value is, both R and python have functions to help you with that.\n\n\n\n\n\n\nDetermining Variable Types\n\n\n\n\n\nR\nPython\n\n\n\nIf you are unsure what the type of a variable is, use the typeof() function to find out.\n\nw <- \"a string\"\nx <- 3L\ny <- 3.1415\nz <- FALSE\n\ntypeof(w)\n## [1] \"character\"\ntypeof(x)\n## [1] \"integer\"\ntypeof(y)\n## [1] \"double\"\ntypeof(z)\n## [1] \"logical\"\n\n\n\nIf you are unsure what the type of a variable is, use the type() function to find out.\n\nw = \"a string\"\nx = 3\ny = 3.1415\nz = False\n\ntype(w)\n## <class 'str'>\ntype(x)\n## <class 'int'>\ntype(y)\n## <class 'float'>\ntype(z)\n## <class 'bool'>\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: Variables and Types\n\n\n\n\n\nR\nPython\nR Solution\nPython Solution\n\n\n\n\nCreate variables string, integer, decimal, and logical, with types that match the relevant variable names.\n\n\nstring <- \ninteger <- \ndecimal <- \nlogical <- \n\n\nCan you get rid of the error that occurs when this chunk is run?\n\n\nlogical + decimal\ninteger + decimal\nstring + integer\n\n\nWhat happens when you add string to string? logical to logical?\n\n\n\n\nCreate variables string, integer, decimal, and logical, with types that match the relevant variable names.\n\n\nstring = \ninteger = \ndecimal = \nlogical = \n\n\nCan you get rid of the error that occurs when this chunk is run?\n\n\nlogical + decimal\ninteger + decimal\nstring + integer\n\n\nWhat happens when you add string to string? logical to logical?\n\n\n\n\nstring <- \"hi, I'm a string\"\ninteger <- 4L\ndecimal <- 5.412\nlogical <- TRUE\n\nlogical + decimal\n## [1] 6.412\ninteger + decimal\n## [1] 9.412\nas.numeric(string) + integer\n## [1] NA\n\n\"abcd\" + \"efgh\"\n## Error in \"abcd\" + \"efgh\": non-numeric argument to binary operator\nTRUE + TRUE\n## [1] 2\n\nIn R, adding a string to a string creates an error (“non-numeric argument to binary operator”). Adding a logical to a logical, e.g. TRUE + TRUE, results in 2, which is a numeric value.\nTo concatenate strings in R (like the default behavior in python), we would use the paste0 function: paste0(\"abcd\", \"efgh\"), which returns abcdefgh.\n\n\n\nimport pandas as pd\n\nstring = \"hi, I'm a string\"\ninteger = 4\ndecimal = 5.412\nlogical = True\n\nlogical + decimal\n## 6.412\ninteger + decimal\n## 9.411999999999999\npd.to_numeric(string, errors='coerce') + integer\n## nan\n\"abcd\" + \"efgh\"\n## 'abcdefgh'\nTrue + True\n## 2\n\nIn Python, when a string is added to another string, the two strings are concatenated. This differs from the result in R, which is a “non-numeric argument to binary operator” error."
  },
  {
    "objectID": "part-gen-prog/01-basic-var-types.html#sec-basic-var-types-refs",
    "href": "part-gen-prog/01-basic-var-types.html#sec-basic-var-types-refs",
    "title": "7  Variables and Basic Data Types",
    "section": "\n7.7 References",
    "text": "7.7 References\n\n\n\n\n[1] \n\nWhy TRUE + TRUE = 2: Data Types. (Feb. 03, 2020) [Online]. Available: https://www.youtube.com/watch?v=6otW6OXjR8c. [Accessed: May 18, 2022]\n\n\n[2] \nRyan, “Answer to \"how do i detect whether a variable is a function?\". Stack overflow,” Mar. 09, 2009. [Online]. Available: https://stackoverflow.com/a/624948/2859168. [Accessed: Jan. 10, 2023]"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#fa-bullseye-objectives",
    "href": "part-gen-prog/02-prog-functions.html#fa-bullseye-objectives",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.1  Objectives",
    "text": "8.1  Objectives\n\nUnderstand how functions are used in R and python\nUnderstand how to install packages in R and python\nUnderstand how to load packages in R and python\nUse pipes to restructure code so that it is more readable"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#mathematical-operators",
    "href": "part-gen-prog/02-prog-functions.html#mathematical-operators",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.2 Mathematical Operators",
    "text": "8.2 Mathematical Operators\nLet’s first start with a special class of functions that you’re probably familiar with from your math classes - mathematical operators.\nHere are a few of the most important ones:\n\n\nOperation\nR symbol\nPython symbol\n\n\n\nAddition\n+\n+\n\n\nSubtraction\n-\n-\n\n\nMultiplication\n*\n*\n\n\nDivision\n/\n/\n\n\nInteger Division\n%/%\n//\n\n\nModular Division\n%%\n%\n\n\nExponentiation\n^\n**\n\n\n\nThese operands are all for scalar operations (operations on a single number) - vectorized versions, such as matrix multiplication, are somewhat more complicated (and different between R and python).\n\n\n\n\n\n\nExample: Integer and Modular Division\n\n\n\nInteger division is the whole number answer to A/B, and modular division is the fractional remainder when A/B.\nLet’s demonstrate with the problem 14/3, which evaluates to 4.6666667 when division is used, but has integer part 4 and remainder 2.\n\n\nR\nPython\n\n\n\n14 %/% 3 in R would be 4, and 14 %% 3 in R would be 2.\n\n14 %/% 3\n## [1] 4\n14 %% 3\n## [1] 2\n\n\n\n\n14 // 3\n## 4\n14 % 3\n## 2"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#order-of-operations",
    "href": "part-gen-prog/02-prog-functions.html#order-of-operations",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.3 Order of Operations",
    "text": "8.3 Order of Operations\nBoth R and Python operate under the same mathematical rules of precedence that you learned in school. You may have learned the acronym PEMDAS, which stands for Parentheses, Exponents, Multiplication/Division, and Addition/Subtraction. That is, when examining a set of mathematical operations, we evaluate parentheses first, then exponents, and then we do multiplication/division, and finally, we add and subtract.\n\n\nR\nPython\n\n\n\n\n(1+1)^(5-2) \n## [1] 8\n1 + 2^3 * 4 \n## [1] 33\n3*1^3 \n## [1] 3\n\n\n\n\n(1+1)**(5-2)\n## 8\n1 + 2**3*4\n## 33\n3*1**3\n## 3"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#simple-string-operations",
    "href": "part-gen-prog/02-prog-functions.html#simple-string-operations",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.4 Simple String Operations",
    "text": "8.4 Simple String Operations\nPython has some additional operators that work on strings. In R, you will have to use functions to perform these operations, as R does not have string operators.\n\n\n\n\n\n\nDemo\n\n\n\n\n\nPython\nR\n\n\n\nIn Python, + will concatenate (stick together) two strings. Multiplying a string by an integer will repeat the string the specified number of times.\n\n\"first \" + \"second\"\n## 'first second'\n\"hello \" * 3\n## 'hello hello hello '\n\n\n\nIn R, to concatenate things, we need to use functions: paste or paste0:\n\npaste(\"first\", \"second\", sep = \" \")\n## [1] \"first second\"\npaste(\"first\", \"second\", collapse = \" \")\n## [1] \"first second\"\npaste(c(\"first\", \"second\"), sep = \" \") # sep only works w/ 2 objects passed in\n## [1] \"first\"  \"second\"\npaste(c(\"first\", \"second\"), collapse = \" \") # collapse works on vectors\n## [1] \"first second\"\n\npaste(c(\"a\", \"b\", \"c\", \"d\"), \n      c(\"first\", \"second\", \"third\", \"fourth\"), \n      sep = \"-\", collapse = \" \")\n## [1] \"a-first b-second c-third d-fourth\"\n# sep is used to collapse parameters, then collapse is used to collapse vectors\n\npaste0(c(\"a\", \"b\", \"c\"))\n## [1] \"a\" \"b\" \"c\"\npaste0(\"a\", \"b\", \"c\") # equivalent to paste(..., sep = \"\")\n## [1] \"abc\"\n\nYou don’t need to understand the details of this code at this point in the class, but it is useful to know how to combine strings in both languages."
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#using-functions",
    "href": "part-gen-prog/02-prog-functions.html#using-functions",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.5 Using Functions",
    "text": "8.5 Using Functions\nFunctions are sets of instructions that take arguments and return values. Strictly speaking, mathematical operators (like those above) are a special type of functions – but we aren’t going to get into that now.\nWe’re also not going to talk about how to create our own functions just yet. Instead, I’m going to show you how to use functions.\n\n\n\n\n\n\nCheat Sheets!\n\n\n\nIt may be helpful at this point to print out the R reference card1 and the Python reference card2 . These cheat sheets contain useful functions for a variety of tasks in each language .\n\n\nMethods are a special type of function that operate on a specific variable type. In Python, methods are applied using the syntax variable.method_name(). So, you can get the length of a string variable my_string using my_string.length().\nR has methods too, but they are invoked differently. In R, you would get the length of a string variable using length(my_string).\nRight now, it is not really necessary to know too much more about functions than this: you can invoke a function by passing in arguments, and the function will do a task and return the value.\n\n\n\n\n\n\nYour Turn\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nTry out some of the functions mentioned on the R and Python cheatsheets.\nCan you figure out how to define a list or vector of numbers? If so, can you use a function to calculate the maximum value?\nCan you find the R functions that will allow you to repeat a string variable multiple times or concatenate two strings? Can you do this task in Python?\n\n\n\n# Define a vector of numbers\nx <- c(1, 2, 3, 4, 5)\n\n# Calculate the maximum\nmax(x)\n## [1] 5\n\n# function to repeat a variable multiple times\nrep(\"test\", 3)\n## [1] \"test\" \"test\" \"test\"\n# Concatenate strings, using \"ing... \" as the separator\npaste(rep(\"test\", 3), collapse = \"ing... \")\n## [1] \"testing... testing... test\"\n\n\n\n\n# Define a list of numbers\nx = [1, 2, 3, 4, 5]\n\n# Calculate the maximum\nmax(x)\n\n# Repeat a string multiple times\n## 5\nx = (\"test\", )*3 # String multiplication \n                 # have to use a tuple () to get separate items\n# Then use 'yyy'.join(x) to paste items of x together with yyy as separators\n'ing... '.join(x)\n## 'testing... testing... test'"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#overpowerd-calculators",
    "href": "part-gen-prog/02-prog-functions.html#overpowerd-calculators",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.6 Overpowerd Calculators",
    "text": "8.6 Overpowerd Calculators\nNow that you’re familiar with how to use functions, if not how to define them, you are capable of using R or python as a very fancy calculator. Obviously, both languages can do many more interesting things, which we’ll get to, but let’s see if we can make R and Python do some very basic stuff that hopefully isn’t too foreign to you.\n\n\n\n\n\n\nExample: Triangle Side Length\n\n\n\n\n\nA right triangle with sides a, b, and hypotenuse c labeled.\n\n\nConsider this triangle. I’ve measured the sides in an image editor and determined that \\(a = 212\\) pixels, \\(b = 345\\) pixels, and \\(c = 406\\) pixels. I suspect, however, that my measurements aren’t quite right - for one thing, I tried to measure in the center of the line, but it wasn’t easy on the diagonal.\nLet’s assume that my measurements for \\(a\\) and \\(b\\) are accurate and calculate how far off my estimate was for side \\(c\\).\n\n\nR\nPython\n\n\n\n\n# Define variables for the 3 sides of the triangle\na <- 212\nb <- 345\nc_meas <- 406\nc_actual <- sqrt(a^2 + b^2)\n\n# Calculate difference between measured and actual\n# relative to actual \n# and make it a percentage\npct_error <- (c_meas - c_actual)/c_actual * 100\npct_error\n## [1] 0.2640307\n\n\n\n\n# To get the sqrt function, we have to import the math package\nimport math\n\n# Define variables for the 3 sides of the triangle\na = 212\nb = 345\nc_meas = 406\nc_actual = math.sqrt(a**2 + b**2)\n\n# Calculate difference between measured and actual\n# relative to actual \n# and make it a percentage\npct_error = (c_meas - c_actual)/c_actual * 100\npct_error\n## 0.264030681414134\n\n\n\n\nInteresting, I wasn’t as inaccurate as I thought!\n\n\n\n\n\n\n\n\nYour Turn\n\n\n\nOf course, if you remember trigonometry, we don’t have to work with right triangles. Let’s see if we can use trigonometric functions to do the same task with an oblique triangle.\n\n\nProblem\nR solution\nPython solution\n\n\n\nJust in case you’ve forgotten your Trig, the Law of Cosines says that \\[c^2 = a^2 + b^2 - 2 a b \\cos(C),\\] where \\(C\\) is the angle between sides \\(a\\) and \\(b\\).\n\n\nAn oblique triangle with sides labeled a, b, and c, and angles labeled as A, B, C with capital letter opposite the lowercase side.\n\n\nI measure side \\(a = 291\\) pixels, side \\(b = 414\\) pixels, and the angle between \\(a\\) and \\(b\\) to be \\(67.6^\\circ\\). What will I likely get for the length of side \\(c\\) in pixels?\nRemember to check whether R and python compute trig functions using radians or degrees! As a reminder, \\(\\pi\\) radians = \\(180^\\circ\\).\n\n\n\n# Define variables for the 3 sides of the triangle\na <- 291\nb <- 414\nc_angle <- 67.6\nc_actual <- sqrt(a^2 + b^2 - 2*a*b*cos(c_angle/180*pi))\nc_actual\n## [1] 405.2886\n\nI measured the length of side \\(c\\) as 407 pixels.\n\n\n\n# To get the sqrt and cos functions, we have to import the math package\nimport math\n\n# Define variables for the 3 sides of the triangle\na = 291\nb = 414\nc_angle = 67.6\nc_actual = math.sqrt(a**2 + b**2 - 2*a*b*math.cos(c_angle/180*math.pi))\nc_actual\n## 405.28860699402117\n\nI measured the length of side \\(c\\) as 407 pixels.\n\n\n\n\n\nCongratulations, if you used a TI-83 in high school to do this sort of stuff, you’re now just about as proficient with R and python as you were with that!"
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#libraries",
    "href": "part-gen-prog/02-prog-functions.html#libraries",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.7 Libraries",
    "text": "8.7 Libraries\nBoth R and python have a very robust system for extending the language with user-written packages. These packages will give you access to features that aren’t present in the base language, including new statistical methods, all sorts of plotting and visualization libraries, ways of interacting with data that are way more convenient than the default base language methods, and more.\nThere are tons of considerations to think about when using a new library, like how well it’s maintained, how many dependencies it has, and whether the developers of the package prioritize backwards-compatibility. For the moment, we’re going to ignore most of those considerations in favor of learning how to install packages and how to use functions from packages in our code.\n\n8.7.1 Package management\nBefore we talk about how to install packages, though, we need to step back and think a little bit about the pros and cons of different ways of managing packages, if only because the most common R and python setups use very different approaches.\nImagine that you’re an accomplished programmer, and you are juggling multiple different projects. Each project uses some of the same packages, but some different packages as well. You open up a project that you haven’t run in a year, and you find out that one of the packages you’ve updated more recently breaks a bunch of code you wrote a year ago, because the functions in the package have been renamed.\nWhat could prevent this from happening?\nOne way to solve this problem is to store the packages used in each project inside the project directory, in what we might call a project environment. This will keep each project isolated from the others, so that if you update a package in one project, it doesn’t affect any other project.\nHowever, this approach results in a lot of duplication: for one thing, you have copies of each package hanging around in every folder on your computer. That’s not storage efficient, but it does keep your code from breaking as frequently.\nTypically, Python programmers prefer the first approach (project-specific virtual environments), and R programmers default to the second approach (installing packages at the user or system level).\nThis is one of the things that can make starting to learn python so difficult - it can be hard to make sure you’re using the right environment. It doesn’t help that there are several different environment management systems in python - virtualenv, pipenv, and conda are the main options.\n\n\n\n\n\n\nPackage Management in Python with RStudio\n\n\n\nMany of the instructions here modified from [1].\nconda and virtualenv are both virtual environment management systems.\nConda is sometimes preferred for scientific computing because it handles the complex dependencies that arise from large packages like numpy and scipi and pandas a bit better than pip does alone.\nThis guide assumes you have conda set up already. By default, Chapter 2 just installs python at the system level. If you want to use anaconda or miniconda you should go read the documentation for those installers and follow those steps first. Alternately, you can install and load the reticulate R package and then run install_miniconda() - this will install miniconda somewhere that RStudio can find it, but it may make using miniconda outside of RStudio difficult.\n\n\nSystem package installation\nvenv Terminal setup\nvenv RStudio setup\nconda Terminal setup\nconda RStudio setup\n\n\n\nYou can absolutely install all python packages at the user/system level using pip. This has the previously mentioned disadvantages, but has the major advantage of being very simple.\nI highly recommend that you pick one of these options and use that consistently, rather than trying the advantages and disadvantages of each option in different projects. Here is a webcomic to serve as a cautionary tale if you do not heed this warning.\n\n\nPython Environment, by Randall Munroe of [xkcd](https://xkcd.com/1987/). CC-By-NC-2.5The Python environmental protection agency wants to seal it in a cement chamber, with pictorial messages to future civilizations warning them about the danger of using sudo to install random Python packages.\n\n\n\n\nIn your system terminal, navigate to your project directory. Items within < > are intended to be replaced with values specific to your situation.\n\ncd <project-directory>\npip3 install virtualenv # install virtual environments\n\n# Create a virtual environment\nvirtualenv <env-name>\n\n# Activate your virtual environment\nsource <env-name>/bin/activate\n\n# Install packages\npip install <pkg1> <pkg2> <pkg3>\n\nThen, in RStudio, you will want to run the following lines in the R terminal:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"<env-name>/bin/python\") \n\nYou can make this step permanent by modifying the .Rprofile file in your project directory and adding the Sys.setenv() line to that file.\nRestart your R session before you start trying to work in python.\n\n\nOpen your RStudio project. In your R terminal, run the following lines:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\nvirtualenv_create(envname = \"<env-name>\",\n                  packages = c(\"<pkg1>\", \"<pkg2>\", \"<pkg3>\"))\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"<env-name>/bin/python\") \n\n# Activate your virtual environment\nuse_virtualenv(\"<env-name>\")\n\n# Check that the correct python instance is being used\npy_config()\n\n# Check that packages are installed in your virtual env\ngrep(pattern = \"<pkg1>|<pkg2>|<pkg3>\",\n     x = as.character(py_list_packages(envname = \"<env-name>\")$package))\n\nRestart your R session before you start trying to work in python.\n\n\nThese steps constructed from [2].\n\ncd <project-directory>\n# Create conda environment and install specific python version and packages \nconda create --prefix ./<env-name> python=<python-version> <pkg1> <pkg2> <pkg3> \n\n# Activate your virtual environment\nconda activate ./<env-name>\n\nThen, in RStudio, you will want to run the following lines in the R terminal:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"./<env-name>/bin/python\") \n\nYou can make this step permanent by modifying the .Rprofile file in your project directory and adding the Sys.setenv() line to that file.\nRestart your R session before you start trying to work in python.\n\n\nOpen your RStudio project. In your R terminal, run the following lines:\n\ninstall.packages(\"reticulate\")\nlibrary(reticulate)\nconda_create(envname = \"<env-name>\",\n             packages = c(\"<pkg1>\", \"<pkg2>\", \"<pkg3>\"))\n\n# tell R/Rstudio what python to use\nSys.setenv(RETICULATE_PYTHON = \"<env-name>/bin/python\") \n\n# Activate your virtual environment\nuse_condaenv(\"<env-name>\")\n\n# Check that the correct python instance is being used\npy_config()\n\n# Check that packages are installed in your virtual env\ngrep(pattern = \"<pkg1>|<pkg2>|<pkg3>\",\n     x = as.character(py_list_packages(envname = \"<env-name>\")$package))\n\nRestart your R session before you start trying to work in python.\n\n\n\n\n\n\n\n\n\n\n\nPackage Management in R\n\n\n\nPackage management in R is a bit simpler than package management in python.\n\n\nSystem package installation\nrenv\n\n\n\nThis is by far the most common option for R users and it requires just about no setup on your part. Just install packages with install.packages() if you’re installing packages that are on CRAN, or remotes::install_github() if you’re installing packages from github, and you’re good to go, unless something goes wrong.\n\n\nrenv is an attempt to bring virtual environments to R. It’s pretty useful for complex or long-term projects, but it can be pain for very simple and uncomplicated projects.\nIf you want to use renv, you can do that by following these steps:\n\ninstall.packages(\"renv\")\n\nlibrary(renv)\n\n# Activate renv for a project\nrenv::activate()\n\n# this will install from github or CRAN\nrenv::install(c(\"pkg1\", \"pkg2\", \"githubuser/pkg3\")) \n\nI use renv to manage dependencies in this book, but I don’t generally use it for most of my other projects because the overhead is a bit of a pain.\n\n\n\n\n\n\n8.7.2 Package repositories\nBoth R and Python have package systems, though generally, R is a bit more straightforward to deal with than python, because python has more package systems and I don’t fully understand them all, where all R packages seem to go through the same basic installation process and are just hosted in different places.\n\n\n\nFormally Published\nInformally Published/Beta\n\n\n\nR\n\nCRAN, Bioconductor\n\ngithub and other version control. See the remotes package documentation for all of the options.\n\n\nPython\nPyPi\ngithub and other version control systems\n\n\n\n8.7.3 Installing packages\nOnce you’ve configured your package management in R and python, installation is (usually) relatively simple.\n\n\n\n\n\n\nPackage installation\n\n\n\n\n\nR\nPython\n\n\n\n\n# CRAN packages\ninstall.packages(\"<pkg1>\")\n\n# Github packages\nremotes::install_github(\"username/reponame\")\n\n\n\nIn python, you will typically want to install packages using a system terminal.\n\n# If you're using virtualenv\npip install <pkg1>\n\n# If you're using conda, try this first\nconda install <pkg1>\n# If that fails, try pip\n\nIf you’re working in a quarto markdown chunk you can use a special trick to install a package:\n\nimport sys\n# For pip installation\n!{sys.executable} -m pip install <pkg1>\n\n# For conda installation\n!{sys.executable} -m conda install <pkg1>\n\nLoading the sys package ensures that you’re using the version of python that your file will be compiled with to install the package. Once you’ve installed the package on your machine, you can comment these lines out so that they don’t run every time - this makes it a bit easier if you try to run old code on a new machine, as you can just uncomment those lines.\n\n\n\n\n\n\n8.7.4 Loading Packages\nOnce you have the package installed, you need to load the package into memory so that you can use the functions and data contained within. Again, R and python differ slightly in how programmers conventionally handle this process.\n\nR: Load all of the package’s functions, overwriting already loaded functions if necessary\nPython: Load all of the package’s functions, contained within an object that is either the package name or a shortened alias.\n\nNow, both R and python can load packages in either way, so this isn’t an either/or thing - it’s about knowing what the conventions of the language are, and then deciding whether or not it is appropriate to follow those conventions in your project.\n\n\n\n\n\n\nImport the whole package and all functions\n\n\n\nTo demonstrate this approach, let’s create a simple plot with a plotting library (ggplot2 in R, plotnine in Python).\n\n\nR\nPython\n\n\n\nAll of the other packages in this plot are present by default in any new R environment.\n\nlibrary(ggplot2)\n\n# This code lists all the functions available to be called\npkgs <- search()\npkgs <- pkgs[grep(\"package:\",pkgs)]\n# get all the functions in each package that is loaded\nall_fns <- lapply(pkgs, function(x) as.character(lsf.str(x)))\n# create a data frame\npkg_fns <- data.frame(pkg = rep(pkgs, sapply(all_fns, length)), \n                      fn = unlist(all_fns))\npkg_fns$pkg <- gsub(\"package:\", \"\", pkg_fns$pkg)\n\n\nggplot(pkg_fns, aes(x = pkg, y = after_stat(count), fill = pkg)) + \n  geom_bar() + theme(legend.position = \"none\") + \n  ylab(\"# Functions\") + xlab(\"Package\")\n\n\n\n\n\n\n\nfrom plotnine import *\n\n# I have no clue how to get all callable objects in python \n# classes and methods for those classes make this a lot harder... ugh\n\npkg_fns = r.pkg_fns # This is just the same data from R\n\n(\n  ggplot(pkg_fns, aes(x = \"pkg\", fill = \"pkg\")) + \n  geom_bar(aes(y = after_stat(\"count\"))) + \n  theme(legend_position = \"none\") + \n  ylab(\"# Functions\") + xlab(\"Package\")\n)\n## <ggplot: (8760696069078)>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse functions from the package without loading everything\n\n\n\n\n\nR\nPython\n\n\n\n\n# This code lists all the functions available to be called\npkgs <- search()\npkgs <- pkgs[grep(\"package:\",pkgs)]\n# get all the functions in each package that is loaded\nall_fns <- lapply(pkgs, function(x) as.character(lsf.str(x)))\n# create a data frame\npkg_fns <- data.frame(pkg = rep(pkgs, sapply(all_fns, length)), \n                      fn = unlist(all_fns))\npkg_fns$pkg <- gsub(\"package:\", \"\", pkg_fns$pkg)\n\nggplot2::ggplot(pkg_fns, ggplot2::aes(x = pkg, fill = pkg)) + \n  ggplot2::geom_bar(y = ggplot2::after_stat(count)) + \n  ggplot2::theme(legend.position = \"none\") + \n  ggplot2::xlab(\"Package\") + ggplot2::ylab(\"# Functions\")\n## Error in ggplot2::after_stat(count): object 'count' not found\n\n\n\n\nimport plotnine as p9\npkg_fns = r.pkg_fns\n\n(\n  p9.ggplot(pkg_fns, p9.aes(x = \"pkg\", fill = \"pkg\")) + \n  p9.geom_bar(y = p9.after_stat(\"count\")) + \n  p9.theme(legend_position = \"none\") + \n  p9.xlab(\"Package\") + p9.ylab(\"# Functions\")\n)\n## <ggplot: (8760719227538)>\n\n\n\n\n\n\n\n\n\nIn python, you can use import package as nickname, or you can just use import package and reference the package name directly. There are some packages which have typical aliases, and it’s best to use those so that you can look things up and not get too confused.\n\nCommon Python package aliases\n\n\n\n\n\n\nPackage\nCommon Alias\nExplanation\n\n\n\npandas\npd\nshorter\n\n\nnumpy\nnp\nshorter\n\n\nseaborn\nsns\nThis is a reference to Samuel Norman Seaborn, played by Rob Lowe, in the TV show The West Wing\n\n\nplotnine\np9\n\n\n\nBeautifulSoup (bs4)\nbs\nBeautifulSoup is a reference to Alice in Wonderland. The package name in PyPi is actually bs4."
  },
  {
    "objectID": "part-gen-prog/02-prog-functions.html#pipes",
    "href": "part-gen-prog/02-prog-functions.html#pipes",
    "title": "8  Using Functions and Libraries",
    "section": "\n8.8 Pipes",
    "text": "8.8 Pipes\nPipes are useful items for moving things from one place to another. In programming, and in particular, in data programming, pipes are operators that let us move data around. In R, we have two primary pipes that are similar (you may see both used if you google for code online). Any R version after 4.1 has a built-in pipe, |>; the tidyverse libraries use a pipe from the magrittr package, %>%.\nFor right now, it’s ok to think of the two pipes as essentially the same (but you can read about the differences [3]).\nFundamentally, a pipe allows you to take a function b() and apply it to x, like b(x), but write it as x |> b() or x %>% b(). This is particularly useful in cases where there are multiple sequential analysis steps, because where in regular notation you have to read the functions from the inside out to understand the sequential steps, with pipes, you have a clear step-by-step list of the order of operations.\nIn Python, there is a pipe function in the Pandas library that works using .pipe(function) notation [4]. From what I’ve seen reading code online, however, pipes are less commonly used in Python code than they are in R code. That’s ok - languages have different conventions, and it is usually best to adopt the convention of the language you’re working in so that your code can be read, run, and maintained by others more easily.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR solution\nPython solution\n\n\n\nGenerate 100 draws from a standard normal distribution and calculate the mean.\nIn R, simulate from a normal distribution with rnorm. In python, use np.random.normal - you’ll have to import numpy as np first.\nUse 3 approaches: 1. Store the data in a variable, then calculate the mean of the variable 2. Calculate the mean of the data by nesting the two functions (e.g. mean(generate_normal(100)) in pseudocode) 3. Calculate the mean of the data using the pipe (e.g. generate_normal(100) |> mean())\nConsider: What are the advantages and disadvantages of each approach? Would your answer change if there were more steps/functions required to get to the right answer?\n\n\n\ndata <- rnorm(100)\nmean(data)\n## [1] -0.1145949\n\nmean(rnorm(100))\n## [1] 0.008995119\n\nlibrary(magrittr) # load the pipe %>%\n\nrnorm(100) %>%\n  mean()\n## [1] 0.009137515\n\nrnorm(100) |> mean()\n## [1] -0.05921739\n\n\n\nIn python, task 3 isn’t really possible, because of the way Python function chaining works, but task 2 is basically the equivalent.\n\nimport numpy as np\nimport pandas as pd\n\nnums = pd.Series(np.random.normal(size = 100))\nnums.mean()\n## 0.21957918419534564\nnp.random.normal(size=100).mean()\n## -0.036586746397973255\n\nThe conclusion here is that it’s far easier to not use the pipe in python because the .function notation that python uses mimics the step-by-step approach of pipes in R even without using the actual pipe function. When you use data frames instead of Series, you might start using the pipe, but only in some circumstances - with user-defined functions, instead of methods. Methods are functions that are attached to a data type (technically, a class) and only work if they are defined for that class - for instance, .mean() is defined for both Pandas series and numpy arrays.\n\n\n\n\n\n\n\n\n\n\n[1] \nG. Makarov, “Use python in rstudio. RPubs,” May 02, 2022. [Online]. Available: https://rpubs.com/georgy_makarov/897844. [Accessed: Jan. 23, 2023]\n\n\n[2] \nD. Blackwood, “How to use python in r with reticulate and conda. Save the data,” Nov. 04, 2021. [Online]. Available: https://medium.com/save-the-data/how-to-use-python-in-r-with-reticulate-and-conda-36685534f06a. [Accessed: Jan. 23, 2023]\n\n\n[3] \nS. Machlis, “Use the new r pipe built into r 4.1. InfoWorld,” Jun. 10, 2021. [Online]. Available: https://www.infoworld.com/article/3621369/use-the-new-r-pipe-built-into-r-41.html. [Accessed: Jan. 13, 2023]\n\n\n[4] \nshadowtalker, “Answer to \"functional pipes in python like %>% from r’s magrittr\". Stack overflow,” Jun. 24, 2015. [Online]. Available: https://stackoverflow.com/a/31037901/2859168. [Accessed: Jan. 13, 2023]"
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#fa-bullseye-objectives",
    "href": "part-gen-prog/03-data-struct.html#fa-bullseye-objectives",
    "title": "9  Data Structures",
    "section": "\n9.1  Objectives",
    "text": "9.1  Objectives\n\nUnderstand the differences between lists, vectors, data frames, matrices, and arrays in R and python\nBe able to use location-based indexing in R or python to pull out subsets of a complex data object"
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#data-structures-overview",
    "href": "part-gen-prog/03-data-struct.html#data-structures-overview",
    "title": "9  Data Structures",
    "section": "\n9.2 Data Structures Overview",
    "text": "9.2 Data Structures Overview\nIn Chapter 7, we discussed 4 different data types: strings/characters, numeric/double/floats, integers, and logical/booleans. As you might imagine, things are about to get more complicated.\nData structures are more complex arrangements of information, but they are still (usually) created using the same data types we have previously discussed.\n\n\n\nHomogeneous\nHeterogeneous\n\n\n\n1D\nvector\nlist\n\n\n2D\nmatrix\ndata frame\n\n\nN-D\narray\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThose of you who have taken programming classes that were more computer science focused will realize that I am leaving out a lot of information about lower-level structures like pointers. I’m making a deliberate choice to gloss over most of those details in this chapter, because it’s already hard enough to learn 2 languages worth of data structures at a time. In addition, R doesn’t have pointers No Pointers in R, [1], so leaving out this material in python streamlines teaching both two languages, at the cost of overly simplifying some python concepts. If you want to read more about the Python concepts I’m leaving out, check out [2]."
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#lists",
    "href": "part-gen-prog/03-data-struct.html#lists",
    "title": "9  Data Structures",
    "section": "\n9.3 Lists",
    "text": "9.3 Lists\nA list is a one-dimensional column of heterogeneous data - the things stored in a list can be of different types.\n\n\nA lego list: the bricks are all different types and colors, but they are still part of the same data structure.\n\n\n\n\nR\nPython\n\n\n\n\nx <- list(\"a\", 3, FALSE)\nx\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] 3\n## \n## [[3]]\n## [1] FALSE\n\n\n\n\nx = [\"a\", 3, False]\nx\n## ['a', 3, False]\n\n\n\n\nThe most important thing to know about lists, for the moment, is how to pull things out of the list. We call that process indexing.\n\n9.3.1 Indexing\nEvery element in a list has an index (a location, indicated by an integer position)1.\n\n\nR concept\nR code\nPython concept\nPython code\n\n\n\nIn R, we count from 1.\n\n\nAn R-indexed lego list, counting from 1 to 5\n\n\n\n\n\nx <- list(\"a\", 3, FALSE)\n\nx[1] # This returns a list\n## [[1]]\n## [1] \"a\"\nx[1:2] # This returns multiple elements in the list\n## [[1]]\n## [1] \"a\"\n## \n## [[2]]\n## [1] 3\n\nx[[1]] # This returns the item\n## [1] \"a\"\nx[[1:2]] # This doesn't work - you can only use [[]] with a single index\n## Error in x[[1:2]]: subscript out of bounds\n\nIn R, list indexing with [] will return a list with the specified elements.\nTo actually retrieve the item in the list, use [[]]. The only downside to [[]] is that you can only access one thing at a time.\n\n\nIn Python, we count from 0.\n\n\nA python-indexed lego list, counting from 0 to 4\n\n\n\n\n\nx = [\"a\", 3, False]\n\nx[0]\n## 'a'\nx[1]\n## 3\nx[0:2]\n## ['a', 3]\n\nIn Python, we can use single brackets to get an object or a list back out, but we have to know how slices work. Essentially, in Python, 0:2 indicates that we want objects 0 and 1, but want to stop at 2 (not including 2). If you use a slice, Python will return a list; if you use a single index, python just returns the value in that location in the list.\n\n\n\nWe’ll talk more about indexing as it relates to vectors, but indexing is a general concept that applies to just about any multi-value object."
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#vectors",
    "href": "part-gen-prog/03-data-struct.html#vectors",
    "title": "9  Data Structures",
    "section": "\n9.4 Vectors",
    "text": "9.4 Vectors\nA vector is a one-dimensional column of homogeneous data. Homogeneous means that every element in a vector has the same data type.\nWe can have vectors of any data type and length we want:\n\n\nvectors of different data types\n\n\n\n9.4.1 Indexing by Location\nEach element in a vector has an index - an integer telling you what the item’s position within the vector is. I’m going to demonstrate indices with the string vector\n\n\n\n\n\n\nR\nPython\n\n\n\n1-indexed language\n0-indexed language\n\n\nCount elements as 1, 2, 3, 4, …, N\nCount elements as 0, 1, 2, 3, , …, N-1\n\n\n\n\n\n\n\n\n\nR\nPython Vectors\nPython Series (Pandas)\n\n\n\nIn R, we create vectors with the c() function, which stands for “concatenate” - basically, we stick a bunch of objects into a row.\n\ndigits_pi <- c(3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5)\n\n# Access individual entries\ndigits_pi[1]\n## [1] 3\ndigits_pi[2]\n## [1] 1\ndigits_pi[3]\n## [1] 4\n\n# R is 1-indexed - a list of 11 things goes from 1 to 11\ndigits_pi[0]\n## numeric(0)\ndigits_pi[11]\n## [1] 5\n\n# Print out the vector\ndigits_pi\n##  [1] 3 1 4 1 5 9 2 6 5 3 5\n\n\n\nIn python, we create vectors using the array function in the numpy module. To add a python module, we use the syntax import <name> as <nickname>. Many modules have conventional (and very short) nicknames - for numpy, we will use np as the nickname. Any functions we reference in the numpy module will then be called using np.fun_name() so that python knows where to find them.2\n\nimport numpy as np\ndigits_list = [3,1,4,1,5,9,2,6,5,3,5]\ndigits_pi = np.array(digits_list)\n\n# Access individual entries\ndigits_pi[0]\n## 3\ndigits_pi[1]\n## 1\ndigits_pi[2]\n\n# Python is 0 indexed - a list of 11 things goes from 0 to 10\n## 4\ndigits_pi[0]\n## 3\ndigits_pi[11] \n\n# multiplication works on the whole vector at once\n## Error in py_call_impl(callable, dots$args, dots$keywords): IndexError: index 11 is out of bounds for axis 0 with size 11\ndigits_pi * 2\n\n# Print out the vector\n## array([ 6,  2,  8,  2, 10, 18,  4, 12, 10,  6, 10])\nprint(digits_pi)\n## [3 1 4 1 5 9 2 6 5 3 5]\n\n\n\nPython has multiple things that look like vectors, including the pandas library’s Series structure. A Series is a one-dimensional array-like object containing a sequence of values and an associated array of labels (called its index).\n\nimport pandas as pd\ndigits_pi = pd.Series([3,1,4,1,5,9,2,6,5,3,5])\n\n# Access individual entries\ndigits_pi[0]\n## 3\ndigits_pi[1]\n## 1\ndigits_pi[2]\n\n# Python is 0 indexed - a list of 11 things goes from 0 to 10\n## 4\ndigits_pi[0]\n## 3\ndigits_pi[11] \n\n# logical indexing works here too\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: 11\ndigits_pi[digits_pi > 3]\n# simple multiplication works in a vectorized manner\n# that is, the whole vector is multiplied at once\n## 2     4\n## 4     5\n## 5     9\n## 7     6\n## 8     5\n## 10    5\n## dtype: int64\ndigits_pi * 2\n\n# Print out the series\n## 0      6\n## 1      2\n## 2      8\n## 3      2\n## 4     10\n## 5     18\n## 6      4\n## 7     12\n## 8     10\n## 9      6\n## 10    10\n## dtype: int64\nprint(digits_pi)\n## 0     3\n## 1     1\n## 2     4\n## 3     1\n## 4     5\n## 5     9\n## 6     2\n## 7     6\n## 8     5\n## 9     3\n## 10    5\n## dtype: int64\n\nThe Series object has a list of labels in the first printed column, and a list of values in the second. If we want, we can specify the labels manually to use as e.g. plot labels later:\n\nimport pandas as pd\nweekdays = pd.Series(['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'], index = ['S', 'M', 'T', 'W', 'R', 'F', 'Sat'])\n\n# access individual objs\nweekdays[0]\n## 'Sunday'\nweekdays[1]\n## 'Monday'\nweekdays['S']\n## 'Sunday'\nweekdays['Sat']\n\n# access the index\n## 'Saturday'\nweekdays.index\n## Index(['S', 'M', 'T', 'W', 'R', 'F', 'Sat'], dtype='object')\nweekdays.index[6] = 'Z' # you can't assign things to the index to change it\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: Index does not support mutable operations\nweekdays\n## S         Sunday\n## M         Monday\n## T        Tuesday\n## W      Wednesday\n## R       Thursday\n## F         Friday\n## Sat     Saturday\n## dtype: object\n\n\n\n\nWe can pull out items in a vector by indexing, but we can also replace specific things as well:\n\n\n\nR\nPython\n\n\n\n\nfavorite_cats <- c(\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\")\n\nfavorite_cats\n## [1] \"Grumpy\"   \"Garfield\" \"Jorts\"    \"Jean\"\n\nfavorite_cats[2] <- \"Nyan Cat\"\n\nfavorite_cats\n## [1] \"Grumpy\"   \"Nyan Cat\" \"Jorts\"    \"Jean\"\n\n\n\n\nfavorite_cats = [\"Grumpy\", \"Garfield\", \"Jorts\", \"Jean\"]\n\nfavorite_cats\n## ['Grumpy', 'Garfield', 'Jorts', 'Jean']\nfavorite_cats[1] = \"Nyan Cat\"\n\nfavorite_cats\n## ['Grumpy', 'Nyan Cat', 'Jorts', 'Jean']\n\n\n\n\nIf you’re curious about any of these cats, see the footnotes3.\n\n\n9.4.2 Indexing with Logical Vectors\nAs you might imagine, we can create vectors of all sorts of different data types. One particularly useful trick is to create a logical vector that goes along with a vector of another type to use as a logical index.\n\n\nlego vectors - a pink/purple hued set of 1x3 bricks representing the data and a corresponding set of 1x1 grey and black bricks representing the logical index vector of the same length\n\n\nIf we let the black lego represent “True” and the grey lego represent “False”, we can use the logical vector to pull out all values in the main vector.\n\n\n\n\n\n\nBlack = True, Grey = False\nGrey = True, Black = False\n\n\n\n\n\n\nNote that for logical indexing to work properly, the logical index must be the same length as the vector we’re indexing. This constraint will return when we talk about data frames, but for now just keep in mind that logical indexing doesn’t make sense when this constraint isn’t true.\n\n\nIndexing with logical vectors in R\nIndexing with logical vectors in python\n\n\n\n\n# Define a character vector\nweekdays <- c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\")\nweekend <- c(\"Sunday\", \"Saturday\")\n\n# Create logical vectors\nrelax_days <- c(1, 0, 0, 0, 0, 0, 1) # doing this the manual way\nrelax_days <- weekdays %in% weekend # This creates a logical vector \n                                    # with less manual construction\nrelax_days\n## [1]  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n\nschool_days <- !relax_days # FALSE if weekend, TRUE if not\nschool_days\n## [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n\n# Using logical vectors to index the character vector\nweekdays[school_days] # print out all school days\n## [1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"\n\n\n\n\nimport numpy as np;\n\nanimals = np.array([\"Cat\", \"Dog\", \"Snake\", \"Lizard\", \"Tarantula\", \"Hamster\", \"Gerbil\", \"Otter\"])\n\n# Define a logical vector\ngood_pets = np.array([True, True, False, False, False, True, True, False])\nbad_pets = np.invert(good_pets) # Invert the logical vector \n                                # so True -> False and False -> True\n\nanimals[good_pets]\n## array(['Cat', 'Dog', 'Hamster', 'Gerbil'], dtype='<U9')\nanimals[bad_pets]\n## array(['Snake', 'Lizard', 'Tarantula', 'Otter'], dtype='<U9')\nanimals[~good_pets] # equivalent to using bad_pets\n## array(['Snake', 'Lizard', 'Tarantula', 'Otter'], dtype='<U9')\n\n\n\n\n\n9.4.3 Reviewing Types\nAs vectors are a collection of things of a single type, what happens if we try to make a vector with differently-typed things?\n\n\nR\nPython\n\n\n\n\nc(2L, FALSE, 3.1415, \"animal\") # all converted to strings\n## [1] \"2\"      \"FALSE\"  \"3.1415\" \"animal\"\n\nc(2L, FALSE, 3.1415) # converted to numerics\n## [1] 2.0000 0.0000 3.1415\n\nc(2L, FALSE) # converted to integers\n## [1] 2 0\n\n\n\n\nimport numpy as np\n\nnp.array([2, False, 3.1415, \"animal\"]) # all converted to strings\n## array(['2', 'False', '3.1415', 'animal'], dtype='<U32')\nnp.array([2, False, 3.1415]) # converted to floats\n## array([2.    , 0.    , 3.1415])\nnp.array([2, False]) # converted to integers\n## array([2, 0])\n\n\n\n\nAs a reminder, this is an example of implicit type conversion - R and python decide what type to use for you, going with the type that doesn’t lose data but takes up as little space as possible.\nTry it Out!\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nCreate a vector of the integers from one to 30. Use logical indexing to pick out only the numbers which are multiples of 3.\n\n\n\nx <- 1:30\nx [ x %% 3 == 0]\n##  [1]  3  6  9 12 15 18 21 24 27 30\n\n\n\n\nimport numpy as np\nx = np.array(range(1, 31)) # because python is 0 indexed\nx[ x % 3 == 0]\n## array([ 3,  6,  9, 12, 15, 18, 21, 24, 27, 30])\n\n\n\n\n\n\nChallenge\nGeneral Solution\nR Solution\nPython Solution\n\n\n\nExtra challenge: Pick out numbers which are multiples of 2 or 3, but not multiples of 6!\n\n\nThis operation is xor, a.k.a. exclusive or. That is, X or Y, but not X AND Y.\nWe can write xor as (X OR Y) & !(X AND Y) – or we can use a predefined function: xor() in R, ^ in python.\n\n\n\nx <- 1:30\n\nx2 <- x %% 2 == 0 # multiples of 2\nx3 <- x %% 3 == 0 # multiples of 3\nx2xor3 <- xor(x2, x3)\nx2xor3_2 <- (x2 | x3) & !(x2 & x3)\nx[x2xor3]\n##  [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\nx[x2xor3_2]\n##  [1]  2  3  4  8  9 10 14 15 16 20 21 22 26 27 28\n\n\n\n\nimport numpy as np\nx = np.array(range(1, 31))\n\nx2 = x % 2 == 0 # multiples of 2\nx3 = x % 3 == 0 # multiples of 3\nx2xor3 = x2 ^ x3\n\nx[x2xor3]\n## array([ 2,  3,  4,  8,  9, 10, 14, 15, 16, 20, 21, 22, 26, 27, 28])"
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#matrices",
    "href": "part-gen-prog/03-data-struct.html#matrices",
    "title": "9  Data Structures",
    "section": "\n9.5 Matrices",
    "text": "9.5 Matrices\nA matrix is the next step after a vector - it’s a set of values arranged in a two-dimensional, rectangular format.\n\n\nMatrix (Lego)\nR\nPython\n\n\n\n\n\nlego depiction of a 3-row, 4-column matrix of 2x2 red-colored blocks\n\n\n\n\n\n# Minimal matrix in R: take a vector, \n# tell R how many rows you want\nmatrix(1:12, nrow = 3)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    4    7   10\n## [2,]    2    5    8   11\n## [3,]    3    6    9   12\n\nmatrix(1:12, ncol = 3) # or columns\n##      [,1] [,2] [,3]\n## [1,]    1    5    9\n## [2,]    2    6   10\n## [3,]    3    7   11\n## [4,]    4    8   12\n\n# by default, R will fill in column-by-column\n# the byrow parameter tells R to go row-by-row\nmatrix(1:12, nrow = 3, byrow = T)\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\n\n# We can also easily create square matrices \n# with a specific diagonal (this is useful for modeling)\ndiag(rep(1, times = 4))\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    0    0    0\n## [2,]    0    1    0    0\n## [3,]    0    0    1    0\n## [4,]    0    0    0    1\n\n\n\nIn python, matrices are just a special case of a class called ndarray - n-dimensional arrays.\n\nimport numpy as np\n# Minimal ndarray in python by typing in the values in a structured format\nnp.array([[0,  1,  2],\n          [3,  4,  5],\n          [6,  7,  8],\n          [9, 10, 11]])\n# This syntax creates a list of the rows we want in our matrix\n\n# Matrix in python using a data vector and size parameters\n## array([[ 0,  1,  2],\n##        [ 3,  4,  5],\n##        [ 6,  7,  8],\n##        [ 9, 10, 11]])\nnp.reshape(range(0,12), (3,4))\n## array([[ 0,  1,  2,  3],\n##        [ 4,  5,  6,  7],\n##        [ 8,  9, 10, 11]])\nnp.reshape(range(0,12), (4,3))\n## array([[ 0,  1,  2],\n##        [ 3,  4,  5],\n##        [ 6,  7,  8],\n##        [ 9, 10, 11]])\nnp.reshape(range(0,12), (3,4), order = 'F')\n## array([[ 0,  3,  6,  9],\n##        [ 1,  4,  7, 10],\n##        [ 2,  5,  8, 11]])\n\nIn python, we create 2-dimensional arrays (aka matrices) either by creating a list of rows to join together or by reshaping a 1-dimensional array. The trick with reshaping the 1-dimensional array is the order argument: ‘F’ stands for “Fortran-like” and ‘C’ stands for “C-like”… so to go by column, you use ‘F’ and to go by row, you use ‘C’. Totally intuitive, right?\n\n\n\nMost of the problems we’re going to work on will not require much in the way of matrix or array operations. For now, you need the following:\n\nKnow that matrices exist and what they are (2-dimensional arrays of numbers)\nUnderstand how they are indexed (because it is extremely similar to data frames that we’ll work with in the next chapter)\nBe aware that there are lots of functions that depend on matrix operations at their core (including linear regression)\n\nFor more on matrix operations and matrix calculations, see Chapter 10.\n\n9.5.1 Indexing in Matrices\nBoth R and python use [row, column] to index matrices. To extract the bottom-left element of a 3x4 matrix in R, we would use [3,1] to get to the third row and first column entry; in python, we would use [2,0] (remember that Python is 0-indexed).\nAs with vectors, you can replace elements in a matrix using assignment.\n\n\nR\nPython\n\n\n\n\nmy_mat <- matrix(1:12, nrow = 3, byrow = T)\n\nmy_mat[3,1] <- 500\n\nmy_mat\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]  500   10   11   12\n\n\n\nRemember that zero-indexing!\n\nimport numpy as np\n\nmy_mat = np.reshape(range(1, 13), (3,4))\n\nmy_mat[2,0] = 500\n\nmy_mat\n## array([[  1,   2,   3,   4],\n##        [  5,   6,   7,   8],\n##        [500,  10,  11,  12]])\n\n\n\n\n\n9.5.2 Matrix Operations\nThere are a number of matrix operations that we need to know for basic programming purposes:\n\nscalar multiplication \\[c*\\textbf{X} = c * \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] = \\left[\\begin{array}{cc} c*x_{1,1} & c*x_{1, 2}\\\\c*x_{2,1} & c*x_{2,2}\\end{array}\\right]\\]\n\ntranspose - flip the matrix across the left top -> right bottom diagonal. \\[t(\\textbf{X}) = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right]^T = \\left[\\begin{array}{cc} x_{1,1} & x_{2,1}\\\\x_{1,2} & x_{2,2}\\end{array}\\right]\\]\n\nmatrix multiplication (dot product) - If you haven’t had this in Linear Algebra, here’s a preview. See [3] for a better explanation \\[\\textbf{X}*\\textbf{Y} = \\left[\\begin{array}{cc} x_{1,1} & x_{1, 2}\\\\x_{2,1} & x_{2,2}\\end{array}\\right] * \\left[\\begin{array}{cc} y_{1,1} \\\\y_{2,1} \\end{array}\\right] = \\left[\\begin{array}{c}x_{1,1}*y_{1,1} + x_{1,2}*y_{2,1} \\\\x_{2, 1}*y_{1,1} + x_{2,2}*y_{2,1}\\end{array}\\right]\\] Note that matrix multiplication depends on having matrices of compatible dimensions. If you have two matrices of dimension \\((a \\times b)\\) and \\((c \\times d)\\), then \\(b\\) must be equal to \\(c\\) for the multiplication to work, and your result will be \\((a \\times d)\\).\n\n\n\nR\nPython\n\n\n\n\nx <- matrix(c(1, 2, 3, 4), nrow = 2, byrow = T)\ny <- matrix(c(5, 6), nrow = 2)\n\n# Scalar multiplication\nx * 3\n##      [,1] [,2]\n## [1,]    3    6\n## [2,]    9   12\n3 * x\n##      [,1] [,2]\n## [1,]    3    6\n## [2,]    9   12\n\n# Transpose\nt(x)\n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\nt(y)\n##      [,1] [,2]\n## [1,]    5    6\n\n# matrix multiplication (dot product)\nx %*% y\n##      [,1]\n## [1,]   17\n## [2,]   39\n\n\n\n\nimport numpy as np\nx = np.array([[1,2],[3,4]])\ny = np.array([[5],[6]])\n\n# scalar multiplication\nx*3\n## array([[ 3,  6],\n##        [ 9, 12]])\n3*x\n\n# transpose\n## array([[ 3,  6],\n##        [ 9, 12]])\nx.T # shorthand\n## array([[1, 3],\n##        [2, 4]])\nx.transpose() # Long form\n\n# Matrix multiplication (dot product)\n## array([[1, 3],\n##        [2, 4]])\nnp.dot(x, y)\n## array([[17],\n##        [39]])"
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#arrays",
    "href": "part-gen-prog/03-data-struct.html#arrays",
    "title": "9  Data Structures",
    "section": "\n9.6 Arrays",
    "text": "9.6 Arrays\nArrays are a generalized n-dimensional version of a vector: all elements have the same type, and they are indexed using square brackets in both R and python: [dim1, dim2, dim3, ...]\nI don’t think you will need to create 3+ dimensional arrays in this class, but if you want to try it out, here is some code.\n\n\nR\nPython\n\n\n\n\narray(1:8, dim = c(2,2,2))\n## , , 1\n## \n##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n## \n## , , 2\n## \n##      [,1] [,2]\n## [1,]    5    7\n## [2,]    6    8\n\nNote that displaying this requires 2 slices, since it’s hard to display 3D information in a 2D terminal arrangement.\n\n\n\nimport numpy as np\n\nnp.array([[[1,2],[3,4]],[[5,6], [7,8]]])\n## array([[[1, 2],\n##         [3, 4]],\n## \n##        [[5, 6],\n##         [7, 8]]])"
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#data-frames",
    "href": "part-gen-prog/03-data-struct.html#data-frames",
    "title": "9  Data Structures",
    "section": "\n9.7 Data Frames",
    "text": "9.7 Data Frames\nIn the previous sections, we talked about homogeneous structures: arrangements of data, like vectors and matrices, where every entry in the larger structure has the same type. In the rest of this chapter, we’ll be talking about the root of most data science analysis projects: the data frame.\nLike an excel spreadsheet, data frames are arrangements of data in columns and rows.\nThis format has two main restrictions:\n\nEvery entry in each column must have the same data type\nEvery column must have the same number of rows\n\n\n\nA lego data frame of 4 columns and 12 rows. Each column is a separate color hue (data type), with slight variations in the hue of each individual bricks.\n\n\nThe picture above shows a data frame of 4 columns, each with a different data type (brick size/hue). The data frame has 12 rows. This picture may look similar to one that we used to show logical indexing in the last chapter, and that is not a coincidence. You can get everything from a data frame that you would get from a collection of 4 separate vectors… but there are advantages to keeping things in a data frame instead.\n\n\n\n\n\n\nMotivating Data Frames: Working with Multiple Vectors\n\n\n\nConsider for a moment https://worldpopulationreview.com/states, which lists the population of each state. You can find this dataset in CSV form here.\nIn the previous sections, we learned how to make different vectors in R, numpy, and pandas. Let’s see what happens when we work with the data above as a set of vectors/Series compared to what happens when we work with data frames.\n\n\nPython\nR\n\n\n\n(I’m going to cheat and read this in using pandas functions we haven’t learned yet to demonstrate why this stuff matters.)\n\nimport pandas as pd\n\ndata = pd.read_html(\"https://worldpopulationreview.com/states\")[0]\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: No tables found\nlist(data.columns) # get names\n\n# Create a few population series\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2022 = pd.Series(data['2022 Population'].values, index = data['State'].values)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2021 = pd.Series(data['2021 Population'].values, index = data['State'].values)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2010 = pd.Series(data['2010 Census'].values, index = data['State'].values)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\n\nSuppose that we want to sort each population vector by the population in that year.\n\nimport pandas as pd\ndata = pd.read_html(\"https://worldpopulationreview.com/states\")[0]\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: No tables found\npopulation2022 = pd.Series(data['2022 Population'].values, index = data['State'].values).sort_values()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2021 = pd.Series(data['2021 Population'].values, index = data['State'].values).sort_values()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2010 = pd.Series(data['2010 Census'].values, index = data['State'].values).sort_values()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'data' is not defined\npopulation2022.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'population2022' is not defined\npopulation2021.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'population2021' is not defined\npopulation2010.head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'population2010' is not defined\n\nThe only problem is that by doing this, we’ve now lost the ordering that matched across all 3 vectors. Pandas Series are great for this, because they use labels that allow us to reconstitute which value corresponds to which label, but in R or even in numpy arrays, vectors don’t inherently come with labels. In these situations, sorting by one value can actually destroy the connection between two vectors!\n\n\n\ndf <- read.csv(\"https://raw.githubusercontent.com/srvanderplas/Stat151/main/data/population2022.csv\")\n\n# Use vectors instead of the data frame\nstate <- df$State\npop2022 <- df$Pop\npop2021 <- df$Pop2021\npop2010 <- df$Pop2010\n\n# Create a vector to index population in 2022 in order\norder2022 <- order(pop2022)\n\n# To keep variables together, we have to do things like this:\nhead(state[order2022])\n## [1] \"Wyoming\"              \"Vermont\"              \"District of Columbia\"\n## [4] \"Alaska\"               \"North Dakota\"         \"South Dakota\"\nhead(pop2022[order2022])\n## [1] 582233 622882 718355 720763 774008 902542\n\n# It makes more sense just to reorder the whole data frame:\nhead(df[order2022,])\n##    rank                State    Pop  Growth Pop2021 Pop2010 growthSince2010\n## 52   52              Wyoming 582233  0.0020  581075  564487          0.0314\n## 51   51              Vermont 622882 -0.0006  623251  625879         -0.0048\n## 50   50 District of Columbia 718355  0.0059  714153  605226          0.1869\n## 49   49               Alaska 720763 -0.0050  724357  713910          0.0096\n## 48   48         North Dakota 774008  0.0052  770026  674715          0.1472\n## 47   47         South Dakota 902542  0.0066  896581  816166          0.1058\n##    Percent    density\n## 52  0.0017     5.9967\n## 51  0.0019    67.5797\n## 50  0.0021 11776.3115\n## 49  0.0021     1.2631\n## 48  0.0023    11.2173\n## 47  0.0027    11.9052\n\n\n\n\n\n\nThe primary advantage to data frames is that rows of data are kept together. Since we often think of a row of data as a single observation in a sample, this is an extremely important feature that makes data frames a huge improvement on a collection of vectors of the same length: it’s much harder for observations in a single row to get shuffled around and mismatched!\n\n9.7.1 Data Frame Basics\nIn R, data frames are built in as type data.frame, though there are packages that provide other implementations of data frames that have additional features, such as the tibble package used in many other common packages. We will cover functions from both base R and the tibble package in this chapter.\nIn Python, we will use the pandas library, which is conventionally abbreviated pd. So before you use any data frames in python, you will need to add the following line to your code: import pandas as pd.\n\n\n\n\n\n\nExamining Data Frames\n\n\n\n\n\nR\nPython\n\n\n\nWhen you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The head(n) command shows the first \\(n\\) rows of a data frame (enough to see what’s there, not enough to overflow your screen).\n\ndata(mtcars) # Load the data -- included in base R\nhead(mtcars) # Look at the first 6 rows\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nstr(mtcars) # Examine the structure of the object\n## 'data.frame':    32 obs. of  11 variables:\n##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n##  $ disp: num  160 160 108 258 360 ...\n##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n##  $ qsec: num  16.5 17 18.6 19.4 17 ...\n##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\nYou can change column values or add new columns easily using assignment. The summary() function can be used on specific columns to perform summary operations (a 5-number summary useful for making e.g. boxplots is provided by default).\n\nmtcars$gpm <- 1/mtcars$mpg # gpm is sometimes used to assess efficiency\n\nsummary(mtcars$gpm)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n## 0.02950 0.04386 0.05208 0.05423 0.06483 0.09615\nsummary(mtcars$mpg)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   10.40   15.43   19.20   20.09   22.80   33.90\n\nOften, it is useful to know the dimensions of a data frame. The number of rows can be obtained by using nrow(df) and similarly, the columns can be obtained using ncol(df) (or, get both with dim()). There is also an easy way to get a summary of each column in the data frame, using summary().\n\nsummary(mtcars)\n##       mpg             cyl             disp             hp       \n##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n##       drat             wt             qsec             vs        \n##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n##        am              gear            carb            gpm         \n##  Min.   :0.0000   Min.   :3.000   Min.   :1.000   Min.   :0.02950  \n##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000   1st Qu.:0.04386  \n##  Median :0.0000   Median :4.000   Median :2.000   Median :0.05208  \n##  Mean   :0.4062   Mean   :3.688   Mean   :2.812   Mean   :0.05423  \n##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:0.06483  \n##  Max.   :1.0000   Max.   :5.000   Max.   :8.000   Max.   :0.09615\ndim(mtcars)\n## [1] 32 12\nnrow(mtcars)\n## [1] 32\nncol(mtcars)\n## [1] 12\n\nMissing variables in an R data frame are indicated with NA.\n\n\nWhen you examine the structure of a data frame, as shown below, you get each column shown in a row, with its type and the first few values in the column. The df.head(n) command shows the first \\(n\\) rows of a data frame (enough to see what’s there, not enough to overflow your screen).\n\nmtcars = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/mtcars.csv\")\n\nmtcars.head(5)\n##           Unnamed: 0   mpg  cyl   disp   hp  ...   qsec  vs  am  gear  carb\n## 0          Mazda RX4  21.0    6  160.0  110  ...  16.46   0   1     4     4\n## 1      Mazda RX4 Wag  21.0    6  160.0  110  ...  17.02   0   1     4     4\n## 2         Datsun 710  22.8    4  108.0   93  ...  18.61   1   1     4     1\n## 3     Hornet 4 Drive  21.4    6  258.0  110  ...  19.44   1   0     3     1\n## 4  Hornet Sportabout  18.7    8  360.0  175  ...  17.02   0   0     3     2\n## \n## [5 rows x 12 columns]\nmtcars.info()\n## <class 'pandas.core.frame.DataFrame'>\n## RangeIndex: 32 entries, 0 to 31\n## Data columns (total 12 columns):\n##  #   Column      Non-Null Count  Dtype  \n## ---  ------      --------------  -----  \n##  0   Unnamed: 0  32 non-null     object \n##  1   mpg         32 non-null     float64\n##  2   cyl         32 non-null     int64  \n##  3   disp        32 non-null     float64\n##  4   hp          32 non-null     int64  \n##  5   drat        32 non-null     float64\n##  6   wt          32 non-null     float64\n##  7   qsec        32 non-null     float64\n##  8   vs          32 non-null     int64  \n##  9   am          32 non-null     int64  \n##  10  gear        32 non-null     int64  \n##  11  carb        32 non-null     int64  \n## dtypes: float64(5), int64(6), object(1)\n## memory usage: 3.1+ KB\n\nYou can change column values or add new columns easily using assignment. It’s also easy to access specific columns to perform summary operations. You can access a column named xyz using df.xyz or using df[\"xyz\"]. To create a new column, you must use df[\"xyz\"].\n\nmtcars[\"gpm\"] = 1/mtcars.mpg # gpm is sometimes used to assess efficiency\n\nmtcars.gpm.describe()\n## count    32.000000\n## mean      0.054227\n## std       0.016424\n## min       0.029499\n## 25%       0.043860\n## 50%       0.052083\n## 75%       0.064834\n## max       0.096154\n## Name: gpm, dtype: float64\nmtcars.mpg.describe()\n## count    32.000000\n## mean     20.090625\n## std       6.026948\n## min      10.400000\n## 25%      15.425000\n## 50%      19.200000\n## 75%      22.800000\n## max      33.900000\n## Name: mpg, dtype: float64\n\nOften, it is useful to know the dimensions of a data frame. The dimensions of a data frame (rows x columns) can be accessed using df.shape. There is also an easy way to get a summary of each column in the data frame, using df.describe().\n\nmtcars.describe()\n##              mpg        cyl        disp  ...       gear     carb        gpm\n## count  32.000000  32.000000   32.000000  ...  32.000000  32.0000  32.000000\n## mean   20.090625   6.187500  230.721875  ...   3.687500   2.8125   0.054227\n## std     6.026948   1.785922  123.938694  ...   0.737804   1.6152   0.016424\n## min    10.400000   4.000000   71.100000  ...   3.000000   1.0000   0.029499\n## 25%    15.425000   4.000000  120.825000  ...   3.000000   2.0000   0.043860\n## 50%    19.200000   6.000000  196.300000  ...   4.000000   2.0000   0.052083\n## 75%    22.800000   8.000000  326.000000  ...   4.000000   4.0000   0.064834\n## max    33.900000   8.000000  472.000000  ...   5.000000   8.0000   0.096154\n## \n## [8 rows x 12 columns]\nmtcars.shape\n## (32, 13)\n\nMissing variables in a pandas data frame are indicated with nan or NULL.\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nSetup\nProblem\nR Solution\nPython Solution\n\n\n\nThe dataset state.x77 contains information on US state statistics in the 1970s. By default, it is a matrix, but we can easily convert it to a data frame, as shown below.\n\ndata(state)\nstate_facts <- data.frame(state.x77)\nstate_facts <- cbind(state = row.names(state_facts), state_facts, stringsAsFactors = F) \n# State names were stored as row labels\n# Store them in a variable instead, and add it to the data frame\n\nrow.names(state_facts) <- NULL # get rid of row names\n\nhead(state_facts)\n##        state Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\n## 1    Alabama       3615   3624        2.1    69.05   15.1    41.3    20  50708\n## 2     Alaska        365   6315        1.5    69.31   11.3    66.7   152 566432\n## 3    Arizona       2212   4530        1.8    70.55    7.8    58.1    15 113417\n## 4   Arkansas       2110   3378        1.9    70.66   10.1    39.9    65  51945\n## 5 California      21198   5114        1.1    71.71   10.3    62.6    20 156361\n## 6   Colorado       2541   4884        0.7    72.06    6.8    63.9   166 103766\n\n# Write data out so that we can read it in using Python\nwrite.csv(state_facts, file = \"data/state_facts.csv\", row.names = F)\n## Error in file(file, ifelse(append, \"a\", \"w\")): cannot open the connection\n\nWe can write out the built in R data and read it in using pd.read_csv, which creates a DataFrame in pandas.\n\nimport pandas as pd\n\nstate_facts = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/state_facts.csv\")\n\n\n\n\nHow many rows and columns does it have? Can you find different ways to get that information?\nThe Illiteracy column contains the percent of the population of each state that is illiterate. Calculate the number of people in each state who are illiterate, and store that in a new column called TotalNumIlliterate. Note: Population contains the population in thousands.\nCalculate the average population density of each state (population per square mile) and store it in a new column PopDensity. Using the R reference card, can you find functions that you can combine to get the state with the minimum population density?\n\n\n\n\n# 3 ways to get rows and columns\nstr(state_facts)\n## 'data.frame':    50 obs. of  9 variables:\n##  $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n##  $ Population: num  3615 365 2212 2110 21198 ...\n##  $ Income    : num  3624 6315 4530 3378 5114 ...\n##  $ Illiteracy: num  2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ...\n##  $ Life.Exp  : num  69 69.3 70.5 70.7 71.7 ...\n##  $ Murder    : num  15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ...\n##  $ HS.Grad   : num  41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ...\n##  $ Frost     : num  20 152 15 65 20 166 139 103 11 60 ...\n##  $ Area      : num  50708 566432 113417 51945 156361 ...\ndim(state_facts)\n## [1] 50  9\nnrow(state_facts)\n## [1] 50\nncol(state_facts)\n## [1] 9\n\n# Illiteracy\nstate_facts$TotalNumIlliterate <- state_facts$Population * 1e3 * (state_facts$Illiteracy/100) \n\n# Population Density\nstate_facts$PopDensity <- state_facts$Population * 1e3/state_facts$Area \n# in people per square mile\n\n# minimum population\nstate_facts$state[which.min(state_facts$PopDensity)]\n## [1] \"Alaska\"\n\n\n\n\n# Ways to get rows and columns\nstate_facts.shape\n## (50, 9)\nstate_facts.index.size # rows\n## 50\nstate_facts.columns.size # columns\n## 9\nstate_facts.info() # columns + rows + missing counts + data types\n\n# Illiteracy\n## <class 'pandas.core.frame.DataFrame'>\n## RangeIndex: 50 entries, 0 to 49\n## Data columns (total 9 columns):\n##  #   Column      Non-Null Count  Dtype  \n## ---  ------      --------------  -----  \n##  0   state       50 non-null     object \n##  1   Population  50 non-null     int64  \n##  2   Income      50 non-null     int64  \n##  3   Illiteracy  50 non-null     float64\n##  4   Life.Exp    50 non-null     float64\n##  5   Murder      50 non-null     float64\n##  6   HS.Grad     50 non-null     float64\n##  7   Frost       50 non-null     int64  \n##  8   Area        50 non-null     int64  \n## dtypes: float64(4), int64(4), object(1)\n## memory usage: 3.6+ KB\nstate_facts[\"TotalNumIlliterate\"] = state_facts[\"Population\"] * 1e3 * state_facts[\"Illiteracy\"]/100\n\n# Population Density\nstate_facts[\"PopDensity\"] = state_facts[\"Population\"] * 1e3/state_facts[\"Area\"] \n# in people per square mile\n\n# minimum population\nmin_dens = state_facts[\"PopDensity\"].min()\n# Get location of minimum population\nloc_min_dens = state_facts.PopDensity.isin([min_dens])\n# Pull out matching state\nstate_facts.state[loc_min_dens]\n## 1    Alaska\n## Name: state, dtype: object\n\n\n\n\n\n\n\n9.7.2 Creating Data Frames\nIt is possible to create data frames from scratch by building them out of simpler components, such as lists of vectors or dicts of Series. This tends to be useful for small data sets, but it is more common to read data in from e.g. CSV files, which I’ve used several times already but haven’t yet shown you how to do (see Chapter 14 for the full how-to).\n\n\n\n\n\n\nData Frames from Scratch\n\n\n\n\n\nR\nPython\n\n\n\n\nmath_and_lsd <- data.frame(\n  lsd_conc = c(1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41),\n  test_score = c(78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97))\nmath_and_lsd\n##   lsd_conc test_score\n## 1     1.17      78.93\n## 2     2.97      58.20\n## 3     3.26      67.47\n## 4     4.69      37.47\n## 5     5.83      45.65\n## 6     6.00      32.92\n## 7     6.41      29.97\n\n# add a column - character vector\nmath_and_lsd$subjective <- c(\"finally coming back\", \"getting better\", \"it's totally better\", \"really tripping out\", \"is it over?\", \"whoa, man\", \"I can taste color, but I can't do math\")\n\nmath_and_lsd\n##   lsd_conc test_score                             subjective\n## 1     1.17      78.93                    finally coming back\n## 2     2.97      58.20                         getting better\n## 3     3.26      67.47                    it's totally better\n## 4     4.69      37.47                    really tripping out\n## 5     5.83      45.65                            is it over?\n## 6     6.00      32.92                              whoa, man\n## 7     6.41      29.97 I can taste color, but I can't do math\n\n\n\n\nmath_and_lsd = pd.DataFrame({\n  \"lsd_conc\": [1.17, 2.97, 3.26, 4.69, 5.83, 6.00, 6.41],\n  \"test_score\": [78.93, 58.20, 67.47, 37.47, 45.65, 32.92, 29.97]})\nmath_and_lsd\n\n# add a column - character vector\n##    lsd_conc  test_score\n## 0      1.17       78.93\n## 1      2.97       58.20\n## 2      3.26       67.47\n## 3      4.69       37.47\n## 4      5.83       45.65\n## 5      6.00       32.92\n## 6      6.41       29.97\nmath_and_lsd[\"subjective\"] = [\"finally coming back\", \"getting better\", \"it's totally better\", \"really tripping out\", \"is it over?\", \"whoa, man\", \"I can taste color, but I can't do math\"]\n\nmath_and_lsd\n##    lsd_conc  test_score                              subjective\n## 0      1.17       78.93                     finally coming back\n## 1      2.97       58.20                          getting better\n## 2      3.26       67.47                     it's totally better\n## 3      4.69       37.47                     really tripping out\n## 4      5.83       45.65                             is it over?\n## 5      6.00       32.92                               whoa, man\n## 6      6.41       29.97  I can taste color, but I can't do math\n\n\n\n\n\n\nWhile it’s not so hard to create data frames from scratch for small data sets, it’s very tedious if you have a lot of data (or if you can’t type accurately). An easier way to create a data frame (rather than typing the whole thing in) is to read in data from somewhere else - a file, a table on a webpage, etc. We’re not going to go into the finer points of this (you’ll get into that in Chapter 14), but it is useful to know how to read neatly formatted data.\nOne source of (relatively neat) data is the TidyTuesday github repository4\n\n\n\n\n\n\nReading in Data\n\n\n\n\n\nBase R\nreadR package\nPandas\n\n\n\nIn Base R, we can read the data in using the read.csv function\n\nairmen <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv')\nhead(airmen)\n##                    name last_name    first_name      graduation_date\n## 1   Adams, John H., Jr.     Adams  John H., Jr. 1945-04-15T00:00:00Z\n## 2           Adams, Paul     Adams          Paul 1943-04-29T00:00:00Z\n## 3 Adkins, Rutherford H.    Adkins Rutherford H. 1944-10-16T00:00:00Z\n## 4    Adkins, Winston A.    Adkins    Winston A. 1944-02-08T00:00:00Z\n## 5 Alexander, Halbert L. Alexander    Halbert L. 1944-11-20T00:00:00Z\n## 6  Alexander, Harvey R. Alexander     Harvey R. 1944-04-15T00:00:00Z\n##   rank_at_graduation     class graduated_from    pilot_type\n## 1             2nd Lt   SE-45-B           TAAF Single engine\n## 2             2nd Lt   SE-43-D           TAAF Single engine\n## 3             2nd Lt SE-44-I-1           TAAF Single engine\n## 4             2nd Lt   TE-44-B           TAAF   Twin engine\n## 5             2nd Lt   SE-44-I           TAAF Single engine\n## 6             2nd Lt   TE-44-D           TAAF   Twin engine\n##   military_hometown_of_record state aerial_victory_credits\n## 1                 Kansas City    KS                   <NA>\n## 2                  Greenville    SC                   <NA>\n## 3                  Alexandria    VA                   <NA>\n## 4                     Chicago    IL                   <NA>\n## 5                  Georgetown    IL                   <NA>\n## 6                  Georgetown    IL                   <NA>\n##   number_of_aerial_victory_credits reported_lost reported_lost_date\n## 1                                0          <NA>               <NA>\n## 2                                0          <NA>               <NA>\n## 3                                0          <NA>               <NA>\n## 4                                0          <NA>               <NA>\n## 5                                0          <NA>               <NA>\n## 6                                0          <NA>               <NA>\n##   reported_lost_location                                   web_profile\n## 1                   <NA>     https://cafriseabove.org/john-h-adams-jr/\n## 2                   <NA>          https://cafriseabove.org/paul-adams/\n## 3                   <NA> https://cafriseabove.org/rutherford-h-adkins/\n## 4                   <NA>                                          <NA>\n## 5                   <NA> https://cafriseabove.org/halbert-l-alexander/\n## 6                   <NA>  https://cafriseabove.org/harvey-r-alexander/\n\n\n\nIf we want instead to create a tibble, we can use the readr package’s read_csv function, which is a bit more robust and has a few additional features.\n\nlibrary(readr)\nairmen <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv')\nhead(airmen)\n## # A tibble: 6 × 16\n##   name         last_…¹ first…² graduation_date     rank_…³ class gradu…⁴ pilot…⁵\n##   <chr>        <chr>   <chr>   <dttm>              <chr>   <chr> <chr>   <chr>  \n## 1 Adams, John… Adams   John H… 1945-04-15 00:00:00 2nd Lt  SE-4… TAAF    Single…\n## 2 Adams, Paul  Adams   Paul    1943-04-29 00:00:00 2nd Lt  SE-4… TAAF    Single…\n## 3 Adkins, Rut… Adkins  Ruther… 1944-10-16 00:00:00 2nd Lt  SE-4… TAAF    Single…\n## 4 Adkins, Win… Adkins  Winsto… 1944-02-08 00:00:00 2nd Lt  TE-4… TAAF    Twin e…\n## 5 Alexander, … Alexan… Halber… 1944-11-20 00:00:00 2nd Lt  SE-4… TAAF    Single…\n## 6 Alexander, … Alexan… Harvey… 1944-04-15 00:00:00 2nd Lt  TE-4… TAAF    Twin e…\n## # … with 8 more variables: military_hometown_of_record <chr>, state <chr>,\n## #   aerial_victory_credits <chr>, number_of_aerial_victory_credits <dbl>,\n## #   reported_lost <chr>, reported_lost_date <dttm>,\n## #   reported_lost_location <chr>, web_profile <chr>, and abbreviated variable\n## #   names ¹​last_name, ²​first_name, ³​rank_at_graduation, ⁴​graduated_from,\n## #   ⁵​pilot_type\n\n\n\nIn pandas, we can read the csv using pd.read_csv\n\nimport pandas as pd\n\nairmen = pd.read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-02-08/airmen.csv\")\nairmen.head()\n##                     name  ...                                    web_profile\n## 0    Adams, John H., Jr.  ...      https://cafriseabove.org/john-h-adams-jr/\n## 1            Adams, Paul  ...           https://cafriseabove.org/paul-adams/\n## 2  Adkins, Rutherford H.  ...  https://cafriseabove.org/rutherford-h-adkins/\n## 3     Adkins, Winston A.  ...                                            NaN\n## 4  Alexander, Halbert L.  ...  https://cafriseabove.org/halbert-l-alexander/\n## \n## [5 rows x 16 columns]\n\n\n\n\n\n\n\n9.7.3 Working with Data Frames\nOften, we want to know what a data frame contains. R and pandas both have easy summary methods for data frames.\n\n\n\n\n\n\nData Frame Summaries\n\n\n\nNotice that the type of summary depends on the data type.\n\n\nR\nPython\n\n\n\n\nsummary(airmen)\n##      name            last_name          first_name       \n##  Length:1006        Length:1006        Length:1006       \n##  Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character  \n##                                                          \n##                                                          \n##                                                          \n##                                                          \n##  graduation_date                   rank_at_graduation    class          \n##  Min.   :1942-03-06 00:00:00.000   Length:1006        Length:1006       \n##  1st Qu.:1943-10-22 00:00:00.000   Class :character   Class :character  \n##  Median :1944-05-23 00:00:00.000   Mode  :character   Mode  :character  \n##  Mean   :1944-07-02 13:18:52.462                                        \n##  3rd Qu.:1945-04-15 00:00:00.000                                        \n##  Max.   :1948-10-12 00:00:00.000                                        \n##  NA's   :11                                                             \n##  graduated_from      pilot_type        military_hometown_of_record\n##  Length:1006        Length:1006        Length:1006                \n##  Class :character   Class :character   Class :character           \n##  Mode  :character   Mode  :character   Mode  :character           \n##                                                                   \n##                                                                   \n##                                                                   \n##                                                                   \n##     state           aerial_victory_credits number_of_aerial_victory_credits\n##  Length:1006        Length:1006            Min.   :0.0000                  \n##  Class :character   Class :character       1st Qu.:0.0000                  \n##  Mode  :character   Mode  :character       Median :0.0000                  \n##                                            Mean   :0.1118                  \n##                                            3rd Qu.:0.0000                  \n##                                            Max.   :4.0000                  \n##                                                                            \n##  reported_lost      reported_lost_date   reported_lost_location\n##  Length:1006        Min.   :1943-07-02   Length:1006           \n##  Class :character   1st Qu.:1943-07-02   Class :character      \n##  Mode  :character   Median :1943-07-02   Mode  :character      \n##                     Mean   :1943-07-02                         \n##                     3rd Qu.:1943-07-02                         \n##                     Max.   :1943-07-02                         \n##                     NA's   :1004                               \n##  web_profile       \n##  Length:1006       \n##  Class :character  \n##  Mode  :character  \n##                    \n##                    \n##                    \n## \n\nlibrary(skimr) # Fancier summaries\nskim(airmen)\n\n\nData summary\n\n\nName\nairmen\n\n\nNumber of rows\n1006\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nnumeric\n1\n\n\nPOSIXct\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nname\n0\n1.00\n9\n28\n0\n1003\n0\n\n\nlast_name\n0\n1.00\n3\n12\n0\n617\n0\n\n\nfirst_name\n0\n1.00\n3\n17\n0\n804\n0\n\n\nrank_at_graduation\n5\n1.00\n3\n14\n0\n7\n0\n\n\nclass\n20\n0.98\n3\n9\n0\n72\n0\n\n\ngraduated_from\n0\n1.00\n4\n23\n0\n4\n0\n\n\npilot_type\n0\n1.00\n11\n13\n0\n5\n0\n\n\nmilitary_hometown_of_record\n9\n0.99\n3\n19\n0\n366\n0\n\n\nstate\n11\n0.99\n2\n5\n0\n48\n0\n\n\naerial_victory_credits\n934\n0.07\n31\n137\n0\n50\n0\n\n\nreported_lost\n1004\n0.00\n1\n1\n0\n1\n0\n\n\nreported_lost_location\n1004\n0.00\n23\n23\n0\n1\n0\n\n\nweb_profile\n813\n0.19\n34\n95\n0\n190\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nnumber_of_aerial_victory_credits\n0\n1\n0.11\n0.46\n0\n0\n0\n0\n4\n▇▁▁▁▁\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\ngraduation_date\n11\n0.99\n1942-03-06\n1948-10-12\n1944-05-23\n52\n\n\nreported_lost_date\n1004\n0.00\n1943-07-02\n1943-07-02\n1943-07-02\n1\n\n\n\n\n\n\n\n\n# All variables - strings are summarized with NaNs\nairmen.describe(include = 'all')\n\n# Only summarize numeric variables\n##                       name  ...                                        web_profile\n## count                 1006  ...                                                193\n## unique                1003  ...                                                190\n## top     Brothers, James E.  ...  https://cafriseabove.org/captain-graham-smith-...\n## freq                     2  ...                                                  2\n## mean                   NaN  ...                                                NaN\n## std                    NaN  ...                                                NaN\n## min                    NaN  ...                                                NaN\n## 25%                    NaN  ...                                                NaN\n## 50%                    NaN  ...                                                NaN\n## 75%                    NaN  ...                                                NaN\n## max                    NaN  ...                                                NaN\n## \n## [11 rows x 16 columns]\nairmen.describe(include = [np.number])\n\n# Only summarize string variables (objects)\n##        number_of_aerial_victory_credits\n## count                       1006.000000\n## mean                           0.111829\n## std                            0.457844\n## min                            0.000000\n## 25%                            0.000000\n## 50%                            0.000000\n## 75%                            0.000000\n## max                            4.000000\nairmen.describe(include = ['O'])\n\n# Get counts of how many NAs in each column\n##                       name  ...                                        web_profile\n## count                 1006  ...                                                193\n## unique                1003  ...                                                190\n## top     Brothers, James E.  ...  https://cafriseabove.org/captain-graham-smith-...\n## freq                     2  ...                                                  2\n## \n## [4 rows x 15 columns]\nairmen.info(show_counts=True)\n## <class 'pandas.core.frame.DataFrame'>\n## RangeIndex: 1006 entries, 0 to 1005\n## Data columns (total 16 columns):\n##  #   Column                            Non-Null Count  Dtype  \n## ---  ------                            --------------  -----  \n##  0   name                              1006 non-null   object \n##  1   last_name                         1006 non-null   object \n##  2   first_name                        1006 non-null   object \n##  3   graduation_date                   995 non-null    object \n##  4   rank_at_graduation                999 non-null    object \n##  5   class                             986 non-null    object \n##  6   graduated_from                    1006 non-null   object \n##  7   pilot_type                        1006 non-null   object \n##  8   military_hometown_of_record       997 non-null    object \n##  9   state                             995 non-null    object \n##  10  aerial_victory_credits            72 non-null     object \n##  11  number_of_aerial_victory_credits  1006 non-null   float64\n##  12  reported_lost                     2 non-null      object \n##  13  reported_lost_date                2 non-null      object \n##  14  reported_lost_location            2 non-null      object \n##  15  web_profile                       193 non-null    object \n## dtypes: float64(1), object(15)\n## memory usage: 125.9+ KB\n\nIn pandas, you will typically want to separate out .describe() calls for numeric and non-numeric columns. Another handy function in pandas is .info(), which you can use to show the number of non-NA values. This is particularly useful in sparse datasets where there may be a LOT of missing values and you may want to find out which columns have useful information for more than just a few rows."
  },
  {
    "objectID": "part-gen-prog/03-data-struct.html#sec-data-struct-refs",
    "href": "part-gen-prog/03-data-struct.html#sec-data-struct-refs",
    "title": "9  Data Structures",
    "section": "\n9.8 References",
    "text": "9.8 References\n\n\n\n\n[1] \nN. Matloff, The art of r programming: A tour of statistical software design. No Starch Press, 2011 [Online]. Available: https://books.google.com/books?id=o2aLBAAAQBAJ\n\n\n\n[2] \nM. Fripp, “Answer to \"python pandas dataframe, is it pass-by-value or pass-by-reference\". Stack overflow,” Aug. 12, 2016. [Online]. Available: https://stackoverflow.com/a/38925257/2859168. [Accessed: Jan. 10, 2023]\n\n\n[3] \nMathIsFun.com, “How to multiply matrices. Math is fun,” 2021. [Online]. Available: https://www.mathsisfun.com/algebra/matrix-multiplying.html. [Accessed: Jan. 10, 2023]"
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#fa-bullseye-objectives",
    "href": "part-gen-prog/035-matrix-calcs.html#fa-bullseye-objectives",
    "title": "10  Matrix Calculations",
    "section": "\n10.1  Objectives",
    "text": "10.1  Objectives\n\nUnderstand how to do matrix algebra in relevant programming languages\n\nWhile R, SAS, and Python are all extremely powerful statistical programming languages, the core of most programming languages is the ability to do basic calculations and matrix arithmetic. As almost every dataset is stored as a matrix-like structure (data sets and data frames both allow for multiple types, which isn’t quite compatible with more canonical matrices), it is useful to know how to do matrix-level calculations in whatever language you are planning to use to work with data.\nIn this section, we will essentially be using our programming language as overgrown calculators."
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#matrix-operations",
    "href": "part-gen-prog/035-matrix-calcs.html#matrix-operations",
    "title": "10  Matrix Calculations",
    "section": "\n10.2 Matrix Operations",
    "text": "10.2 Matrix Operations\n\n\nTable 10.1: Table of common mathematical and matrix operations in R, SAS, and Python [1].\n\n\n\n\n\n\n\nOperation\nR\nSAS\nPython\n\n\n\nAddition\n+\n+\n+\n\n\nSubtraction\n-\n-\n-\n\n\nElementwise Multiplication\n*\n#\n*\n\n\nDivision\n/\n/\n/\n\n\nModulo (Remainder)\n%%\nMOD\n%\n\n\nInteger Division\n%/%\nFLOOR(x\\y)\n//\n\n\nElementwise Exponentiation\n^\n##\n**\n\n\nMatrix/Vector Multiplication\n%*%\n*\nnp.dot()\n\n\nMatrix Exponentiation\n^\n**\nnp.exp()\n\n\nMatrix Transpose\nt(A)\n\nA`\nnp.transpose(A)\n\n\nMatrix Determinant\ndet(A)\ndet(A)\nnp.linalg.det(A)\n\n\nMatrix Diagonal\ndiag(A)\ndiag(A)\nnp.linalg.diag(A)\n\n\nMatrix Inverse\nsolve(A)\nsolve(A, diag({...}))\nnp.linalg.inv(A)\n\n\n\n\n\n\n\n\n\n\nBasic Mathematical Operators\n\n\n\n\n\nR\nPython\nSAS\n\n\n\n\nx <- 1:10\ny <- seq(3, 30, by = 3)\n\nx + y\n##  [1]  4  8 12 16 20 24 28 32 36 40\nx - y\n##  [1]  -2  -4  -6  -8 -10 -12 -14 -16 -18 -20\nx * y\n##  [1]   3  12  27  48  75 108 147 192 243 300\nx / y\n##  [1] 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333 0.3333333\n##  [8] 0.3333333 0.3333333 0.3333333\nx^2\n##  [1]   1   4   9  16  25  36  49  64  81 100\nt(x) %*% y\n##      [,1]\n## [1,] 1155\n\n\n\n\nimport numpy as np\n\nx = np.array(range(1, 11))\ny = np.array(range(3, 33, 3)) # python indexes are not inclusive\n\nx + y\n## array([ 4,  8, 12, 16, 20, 24, 28, 32, 36, 40])\nx - y\n## array([ -2,  -4,  -6,  -8, -10, -12, -14, -16, -18, -20])\nx * y\n## array([  3,  12,  27,  48,  75, 108, 147, 192, 243, 300])\nx / y\n## array([0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333,\n##        0.33333333, 0.33333333, 0.33333333, 0.33333333, 0.33333333])\nx ** 2\n## array([  1,   4,   9,  16,  25,  36,  49,  64,  81, 100])\nnp.dot(x.T, y)\n## 1155\n\n\n\nBy default, SAS creates row vectors with do(a, b, by = c) syntax. The transpose operator (a single backtick) can be used to transform A into A`.\nproc iml; \n  x = do(1, 10, 1);\n  y = do(3, 30, 3);\n\n  z = x + y;\n  z2 = x - y;\n  z3 = x # y;\n  z4 = x/y;\n  z5 = x##2;\n  z6 = x` * y;\n  print z, z2, z3, z4, z5, z6;\nquit;\n\n\n\n\n\n\n\n\n\n\n\nMatrix Operations\n\n\n\nOther matrix operations, such as determinants and extraction of the matrix diagonal, are similarly easy:\n\n\nR\nPython\nSAS\n\n\n\n\nmat <- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T)\nmat\n##      [,1] [,2] [,3]\n## [1,]    1    2    3\n## [2,]    6    4    5\n## [3,]    7    8    9\nt(mat) # transpose\n##      [,1] [,2] [,3]\n## [1,]    1    6    7\n## [2,]    2    4    8\n## [3,]    3    5    9\ndet(mat) # get the determinant\n## [1] 18\ndiag(mat) # get the diagonal\n## [1] 1 4 9\ndiag(diag(mat)) # get a square matrix with off-diag 0s\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    4    0\n## [3,]    0    0    9\ndiag(1:3) # diag() also will create a diagonal matrix if given a vector\n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    2    0\n## [3,]    0    0    3\n\n\n\n\nimport numpy as np\nmat = np.array([[1, 2, 3],[6, 4, 5],[7, 8, 9]], dtype = int, order ='C')\n\nmat\n## array([[1, 2, 3],\n##        [6, 4, 5],\n##        [7, 8, 9]])\nmat.T\n## array([[1, 6, 7],\n##        [2, 4, 8],\n##        [3, 5, 9]])\nnp.linalg.det(mat) # numerical precision...\n## 18.000000000000004\nnp.diag(mat)\n## array([1, 4, 9])\nnp.diag(np.diag(mat))\n## array([[1, 0, 0],\n##        [0, 4, 0],\n##        [0, 0, 9]])\nnp.diag(range(1, 4))\n## array([[1, 0, 0],\n##        [0, 2, 0],\n##        [0, 0, 3]])\n\n\n\nproc iml;\n  mat = {1 2 3, 6 4 5, 7 8 9}; \n  tmat = mat`; /* transpose */\n  determinant = det(mat); /* get the determinant */\n  diagonal_vector = vecdiag(mat); /* get the diagonal as a vector */\n  diagonal_mat = diag(mat); /* get the diagonal as a square matrix */\n                            /* with 0 on off-diagonal entries */\n  \n  dm = diag({1 2 3}); /* make a square matrix with vector as the diagonal */\n  \n  print tmat, determinant, diagonal_vector, diagonal_mat, dm;\nquit;\n\n\n\n\n\n\n\n\n\n\n\nMatrix Inverse\n\n\n\nThe other important matrix-related function is the inverse. In R, A^-1 will get you the elementwise reciprocal of the matrix. Not exactly what we’d like to see… Instead, in R and SAS, we use the solve() function. The inverse is defined as the matrix B such that AB = I where I is the identity matrix (1’s on diagonal, 0’s off-diagonal). So if we solve(A) (in R) or solve(A, diag(n)) in SAS (where n is a vector of 1s the size of A), we will get the inverse matrix. In Python, we use the np.linalg.inv() function to invert a matrix, which may be a bit more linguistically familiar.\n\n\nR\nPython\nSAS\n\n\n\n\nmat <- matrix(c(1, 2, 3, 6, 4, 5, 7, 8, 9), nrow = 3, byrow = T)\n\nminv <- solve(mat) # get the inverse\n\nminv\n##            [,1]       [,2]       [,3]\n## [1,] -0.2222222  0.3333333 -0.1111111\n## [2,] -1.0555556 -0.6666667  0.7222222\n## [3,]  1.1111111  0.3333333 -0.4444444\nmat %*% minv \n##      [,1] [,2] [,3]\n## [1,]    1    0    0\n## [2,]    0    1    0\n## [3,]    0    0    1\n\n\n\n\nimport numpy as np\nmat = np.array([[1, 2, 3],[6, 4, 5],[7, 8, 9]], dtype = int, order ='C')\n\nminv = np.linalg.inv(mat)\nminv\n## array([[-0.22222222,  0.33333333, -0.11111111],\n##        [-1.05555556, -0.66666667,  0.72222222],\n##        [ 1.11111111,  0.33333333, -0.44444444]])\nnp.dot(mat, minv)\n## array([[1., 0., 0.],\n##        [0., 1., 0.],\n##        [0., 0., 1.]])\nnp.round(np.dot(mat, minv), 2)\n## array([[1., 0., 0.],\n##        [0., 1., 0.],\n##        [0., 0., 1.]])\n\n\n\nDocumentation\n    proc iml;\n      mat = {1 2 3, 6 4 5, 7 8 9};\n\n      mat_inv = solve(mat, diag({1 1 1})); /* get the inverse */\n      mat_inv2 = inv(mat); /* less efficient and less accurate */\n      print mat_inv, mat_inv2;\n\n      id = mat * mat_inv;\n      id2 = mat * mat_inv2;\n      print id, id2; \n    quit;"
  },
  {
    "objectID": "part-gen-prog/035-matrix-calcs.html#references",
    "href": "part-gen-prog/035-matrix-calcs.html#references",
    "title": "10  Matrix Calculations",
    "section": "\n10.3 References",
    "text": "10.3 References\n\n\n\n\n[1] \nQuartz25, Jesdisciple, H. Röst, D. Ross, L. D’Oliveiro, and BLibrestez55, Python Programming. Wikibooks, 2016 [Online]. Available: https://en.wikibooks.org/wiki/Python_Programming. [Accessed: May 28, 2022]"
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#fa-bullseye-objectives",
    "href": "part-gen-prog/04-control-struct.html#fa-bullseye-objectives",
    "title": "11  Control Structures",
    "section": "\n11.1  Objectives",
    "text": "11.1  Objectives\n\nUnderstand how to use conditional statements\nUnderstand how conditional statements are evaluated by a program\nUse program flow diagrams to break a problem into parts and evaluate how a program will execute\nUnderstand how to use loops\nSelect the appropriate type of loop for a problem"
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#mindset",
    "href": "part-gen-prog/04-control-struct.html#mindset",
    "title": "11  Control Structures",
    "section": "\n11.2 Mindset",
    "text": "11.2 Mindset\nBefore we start on the types of control structures, let’s get in the right mindset. We’re all used to “if-then” logic, and use it in everyday conversation, but computers require another level of specificity when you’re trying to provide instructions.\nCheck out this video of the classic “make a peanut butter sandwich instructions challenge”:\n\n\n\n\nHere’s another example:\n\n\n‘If you’re done being pedantic, we should get dinner.’ ‘You did it again!’ ‘No, I didn’t.’ Image from Randal Munroe, xkcd.com, available under a CC-By 2.5 license.\n\n\nThe key takeaways from these bits of media are that you should read this section with a focus on exact precision - state exactly what you mean, and the computer will do what you say. If you instead expect the computer to get what you mean, you’re going to have a bad time."
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#conditional-statements",
    "href": "part-gen-prog/04-control-struct.html#conditional-statements",
    "title": "11  Control Structures",
    "section": "\n11.3 Conditional Statements",
    "text": "11.3 Conditional Statements\nConditional statements determine if code is evaluated.\nThey look like this:\nif (condition)\n  then\n    (thing to do)\n  else\n    (other thing to do)\nThe else (other thing to do) part may be omitted.\nWhen this statement is read by the computer, the computer checks to see if condition is true or false. If the condition is true, then (thing to do) is also run. If the condition is false, then (other thing to do) is run instead.\nLet’s try this out:\n\n\nR\nPython\n\n\n\n\nx <- 3\ny <- 1\n\nif (x > 2) { \n  y <- 8\n} else {\n  y <- 4\n}\n\nprint(paste(\"x =\", x, \"; y =\", y))\n## [1] \"x = 3 ; y = 8\"\n\nIn R, the logical condition after if must be in parentheses. It is common to then enclose the statement to be run if the condition is true in {} so that it is clear what code matches the if statement. You can technically put the condition on the line after the if (x > 2) line, and everything will still work, but then it gets hard to figure out what to do with the else statement - it technically would also go on the same line, and that gets hard to read.\n\nx <- 3\ny <- 1\n\nif (x > 2) y <- 8 else y <- 4\n\nprint(paste(\"x =\", x, \"; y =\", y))\n## [1] \"x = 3 ; y = 8\"\n\nSo while the 2nd version of the code technically works, the first version with the brackets is much easier to read and understand. Please try to emulate the first version!\n\n\n\nx = 3\ny = 1\n\nif x > 2:\n  y = 8\nelse:\n  y = 4\n\nprint(\"x =\", x, \"; y =\", y)\n## x = 3 ; y = 8\n\nIn python, all code grouping is accomplished with spaces instead of with brackets. So in python, we write our if statement as if x > 2: with the colon indicating that what follows is the code to evaluate. The next line is indented with 2 spaces to show that the code on those lines belongs to that if statement. Then, we use the else: statement to provide an alternative set of code to run if the logical condition in the if statement is false. Again, we indent the code under the else statement to show where it “belongs”.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPython will throw errors if you mess up the spacing. This is one thing that is very annoying about Python… but it’s a consequence of trying to make the code more readable.\n\n\n\n11.3.1 Representing Conditional Statements as Diagrams\nA common way to represent conditional logic is to draw a flow chart diagram.\nIn a flow chart, conditional statements are represented as diamonds, and other code is represented as a rectangle. Yes/no or True/False branches are labeled. Typically, after a conditional statement, the program flow returns to a single point.\n\n\nProgram flow diagram outline of a simple if/else statement\n\n\n\n11.3.2 Chaining Conditional Statements: Else-If\nIn many cases, it can be helpful to have a long chain of conditional statements describing a sequence of alternative statements.\n\n\n\n\n\n\nExample - Conditional Evaluation\n\n\n\nSuppose I want to determine what categorical age bracket someone falls into based on their numerical age. All of the bins are mutually exclusive - you can’t be in the 25-40 bracket and the 41-55 bracket.\n\n\nProgram Flow Map\nR\nPython\n\n\n\n\n\nProgram flow map for a series of mutually exclusive categories. If our goal is to take a numeric age variable and create a categorical set of age brackets, such as <18, 18-25, 26-40, 41-55, 56-65, and >65, we can do this with a series of if-else statements chained together. Only one of the bracket assignments is evaluated, so it is important to place the most restrictive condition first.\n\n\nThe important thing to realize when examining this program flow map is that if age <= 18 is true, then none of the other conditional statements even get evaluated. That is, once a statement is true, none of the other statements matter. Because of this, it is important to place the most restrictive statement first.\n\n\nProgram flow map for a series of mutually exclusive categories, emphasizing that only some statements are evaluated. When age = 40, only (age <= 18), (age <= 25), and (age <= 40) are evaluated conditionally. Of the assignment statements, only bracket = ‘26-40’ is evaluated when age = 40.\n\n\nIf for some reason you wrote your conditional statements in the wrong order, the wrong label would get assigned:\n\n\nProgram flow map for a series of mutually exclusive categories, with category labels in the wrong order - <40 is evaluated first, and so <= 25 and <= 18 will never be evaluated and the wrong label will be assigned for anything in those categories.\n\n\nIn code, we would write this statement using else-if (or elif) statements.\n\n\n\nage <- 40 # change this as you will to see how the code works\n\nif (age < 18) {\n  bracket <- \"<18\"\n} else if (age <= 25) {\n  bracket <- \"18-25\"\n} else if (age <= 40) {\n  bracket <- \"26-40\"\n} else if (age <= 55) {\n  bracket <- \"41-55\" \n} else if (age <= 65) {\n  bracket <- \"56-65\"\n} else {\n  bracket <- \">65\"\n}\n\nbracket\n## [1] \"26-40\"\n\n\n\nPython uses elif as a shorthand for else if statements. As always, indentation/white space in python matters. If you put an extra blank line between two elif statements, then the interpreter will complain. If you don’t indent properly, the interpreter will complain.\n\nage = 40 # change this to see how the code works\n\nif age < 18:\n  bracket = \"<18\"\nelif age <= 25:\n  bracket = \"18-25\"\nelif age <= 40:\n  bracket = \"26-40\"\nelif age <= 55:\n  bracket = \"41-55\"\nelif age <= 65:\n  bracket = \"56-65\"\nelse:\n  bracket = \">65\"\n  \nbracket\n## '26-40'\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out - Chained If/Else Statements\n\n\n\n\n\nProblem\nFlow Map\nR Solution\nPython Solution\n\n\n\nThe US Tax code has brackets, such that the first $10,275 of your income is taxed at 10%, anything between $10,275 and $41,775 is taxed at 12%, and so on.\nHere is the table of tax brackets for single filers in 2022:\n\n\nrate\nIncome\n\n\n\n10%\n$0 to $10,275\n\n\n12%\n$10,275 to $41,775\n\n\n22%\n$41,775 to $89,075\n\n\n24%\n$89,075 to $170,050\n\n\n32%\n$170,050 to $215,950\n\n\n35%\n$215,950 to $539,900\n\n\n37%\n$539,900 or more\n\n\n\nNote: For the purposes of this problem, we’re ignoring the personal exemption and the standard deduction, so we’re already simplifying the tax code.\nWrite a set of if statements that assess someone’s income and determine what their overall tax rate is.\nHint: You may want to keep track of how much of the income has already been taxed in a variable and what the total tax accumulation is in another variable.\n\n\n\n\n\n\n\n\nThe control flow diagram for the tax brackets\n\n\nControl flow diagrams can be extremely helpful when figuring out how programs work (and where gaps in your logic are when you’re debugging). It can be very helpful to map out your program flow as you’re untangling a problem.\n\n\n\n# Start with total income\nincome <- 200000\n\n# x will hold income that hasn't been taxed yet\nx <- income\n# y will hold taxes paid\ny <- 0\n\nif (x <= 10275) {\n  y <- x*.1 # tax paid\n  x <- 0 # All money has been taxed\n} else {\n  y <- y + 10275 * .1\n  x <- x - 10275 # Money remaining that hasn't been taxed\n}\n\nif (x <= (41775 - 10275)) {\n  y <- y + x * .12\n  x <- 0\n} else {\n  y <- y + (41775 - 10275) * .12\n  x <- x - (41775 - 10275) \n}\n\nif (x <= (89075 - 41775)) {\n  y <- y + x * .22\n  x <- 0\n} else {\n  y <- y + (89075 - 41775) * .22\n  x <- x - (89075 - 41775)\n}\n\nif (x <= (170050 - 89075)) {\n  y <- y + x * .24\n  x <- 0\n} else {\n  y <- y + (170050 - 89075) * .24\n  x <- x - (170050 - 89075)\n}\n\nif (x <= (215950 - 170050)) {\n  y <- y + x * .32\n  x <- 0\n} else {\n  y <- y + (215950 - 170050) * .32\n  x <- x - (215950 - 170050)\n}\n\nif (x <= (539900 - 215950)) {\n  y <- y + x * .35\n  x <- 0\n} else {\n  y <- y + (539900 - 215950) * .35\n  x <- x - (539900 - 215950)\n}\n\nif (x > 0) {\n  y <- y + x * .37\n}\n\n\nprint(paste(\"Total Tax Rate on $\", income, \" in income = \", round(y/income, 4)*100, \"%\"))\n## [1] \"Total Tax Rate on $ 2e+05  in income =  22.12 %\"\n\n\n\n\n# Start with total income\nincome = 200000\n\n# untaxed will hold income that hasn't been taxed yet\nuntaxed = income\n# taxed will hold taxes paid\ntaxes = 0\n\nif untaxed <= 10275:\n  taxes = untaxed*.1 # tax paid\n  untaxed = 0 # All money has been taxed\nelse:\n  taxes = taxes + 10275 * .1\n  untaxed = untaxed - 10275 # money remaining that hasn't been taxed\n\nif untaxed <= (41775 - 10275):\n  taxes = taxes + untaxed * .12\n  untaxed = 0\nelse:\n  taxes = taxes + (41775 - 10275) * .12\n  untaxed = untaxed - (41775 - 10275) \n\n\nif untaxed <= (89075 - 41775):\n  taxes = taxes + untaxed * .22\n  untaxed = 0\nelse: \n  taxes = taxes + (89075 - 41775) * .22\n  untaxed = untaxed - (89075 - 41775)\n\nif untaxed <= (170050 - 89075):\n  taxes = taxes + untaxed * .24\n  untaxed = 0\nelse: \n  taxes = taxes + (170050 - 89075) * .24\n  untaxed = untaxed - (170050 - 89075)\n\nif untaxed <= (215950 - 170050):\n  taxes = taxes + untaxed * .32\n  untaxed = 0\nelse:\n  taxes = taxes + (215950 - 170050) * .32\n  untaxed = untaxed - (215950 - 170050)\n\nif untaxed <= (539900 - 215950):\n  taxes = taxes + untaxed * .35\n  untaxed = 0\nelse: \n  taxes = taxes + (539900 - 215950) * .35\n  untaxed = untaxed - (539900 - 215950)\n\n\nif untaxed > 0:\n  taxes = taxes + untaxed * .37\n\n\n\nprint(\"Total Tauntaxed Rate on $\", income, \" in income = \", round(taxes/income, 4)*100, \"%\")\n## Total Tauntaxed Rate on $ 200000  in income =  22.12 %\n\nWe will find a better way to represent this calculation once we discuss loops - we can store each bracket’s start and end point in a vector and loop through them. Any time you find yourself copy-pasting code and changing values, you should consider using a loop (or eventually a function) instead."
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#loops",
    "href": "part-gen-prog/04-control-struct.html#loops",
    "title": "11  Control Structures",
    "section": "\n11.4 Loops",
    "text": "11.4 Loops\n\nOften, we write programs which update a variable in a way that the new value of the variable depends on the old value:\nx = x + 1\nThis means that we add one to the current value of x.\nBefore we write a statement like this, we have to initialize the value of x because otherwise, we don’t know what value to add one to.\nx = 0\nx = x + 1\nWe sometimes use the word increment to talk about adding one to the value of x; decrement means subtracting one from the value of x.\nA particularly powerful tool for making these types of repetitive changes in programming is the loop, which executes statements a certain number of times. Loops can be written in several different ways, but all loops allow for executing a block of code a variable number of times.\n\n11.4.1 While Loops\nIn the previous section, we discussed conditional statements, where a block of code is only executed if a logical statement is true. The simplest type of loop is the while loop, which executes a block of code until a statement is no longer true.\n\n\n\n\n\n\nExample - While Loops\n\n\n\n\n\nFlow Map\nR\nPython\n\n\n\n\n\nFlow map showing while-loop pseudocode (while x <= N) { # code that changes x in some way} and the program flow map expansion where we check if x > N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then change x and start over.\n\n\n\n\n\nx <- 0\n\nwhile (x < 10) { \n  # Everything in here is executed \n  # during each iteration of the loop\n  print(x)\n  x <- x + 1\n}\n## [1] 0\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n## [1] 7\n## [1] 8\n## [1] 9\n\n\n\n\nx = 0\n\nwhile x < 10:\n  print(x)\n  x = x + 1\n## 0\n## 1\n## 2\n## 3\n## 4\n## 5\n## 6\n## 7\n## 8\n## 9\n\n\n\n\n\n\n\n\n\n\n\n\nTry it Out - While Loops\n\n\n\n\n\nProblem\nMath Notation\nR Solution\nPython solution\n\n\n\nWrite a while loop that verifies that \\[\\lim_{N \\rightarrow \\infty} \\prod_{k=1}^N \\left(1 + \\frac{1}{k^2}\\right) = \\frac{e^\\pi - e^{-\\pi}}{2\\pi}.\\]\nTerminate your loop when you get within 0.0001 of \\(\\frac{e^\\pi - e^{-\\pi}}{2\\pi}\\). At what value of \\(k\\) is this point reached?\n\n\nBreaking down math notation for code:\n\nIf you are unfamiliar with the notation \\(\\prod_{k=1}^N f(k)\\), this is the product of \\(f(k)\\) for \\(k = 1, 2, ..., N\\), \\[f(1)\\cdot f(2)\\cdot ... \\cdot f(N)\\]\nTo evaluate a limit, we just keep increasing \\(N\\) until we get arbitrarily close to the right hand side of the equation.\n\nIn this problem, we can just keep increasing \\(k\\) and keep track of the cumulative product. So we define k=1, prod = 1, and ans before the loop starts. Then, we loop over k, multiplying prod by \\((1 + 1/k^2)\\) and then incrementing \\(k\\) by one each time. At each iteration, we test whether prod is close enough to ans to stop the loop.\n\n\nIn R, you will use pi and exp() - these are available by default without any additional libraries or packages.\n\nk <- 1\nprod <- 1\nans <- (exp(pi) - exp(-pi))/(2*pi)\ndelta <- 0.0001\n\nwhile (abs(prod - ans) >= 0.0001) {\n  prod <- prod * (1 + 1/k^2)\n  k <- k + 1\n}\n\nk\n## [1] 36761\nprod\n## [1] 3.675978\nans\n## [1] 3.676078\n\n\n\nNote that in python, you will have to import the math library to get the values of pi and the exp function. You can refer to these as math.pi and math.exp() respectively.\n\nimport math\n\nk = 1\nprod = 1\nans = (math.exp(math.pi) - math.exp(-math.pi))/(2*math.pi)\ndelta = 0.0001\n\nwhile abs(prod - ans) >= 0.0001:\n  prod = prod * (1 + k**-2)\n  k = k + 1\n  if k > 500000:\n    break\n\n\nprint(\"At \", k, \" iterations, the product is \", prod, \"compared to the limit \", ans,\".\")\n## At  36761  iterations, the product is  3.675977910975878 compared to the limit  3.676077910374978 .\n\n\n\n\n\n\n\n\n\n\n\n\nWarning: Avoid Infinite Loops\n\n\n\nIt is very easy to create an infinite loop when you are working with while loops. Infinite loops never exit, because the condition is always true. If in the while loop example we decrement x instead of incrementing x, the loop will run forever.\nYou want to try very hard to avoid ever creating an infinite loop - it can cause your session to crash.\nOne common way to avoid infinite loops is to create a second variable that just counts how many times the loop has run. If that variable gets over a certain threshold, you exit the loop.\n\n\nR\nPython\n\n\n\nThis while loop runs until either x < 10 or n > 50 - so it will run an indeterminate number of times and depends on the random values added to x. Since this process (a ‘random walk’) could theoretically continue forever, we add the n>50 check to the loop so that we don’t tie up the computer for eternity.\n\nx <- 0\nn <- 0 # count the number of times the loop runs\n\nwhile (x < 10) { \n  print(x)\n  x <- x + rnorm(1) # add a random normal (0, 1) draw each time\n  n <- n + 1\n  if (n > 50) \n    break # this stops the loop if n > 50\n}\n## [1] 0\n## [1] 0.5784109\n## [1] 0.8110585\n## [1] -0.3370091\n## [1] 2.192678\n## [1] 2.297477\n## [1] 1.579666\n## [1] 1.439367\n## [1] 1.027423\n## [1] 0.906022\n## [1] 1.446936\n## [1] 1.918215\n## [1] 2.025918\n## [1] 3.961098\n## [1] 4.724725\n## [1] 4.544116\n## [1] 5.136827\n## [1] 4.842806\n## [1] 4.637494\n## [1] 5.125483\n## [1] 4.416551\n## [1] 4.549008\n## [1] 4.922749\n## [1] 5.997388\n## [1] 6.472741\n## [1] 6.612704\n## [1] 7.35135\n## [1] 6.521946\n## [1] 7.469193\n## [1] 8.108795\n## [1] 8.385834\n## [1] 9.676331\n## [1] 9.303546\n## [1] 9.19358\n\n\n\n\nimport numpy as np; # for the random normal draw\n\nx = 0\nn = 0 # count the number of times the loop runs\n\nwhile x < 10:\n  print(x)\n  x = x + np.random.normal(0, 1, 1) # add a random normal (0, 1) draw each time\n  n = n + 1\n  if n > 50:\n    break # this stops the loop if n > 50\n## 0\n## [-1.64940985]\n## [-2.2340515]\n## [-2.3877608]\n## [-3.33135283]\n## [-5.38990466]\n## [-2.91954154]\n## [-0.71063423]\n## [-1.08510254]\n## [-0.2618705]\n## [1.06681958]\n## [1.46290131]\n## [2.3182848]\n## [2.20515055]\n## [2.14994895]\n## [1.94006444]\n## [0.83844597]\n## [-0.33299111]\n## [0.25509602]\n## [-1.12183468]\n## [0.39617935]\n## [0.19425329]\n## [2.49059488]\n## [2.98187871]\n## [3.83034699]\n## [2.92901788]\n## [2.38754926]\n## [1.53581488]\n## [2.72101825]\n## [1.75852595]\n## [2.86681232]\n## [3.39819943]\n## [4.20076909]\n## [5.45239613]\n## [6.58077036]\n## [6.84843888]\n## [5.95069216]\n## [4.01780255]\n## [4.17544738]\n## [4.46836679]\n## [4.41057405]\n## [4.84290737]\n## [4.97956382]\n## [3.27153942]\n## [4.39145735]\n## [4.7031069]\n## [4.12096166]\n## [2.83427391]\n## [2.15720095]\n## [1.79782177]\n## [2.33888155]\n\n\n\n\nIn both of the examples above, there are more efficient ways to write a random walk, but we will get to that later. The important thing here is that we want to make sure that our loops don’t run for all eternity.\n\n\n\n11.4.2 For Loops\nAnother common type of loop is a for loop. In a for loop, we run the block of code, iterating through a series of values (commonly, one to N, but not always). Generally speaking, for loops are known as definite loops because the code inside a for loop is executed a specific number of times. While loops are known as indefinite loops because the code within a while loop is evaluated until the condition is falsified, which is not always a known number of times.\n\n\n\n\n\n\nExample - For Loop Syntax\n\n\n\n\n\nFlow Map\nR\nPython\n\n\n\n\n\nFlow map showing for-loop pseudocode (for j in 1 to N) { # code} and the program flow map expansion where j starts at 1 and we check if j > N (exiting the loop if true); otherwise, we continue into the loop, execute the main body of #code and then increment j and start over.\n\n\n\n\n\nfor (i in 1:5 ) {\n  print(i)\n}\n## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n\n\n\n\nfor i in range(5):\n  print(i)\n## 0\n## 1\n## 2\n## 3\n## 4\n\nBy default range(5) goes from 0 to 5, the upper bound. When i = 5 the loop exits. This is because range(5) creates a vector [0, 1, 2, 3, 4].\n\n\n\n\n\nFor loops are often run from 1 to N (or 0 to N-1 in python) but in essence, a for loop is very commonly used to do a task for every value of a vector.\n\n11.4.2.1 Example - For Loops\n\n\nR\nPython\n\n\n\nFor instance, in R, there is a built-in variable called month.name. Type month.name into your R console to see what it looks like. If we want to iterate along the values of month.name, we can:\n\nfor (i in month.name)\n  print(i)\n## [1] \"January\"\n## [1] \"February\"\n## [1] \"March\"\n## [1] \"April\"\n## [1] \"May\"\n## [1] \"June\"\n## [1] \"July\"\n## [1] \"August\"\n## [1] \"September\"\n## [1] \"October\"\n## [1] \"November\"\n## [1] \"December\"\n\n\n\nIn python, we have to define our vector or list to start out with, but that’s easy enough:\n\nfuturama_crew = ['Fry', 'Leela', 'Bender', 'Amy', 'the Professor', 'Hermes', 'Zoidberg', 'Nibbler']\nfor i in futurama_crew:\n  print(i)\n## Fry\n## Leela\n## Bender\n## Amy\n## the Professor\n## Hermes\n## Zoidberg\n## Nibbler"
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#other-control-structures",
    "href": "part-gen-prog/04-control-struct.html#other-control-structures",
    "title": "11  Control Structures",
    "section": "\n11.5 Other Control Structures",
    "text": "11.5 Other Control Structures\n\n11.5.1 Conditional Statements\ncase statements, e.g. case_when in tidyverse\n\n11.5.2 Loops\n\n11.5.2.1 Controlling Loops\nWhile I do not often use break, next, and continue statements, they do exist in both languages and can be useful for controlling the flow of program execution. I have moved the section on this to Section 24.2 for the sake of brevity and to reduce the amount of new material those without programming experience are being exposed to in this section.\n\n11.5.2.2 Other Types of Loops\nThere are other types of loops in most languages, such as the do-while loop, which runs the code first and then evaluates the logical condition to determine whether the loop will be run again.\n\n\n\n\n\n\nExample: do-while loops\n\n\n\n\n\nR\nPython\n\n\n\nIn R, do-while loops are most naturally implemented using a very primitive type of iteration: a repeat statement.\n\nrepeat {\n  # statements go here\n  if (condition)\n    break # this exits the repeat statement\n}\n\n\n\nIn python, do-while loops are most naturally implemented using a while loop with condition TRUE:\n\nwhile TRUE:\n  # statements go here\n  if condition:\n    break\n\n\n\n\n\n\nAn additional means of running code an indeterminate number of times is the use of recursion, which we cannot cover until we learn about functions. I have added an additional section, Section 24.3, to cover this topic, but it is not essential to being able to complete most basic data programming tasks. Recursion is useful when working with structures such as trees (including phylogenetic trees) and nested lists."
  },
  {
    "objectID": "part-gen-prog/04-control-struct.html#sec-control-struct-refs",
    "href": "part-gen-prog/04-control-struct.html#sec-control-struct-refs",
    "title": "11  Control Structures",
    "section": "\n11.6 References",
    "text": "11.6 References"
  },
  {
    "objectID": "part-gen-prog/05-functions.html#fa-bullseye-objectives",
    "href": "part-gen-prog/05-functions.html#fa-bullseye-objectives",
    "title": "12  Writing Functions",
    "section": "\n12.1  Objectives",
    "text": "12.1  Objectives\n\nIdentify the parts of a function from provided source code\nPredict what the function will return when provided with input values and source code\nGiven a task, lay out the steps necessary to complete the task in pseudocode\nWrite a function which uses necessary input values to complete a task"
  },
  {
    "objectID": "part-gen-prog/05-functions.html#when-to-write-a-function",
    "href": "part-gen-prog/05-functions.html#when-to-write-a-function",
    "title": "12  Writing Functions",
    "section": "\n12.2 When to write a function?",
    "text": "12.2 When to write a function?\nIf you’ve written the same code (with a few minor changes, like variable names) more than twice, you should probably write a function instead. There are a few benefits to this rule:\n\nYour code stays neater (and shorter), so it is easier to read, understand, and maintain.\nIf you need to fix the code because of errors, you only have to do it in one place.\nYou can re-use code in other files by keeping functions you need regularly in a file (or if you’re really awesome, in your own package!)\nIf you name your functions well, your code becomes easier to understand thanks to grouping a set of actions under a descriptive function name.\n\n\n\n\n\n\n\nLearn more about functions\n\n\n\nThere is some extensive material on this subject in R for Data Science [1] on functions. If you want to really understand how functions work in R, that is a good place to go.\n\n\n\n\n\n\n\n\nExample: Turning Code into Functions\n\n\n\nThis example is modified from R for Data Science [2, Ch. 19].\nWhat does this code do? Does it work as intended?\n\n\nR\nPython\n\n\n\n\ndf <- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b <- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c <- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d <- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n  'a': np.random.randn(10), \n  'b': np.random.randn(10), \n  'c': np.random.randn(10), \n  'd': np.random.randn(10)})\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\ndf.b = (df.b - min(df.b))/(max(df.b) - min(df.a))\ndf.c = (df.c - min(df.c))/(max(df.c) - min(df.c))\ndf.d = (df.d - min(df.d))/(max(df.d) - min(df.d))\n\n\n\n\nThe code rescales a set of variables to have a range from 0 to 1. But, because of the copy-pasting, the code’s author made a mistake and forgot to change an a to b.\nWriting a function to rescale a variable would prevent this type of copy-paste error.\nTo write a function, we first analyze the code to determine how many inputs it has:\n\n\nR\nPython\n\n\n\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\n\nThis code has only one input: df$a.\n\n\n\n\ndf.a = (df.a - min(df.a))/(max(df.a) - min(df.a))\n\nThis code has only one input: df.a\n\n\n\nTo convert the code into a function, we start by rewriting it using general names:\n\n\nR\nPython\n\n\n\nIn this case, it might help to replace df$a with x.\n\nx <- df$a \n\n(x - min(x, na.rm = TRUE)) / \n  (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n##  [1] 0.3266877 0.8454891 0.3755147 0.6827532 1.0000000 0.5599201 0.6903440\n##  [8] 0.6067227 0.0000000 0.3585888\n\n\n\nIn this case, it might help to replace df.a with x.\n\nx = df.a\n\n(x - min(x))/(max(x) - min(x))\n## 0    1.000000\n## 1    0.561338\n## 2    0.431346\n## 3    0.000000\n## 4    0.563970\n## 5    0.489798\n## 6    0.967892\n## 7    0.571396\n## 8    0.568369\n## 9    0.989954\n## Name: a, dtype: float64\n\n\n\n\nThen, we make it a bit easier to read, removing duplicate computations if possible (for instance, computing min two times).\n\n\nR\nPython\n\n\n\nIn R, we can use the range function, which computes the maximum and minimum at the same time and returns the result as c(min, max)\n\nrng <- range(x, na.rm = T)\n\n(x - rng[1])/(rng[2] - rng[1])\n##  [1] 0.3266877 0.8454891 0.3755147 0.6827532 1.0000000 0.5599201 0.6903440\n##  [8] 0.6067227 0.0000000 0.3585888\n\n\n\nIn python, range is the equivalent of seq() in R, so we are better off just using min and max.\n\nx = df.a\n\n\nxmin, xmax = [x.min(), x.max()]\n(x - xmin)/(xmax - xmin)\n## 0    1.000000\n## 1    0.561338\n## 2    0.431346\n## 3    0.000000\n## 4    0.563970\n## 5    0.489798\n## 6    0.967892\n## 7    0.571396\n## 8    0.568369\n## 9    0.989954\n## Name: a, dtype: float64\n\n\n\n\nFinally, we turn this code into a function:\n\n\nR\nPython\n\n\n\n\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = T)\n  (x - rng[1])/(rng[2] - rng[1])\n}\n\nrescale01(df$a)\n##  [1] 0.3266877 0.8454891 0.3755147 0.6827532 1.0000000 0.5599201 0.6903440\n##  [8] 0.6067227 0.0000000 0.3585888\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df$a, df$b, df$c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, between { and } (this is true in R, in python, there are different conventions, but the same principle applies)\nThe function returns the last value computed: in this case, (x - rng[1])/(rng[2]-rng[1]). You can make this explicit by adding a return() statement around that calculation.\n\n\n\n\ndef rescale01(x):\n  xmin, xmax = [x.min(), x.max()]\n  return (x - xmin)/(xmax - xmin)\n\nrescale01(df.a)\n## 0    1.000000\n## 1    0.561338\n## 2    0.431346\n## 3    0.000000\n## 4    0.563970\n## 5    0.489798\n## 6    0.967892\n## 7    0.571396\n## 8    0.568369\n## 9    0.989954\n## Name: a, dtype: float64\n\n\nThe name of the function, rescale01, describes what the function does - it rescales the data to between 0 and 1.\nThe function takes one argument, named x; any references to this value within the function will use x as the name. This allows us to use the function on df.a, df.b, df.c, and so on, with x as a placeholder name for the data we’re working on at the moment.\nThe code that actually does what your function is supposed to do goes in the body of the function, indented relative to the line with def: function_name():. At the end of the function, you should have a blank line with no spaces or tabs.\nThe function returns the value it is told to return: in this case, (x - xmin)/(xmax - xmin). In Python, you must return a value if you want the function to perform a computation. 1\n\n\n\n\nThe process for creating a function is important: first, you figure out how to do the thing you want to do. Then, you simplify the code as much as possible. Only at the end of that process do you create an actual function."
  },
  {
    "objectID": "part-gen-prog/05-functions.html#syntax",
    "href": "part-gen-prog/05-functions.html#syntax",
    "title": "12  Writing Functions",
    "section": "\n12.3 Syntax",
    "text": "12.3 Syntax\n\n\nR and python syntax for defining functions. Portions of the command that indicate the function name, function scope, and return statement are highlighted in each block.\n\n\nIn R, functions are defined as other variables, using <-, but we specify the arguments a function takes by using the function() statement. The contents of the function are contained within { and }. If the function returns a value, a return() statement can be used; alternately, if there is no return statement, the last computation in the function will be returned.\nIn python, functions are defined using the def command, with the function name, parentheses, and the function arguments to follow. The first line of the function definition ends with a :, and all subsequent lines of the function are indented (this is how python knows where the end of the function is). A python function return statement is return <value>, with no parentheses needed.\nNote that in python, the return statement is not optional. It is not uncommon to have python functions that don’t return anything; in R, this is a bit less common, for reasons we won’t get into here."
  },
  {
    "objectID": "part-gen-prog/05-functions.html#arguments-and-parameters",
    "href": "part-gen-prog/05-functions.html#arguments-and-parameters",
    "title": "12  Writing Functions",
    "section": "\n12.4 Arguments and Parameters",
    "text": "12.4 Arguments and Parameters\nAn argument is the name for the object you pass into a function.\nA parameter is the name for the object once it is inside the function (or the name of the thing as defined in the function).\n\n\n\n\n\n\nExample: Parts of a Function\n\n\n\nLet’s examine the difference between arguments and parameters by writing a function that takes a dog’s name and returns “ is a good pup!”.\n\n\nR\nPython\n\n\n\n\ndog <- \"Eddie\"\n\ngoodpup <- function(name) {\n  paste(name, \"is a good pup!\")\n}\n\ngoodpup(dog)\n## [1] \"Eddie is a good pup!\"\n\n\n\n\ndog = \"Eddie\"\n\ndef goodpup(name):\n  return name + \" is a good pup!\"\n\ngoodpup(dog)\n## 'Eddie is a good pup!'\n\n\n\n\nIn this example function, when we call goodpup(dog), dog is the argument. name is the parameter.\nWhat is happening inside the computer’s memory as goodpup runs?\n\n\nA sketch of the execution of the program goodpup, showing that name is only defined within the local environment that is created while goodpup is running. We can never access name in our global environment.\n\n\n\n\nThis is why the distinction between arguments and parameters matters. Parameters are only accessible while inside of the function - and in that local environment, we need to call the object by the parameter name, not the name we use outside the function (the argument name).\nWe can even call a function with an argument that isn’t defined outside of the function call: goodpup(\"Tesla\") produces “Tesla is a good pup!”. Here, I do not have a variable storing the string “Tesla”, but I can make the function run anyways. So “Tesla” here is an argument to goodpup but it is not a variable in my environment.\nThis is a confusing set of concepts and it’s ok if you only just sort of get what I’m trying to explain here. Hopefully it will become more clear as you write more code.\n\n\n\n\n\n\nTry it out: Function Parts\n\n\n\nFor each of the following blocks of code, identify the function name, function arguments, parameter names, and return statements. When the function is called, see if you can predict what the output will be. Also determine whether the function output is stored in memory or just printed to the command line.\n\n\nFunction 1\nAnswer\n\n\n\n\n\ndef hello_world():\n  print(\"Hello World\")\n\n\nhello_world()\n\n\n\n\nFunction name: hello_world\n\nFunction parameters: none\nFunction arguments: none\nFunction output:\n\n\nhello_world()\n## Hello World\n\n\nFunction output is not stored in memory and is printed to the command line.\n\n\n\n\n\n\nFunction 2\nAnswer\n\n\n\n\n\nmy_mean <- function(x) {\n  censor_x <- sample(x, size = length(x) - 2, replace = F)\n  mean(censor_x)\n}\n\n\nset.seed(3420523)\nx = my_mean(1:10)\nx\n\n\n\n\nFunction name: my_mean\n\nFunction parameters: x\nFunction arguments: 1:10\nFunction output: (varies each time the function is run unless you set the seed)\n\n\nset.seed(3420523)\nx = my_mean(1:10)\nx\n## [1] 6\n\n\nFunction output is saved to memory (x) and printed to the command line\n\n\n\n\n\n\n\n12.4.1 Named Arguments and Parameter Order\nIn the examples above, you didn’t have to worry about what order parameters were passed into the function, because there were 0 and 1 parameters, respectively. But what happens when we have a function with multiple parameters?\n\n\nR\nPython\n\n\n\n\n\ndivide <- function(x, y) {\n  x / y\n}\n\n\n\n\n\ndef divide(x, y):\n  return x / y\n\n\n\n\nIn this function, the order of the parameters matters! divide(3, 6) does not produce the same result as divide(6, 3). As you might imagine, this can quickly get confusing as the number of parameters in the function increases.\nIn this case, it can be simpler to use the parameter names when you pass in arguments.\n\n\nR\nPython\n\n\n\n\ndivide(3, 6)\n## [1] 0.5\n\ndivide(x = 3, y = 6)\n## [1] 0.5\n\ndivide(y = 6, x = 3)\n## [1] 0.5\n\ndivide(6, 3)\n## [1] 2\n\ndivide(x = 6, y = 3)\n## [1] 2\n\ndivide(y = 3, x = 6)\n## [1] 2\n\n\n\n\ndivide(3, 6)\n## 0.5\ndivide(x = 3, y = 6)\n## 0.5\ndivide(y = 6, x = 3)\n## 0.5\ndivide(6, 3)\n## 2.0\ndivide(x = 6, y = 3)\n## 2.0\ndivide(y = 3, x = 6)\n## 2.0\n\n\n\n\nAs you can see, the order of the arguments doesn’t much matter, as long as you use named arguments, but if you don’t name your arguments, the order very much matters.\n\n12.4.2 Input Validation\nWhen you write a function, you often assume that your parameters will be of a certain type. But you can’t guarantee that the person using your function knows that they need a certain type of input. In these cases, it’s best to validate your function input.\n\n\n\n\n\n\nInput Validation Example\n\n\n\n\n\nR\nPython\n\n\n\nIn R, you can use stopifnot() to check for certain essential conditions. If you want to provide a more illuminating error message, you can check your conditions using if() and then use stop(\"better error message\") in the body of the if statement.\n\nadd <- function(x, y) {\n  x + y\n}\n\nadd(\"tmp\", 3)\n## Error in x + y: non-numeric argument to binary operator\n\nadd <- function(x, y) {\n  stopifnot(is.numeric(x))\n  stopifnot(is.numeric(y))\n  x + y\n}\n\nadd(\"tmp\", 3)\n## Error in add(\"tmp\", 3): is.numeric(x) is not TRUE\nadd(3, 4)\n## [1] 7\n\n\n\nIn Python, the easiest way to handle errors is to use a try statement, which operates rather like an if statement: if the statement executes, then we’re good to go; if not, we can use except to handle different types of errors. The else clause is there to handle anything that needs to happen if the statement in the try clause executes without any errors.\n\n\ndef add(x, y):\n  x + y\n\nadd(\"tmp\", 3)\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: can only concatenate str (not \"int\") to str\ndef add(x, y):\n  try:\n    return x + y\n  except TypeError:\n    print(\"x and y must be add-able\")\n  else:\n    # We should never get here, because the try clause has a return statement\n    print(\"Else clause?\")\n  return\n\nadd(\"tmp\", 3)\n## x and y must be add-able\nadd(3, 4)\n## 7\n\nYou can read more about error handling in Python here\n\n\n\n\n\nInput validation is one aspect of defensive programming - programming in such a way that you try to ensure that your programs don’t error out due to unexpected bugs by anticipating ways your programs might be misunderstood or misused [3]."
  },
  {
    "objectID": "part-gen-prog/05-functions.html#scope",
    "href": "part-gen-prog/05-functions.html#scope",
    "title": "12  Writing Functions",
    "section": "\n12.5 Scope",
    "text": "12.5 Scope\nWhen talking about functions, for the first time we start to confront a critical concept in programming, which is scope. Scope is the part of the program where the name you’ve given a variable is valid - that is, where you can use a variable.\n\nA variable is only available from inside the region it is created.\n\nWhat do I mean by the part of a program? The lexical scope is the portion of the code (the set of lines of code) where the name is valid.\nThe concept of scope is best demonstrated through a series of examples, so in the rest of this section, I’ll show you some examples of how scope works and the concepts that help you figure out what “scope” actually means in practice.\n\n12.5.1 Name Masking\nScope is most clearly demonstrated when we use the same variable name inside and outside a function. Note that this is 1) bad programming practice, and 2) fairly easily avoided if you can make your names even slightly more creative than a, b, and so on. But, for the purposes of demonstration, I hope you’ll forgive my lack of creativity in this area so that you can see how name masking works.\n\n\n\n\n\n\nDanger\n\n\n\nWhat does this function return, 10 or 20?\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\na = 10\n\nmyfun = function() {\n  a = 20\n  return a\n}\n\nmyfun()\n\n\n\n\nA sketch of the global environment as well as the environment within myfun(). Because a=20 inside myfun(), when we call myfun(), we get the value of a within that environment, instead of within the global environment.\n\n\n\n\n\na <- 10\n\nmyfun <- function() {\n  a <- 20\n  a\n}\n\nmyfun()\n## [1] 20\n\n\n\n\n\na = 10\n\ndef myfun():\n  a = 20\n  return a\n\nmyfun()\n## 20\n\n\n\n\n\n\nThe lexical scope of the function is the area that is between the braces (in R) or the indented region (in python). Outside the function, a has the value of 10, but inside the function, a has the value of 20. So when we call myfun(), we get 20, because the scope of myfun is the local context where a is evaluated, and the value of a in that environment dominates.\nThis is an example of name masking, where names defined inside of a function mask names defined outside of a function.\n\n12.5.2 Environments and Scope\nAnother principle of scoping is that if you call a function and then call the same function again, the function’s environment is re-created each time. Each function call is unrelated to the next function call when the function is defined using local variables.\n\n\n\n\n\n\nDanger\n\n\n\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\nmyfun = function() {\n  if a is not defined\n    a = 1\n  else\n    a = a + 1\n}\n\nmyfun()\nmyfun()\n\nWhat does this output?\n\n\n\n\nWhen we define myfun, we create a template for an environment with variables and code to excecute. Each time myfun() is called, that template is used to create a new environment. This prevents successive calls to myfun() from affecting each other – which means a = 1 every time.\n\n\n\n\n\nmyfun <- function() {\n  if (!exists(\"aa\")) {\n    aa <- 1\n  } else {\n    aa <- aa + 1\n  }\n  return(aa)\n}\n\nmyfun()\n## [1] 1\nmyfun()\n## [1] 1\n\n\n\n\ndef myfun():\n  try: aa\n  except NameError: aa = 1\n  else: aa = aa + 1\n  return aa\n\nmyfun()\n## 1\nmyfun()\n## 1\n\nNote that the try command here is used to handle the case where a doesn’t exist. If there is a NameError (which will happen if aa is not defined) then we define aa = 1, if there is not a NameError, then aa = aa + 1.\nThis is necessary because Python does not have a built-in way to test if a variable exists before it is used [4], Ch 17.\n\n\n\n\n\n\n12.5.3 Dynamic Lookup\nScoping determines where to look for values – when, however, is determined by the sequence of steps in the code. When a function is called, the calling environment (the global environment or set of environments at the time the function is called) determines what values are used.\nIf an object doesn’t exist in the function’s environment, the global environment will be searched next; if there is no object in the global environment, the program will error out. This behavior, combined with changes in the calling environment over time, can mean that the output of a function can change based on objects outside of the function.\n\n\n\n\n\n\nDanger\n\n\n\n\n\nPseudocode\nSketch\nR\nPython\n\n\n\nmyfun = function() x + 1\n\nx = 14\n\nmyfun()\n\nx = 20\n\nmyfun()\n\nWhat will the output be of this code?\n\n\n\n\nThe state of the global environment at the time the function is called (that is, the state of the calling environment) can change the results of the function\n\n\n\n\n\nmyfun <- function() {\n  x + 1\n}\n\nx <- 14\n\nmyfun()\n## [1] 15\n\nx <- 20\n\nmyfun()\n## [1] 21\n\n\n\n\n\ndef myfun():\n  return x + 1\n\n\nx = 14\n\nmyfun()\n## 15\nx = 20\n\nmyfun()\n## 21\n\n\n\n\n\n\n\n\n\n\n\n\nTry It Out: Function Scope\n\n\n\nWhat does the following function return? Make a prediction, then run the code yourself. From [2, Ch. 6]\n\n\nR code\nR solution\nPython code\nPython solution\n\n\n\n\nf <- function(x) {\n  f <- function(x) {\n    f <- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n\n\n\n\nf <- function(x) {\n  f <- function(x) {\n    f <- function() {\n      x ^ 2\n    }\n    f() + 1\n  }\n  f(x) * 2\n}\nf(10)\n## [1] 202\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ^ 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n\n\n\n\ndef f(x):\n  def f(x):\n    def f():\n      return x ** 2\n    return f() + 1\n  return f(x) * 2\n\nf(10)\n## 202"
  },
  {
    "objectID": "part-gen-prog/05-functions.html#sec-functions-refs",
    "href": "part-gen-prog/05-functions.html#sec-functions-refs",
    "title": "12  Writing Functions",
    "section": "\n12.6 References",
    "text": "12.6 References\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[3] \nWikipedia Contributors, “Defensive programming,” Wikipedia. Wikimedia Foundation, Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Defensive_programming&oldid=1084121123. [Accessed: May 31, 2022]\n\n\n[4] \nA. Martelli and D. Ascher, Python Cookbook. O’Reilly Media, 2002 [Online]. Available: https://learning.oreilly.com/library/view/python-cookbook/0596001673/ch05s24.html. [Accessed: May 31, 2022]"
  },
  {
    "objectID": "part-gen-prog/06-debugging.html",
    "href": "part-gen-prog/06-debugging.html",
    "title": "13  Debugging",
    "section": "",
    "text": "14 I’ve deleted the intermediate chunks because they screw\n’ knitr::knit(text=indoc, output=“test.md”) rmarkdown::render(“test.md”)"
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#fa-bullseye-objectives",
    "href": "part-gen-prog/06-debugging.html#fa-bullseye-objectives",
    "title": "13  Debugging",
    "section": "\n13.1  Objectives",
    "text": "13.1  Objectives\n\nCreate reproducible examples of problems\nUse built in debugging tools to trace errors\nUse online resources to research errors\n\n\n\nThe faces of debugging (by Allison Horst)"
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#avoiding-errors-defensive-programming",
    "href": "part-gen-prog/06-debugging.html#avoiding-errors-defensive-programming",
    "title": "13  Debugging",
    "section": "\n13.2 Avoiding Errors: Defensive Programming",
    "text": "13.2 Avoiding Errors: Defensive Programming\nOne of the best debugging strategies (that isn’t a debugging strategy at all, really) is to code defensively [1]. By that, I mean, code in a way that you will make debugging things easier later.\n\nModularize your code. Each function should do only one task, ideally in the least-complex way possible.\nMake your code readable. If you can read the code easily, you’ll be able to narrow down the location of the bug more quickly.\nComment your code. This makes it more likely that you will be able to locate the spot where the bug is likely to have occurred, and will remind you how things are calculated. Remember, comments aren’t just for your collaborators or others who see the code. They’re for future you.\nDon’t duplicate code. If you have the same code (or essentially the same code) in two or three different places, put that code in a function and call the function instead. This will save you trouble when updating the code in the future, but also makes narrowing down the source of the bug less complex.\nReduce the number of dependencies you have on outside software packages. Often bugs are introduced when a dependency is updated and the functionality changes slightly. The tidyverse [2] is notorious for this.\n\n\n\n\n\n\n\nNote\n\n\n\nIt’s ok to write code using lots of dependencies, but as you transition from “experimental” code to “production” code (you’re using the code without tinkering with it) you should work to reduce the dependencies, where possible. In addition, if you do need packages with lots of dependencies, try to make sure those packages are relatively popular, used by a lot of people, and currently maintained. (The tidyverse is a bit better from this perspective, because the constituent packages are some of the most installed R packages on CRAN.)\n\n\nAnother way to handle dependency management is to use the renv package [3], which creates a local package library with the appropriate versions of your packages stored in the same directory as your project. renv was inspired by the python concept of virtual environments, and it does also work with python if you’re using both R and python inside a project (e.g. this book uses renv). renv will at the very least help you minimize issues with code not working after unintentional package updates.\n\nAdd safeguards against unexpected inputs. Check to make sure inputs to the function are valid. Check to make sure intermediate results are reasonable (e.g. you don’t compute the derivative of a function and come up with “a”.)\nDon’t reinvent the wheel. If you have working, tested code for a task, use that! If someone else has working code that’s used by the community, don’t write your own unless you have a very good reason. The implementation of lm has been better tested than your homegrown linear regression.\nCollect your often-reused code in packages that you can easily load and make available to “future you”. The process of making a package often encourages you to document your code better than you would a script. A good resource for getting started making R packages is [4], and a similar python book is [5]."
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#working-through-errors",
    "href": "part-gen-prog/06-debugging.html#working-through-errors",
    "title": "13  Debugging",
    "section": "\n13.3 Working through Errors",
    "text": "13.3 Working through Errors\n\n13.3.1 First steps\n\n13.3.1.1 Get into the right mindset\nYou can’t debug something effectively if you’re upset. You have to be in a puzzle-solving, detective mindset to actually solve a problem. If you’re already stressed out, try to relieve that stress before you tackle the problem: take a shower, go for a walk, pet a puppy.\n\n\nA debugging manifesto [6]\n\n\n\n13.3.1.2 Check your spelling\nI’ll guess that 80% of my personal debugging comes down to spelling errors and misplaced punctuation.\n\n\nTitle: user.fist_name [7]\n\n\n\n13.3.2 General Debugging Strategies\n\n\n\nDebugging: Being the detective in a crime movie where you are also the murderer. - some t-shirt I saw once\n\nWhile defensive programming is a nice idea, if you’re already at the point where you have an error you can’t diagnose, then… it doesn’t help that much. At that point, you’ll need some general debugging strategies to work with. The overall process is well described in [8]; I’ve added some steps that are commonly overlooked and modified the context from the original package development to introductory programming. I’ve also integrated some lovely illustrations from Julia Evans (@b0rk) to lighten the mood.\n\nRealize that you have a bug\nRead the error message\n\n\n\nDebugging strategy: Reread the error message[9]\n\n\n\n\nGoogle! Seriously, just google the whole error message.\nIn R you can automate this with the errorist and searcher packages. Python is so commonly used that you’ll likely be able to find help for your issue if you are specific enough.\n\n\n\n\n\nDebugging strategy: Shorten your feedback loop [10]\n\n\n\n\nMake the error repeatable: This makes it easier to figure out what the error is, faster to iterate, and easier to ask for help.\n\nUse binary search (remove 1/2 of the code, see if the error occurs, if not go to the other 1/2 of the code. Repeat until you’ve isolated the error.)\nGenerate the error faster - use a minimal test dataset, if possible, so that you can ask for help easily and run code faster. This is worth the investment if you’ve been debugging the same error for a while. \nNote which inputs don’t generate the bug – this negative “data” is helpful when asking for help.\n\n\n\nDebugging strategy: Change working code into broken code [12]\n\n\n\nFigure out where it is. Debuggers may help with this, but you can also use the scientific method to explore the code, or the tried-and-true method of using lots of print() statements.\nCome up with one question. If you’re stuck, it can be helpful to break it down a bit and ask one tiny question about the bug.\n\n\n\nDebugging strategy: Come up with one question [13]\n\n\n\n\nFix it and test it. The goal with tests is to ensure that the same error doesn’t pop back up in a future version of your code. Generate an example that will test for the error, and add it to your documentation. If you’re developing a package, unit test suites offer a more formalized way to test errors and you can automate your testing so that every time your code is changed, tests are run and checked.\n\n\n\n\n\nDebugging strategy: Write a unit test [14]\n\n\nThere are several other general strategies for debugging:\n\nRetype (from scratch) your code\nThis works well if it’s a short function or a couple of lines of code, but it’s less useful if you have a big script full of code to debug. However, it does sometimes fix really silly typos that are hard to spot, like having typed <-- instead of <- in R and then wondering why your answers are negative.\nVisualize your data as it moves through the program. This may be done using print() statements, or the debugger, or some other strategy depending on your application.\nTracing statements. Again, this is part of print() debugging, but these messages indicate progress - “got into function x”, “returning from function y”, and so on.\nRubber ducking. Have you ever tried to explain a problem you’re having to someone else, only to have a moment of insight and “oh, never mind”? Rubber ducking outsources the problem to a nonjudgmental entity, such as a rubber duck1. You simply explain, in terms simple enough for your rubber duck to understand, exactly what your code does, line by line, until you’ve found the problem. See [15] for a more thorough explanation.\n\nDo not be surprised if, in the process of debugging, you encounter new bugs. This is a problem that’s well-known enough that it has its own xkcd comic. At some point, getting up and going for a walk may help. Redesigning your code to be more modular and more organized is also a good idea."
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#dividing-problems-into-smaller-parts",
    "href": "part-gen-prog/06-debugging.html#dividing-problems-into-smaller-parts",
    "title": "13  Debugging",
    "section": "\n13.4 Dividing Problems into Smaller Parts",
    "text": "13.4 Dividing Problems into Smaller Parts\n\n“Divide each difficulty into as many parts as is feasible and necessary to resolve it.” -René Descartes, Discourse on Method\n\nIn programming, as in life, big, general problems are very hard to solve effectively. Instead, the goal is to break a problem down into smaller pieces that may actually be solvable.\n\n\n\n\n\n\nExample: Exhaustion\n\n\n\nThis example inspired by [16].\n\n\nGeneral problem\nSpecific problem\nSubproblems\nBrainstorm\nSubproblem solutions\n\n\n\n“I’m exhausted all the time”\nOk, so this is a problem that many of us have from time to time (or all the time). If we get a little bit more specific at outlining the problem, though, we can sometimes get a bit more insight into how to solve it.\n\n\n“I wake up in the morning and I don’t have any energy to do anything. I want to go back to sleep, but I have too much to do to actually give in and sleep. I spend my days worrying about how I’m going to get all of the things on my to-do list done, and then I lie awake at night thinking about how many things there are to do tomorrow. I don’t have time for hobbies or exercise, so I drink a lot of coffee instead to make it through the day.”\nThis is a much more specific list of issues, and some of these issues are actually things we can approach separately.\n\n\nMoving through the list in the previous tab, we can isolate a few issues. Some of these issues are undoubtedly related to each other, but we can approach them separately (for the most part).\n\nPoor quality sleep (tired in the morning, lying awake at night)\nToo many things to do (to-do list)\nChemical solutions to low energy (coffee during the day)\nAnxiety about completing tasks (worrying, insomnia)\nLack of personal time for hobbies or exercise\n\n\n\n\nGet a check-up to rule out any other issues that could cause sleep quality degradation - depression, anxiety, sleep apnea, thyroid conditions, etc.\n\nAsk the doctor about taking melatonin supplements for a short time to ensure that sleep starts off well (note, don’t take medical advice from a stats textbook!)\n\n\nReformat your to-do list:\n\nSet time limits for things on the to-do list\nBreak the to-do list into smaller, manageable tasks that can be accomplished within a relatively short interval - such as an hour\nSort the to-do list by priority and level of “fun” so that each day has a few hard tasks and a couple of easy/fun tasks. Do the hard tasks first, and use the easy/fun tasks as a reward.\n\n\nSet a time limit for caffeine (e.g. no coffee after noon) so that caffeine doesn’t contribute to poor quality sleep\nAddress anxiety with medication (from 1), scheduled time for mindfulness meditation, and/or self-care activities\nScheduling time for exercise/hobbies\n\nscheduling exercise in the morning to take advantage of the endorphins generated by working out\nscheduling hobbies in the evening to reward yourself for a day’s work and wind down work well before bedtime\n\n\n\n\n\nWhen the sub-problem has a viable solution, move on to the next sub-problem. Don’t try to tackle everything at once. Here, that might look like this list, where each step is taken separately and you give each thing a few days to see how it affects your sleep quality. In programming, of course, this list would perhaps be a bit more sequential, but real life is messy and the results take a while to populate.\n\n[1] Make the doctor’s appointment.\n[5] While waiting for the appointment, schedule exercise early in the day and hobbies later in the day to create a “no-work” period before bedtime.\n[1] Go to the doctor’s appointment, follow up with any concerns.\n\n[1] If doctor approves, start taking melatonin according to directions\n\n\n[2] Work on reformatting the to-do list into manageable chunks. Schedule time to complete chunks using your favorite planning method.\n[4] If anxiety is still an issue after following up with the doctor, add some mindfullness meditation or self-care to the schedule in the mornings or evenings.\n[3] If sleep quality is still an issue, set a time limit for caffeine\n[2] Revise your to-do list and try a different tactic if what you were trying didn’t work."
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#minimal-working-or-reproducible-examples",
    "href": "part-gen-prog/06-debugging.html#minimal-working-or-reproducible-examples",
    "title": "13  Debugging",
    "section": "\n13.5 Minimal Working (or Reproducible) Examples",
    "text": "13.5 Minimal Working (or Reproducible) Examples\n\n\n\n\nThe reprex R package will help you make a reproducible example (drawing by Allison Horst)\n\n\nIf all else has failed, and you can’t figure out what is causing your error, it’s probably time to ask for help. If you have a friend or buddy that knows the language you’re working in, by all means ask for help sooner - use them as a rubber duck if you have to. But when you ask for help online, often you’re asking people who are much more knowledgeable about the topic - members of R core and really famous python developers browse stackoverflow and may drop in and help you out. Under those circumstances, it’s better to make the task of helping you as easy as possible because it shows respect for their time. The same thing goes for your supervisors and professors.\nThere are numerous resources for writing what’s called a “minimal working example”, “reproducible example” (commonly abbreviated reprex), or MCVE (minimal complete verifiable example). Much of this is lifted directly from the StackOverflow post describing a minimal reproducible example.\nThe goal is to reproduce the error message with information that is\n\n\nminimal - as little code as possible to still reproduce the problem\n\ncomplete - everything necessary to reproduce the issue is contained in the description/question\n\nreproducible - test the code you provide to reproduce the problem.\n\nYou should format your question to make it as easy as possible to help you. Make it so that code can be copied from your post directly and pasted into a terminal. Describe what you see and what you’d hope to see if the code were working.\n\n\n\n\n\n\nOther Minimum Working Example/Reprex resources\n\n\n\n\nreprex package: Do’s and Don’ts\n\nHow to use the reprex package - vignette with videos from Jenny Bryan\nreprex magic - Vignette adapted from a blog post by Nick Tierney\n\n\n\n\n\n\n\n\n\nExample: MWEs\n\n\n\nNote: You don’t need to know anything about SAS to understand this example.\n\nSAS markdown\n\n\nA long time ago, when this book covered R and SAS, I had issues with SAS graphs rendering in black and white most of the time.\nI started debugging the issue with the following code chunk:\n```{{r sas-cat-aes-map-07, engine=“sashtml”, engine.path=“sas”, fig.path = “image/”}} libname classdat “sas/”;\nPROC SGPLOT data=classdat.fbiwide; SCATTER x = Population y = Assault / markerattrs=(size=8pt symbol=circlefilled) group = Abb; /* maps to point color by default */ RUN; QUIT;\nPROC SGPLOT data=classdat.fbiwide NOAUTOLEGEND; /* dont generate a legend / SCATTER x = Population y = Assault / markercharattrs=(size=8) markerchar = Abb / specify marker character variable */ group = Abb ; RUN; QUIT;\n\n\nAfter running the code separately in SAS and getting a figure that looked like what I'd expected, I set out to construct a reproducible example so that I could post to the [SASmarkdown github issues page](https://github.com/Hemken/SASmarkdown/issues/14) and ask for help.\n\nThe first thing I did was strip out all of the extra stuff that didn't need to be in the chunk - this chunk generates 2 pictures; I only need one. \nThis chunk requires the fbiwide data from the `classdata` R package (that I exported to CSV); I replaced it with a dataset in the `sashelp` library.\n\nWhen I was done, the chunk looked like this:\n\nPROC SGPLOT data=sashelp.snacks; SCATTER x = date y = QtySold / markerattrs=(size=8pt symbol=circlefilled) group = product; /* maps to point color by default */ RUN; QUIT;\n\nThen, I started constructing my reproducible example. \nI ran `?sas_enginesetup` to get to a SASmarkdown help page, because I remembered it had a nice way to generate and run markdown files from R directly (without saving the Rmd file). \n\nI copied the example from that page:\n\n13.6 indoc <- ’\ntitle: “Basic SASmarkdown Doc” author: “Doug Hemken” output: html_document —\n\n\n15 everything up when I print this chunk out\n\n\nThen, I created several chunks which would do the following:\n1. Write the minimal example SAS code above to a [file](files/reprex.sas)\n2. Call that file in a SASmarkdown chunk using the `%include` macro, which dumps the listed file into the SAS program. This generates the [plot](files/SASmarkdown-reprex/unnamed-chunk-1.png) using SASmarkdown.\n3. Call the file using SAS batch mode    \n(this runs the code and produces a [plot](files/SASmarkdown-reprex/SGPlot.png) outside of SASmarkdown, to prove that the issue is SASmarkdown itself)\n\nFinally, I included the image generated from the batch mode call manually.\n\nYou can see the resulting code [here](https://github.com/Hemken/SASmarkdown/issues/14).\n\nI pasted my example into the issues page, and then included some additional information: \n\n1. A screenshot of the rendered page\n2. The image files themselves\n3. A description of what happened\n4. My suspicions (some obvious option I'm missing?)\n5. An additional line of R code that would delete any files created if someone ran my example. Because file clutter sucks.\n\nThis process took me about 45 minutes, but that was still much shorter than the time I'd spent rerunning code trying to get it to work with no success. \n\nIn less than 24 hours, the package maintainer responded with a (admittedly terse) explanation of what he thought caused the problem. \nI had to do some additional research to figure out what that meant, but once I had my reproducible example working in color, I posted that code (so that anyone else with the same problem would know what to do).\n\nThen, I had to tinker with the book a bit to figure out if there were easier ways to get the same result.\nThe end result, though, was that I got what I wanted - color figures throughout the book!\n\n#### Python/Quarto\n\nWhile converting the book from Rmarkdown to quarto, I ran into an issue setting up GitHub Actions (basically, when I push changes, GitHub rebuilds the book from scratch automatically). \n\n\nI found an issue describing the same segfault issue I had been getting, and so I made a [post there](https://github.com/rstudio/reticulate/issues/1133) with a new github repository containing a [minimal working example](https://github.com/srvanderplas/test-quarto) that I set up to test the problem. \n\nWithin 24h, I had gotten replies from people working at RStudio, and one of them had [diagnosed the problem](https://github.com/rstudio/reticulate/issues/1133#issuecomment-1021783041). \nAfter I asked a few more questions, one of them submitted a pull request to my repository with a solution.\n\nI didn't know enough python or enough about GitHub Actions to diagnose the problem myself, but because I managed to create a reproducible example, I got the answers I needed from people with more experience.\n\n:::\n\n:::\n\n::: callout-tip\n### Try It Out \n\nUse [this list of StackOverflow posts](files/Debugging_exercise.html) to try out your new debugging techniques. \nCan you figure out what's wrong? \nWhat information would you need from the poster in order to come up with a solution?\nHow much time did you spend trying to figure out what the poster was actually asking?\n:::\n\n\n\n## Debugging Tools\n\nNow that we've discussed general strategies for debugging that will work in any language, lets get down to the dirty details of debugging. \n\n### Low tech debugging with print() and other tools\nSometimes called \"tracing\" techniques, the most common, universal, and low tech strategy for debugging involves scattering messages throughout your code. \nWhen the code is executed, you get a window into what the variables look like during execution.\n\nThis is called **print debugging** and it is an incredibly useful tool.\n\n::: callout-caution\n#### Example: Nested Functions\n\n::: panel-tabset\n#### R\nImagine we start with this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = 1\ny = 2\nz = 0\n\naa <- function(x) {\n  bb <- function(y) {\n    cc <- function(z) {\n      z + y\n    }\n    cc(3) + 2\n  }\n  x + bb(4)\n}\n\naa(5)\n## [1] 14\n\n\n\n\nand the goal is to understand what’s happening in the code. We might add some lines:\n\nx = 1\ny = 2\nz = 0\n\naa <- function(x) {\n  print(paste(\"Entering aa(). x = \", x))\n  bb <- function(y) {\n    print(paste(\"Entering bb(). x = \", x, \"y = \", y))\n    cc <- function(z) {\n      print(paste(\"Entering cc(). x = \", x, \"y = \", y, \"z = \", z))\n      cres <- z + y\n      print(paste(\"Returning\", cres, \"from cc()\"))\n      cres\n    }\n    bres <- cc(3) + 2\n    print(paste(\"Returning\", bres, \"from bb()\"))\n    bres\n  }\n  ares <- x + bb(4)\n  print(paste(\"Returning\",ares, \"from aa()\"))\n  ares\n}\n\naa(5)\n## [1] \"Entering aa(). x =  5\"\n## [1] \"Entering bb(). x =  5 y =  4\"\n## [1] \"Entering cc(). x =  5 y =  4 z =  3\"\n## [1] \"Returning 7 from cc()\"\n## [1] \"Returning 9 from bb()\"\n## [1] \"Returning 14 from aa()\"\n## [1] 14\n\n\n15.0.0.1 Python\nImagine we start with this:\n\nx = 1\ny = 2\nz = 0\n\ndef aa(x):\n  def bb(y):\n    def cc(z):\n      return z + y\n    return cc(3) + 2\n  return x + bb(4)\n\naa(5)\n## 14\n\nand the goal is to understand what’s happening in the code. We might add some lines:\n\nx = 1\ny = 2\nz = 0\n\ndef aa(x):\n  print(\"Entering aa(). x = \" + str(x))\n  def bb(y):\n    print(\"Entering bb(). x = \" + str(x) + \", y = \" + str(y))\n    def cc(z):\n      print(\"Entering cc(). x = \" + str(x) + \", y = \" + str(y) + \", z = \" + str(z))\n      cres = z + y\n      print(\"Returning \" + str(cres) + \" from cc()\")\n      return cres\n    bres = cc(3) + 2\n    print(\"Returning \" + str(bres) + \" from bb()\")\n    return bres\n  ares = x + bb(4)\n  print(\"Returning \" + str(ares) + \" from aa()\")\n  return ares\n\naa(5)\n## Entering aa(). x = 5\n## Entering bb(). x = 5, y = 4\n## Entering cc(). x = 5, y = 4, z = 3\n## Returning 7 from cc()\n## Returning 9 from bb()\n## Returning 14 from aa()\n## 14\n\n\n\n\n:::\nFor more complex data structures, it can be useful to add str(), head(), or summary() functions.\n\n\n\n\n\n\nReal world example: Web Scraping\n\n\n\nIn fall 2020, I wrote a webscraper to get election polling data from the RealClearPolitics site as part of the electionViz package. I wrote the function search_for_parent() to get the parent HTML tag which matched the “tag” argument, that had the “node” argument as a descendant. I used print debugging to show the sequence of tags on the page.\nI was assuming that the order of the parents would be “html”, “body”, “div”, “table”, “tbody”, “tr” - descending from outer to inner (if you know anything about HTML/XML structure).\nTo prevent the site from changing on me (as websites tend to do…), I’ve saved the HTML file here.\n\n\nR\nPython\n\n\n\n\nlibrary(xml2) # read html\n\nsearch_for_parent <- function(node, tag) {\n  # Get all of the parent nodes \n  parents <- xml2::xml_parents(node)\n  # Get the tags of every parent node\n  tags <- purrr::map_chr(parents, rvest::html_name)\n  print(tags)\n  \n  # Find matching tags\n  matches <- which(tags == tag)\n  print(matches)\n  \n  # Take the minimum matching tag\n  min_match <- min(matches)\n  if (length(matches) == 1) return(parents[min_match]) else return(NULL)\n}\n\npage <- read_html(\"shorturl.at/jkS59\")\n## Error: 'shorturl.at/jkS59' does not exist in current working directory ('/home/susan/Projects/Class/stat-computing-r-python/part-gen-prog').\n# find all poll results in any table\npoll_results <- xml_find_all(page, \"//td[@class='lp-results']\") \n## Error in UseMethod(\"xml_find_all\"): no applicable method for 'xml_find_all' applied to an object of class \"function\"\n# find the table that contains it\nsearch_for_parent(poll_results[1], \"table\") \n## Error in nodeset_apply(x, function(x) .Call(node_parents, x)): object 'poll_results' not found\n\n\n\nYou may need to pip install lxml requests bs4 to run this code.\n\n# !pip install lxml requests bs4\nfrom bs4 import BeautifulSoup\nimport requests as req\nimport numpy as np\n\n\ndef search_for_parent(node, tag):\n  # Get all of the parent nodes\n  parents = node.find_parents()\n  # get tag type for each parent node\n  tags = [x.name for x in parents]\n  print(tags)\n  \n  # Find matching tags\n  matches = np.array([i for i, val in enumerate(tags) if val == tag])\n  print(matches)\n  \n  # Take the minimum matching tag\n  min_match = np.min(matches)\n  if matches.size == 1:\n    ret = parents[min_match]\n  \n  return ret\n\n\nhtml_file = open('shorturl.at/jkS59', 'r')\n## Error in py_call_impl(callable, dots$args, dots$keywords): FileNotFoundError: [Errno 2] No such file or directory: 'shorturl.at/jkS59'\npage = html_file.read() \n# Read the page as HTML\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'html_file' is not defined\nsoup = BeautifulSoup(page, 'html')\n# Find all poll results in any table\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'page' is not defined\npoll_results = soup.findAll('td', {'class': 'lp-results'})\n# Find the table that contains the first poll result\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'soup' is not defined\nsearch_for_parent(poll_results[0], 'table')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'poll_results' is not defined\n\n\n\n\nBy printing out all of the tags that contain node, I could see the order – inner to outer. I asked the function to return the location of the first table node, so the index (2nd value printed out) should match table in the character vector that was printed out first. I could then see that the HTML node that is returned is in fact the table node.\n\n\n\n\n\n\n\n\nTry it out: Hurricanes in R\n\n\n\nNot all bugs result in error messages, unfortunately, which makes higher-level techniques like traceback() less useful. The low-tech debugging tools, however, still work wonderfully.\n\n\nSetup\nBuggy code\nSolution 1: Identification\nSolution 2: Fixing\nSolution 3: Verifying\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(maps)\nlibrary(ggthemes)\nworldmap <- map_data(\"world\")\n\n# Load the data\ndata(storms, package = \"dplyr\")\n\n\n\nThe code below is supposed to print out a map of the tracks of all hurricanes of a specific category, 1 to 5, in 2013. Use print statements to figure out what’s wrong with my code.\n\n# Make base map to be used for each iteration\nbasemap <-  ggplot() + \n  # Country shapes\n  geom_polygon(aes(x = long, y = lat, group = group), \n               data = worldmap, fill = \"white\", color = \"black\") + \n  # Zoom in \n  coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + \n  # Don't need scales b/c maps provide their own geographic context...\n  theme_map()\n\nfor (i in 1:5) {\n  # Subset the data\n  subdata <- storms %>%\n    filter(year == 2013) %>%\n    filter(status == i)\n  \n  # Plot the data - path + points to show the observations\n  plot <- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  print(plot)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, lets split the setup from the loop.\n\n# Make base map to be used for each iteration\nbasemap <-  ggplot() + \n  # Country shapes\n  geom_polygon(aes(x = long, y = lat, group = group), \n               data = worldmap, fill = \"white\", color = \"black\") + \n  # Zoom in \n  coord_quickmap(xlim = c(-100, -10), ylim = c(10, 50)) + \n  # Don't need scales b/c maps provide their own geographic context...\n  theme_map()\n\nprint(basemap) # make sure the basemap is fine\n\n\n\n\n# Load the data\ndata(storms, package = \"dplyr\")\n\nstr(storms) # make sure the data exists and is formatted as expected\n## tibble [11,859 × 13] (S3: tbl_df/tbl/data.frame)\n##  $ name                        : chr [1:11859] \"Amy\" \"Amy\" \"Amy\" \"Amy\" ...\n##  $ year                        : num [1:11859] 1975 1975 1975 1975 1975 ...\n##  $ month                       : num [1:11859] 6 6 6 6 6 6 6 6 6 6 ...\n##  $ day                         : int [1:11859] 27 27 27 27 28 28 28 28 29 29 ...\n##  $ hour                        : num [1:11859] 0 6 12 18 0 6 12 18 0 6 ...\n##  $ lat                         : num [1:11859] 27.5 28.5 29.5 30.5 31.5 32.4 33.3 34 34.4 34 ...\n##  $ long                        : num [1:11859] -79 -79 -79 -79 -78.8 -78.7 -78 -77 -75.8 -74.8 ...\n##  $ status                      : chr [1:11859] \"tropical depression\" \"tropical depression\" \"tropical depression\" \"tropical depression\" ...\n##  $ category                    : Ord.factor w/ 7 levels \"-1\"<\"0\"<\"1\"<\"2\"<..: 1 1 1 1 1 1 1 1 2 2 ...\n##  $ wind                        : int [1:11859] 25 25 25 25 25 25 25 30 35 40 ...\n##  $ pressure                    : int [1:11859] 1013 1013 1013 1013 1012 1012 1011 1006 1004 1002 ...\n##  $ tropicalstorm_force_diameter: int [1:11859] NA NA NA NA NA NA NA NA NA NA ...\n##  $ hurricane_force_diameter    : int [1:11859] NA NA NA NA NA NA NA NA NA NA ...\n\nEverything looks ok in the setup chunk…\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata <- storms %>%\n    filter(year == 2013) %>%\n    filter(status == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot <- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  # print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n\nOk, so from this we can see that something is going wrong with our filter statement - we have no rows of data.\n\n\n\nhead(storms)\n## # A tibble: 6 × 13\n##   name   year month   day  hour   lat  long status categ…¹  wind press…² tropi…³\n##   <chr> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>  <ord>   <int>   <int>   <int>\n## 1 Amy    1975     6    27     0  27.5 -79   tropi… -1         25    1013      NA\n## 2 Amy    1975     6    27     6  28.5 -79   tropi… -1         25    1013      NA\n## 3 Amy    1975     6    27    12  29.5 -79   tropi… -1         25    1013      NA\n## 4 Amy    1975     6    27    18  30.5 -79   tropi… -1         25    1013      NA\n## 5 Amy    1975     6    28     0  31.5 -78.8 tropi… -1         25    1012      NA\n## 6 Amy    1975     6    28     6  32.4 -78.7 tropi… -1         25    1012      NA\n## # … with 1 more variable: hurricane_force_diameter <int>, and abbreviated\n## #   variable names ¹​category, ²​pressure, ³​tropicalstorm_force_diameter\n\nWhoops. I meant “category” when I typed “status”.\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata <- storms %>%\n    filter(year == 2013) %>%\n    filter(category == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot <- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  # print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 13 ncol 13\"\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 0 ncol 13\"\n\nOk, that’s something, at least. We now have some data for category 1 storms…\n\nfilter(storms, year == 2013) %>%\n  # Get max category for each named storm\n  group_by(name) %>%\n  filter(category == max(category)) %>%\n  ungroup() %>%\n  # See what categories exist\n  select(name, category) %>%\n  unique()\n## # A tibble: 14 × 2\n##    name      category\n##    <chr>     <ord>   \n##  1 Andrea    0       \n##  2 Barry     0       \n##  3 Chantal   0       \n##  4 Dorian    0       \n##  5 Erin      0       \n##  6 Fernand   0       \n##  7 Gabrielle 0       \n##  8 Eight     -1      \n##  9 Humberto  1       \n## 10 Ingrid    1       \n## 11 Jerry     0       \n## 12 Karen     0       \n## 13 Lorenzo   0       \n## 14 Melissa   0\n\nIt looks like 2013 was just an incredibly quiet year for tropical activity.\n\n\n2013 may have been a quiet year for tropical activity in the Atlantic, but 2004 was not. So let’s just make sure our code works by checking out 2004.\n\nfor (i in 1:5) {\n  print(paste0(\"Category \", i, \" storms\"))\n  # Subset the data\n  subdata <- storms %>%\n    filter(year == 2004) %>%\n    filter(category == i)\n  \n  print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n        # str(subdata) works too, but produces more clutter. I started\n        # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot <- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  print(plot) # Don't print plots - clutters up output at the moment\n}\n## [1] \"Category 1 storms\"\n## [1] \"subdata dims: nrow 45 ncol 13\"\n\n\n\n## [1] \"Category 2 storms\"\n## [1] \"subdata dims: nrow 39 ncol 13\"\n\n\n\n## [1] \"Category 3 storms\"\n## [1] \"subdata dims: nrow 29 ncol 13\"\n\n\n\n## [1] \"Category 4 storms\"\n## [1] \"subdata dims: nrow 32 ncol 13\"\n\n\n\n## [1] \"Category 5 storms\"\n## [1] \"subdata dims: nrow 12 ncol 13\"\n\n\n\n\nIf we want to only print informative plots, we could add an if statement. Now that the code works, we can also comment out our print() statements (we could delete them, too, depending on whether we anticipate future problems with the code).\n\nfor (i in 1:5) {\n  # print(paste0(\"Category \", i, \" storms\"))\n  \n  # Subset the data\n  subdata <- storms %>%\n    filter(year == 2013) %>%\n    filter(category == i)\n  \n  # print(paste0(\"subdata dims: nrow \", nrow(subdata), \" ncol \", ncol(subdata)))\n  #       # str(subdata) works too, but produces more clutter. I started\n  #       # with str() and moved to dim() when I saw the problem\n  \n  # Plot the data - path + points to show the observations\n  plot <- basemap +\n    geom_path(aes(x = long, y = lat, color = name), data = subdata) + \n    geom_point(aes(x = long, y = lat, color = name), data = subdata) + \n    ggtitle(paste0(\"Category \", i, \" storms in 2013\"))\n  \n  if (nrow(subdata) > 0) print(plot) \n}\n\n\n\n\n\n\n\n\n\nOnce you’ve found your problem, go back and delete or comment out your print statements, as they’re no longer necessary. If you think you may need them again, comment them out, otherwise, just delete them so that your code is neat, clean, and concise.\n\n15.0.1 After an error has occurred - traceback()\n\ntraceback() can help you narrow down where an error occurs by taking you through the series of function calls that led up to the error. This may help you identify which function is actually causing the problem, which is especially useful when you have nested functions or are using package functions that depend on other packages.\n\n\n\n\n\n\nUsing traceback\n\n\n\n\n\nR\nPython\n\n\n\n\naa <- function(x) {\n  bb <- function(y) {\n    cc <- function(z) {\n     stop('there was a problem')  # This generates an error\n    }\n    cc()\n  }\n  bb()\n}\n\naa()\n## Error in cc(): there was a problem\n\nFor more information, you could run traceback\n\ntraceback()\n\nWhich will provide the following output:\n4: stop(\"there was a problem\") at #4\n3: c() at #6\n2: b() at #8\n1: a()\nReading through this, we see that a() was called, b() was called, c() was called, and then there was an error. It’s even kind enough to tell us that the error occurred at line 4 of the code.\nIf you are running this code interactively in RStudio, it’s even easier to run traceback() by clicking on the “Show Traceback” option that appears when there is an error.\n\n\nBoth Show Traceback and Rerun with Debug are useful tools\n\n\nIf you are using source() to run the code in Rstudio, it will even provide a link to the file and line location of the error. \n\n\n\nimport sys,traceback\n\ndef aa(x):\n  def bb(y):\n    def cc(z):\n      try: \n        return y + z + tuple()[0] # This generates an error\n      except IndexError:\n        exc_type, exc_value, exc_tb = sys.exc_info()\n        traceback.print_exception(exc_type, exc_value, exc_tb, file = sys.stdout)\n    return cc(3) + 2\n  return x + bb(4)\n\naa(5)\n## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n\nPython’s traceback information is a bit more low-level and requires a bit more from the programmer than R’s version.\n\n\n\n\n\n\n15.0.2 Interactive Debugging\n\n\n\n\n\n\nR browser()\nPython\n\n\n\nThe browser() function is useful for debugging your own code. If you’re writing a function and something isn’t working quite right, you can insert a call to browser() in that function, and examine what’s going on.\n\n\n\n\n\n\nExample : browser()\n\n\n\nSuppose that I want to write a function that will plot an xkcd comic in R.\nI start with\n\nlibrary(png)\nlibrary(xml2)\nlibrary(dplyr)\n\n# get the most current xkcd\nget_xkcd <- function() {\n  url <- \"http://xkcd.com\"\n  page <- read_html(url)\n  # Find the comic\n  image <- xml_find_first(page, \"//div[@id='comic']/img\") %>%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  \n  \n  readPNG(source = image)\n}\n\nget_xkcd() %>%\n  as.raster() %>%\n  plot()\n## Error in readPNG(source = image): unable to open //imgs.xkcd.com/comics/washing_machine_settings.png\n\nHere’s the final function\n\nlibrary(png)\nlibrary(xml2)\n\n# get the most current xkcd\nget_xkcd <- function() {\n  \n  url <- \"http://xkcd.com\"\n  page <- read_html(url)\n  # Find the comic\n  image <- xml_find_first(page, \"//div[@id='comic']/img\") %>%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  \n  # Fix image address so that we can access the image\n  image <- substr(image, 3, nchar(image))\n  \n  # Download the file to a temp file and read from there\n  file_location <- tempfile(fileext = \".png\")\n  download.file(image, destfile = file_location, quiet = T)\n  \n  readPNG(source = file_location)\n}\n\nget_xkcd() %>%\n  as.raster() %>%\n  plot()\n\n\n\n\n\n\n\n\nIn python, the equivalent interactive debugger is ipdb. You can install it with pip install ipdb.\nIf you want to run Python in the interactive ipython console, then you can invoke the ipdb debugging with %debug get_xkcd(). This is similar to browser() in R. If you’re working in Python in RStudio, though, you have to get into debug mode in a more involved way.\nTo run code using ipdb when your code hits an error, add from ipdb import launch_ipdb_on_exception to the top of your python code chunk. Then, at the bottom, put any lines that may trigger the error after these two lines:\nif __name__ == \"__main__\":\n  with launch_ipdb_on_exception():\n    <your properly indented code goes here>\nThis ensures that ipdb is launched when an error is reached.\n\n\n\n\n\n\nExample using ipdb\n\n\n\nSuppose that I want to write a function that will plot an xkcd comic in python.\nI start with\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd():\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic > img')[0].get('src')\n  # Format as a numpy array\n  image = np.array(Image.open(urllib.request.urlopen(imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: unknown url type: '//imgs.xkcd.com/comics/washing_machine_settings.png'\nplt.show()\n\n\n\n\nHere’s the final function\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd():\n  url = \"http://xkcd.com\"\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imlink = soup.select('#comic > img')[0].get('src')\n  # Format as a numpy array\n  image = np.array(Image.open(urllib.request.urlopen('https:' + imlink)))\n  \n  return image\n\nplt.imshow(get_xkcd())\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nEach xkcd has a corresponding ID number (ordered sequentially from 1 to 2722 at the time this was written). Modify the XKCD functions above to make use of the id parameter, so that you can pass in an ID number and get the relevant comic.\nUse interactive debugging tools to help you figure out what logic you need to add. You should not need to change the web scraping code - the only change should be to the URL.\nWhat things might you add to make this function “defensive programming” compatible?\n\n\n\n# get the most current xkcd or the specified number\nget_xkcd <- function(id = NULL) {\n  if (is.null(id)) {\n    # Have to get the location of the image ourselves\n    url <- \"http://xkcd.com\"\n  } else if (is.numeric(id)) {\n    url <- paste0(\"http://xkcd.com/\", id, \"/\")\n  } else {\n    # only allow numeric or null input\n    stop(\"To get current xkcd, pass in NULL, otherwise, pass in a valid comic number\")\n  }\n\n  page <- read_html(url)\n  # Find the comic\n  image <- xml_find_first(page, \"//div[@id='comic']/img\") %>%\n    # pull the address out of the tag\n    xml_attr(\"src\")\n  # Fix image address so that we can access the image\n  image <- substr(image, 3, nchar(image)) # cut the first 2 characters off\n\n  # make temp file\n  location <- tempfile(fileext = \"png\")\n  download.file(image, destfile = location, quiet = T)\n\n  # This checks to make sure we saved the file correctly\n  if (file.exists(location)) {\n    readPNG(source = location)\n  } else {\n    # Give a good informative error message\n    stop(paste(\"Something went wrong saving the image at \", image, \" to \", location))\n  }\n}\n\nget_xkcd(2259) %>%\n  as.raster() %>% \n  plot()\n\n\n\n\n\n\n\nfrom bs4 import BeautifulSoup\nimport urllib.request # work with html\nfrom PIL import Image # work with images\nimport numpy as np\n\n# importing pyplot and image from matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\n# get the most current xkcd\ndef get_xkcd(id=''):\n  image = 0 # Defining a placeholder\n  \n  if id == '':\n    # Have to get the location of the image ourselves\n    url = \"http://xkcd.com\"\n  elif id.isnumeric():\n    url = \"http://xkcd.com/\" + id + \"/\"\n  else:\n    # only allow numeric or null input\n    raise TypeError(\"To get current xkcd, pass in an empty string, otherwise, pass in a valid integer comic number\")\n  \n  # Print debugging left in for your amusement\n  # print(type(id))\n  \n  # Get the URL\n  html_file = urllib.request.urlopen(url)\n  page = html_file.read() \n  decode_page = page.decode(\"utf8\")\n  \n  # Read the page as HTML\n  soup = BeautifulSoup(decode_page, 'html')\n  \n  # Get the comic src from the img tag\n  imnode = soup.select('#comic > img')\n  \n  try:\n    imlink = imnode[0].get('src')\n  except:\n    raise Exception(\"No comic could be found with number \" + id + \" (url = \"+ url+ \" )\")\n  \n  try: \n    # Format as a numpy array\n    image = np.array(Image.open(urllib.request.urlopen('https:' + imlink)))\n    return image\n  except: \n    raise Exception(\"Reading the image failed. Check to make sure an image exists at \" + url)\n    return(None)\n\n\nres = get_xkcd('')\nplt.imshow(res)\nplt.show()\n\n\n\nres = get_xkcd('3000')\n\nError in py_call_impl(callable, dots$args, dots$keywords): urllib.error.HTTPError: HTTP Error 404: Not Found\n\nres = get_xkcd('abcd')\n\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: To get current xkcd, pass in an empty string, otherwise, pass in a valid integer comic number\n\n\n\n\n\n\n\n\n15.0.3 R debug()\n\nIn the traceback() Rstudio output, the other option is “rerun with debug”. In short, debug mode opens up a new interactive session inside the function evaluation environment. This lets you observe what’s going on in the function, pinpoint the error (and what causes it), and potentially fix the error, all in one neat workflow.\ndebug() is most useful when you’re working with code that you didn’t write yourself. So, if you can’t change the code in the function causing the error, debug() is the way to go. Otherwise, using browser() is generally easier. Essentially, debug() places a browser() statement at the first line of a function, but without having to actually alter the function’s source code.\n\n\n\n\n\n\ndebug() example\n\n\n\n\n\n\ndata(iris)\n\ntmp <- lm(Species ~ ., data = iris)\nsummary(tmp)\n## \n## Call:\n## lm(formula = Species ~ ., data = iris)\n## \n## Residuals:\n## Error in quantile.default(resid): (unordered) factors are not allowed\n\nWe get this weird warning, and then an error about factors when we use summary() to look at the coefficients.\n\ndebug(lm) # turn debugging on\n\n\ntmp <- lm(Species ~ ., data = iris)\nsummary(tmp)\n\nundebug(lm) # turn debugging off\n\n\n\nThe first thing I see when I run lm after turning on debug (screenshot)\n\n\n\n\nThe variables passed into the lm function are available as named and used in the function. In addition, we have some handy buttons in the console window that will let us ‘drive’ through the function\n\n\nAfter pressing “next” a few times, you can see that I’ve stepped through the first few lines of the lm function.\n\n\nStepping through the function. The arrow on the left side in the editor window shows which line of code we’re currently at.\n\n\nWe can see that once we’re at line 21, we get a warning about using type with a factor response, and that the warning occurs during a call to the model.response function. So, we’ve narrowed our problem down - we passed in a numeric variable as the response (y) variable, but it’s a factor, so our results aren’t going to mean much. We were using the function wrong.\nWe probably could have gotten there from reading the error message carefully, but this has allowed us to figure out exactly what happened, where it happened, and why it happened.\n\n\nI can hit “Stop” or type “Q” to exit the debug environment.\n\n\nBut, until I run undebug(lm), every call to lm will take me into the debug window.\n\n\n\nundebug(f) will remove the debug flag on the function f. debugonce(f) will only debug f the first time it is run.\n\n\n\n\n\n\nTry it out: debug in R\n\n\n\n\n\nProblem\nSolution\n\n\n\nlarger(x, y) is supposed to return the elementwise maximum of two vectors.\n\nlarger <- function(x, y) { \n  y.is.bigger <- y > x \n  x[y.is.bigger] <- y[y.is.bigger] \n  x\n} \n\nlarger(c(1, 5, 10), c(2, 4, 11))\n## [1]  2  5 11\n\n\nlarger(c(1, 5, 10), 6)\n## [1]  6 NA 10\n\nWhy is there an NA in the second example? It should be a 6. Figure out why this happens, then try to fix it.\n\n\nI’ll replicate “debug” in non-interactive mode by setting up an environment where x and y are defined\n\n\nx <- c(1, 5, 10)\ny <- 6\n\n# Inside of larger() with x = c(1, 5, 10), y = 6\n(y.is.bigger <- y > x ) # putting something in () prints it out\n## [1]  TRUE  TRUE FALSE\ny[y.is.bigger] # This isn't quite what we were going for, but it's what's causing the issue\n## [1]  6 NA\nx[y.is.bigger] # What gets replaced\n## [1] 1 5\n\n\n# Better option\nlarger <- function(x, y) { \n  y.is.bigger <- y > x \n  ifelse(y.is.bigger, y, x)\n} \n\n\n\n\n\n\n\n\n\n\n[1] \nWikipedia Contributors, “Defensive programming,” Wikipedia. Wikimedia Foundation, Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Defensive_programming&oldid=1084121123. [Accessed: May 31, 2022]\n\n\n[2] \nH. Wickham et al., “Welcome to the tidyverse,” Journal of Open Source Software, vol. 4, no. 43, p. 1686, 2019, doi: 10.21105/joss.01686. \n\n\n[3] \nK. Ushey, Renv: Project environments. 2022 [Online]. Available: https://CRAN.R-project.org/package=renv\n\n\n\n[4] \nH. Wickham and J. Bryan, R Packages: Organize, Test, Document, and Share Your Code, 1st ed. Sebastopol, CA: O’Reilly, 2015 [Online]. Available: https://r-pkgs.org/. [Accessed: Sep. 23, 2022]\n\n\n[5] \nT. Beuzen and T. Timbers, Python Packages, 1st edition. Boca Raton: Chapman; Hall/CRC, 2022 [Online]. Available: https://py-pkgs.org/\n\n\n\n[6] \nJ. Evans, “A debugging manifesto https://t.co/3eSOFQj1e1,” Twitter. Sep. 2022 [Online]. Available: https://twitter.com/b0rk/status/1570060516839641092. [Accessed: Sep. 21, 2022]\n\n\n[7] \nNasser_Junior, “User.fist_name https://t.co/lxrf3IFO4x,” Twitter. Aug. 2020 [Online]. Available: https://twitter.com/Nasser_Junior/status/1295805928315531264. [Accessed: Sep. 21, 2022]\n\n\n[8] \nH. Wickham, Advanced R, 2nd ed. CRC Press, 2019 [Online]. Available: http://adv-r.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[9] \nJ. Evans, “Debugging strategy: Reread the error message https://t.co/2BZHhPg04h,” Twitter. Sep. 2022 [Online]. Available: https://twitter.com/b0rk/status/1570463473011920897. [Accessed: Sep. 21, 2022]\n\n\n[10] \nJ. Evans, “Debugging strategy: Shorten your feedback loop https://t.co/1cByDlafsK,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1549164800978059264. [Accessed: Sep. 21, 2022]\n\n\n[11] \nJ. Evans, “Debugging strategy: Write a tiny program https://t.co/Kajr5ZyeIp,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1547247776001654786. [Accessed: Sep. 21, 2022]\n\n\n[12] \nJ. Evans, “Debugging strategy: Change working code into broken code https://t.co/1T5uNDDFs0,” Twitter. Jul. 2022 [Online]. Available: https://twitter.com/b0rk/status/1545099244238946304. [Accessed: Sep. 21, 2022]\n\n\n[13] \nJ. Evans, “Debugging strategy: Come up with one question https://t.co/2Lytzl4laQ,” Twitter. Aug. 2022 [Online]. Available: https://twitter.com/b0rk/status/1554120424602193921. [Accessed: Sep. 21, 2022]\n\n\n[14] \nJ. Evans, “Debugging strategy: Write a unit test https://t.co/mC01DBNyM3,” Twitter. Aug. 2022 [Online]. Available: https://twitter.com/b0rk/status/1561718747504803842. [Accessed: Sep. 21, 2022]\n\n\n[15] \nT. Monteiro, “Improve how you code: Understanding rubber duck debugging. Duckly blog,” Oct. 31, 2019. [Online]. Available: https://duckly.com/blog/improve-how-to-code-with-rubber-duck-debugging/. [Accessed: Jan. 11, 2023]\n\n\n[16] \nS. Grimes, “This 500-Year-Old Piece of Advice Can Help You Solve Your Modern Problems,” Forge. Dec. 2019 [Online]. Available: https://forge.medium.com/the-500-year-old-piece-of-advice-that-will-change-your-life-1e580f115731. [Accessed: Sep. 21, 2022]"
  },
  {
    "objectID": "part-gen-prog/06-debugging.html#indoc--",
    "href": "part-gen-prog/06-debugging.html#indoc--",
    "title": "13  Debugging",
    "section": "\n13.6 indoc <- ’",
    "text": "13.6 indoc <- ’\ntitle: “Basic SASmarkdown Doc” author: “Doug Hemken” output: html_document —"
  },
  {
    "objectID": "part-wrangling/00-wrangling.html#sec-wrangling-refs",
    "href": "part-wrangling/00-wrangling.html#sec-wrangling-refs",
    "title": "Part III: Data Wrangling",
    "section": "References",
    "text": "References\n\n\n\n\n[1] H. Wickham, “Tidy data,” The Journal of Statistical Software, vol. 59, 2014 [Online]. Available: http://www.jstatsoft.org/v59/i10/"
  },
  {
    "objectID": "part-wrangling/01-data-input.html#fa-bullseye-objectives",
    "href": "part-wrangling/01-data-input.html#fa-bullseye-objectives",
    "title": "14  Data Input",
    "section": "\n14.1  Objectives",
    "text": "14.1  Objectives\n\nRead in data from common formats into R or Python\nIdentify delimiters, headers, and other essential components of files\n\n\n\n\n\n\n\nCheatsheets!\n\n\n\nThese may be worth printing off as you work through this module.\nR - tidyverse Python"
  },
  {
    "objectID": "part-wrangling/01-data-input.html#overview-data-formats",
    "href": "part-wrangling/01-data-input.html#overview-data-formats",
    "title": "14  Data Input",
    "section": "\n14.2 Overview: Data Formats",
    "text": "14.2 Overview: Data Formats\nIn order to use statistical software to do anything interesting, we need to be able to get data into the program so that we can work with it effectively. For the moment, we’ll focus on tabular data - data that is stored in a rectangular shape, with rows indicating observations and columns that show variables. This type of data can be stored on the computer in multiple ways:\n\nas raw text, usually in a file that ends with .txt, .tsv, .csv, .dat, or sometimes, there will be no file extension at all. These types of files are human-readable. If part of a text file gets corrupted, the rest of the file may be recoverable.\n\nas a binary file. Binary files are compressed files that are readable by computers but not by humans. They generally take less space to store on disk (but the same amount of space when read into computer memory). If part of a binary file is corrupted, the entire file is usually affected.\n\nR, SAS, Stata, SPSS, and Minitab all have their own formats for storing binary data. Packages such as foreign in R will let you read data from other programs, and packages such as haven in R will let you write data into binary formats used by other programs.\n\n[1] describes why binary file formats exist, and why they’re not necessarily optimal.\n\n\nin a spreadsheet. Spreadsheets, such as those created by MS Excel, Google Sheets, or LibreOffice Calc, are not binary formats, but they’re also not raw text files either. They’re a hybrid - a special type of markup that is specific to the filetype and the program it’s designed to work with. Practically, they may function like a poorly laid-out database, a text file, or a total nightmare, depending on who designed the spreadsheet.\n\n\n\n\n\n\n\nNote\n\n\n\nThere is a collection of spreadsheet horror stories here and a series of even more horrifying tweets here.\nAlso, there’s this amazing comic:\n\n\n\n\nin a database. Databases are typically composed of a set of one or more tables, with information that may be related across tables. Data stored in a database may be easier to access, and may not require that the entire data set be stored in computer memory at the same time, but you may have to join several tables together to get the full set of data you want to work with.\n\nThere are, of course, many other non-tabular data formats – some open and easy to work with, some inpenetrable. A few which you may come across:\n\nWeb data structures: XML (eXtensible markup language), JSON (JavaScript Object Notation), YAML. These structures have their own formats and field delimiters, but more importantly, are not necessarily easily converted to tabular structures. They are, however, useful for handling nested objects, such as trees. When read into R or SAS, these file formats are usually treated as lists, and may be restructured afterwards into a format useful for statistical analysis. See Chapter 22 for some tools to work with these files.\nSpatial files: Shapefiles are the most common version of spatial files, though there are a seemingly infinite number of different formats, and new formats pop up at the most inconvenient times. Spatial files often include structured encodings of geographic information plus corresponding tabular format data that goes with the geographic information. Chapter 23 covers some of the tools available for working with spatial data.\n\nTo be minimally functional in R and Python, it’s important to know how to read in text files (CSV, tab-delimited, etc.). It can be helpful to also know how to read in XLSX files. We will briefly discuss binary files and databases, but it is less critical to remember how to read these in without consulting an online reference."
  },
  {
    "objectID": "part-wrangling/01-data-input.html#text-files",
    "href": "part-wrangling/01-data-input.html#text-files",
    "title": "14  Data Input",
    "section": "\n14.3 Text Files",
    "text": "14.3 Text Files\nThere are several different variants of text data which are relatively common, but for the most part, text data files can be broken down into fixed-width and delimited formats. What’s the difference, you say?\n\n14.3.1 Fixed-width files\nCol1    Col2    Col3\n 3.4     4.2     5.4\n27.3    -2.4    15.9\nIn a fixed-width text file, the position of the data indicates which field (variable/column) it belongs to. These files are fairly common outputs from older FORTRAN-based programs, but may be found elsewhere as well - if you have a very large amount of data, a fixed-width format may be more efficient to read, because you can select only the portions of the file which matter for a particular analysis (and so you don’t have to read the whole thing into memory).\n\n\n\n\n\n\nFixed Width File IO\n\n\n\n\n\nBase R\nreadr\nPython\n\n\n\nIn base R (no extra packages), you can read fixed-width files in using read.fwf, but you must specify the column breaks yourself, which can be painful.\n\n## url <- \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202206070000/mdf/TEXT/\"\ndata <- read.fwf(url, \n         skip = 3, # Skip the first 2 lines (useless) + header line\n         widths = c(5, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 8, 9, 6, 7, 7, 7, 7, 7, 7, \n7, 8, 8, 8)) # There is a row with the column names specified\n\ndata[1:6,] # first 6 rows\n##      V1  V2 V3 V4   V5  V6  V7  V8   V9 V10 V11   V12    V13 V14  V15 V16  V17\n## 1  ACME 110  0 60 29.9 4.4 4.3 111  9.0 0.8 6.4  0.00 959.37 267 29.6 3.6 25.4\n## 2  ADAX   1  0 69 29.3 1.7 1.6  98 24.9 0.6 3.4  0.00 971.26 251 29.0 0.6 24.6\n## 3  ALTU   2  0 52 31.7 5.5 5.4  89  7.6 1.0 7.8  0.00 956.12 287 31.3 3.5 26.5\n## 4  ALV2 116  0 57 30.1 2.5 2.4 108 10.3 0.5 3.6 55.63 954.01 266 30.1 1.7 23.3\n## 5  ANT2 135  0 75 29.1 1.1 1.1  44 21.1 0.3 2.0  0.00 985.35 121 28.9 0.5 25.9\n## 6  APAC 111  0 58 29.9 5.1 5.1 107  8.5 0.7 6.6  0.00 954.47 224 29.7 3.6 26.2\n##    V18  V19  V20    V21  V22  V23     V24\n## 1 29.4 27.4 22.5   20.6 1.55 1.48    1.40\n## 2 28.7 25.6 24.3 -998.0 1.46 1.52 -998.00\n## 3 32.1 27.6 24.0 -998.0 1.72 1.50 -998.00\n## 4 30.3 26.2 21.1 -998.0 1.49 1.40 -998.00\n## 5 29.0 26.3 22.8   21.4 1.51 1.39    1.41\n## 6 29.1 26.6 24.3   20.5 1.59 1.47    1.40\n\nYou can count all of those spaces by hand (not shown), you can use a different function, or you can write code to do it for you.\n\n\n\n\n\n\nCode for counting field width\n\n\n\n\n\n\n\n# I like to cheat a bit....\n# Read the first few lines in\ntmp <- readLines(url, n = 20)[-c(1:2)]\n\n# split each line into a series of single characters\ntmp_chars <- strsplit(tmp, '') \n\n# Bind the lines together into a character matrix\n# do.call applies a function to an entire list - so instead of doing 18 rbinds, \n# one command will put all 18 rows together\ntmp_chars <- do.call(\"rbind\", tmp_chars) # (it's ok if you don't get this line)\n\n# Make into a logical matrix where T = space, F = not space\ntmp_chars_space <- tmp_chars == \" \"\n\n# Add up the number of rows where there is a non-space character\n# space columns would have 0s/FALSE\ntmp_space <- colSums(!tmp_chars_space)\n\n# We need a nonzero column followed by a zero column\nbreaks <- which(tmp_space != 0 & c(tmp_space[-1], 0) == 0)\n\n# Then, we need to get the widths between the columns\nwidths <- diff(c(0, breaks))\n\n# Now we're ready to go\nmesodata <- read.fwf(url, skip = 3, widths = widths, header = F)\n# read header separately - if you use header = T, it errors for some reason.\n# It's easier just to work around the error than to fix it :)\nmesodata_names <- read.fwf(url, skip = 2, n = 1, widths = widths, header = F, \n                           stringsAsFactors = F)\nnames(mesodata) <- as.character(mesodata_names)\n\nmesodata[1:6,] # first 6 rows\n##    STID   STNM   TIME    RELH    TAIR    WSPD    WVEC   WDIR    WDSD    WSSD\n## 1  ACME    110      0      60    29.9     4.4     4.3    111     9.0     0.8\n## 2  ADAX      1      0      69    29.3     1.7     1.6     98    24.9     0.6\n## 3  ALTU      2      0      52    31.7     5.5     5.4     89     7.6     1.0\n## 4  ALV2    116      0      57    30.1     2.5     2.4    108    10.3     0.5\n## 5  ANT2    135      0      75    29.1     1.1     1.1     44    21.1     0.3\n## 6  APAC    111      0      58    29.9     5.1     5.1    107     8.5     0.7\n##      WMAX     RAIN      PRES   SRAD    TA9M    WS2M    TS10    TB10    TS05\n## 1     6.4     0.00    959.37    267    29.6     3.6    25.4    29.4    27.4\n## 2     3.4     0.00    971.26    251    29.0     0.6    24.6    28.7    25.6\n## 3     7.8     0.00    956.12    287    31.3     3.5    26.5    32.1    27.6\n## 4     3.6    55.63    954.01    266    30.1     1.7    23.3    30.3    26.2\n## 5     2.0     0.00    985.35    121    28.9     0.5    25.9    29.0    26.3\n## 6     6.6     0.00    954.47    224    29.7     3.6    26.2    29.1    26.6\n##      TS25    TS60     TR05     TR25     TR60\n## 1    22.5    20.6     1.55     1.48     1.40\n## 2    24.3  -998.0     1.46     1.52  -998.00\n## 3    24.0  -998.0     1.72     1.50  -998.00\n## 4    21.1  -998.0     1.49     1.40  -998.00\n## 5    22.8    21.4     1.51     1.39     1.41\n## 6    24.3    20.5     1.59     1.47     1.40\n\n\n\n\nYou can also write fixed-width files if you really want to:\n\nif (!\"gdata\" %in% installed.packages()) install.packages(\"gdata\")\nlibrary(gdata)\nwrite.fwf(mtcars, file = tempfile())\n\n\n\nThe readr package creates data-frame like objects called tibbles (a souped-up data frame), but it is much friendlier to use.\n\nlibrary(readr) # Better data importing in R\n\nread_table(url, skip = 2) # Gosh, that was much easier!\n## # A tibble: 120 × 24\n##    STID   STNM  TIME  RELH   TAIR  WSPD  WVEC  WDIR  WDSD  WSSD  WMAX  RAIN\n##    <chr> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1 ACME    110     0    60   29.9   4.4   4.3   111   9     0.8   6.4   0  \n##  2 ADAX      1     0    69   29.3   1.7   1.6    98  24.9   0.6   3.4   0  \n##  3 ALTU      2     0    52   31.7   5.5   5.4    89   7.6   1     7.8   0  \n##  4 ALV2    116     0    57   30.1   2.5   2.4   108  10.3   0.5   3.6  55.6\n##  5 ANT2    135     0    75   29.1   1.1   1.1    44  21.1   0.3   2     0  \n##  6 APAC    111     0    58   29.9   5.1   5.1   107   8.5   0.7   6.6   0  \n##  7 ARD2    126     0    61   31.2   3.3   3.2   109   9.1   0.6   4.3   0  \n##  8 ARNE      6     0    49   30.4   4.5   4.4   111  11.1   0.9   6.4   0  \n##  9 BEAV      8     0    42   30.5   6.1   6     127   8.7   0.9   7.9   0  \n## 10 BESS      9     0    53 -999     5.3   5.2   115   8.6   0.6   7     0  \n## # … with 110 more rows, and 12 more variables: PRES <dbl>, SRAD <dbl>,\n## #   TA9M <dbl>, WS2M <dbl>, TS10 <dbl>, TB10 <dbl>, TS05 <dbl>, TS25 <dbl>,\n## #   TS60 <dbl>, TR05 <dbl>, TR25 <dbl>, TR60 <dbl>\n\n\n\nBy default, pandas’ read_fwf will guess at the format of your fixed-width file.\n\nimport pandas as pd\nurl = \"https://www.mesonet.org/index.php/dataMdfMts/dataController/getFile/202006070000/mdf/TEXT/\"\ndata = pd.read_fwf(url, skiprows = 2) # Skip the first 2 lines (useless)\n\n\n\n\n\n\n\n14.3.2 Delimited Text Files\nDelimited text files are files where fields are separated by a specific character, such as space, comma, semicolon, tabs, etc. Often, delimited text files will have the column names as the first row in the file.\n\n\n\n\n\n\nComma Delimited Files\n\n\n\n\n\nBase R\nreadr\nPython\n\n\n\n\nurl <- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\n\npokemon_info <- read.csv(url, header = T, stringsAsFactors = F)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n\n  pokedex_no\n1          1\n2          2\n3          3\n4          3\n5          4\n6          5\n                                                               img_link\n1     https://img.pokemondb.net/sprites/sword-shield/icon/bulbasaur.png\n2       https://img.pokemondb.net/sprites/sword-shield/icon/ivysaur.png\n3      https://img.pokemondb.net/sprites/sword-shield/icon/venusaur.png\n4 https://img.pokemondb.net/sprites/sword-shield/icon/venusaur-mega.png\n5    https://img.pokemondb.net/sprites/sword-shield/icon/charmander.png\n6    https://img.pokemondb.net/sprites/sword-shield/icon/charmeleon.png\n        name       variant         type total\n1  Bulbasaur          <NA> Grass,Poison   318\n2    Ivysaur          <NA> Grass,Poison   405\n3   Venusaur          <NA> Grass,Poison   525\n4   Venusaur Mega Venusaur Grass,Poison   625\n5 Charmander          <NA>         Fire   309\n6 Charmeleon          <NA>         Fire   405\n\n\n\n\nThere is a family of read_xxx functions in readr to read files delimited with commas (read_csv), tabs (read_tsv), or generic delimited files (read_delim).\nThe most common delimited text format is CSV: comma-separated value.\n\nlibrary(readr)\nurl <- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\npokemon_info <- read_csv(url)\npokemon_info[1:6, 1:6] # Show only the first 6 lines & cols\n\n# A tibble: 6 × 6\n  pokedex_no img_link                                  name  variant type  total\n       <dbl> <chr>                                     <chr> <chr>   <chr> <dbl>\n1          1 https://img.pokemondb.net/sprites/sword-… Bulb… <NA>    Gras…   318\n2          2 https://img.pokemondb.net/sprites/sword-… Ivys… <NA>    Gras…   405\n3          3 https://img.pokemondb.net/sprites/sword-… Venu… <NA>    Gras…   525\n4          3 https://img.pokemondb.net/sprites/sword-… Venu… Mega V… Gras…   625\n5          4 https://img.pokemondb.net/sprites/sword-… Char… <NA>    Fire    309\n6          5 https://img.pokemondb.net/sprites/sword-… Char… <NA>    Fire    405\n\n\n\n\nThere is a family of read_xxx functions in pandas including functions to read files delimited with commas (read_csv) as well as generic delimited files (read_table).\n\nimport pandas as pd\n\nurl <- \"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\"\n\nError in py_call_impl(callable, dots$args, dots$keywords): TypeError: bad operand type for unary -: 'str'\n\npokemon_info = pd.read_csv(url)\npokemon_info.iloc[:,2:51]\n\nEmpty DataFrame\nColumns: []\nIndex: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n\n[123 rows x 0 columns]\n\n\n\n\n\n\n\nSometimes, data is available in files that use other characters as delimiters. This can happen when commas are an important part of the data stored in the file, but can also just be a choice made by the person generating the file. Either way, we can’t let it keep us from accessing the data.\n\n\n\n\n\n\nOther Character Delimited Files\n\n\n\n\n\nBase R\nreadr\nPython\n\n\n\n\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = '../data/NE_Features.zip')\n# Unzip to `data/` folder\nunzip('../data/NE_Features.zip', exdir = '../data/')\n# List files matching the file type and pick the first one\nfname <- list.files(\"../data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\n# a file delimited with |\nnebraska_locations <- read.delim(fname, sep = \"|\", header = T)\nnebraska_locations[1:6, 1:6]\n##   FEATURE_ID     FEATURE_NAME FEATURE_CLASS STATE_ALPHA STATE_NUMERIC\n## 1     171013      Peetz Table          Area          CO             8\n## 2     171029      Sidney Draw        Valley          NE            31\n## 3     182687   Highline Canal         Canal          CO             8\n## 4     182688 Cottonwood Creek        Stream          CO             8\n## 5     182689        Sand Draw        Valley          CO             8\n## 6     182690    Sedgwick Draw        Valley          CO             8\n##   COUNTY_NAME\n## 1       Logan\n## 2    Cheyenne\n## 3    Sedgwick\n## 4    Sedgwick\n## 5    Sedgwick\n## 6    Sedgwick\n\n\n\nThere is a family of read_xxx functions in readr to read files delimited with commas (read_csv), tabs (read_tsv), or generic delimited files (read_delim).\n\n# Download from web\ndownload.file(\"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\", destfile = '../data/NE_Features.zip')\n# Unzip to `data/` folder\nunzip('../data/NE_Features.zip', exdir = '../data/')\n# List files matching the file type and pick the first one\nfname <- list.files(\"../data/\", 'NE_Features_20', full.names = T)[1]\n\n# see that the file is delimited with |\nreadLines(fname, n = 5)\n## [1] \"FEATURE_ID|FEATURE_NAME|FEATURE_CLASS|STATE_ALPHA|STATE_NUMERIC|COUNTY_NAME|COUNTY_NUMERIC|PRIMARY_LAT_DMS|PRIM_LONG_DMS|PRIM_LAT_DEC|PRIM_LONG_DEC|SOURCE_LAT_DMS|SOURCE_LONG_DMS|SOURCE_LAT_DEC|SOURCE_LONG_DEC|ELEV_IN_M|ELEV_IN_FT|MAP_NAME|DATE_CREATED|DATE_EDITED\"\n## [2] \"171013|Peetz Table|Area|CO|08|Logan|075|405840N|1030332W|40.9777645|-103.0588116|||||1341|4400|Peetz|10/13/1978|\"                                                                                                                                                        \n## [3] \"171029|Sidney Draw|Valley|NE|31|Cheyenne|033|410816N|1030116W|41.1377213|-103.021044|405215N|1040353W|40.8709614|-104.0646558|1255|4117|Brownson|10/13/1978|03/08/2018\"                                                                                                  \n## [4] \"182687|Highline Canal|Canal|CO|08|Sedgwick|115|405810N|1023137W|40.9694351|-102.5268556|||||1119|3671|Sedgwick|10/13/1978|\"                                                                                                                                              \n## [5] \"182688|Cottonwood Creek|Stream|CO|08|Sedgwick|115|405511N|1023355W|40.9197132|-102.5651893|405850N|1030107W|40.9805426|-103.0185329|1095|3592|Sedgwick|10/13/1978|10/23/2009\"\n\nnebraska_locations <- read_delim(fname, delim = \"|\")\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 × 6\n##   FEATURE_ID FEATURE_NAME     FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_N…¹\n##        <dbl> <chr>            <chr>         <chr>       <chr>         <chr>     \n## 1     171013 Peetz Table      Area          CO          08            Logan     \n## 2     171029 Sidney Draw      Valley        NE          31            Cheyenne  \n## 3     182687 Highline Canal   Canal         CO          08            Sedgwick  \n## 4     182688 Cottonwood Creek Stream        CO          08            Sedgwick  \n## 5     182689 Sand Draw        Valley        CO          08            Sedgwick  \n## 6     182690 Sedgwick Draw    Valley        CO          08            Sedgwick  \n## # … with abbreviated variable name ¹​COUNTY_NAME\n\nWe can actually read in the file without unzipping it, so long as we download it first - readr does not support reading remote zipped files, but it does support reading zipped files locally. If we know ahead of time what our delimiter is, this is the best choice as it reduces the amount of file clutter we have in our working directory.\n\nnebraska_locations <- read_delim(\"../data/NE_Features.zip\", delim = \"|\")\nnebraska_locations[1:6, 1:6]\n## # A tibble: 6 × 6\n##   FEATURE_ID FEATURE_NAME     FEATURE_CLASS STATE_ALPHA STATE_NUMERIC COUNTY_N…¹\n##        <dbl> <chr>            <chr>         <chr>       <chr>         <chr>     \n## 1     171013 Peetz Table      Area          CO          08            Logan     \n## 2     171029 Sidney Draw      Valley        NE          31            Cheyenne  \n## 3     182687 Highline Canal   Canal         CO          08            Sedgwick  \n## 4     182688 Cottonwood Creek Stream        CO          08            Sedgwick  \n## 5     182689 Sand Draw        Valley        CO          08            Sedgwick  \n## 6     182690 Sedgwick Draw    Valley        CO          08            Sedgwick  \n## # … with abbreviated variable name ¹​COUNTY_NAME\n\n\n\nThere is a family of read_xxx functions in pandas including functions to read files delimited with commas (read_csv) as well as generic delimited files (read_table).\nPandas can access zipped data files and unzip them while reading the data in, so we don’t have to download the file and unzip it first.\n\n# a file delimited with |\n\nurl = \"https://geonames.usgs.gov/docs/stategaz/NE_Features.zip\"\nnebraska_locations = pd.read_table(url, delimiter = \"|\")\nnebraska_locations\n##        FEATURE_ID  ... DATE_EDITED\n## 0          171013  ...         NaN\n## 1          171029  ...  03/08/2018\n## 2          182687  ...         NaN\n## 3          182688  ...  10/23/2009\n## 4          182689  ...  12/20/2017\n## ...           ...  ...         ...\n## 31473     2806916  ...  08/12/2021\n## 31474     2806917  ...  08/12/2021\n## 31475     2806918  ...  08/12/2021\n## 31476     2806919  ...         NaN\n## 31477     2806920  ...  08/12/2021\n## \n## [31478 rows x 20 columns]\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Reading CSV files\n\n\n\nRebrickable.com contains tables of almost any information imaginable concerning Lego sets, conveniently available at their download page. Because these data sets are comparatively large, they are available as compressed CSV files - that is, the .gz extension is a gzip compression applied to the CSV.\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe readr package and pandas can handle .csv.gz files with no problems. Try reading in the data using the appropriate function from that package. Can you save the data as an uncompressed csv?\n\n\n\nlibrary(readr)\nlegosets <- read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nwrite_csv(legosets, \"../data/lego_sets.csv\")\n\n\n\n\nimport pandas as pd\n\nlegosets = pd.read_csv(\"https://cdn.rebrickable.com/media/downloads/sets.csv.gz\")\nlegosets.to_csv(\"../data/lego_sets_py.csv\")"
  },
  {
    "objectID": "part-wrangling/01-data-input.html#spreadsheets",
    "href": "part-wrangling/01-data-input.html#spreadsheets",
    "title": "14  Data Input",
    "section": "\n14.4 Spreadsheets",
    "text": "14.4 Spreadsheets\n\n\n\n\n\n\nSpreadsheet IO\n\n\n\nThis example uses from NYC SLice. The author maintains a google sheet of the slices he has photographed, which we can download as an excel sheet and import.\n\n\nR readxl\nPython\n\n\n\nIn R, the easiest way to read Excel data in is to use the readxl package. There are many other packages with different features, however - I have used openxlsx in the past to format spreadsheets to send to clients, for instance. By far and away you are more likely to have problems with the arcane format of the Excel spreadsheet than with the package used to read the data in. It is usually helpful to open the spreadsheet up in a graphical program first to make sure the formatting is as you expected it to be.\n\nif (!\"readxl\" %in% installed.packages()) install.packages(\"readxl\")\nlibrary(readxl)\n\nurl <- \"https://docs.google.com/spreadsheets/d/1EY3oi9ttxybG0A0Obtwey6BFu7QLqdHe02JApijgztg/export?format=xlsx\"\n# Only download the data if it doesn't exist in the data folder\nif (!file.exists(\"../data/nyc_slice.xlsx\")) {\n  download.file(url, destfile = \"../data/nyc_slice.xlsx\", mode = \"wb\")\n}\n\n# Read in the downloaded data\npizza_data <- read_xlsx(\"../data/nyc_slice.xlsx\", sheet = 1)\npizza_data[1:10, 1:6]\n## # A tibble: 10 × 6\n##    `Link to IG Post`                        Name   locat…¹ locat…² Date  Date …³\n##    <chr>                                    <chr>  <chr>   <chr>   <chr> <chr>  \n##  1 https://www.instagram.com/p/CjszJ-fOP5o/ Angel… 40.623… -73.93… 2022… Oct 14…\n##  2 https://www.instagram.com/p/CjdcPNAufPj/ Ozone… 40.680… -73.84… 2022… Oct 8t…\n##  3 https://www.instagram.com/p/CjQdNsaOZlY/ Pino … 40.600… -73.99… 2022… Oct 3r…\n##  4 https://www.instagram.com/p/Ci5XblnOnMA/ La Ro… 40.713… -73.82… 2022… Sep 24…\n##  5 https://www.instagram.com/p/CiiLAtkON_1/ Rony'… 40.748… -73.99… 2022… Sep 15…\n##  6 https://www.instagram.com/p/CiS-44nucNE/ John … 40.854… -73.86… 2022… Sep 9t…\n##  7 https://www.instagram.com/p/CiSmQnjutQy/ Prego… 40.863… -73.85… 2022… Sep 9t…\n##  8 https://www.instagram.com/p/CiIO6oFuxpP/ N & D… 40.600… -73.94… 2022… Sep 5t…\n##  9 https://www.instagram.com/p/ChaZUsxuFsr/ Peppi… 40.903… -73.85… 2022… Aug 18…\n## 10 https://www.instagram.com/p/ChNd9wqOqGD/ Rocco… 40.867… -73.88… 2022… Aug 13…\n## # … with abbreviated variable names ¹​location_lat, ²​location_lng,\n## #   ³​`Date Expanded (times in EST)`\n\n\n\n\nimport pandas as pd\n\npizza_data = pd.read_excel(\"../data/nyc_slice.xlsx\")\npizza_data\n##                               Link to IG Post  ... Notes\n## 0    https://www.instagram.com/p/CjszJ-fOP5o/  ...   NaN\n## 1    https://www.instagram.com/p/CjdcPNAufPj/  ...   NaN\n## 2    https://www.instagram.com/p/CjQdNsaOZlY/  ...   NaN\n## 3    https://www.instagram.com/p/Ci5XblnOnMA/  ...   NaN\n## 4    https://www.instagram.com/p/CiiLAtkON_1/  ...   NaN\n## ..                                        ...  ...   ...\n## 459   https://www.instagram.com/p/rqCdE_hp3N/  ...   NaN\n## 460   https://www.instagram.com/p/rnT-kRhp4h/  ...   NaN\n## 461   https://www.instagram.com/p/rh17_NBp7a/  ...   NaN\n## 462   https://www.instagram.com/p/rfnZKmBp3B/  ...   NaN\n## 463   https://www.instagram.com/p/rfGr-RBp4U/  ...   NaN\n## \n## [464 rows x 11 columns]\n\n\n\n\n\n\nIn general, it is better to avoid working in Excel, as it is not easy to reproduce the results (and Excel is horrible about dates and times, among other issues). Saving your data in more reproducible formats will make writing reproducible code much easier.\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nThe Nebraska Department of Motor Vehicles publishes a database of vehicle registrations by type of license plate. Link\nRead in the data using your language(s) of choice. Be sure to look at the structure of the excel file, so that you can read the data in properly!\n\n\n\nurl <- \"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\"\ndownload.file(url, destfile = \"../data/NE_Licensed_Drivers_by_Type_2021.xls\", mode = \"wb\")\nlibrary(readxl)\nne_plates <- read_xls(path = \"../data/NE_Licensed_Drivers_by_Type_2021.xls\", skip = 2)\nne_plates[1:10,1:6]\n## # A tibble: 10 × 6\n##    Age   `\\nOperator's \\nLicense -\\nClass O` Operator's\\…¹ Motor…² Comme…³ ...6 \n##    <chr>                               <dbl>         <dbl>   <dbl> <chr>   <chr>\n##  1 <NA>                                   NA            NA      NA CDL A   CDL B\n##  2 14                                      0             0       0 0       0    \n##  3 15                                      0             0       0 0       0    \n##  4 16                                      0             0       0 0       0    \n##  5 17                                    961            33       0 0       0    \n##  6 18                                  18903           174       0 25      3    \n##  7 19                                  22159           251       0 97      32   \n##  8 20                                  22844           326       1 144     47   \n##  9 21                                  21589           428       0 233     57   \n## 10 22                                  22478           588       0 292     81   \n## # … with abbreviated variable names\n## #   ¹​`Operator's\\nLicense - \\nClass O/\\nMotorcycle\\nClass M`,\n## #   ²​`Motor-\\ncycle\\nLicense /\\nClass M`, ³​`Commercial Driver's License`\n\n\n\nYou may need to install xlrd via pip for this code to work.\n\nimport pandas as pd\n\nne_plates = pd.read_excel(\"https://dmv.nebraska.gov/sites/dmv.nebraska.gov/files/doc/data/ld-totals/NE_Licensed_Drivers_by_Type_2021.xls\", skiprows = 2)\n## Error in py_call_impl(callable, dots$args, dots$keywords): ImportError: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.\nne_plates\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'ne_plates' is not defined\n\n\n\n\n\n\n\n14.4.1 Google Sheets\nOf course, some spreadsheets are available online via Google sheets. There are specific R and python packages to interface with Google sheets, and these can do more than just read data in - they can create, format, and otherwise manipulate Google sheets programmatically. We’re not going to get into the power of these packages just now, but it’s worth a look if you’re working with collaborators who use Google sheets.\n\n\n\n\n\n\nAdvanced\n\n\n\nThis section is provided for reference, but the details of API authentication are a bit too complicated to require of anyone who is just learning to program. Feel free to skip it and come back later if you need it.\nThe first two tabs below show authentication-free options for publicly available spreadsheets. For anything that is privately available, you will have to use API authentication via GSpread or googlesheets4 in python and R respectively.\n\n\n\n\n\n\n\n\nReading Google Sheets\n\n\n\nLet’s demonstrate reading in data from google sheets in R and python using the Data Is Plural archive. ::: panel-tabset\nPython\nOne simple hack-ish way to read google sheets in Python (so long as the sheet is publicly available) is to modify the sheet url to export the data to CSV and then just read that into pandas as usual. This method is described in [2].\n\nimport pandas as pd\n\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n\ndata_is_plural = pd.read_csv(url)\n\nThis method would likely work just as well in R and would not require the googlesheets4 package.\nR\nThis method is described in [2] for Python, but I have adapted the code to use in R.\n\nlibrary(readr)\nsheet_id = \"1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\"\nsheet_name = \"Items\"\nurl = sprintf(\"https://docs.google.com/spreadsheets/d/%s/gviz/tq?tqx=out:csv&sheet=%s\", sheet_id, sheet_name)\n\ndata_is_plural = read_csv(url)\n\nR: googlesheets4\n\nThis code is set not to run when the textbook is compiled because it requires some interactive authentication.\n\n\nlibrary(googlesheets4)\ngs4_auth(scopes = \"https://www.googleapis.com/auth/drive.readonly\") # Read-only permissions\ndata_is_plural <- read_sheet(\"https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0\")\n\nPython: GSpread\n\nThese instructions are derived from [3]. We will have to install the GSpread package: type pip install gspread into the terminal.\nThen, you will need to obtain a client token JSON file following these instructions.\n\nimport gspread as gs\nimport pandas as pd\n\nI’ve stopped here because I can’t get the authentication working, but the method seems solid if you’re willing to fiddle around with it. \n\n\n\n:::"
  },
  {
    "objectID": "part-wrangling/01-data-input.html#binary-files",
    "href": "part-wrangling/01-data-input.html#binary-files",
    "title": "14  Data Input",
    "section": "\n14.5 Binary Files",
    "text": "14.5 Binary Files\nR has binary file formats which store data in a more compact form. It is relatively common for government websites, in particular, to provide SAS data in binary form. Python, as a more general computing language, has many different ways to interact with binary data files, as each programmer and application might want to save their data in binary form in a different way. As a result, there is not a general-purpose binary data format for Python data. If you are interested in reading binary data in Python, see [4].\n\n\n\n\n\n\nBinary File IO\n\n\n\n\n\nR formats in R\nR formats in Python\nSAS format in R\nSAS format in Python\n\n\n\n.Rdata is perhaps the most common R binary data format, and can store several objects (along with their names) in the same file.\n\nlegos <- read_csv(\"../data/lego_sets.csv\")\nmy_var <- \"This variable contains a string\"\nsave(legos, my_var, file = \"../data/R_binary.Rdata\")\n\nIf we look at the file sizes of lego_sets.csv (619 KB) and R_binary.Rdata(227.8 KB), the size difference between binary and flat file formats is obvious.\nWe can load the R binary file back in using the load() function.\n\nrm(legos, my_var) # clear the files out\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"legosets\"           \"mesodata\"          \n##  [7] \"mesodata_names\"     \"ne_plates\"          \"nebraska_locations\"\n## [10] \"pizza_data\"         \"pokemon_info\"       \"sheet_id\"          \n## [13] \"sheet_name\"         \"tmp\"                \"tmp_chars\"         \n## [16] \"tmp_chars_space\"    \"tmp_space\"          \"url\"               \n## [19] \"widths\"\n\nload(\"../data/R_binary.Rdata\")\n\nls() # all objects in the working environment\n##  [1] \"breaks\"             \"data\"               \"data_is_plural\"    \n##  [4] \"fname\"              \"legos\"              \"legosets\"          \n##  [7] \"mesodata\"           \"mesodata_names\"     \"my_var\"            \n## [10] \"ne_plates\"          \"nebraska_locations\" \"pizza_data\"        \n## [13] \"pokemon_info\"       \"sheet_id\"           \"sheet_name\"        \n## [16] \"tmp\"                \"tmp_chars\"          \"tmp_chars_space\"   \n## [19] \"tmp_space\"          \"url\"                \"widths\"\n\nAnother (less common) binary format used in R is the RDS format. Unlike Rdata, the RDS format does not save the object name - it only saves its contents (which also means you can save only one object at a time). As a result, when you read from an RDS file, you need to store the result of that function into a variable.\n\nsaveRDS(legos, \"../data/RDSlego.rds\")\n\nother_lego <- readRDS(\"../data/RDSlego.rds\")\n\nBecause RDS formats don’t save the object name, you can be sure that you’re not over-writing some object in your workspace by loading a different file. The downside to this is that you have to save each object to its own RDS file separately.\n\n\nWe first need to install the pyreadr package by running pip install pyreadr in the terminal.\n\nimport pyreadr\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'pyreadr'\nrdata_result = pyreadr.read_r('../data/R_binary.Rdata')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pyreadr' is not defined\nrdata_result[\"legos\"] # Access the variables using the variable name as a key\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'rdata_result' is not defined\nrdata_result[\"my_var\"]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'rdata_result' is not defined\nrds_result = pyreadr.read_r('../data/RDSlego.rds')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pyreadr' is not defined\nrds_result[None] # for RDS files, access the data using None as the key since RDS files have no object name.\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'rds_result' is not defined\n\n\n\nFirst, let’s download the NHTS data.\n\nlibrary(httr)\n# Download the file and write to disk\nres <- GET(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \n           write_disk(\"../data/cen10pub.sas7bdat\", overwrite = T))\n\nYou can see more information about this data here [5].\n\nif (!\"sas7bdat\" %in% installed.packages()) install.packages(\"sas7bdat\")\n\nlibrary(sas7bdat)\ndata <- read.sas7bdat(\"../data/cen10pub.sas7bdat\")\nhead(data)\n##    HOUSEID HH_CBSA10 RAIL10 CBSASIZE10 CBSACAT10 URBAN10 URBSIZE10 URBRUR10\n## 1 20000017     XXXXX     02         02        03      04        06       02\n## 2 20000231     XXXXX     02         03        03      01        03       01\n## 3 20000521     XXXXX     02         03        03      01        03       01\n## 4 20001283     35620     01         05        01      01        05       01\n## 5 20001603        -1     02         06        04      04        06       02\n## 6 20001649     XXXXX     02         03        03      01        02       01\n\nIf you are curious about what this data means, then by all means, take a look at the codebook (XLSX file). For now, it’s enough that we can see roughly how it’s structured.\n\n\nFirst, we need to download the SAS data file. This required writing a function to actually write the file downloaded from the URL, which is what this code chunk does.\n\n# Source: https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests\nimport requests\ndef download_file(url, local_filename):\n  # NOTE the stream=True parameter below\n  with requests.get(url, stream=True) as r:\n    r.raise_for_status()\n    with open(local_filename, 'wb') as f:\n      for chunk in r.iter_content(chunk_size=8192): \n        f.write(chunk)\n  return local_filename\n\ndownload_file(\"https://query.data.world/s/y7jo2qmjqfcnmublmwjvkn7wl4xeax\", \"../data/cen10pub.sas7bdat\")\n## '../data/cen10pub.sas7bdat'\n\nYou can see more information about this data here [5].\nTo read SAS files, we use the read_sas function in Pandas.\n\nimport pandas as pd\n\ndata = pd.read_sas(\"../data/cen10pub.sas7bdat\")\ndata\n##             HOUSEID HH_CBSA10 RAIL10  ... URBAN10 URBSIZE10 URBRUR10\n## 0       b'20000017'  b'XXXXX'  b'02'  ...   b'04'     b'06'    b'02'\n## 1       b'20000231'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 2       b'20000521'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 3       b'20001283'  b'35620'  b'01'  ...   b'01'     b'05'    b'01'\n## 4       b'20001603'     b'-1'  b'02'  ...   b'04'     b'06'    b'02'\n## ...             ...       ...    ...  ...     ...       ...      ...\n## 150142  b'69998896'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150143  b'69998980'  b'33100'  b'01'  ...   b'01'     b'05'    b'01'\n## 150144  b'69999718'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150145  b'69999745'  b'XXXXX'  b'02'  ...   b'01'     b'03'    b'01'\n## 150146  b'69999811'  b'31080'  b'01'  ...   b'01'     b'05'    b'01'\n## \n## [150147 rows x 8 columns]\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out\n\n\n\n\n\nProblem\nR Solution\nPython Solution\n\n\n\nRead in two of the files from an earlier example, and save the results as an Rdata file with two objects. Then save each one as an RDS file. (Obviously, use R for this)\nIn RStudio, go to Session -> Clear Workspace. (This will clear your environment)\nNow, using your RDS files, load the objects back into R with different names.\nFinally, load your Rdata file. Are the two objects the same? (You can actually test this with all.equal() if you’re curious)\nThen, load the two RDS files and the Rdata file in Python. Are the objects the same?\n\n\n\nlibrary(readxl)\nlibrary(readr)\npizza <- read_xlsx(\"../data/nyc_slice.xlsx\", sheet = 1, guess_max = 7000)\nlegos <- read_csv(\"../data/lego_sets.csv\")\n\nsave(pizza, legos, file = \"../data/04_Try_Binary.Rdata\")\nsaveRDS(pizza, \"../data/04_Try_Binary1.rds\")\nsaveRDS(legos, \"../data/04_Try_Binary2.rds\")\n\nrm(pizza, legos) # Limited clearing of workspace... \n\n\nload(\"../data/04_Try_Binary.Rdata\")\n\npizza_compare <- readRDS(\"../data/04_Try_Binary1.rds\")\nlego_compare <- readRDS(\"../data/04_Try_Binary2.rds\")\n\nall.equal(pizza, pizza_compare)\n## [1] TRUE\nall.equal(legos, lego_compare)\n## [1] TRUE\n\n\n\n\nimport pyreadr\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'pyreadr'\nrobjs = pyreadr.read_r('data/04_Try_Binary.Rdata')\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pyreadr' is not defined\npizza = robjs[\"pizza\"]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'robjs' is not defined\nlegos = robjs[\"legos\"] # Access the variables using the variable name as a key\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'robjs' is not defined\npizza_compare = pyreadr.read_r('data/04_Try_Binary1.rds')[None]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pyreadr' is not defined\nlego_compare = pyreadr.read_r('data/04_Try_Binary2.rds')[None]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pyreadr' is not defined\npizza.equals(pizza_compare)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'pizza' is not defined\nlegos.equals(lego_compare)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'legos' is not defined\n\n\n\n\n\n\n\n\n\n\n\n\nLearn more\n\n\n\n\n\nSlides from Jenny Bryan’s talk on spreadsheets (sadly, no audio. It was a good talk.)\nThe vroom package works like read_csv but allows you to read in and write to many files at incredible speeds."
  },
  {
    "objectID": "part-wrangling/01-data-input.html#references",
    "href": "part-wrangling/01-data-input.html#references",
    "title": "14  Data Input",
    "section": "\n14.6 References",
    "text": "14.6 References\n\n\n\n\n[1] \nBetterExplained, “A little diddy about binary file formats – BetterExplained. Better explained,” 2017. [Online]. Available: https://betterexplained.com/articles/a-little-diddy-about-binary-file-formats/. [Accessed: Jan. 13, 2023]\n\n\n[2] \nM. Schäfer, “Read Data from Google Sheets into Pandas without the Google Sheets API,” Towards Data Science. Dec. 2020 [Online]. Available: https://towardsdatascience.com/read-data-from-google-sheets-into-pandas-without-the-google-sheets-api-5c468536550. [Accessed: Jun. 07, 2022]\n\n\n[3] \nM. Clarke, “How to read Google Sheets data in Pandas with GSpread,” Practical Data Science. Jun. 2021 [Online]. Available: https://practicaldatascience.co.uk/data-science/how-to-read-google-sheets-data-in-pandas-with-gspread. [Accessed: Jun. 07, 2022]\n\n\n[4] \nC. Maierle, “Loading binary data to NumPy/Pandas,” Towards Data Science. Jul. 2020 [Online]. Available: https://towardsdatascience.com/loading-binary-data-to-numpy-pandas-9caa03eb0672. [Accessed: Jun. 07, 2022]\n\n\n[5] \nUS Department of Transportation, “National Household Travel Survey (NHTS) 2009,” data.world. Mar. 2018 [Online]. Available: https://data.world/dot/national-household-travel-survey-nhts-2009. [Accessed: Jun. 13, 2022]"
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#fa-bullseye-objectives",
    "href": "part-wrangling/02-basic-data-vis.html#fa-bullseye-objectives",
    "title": "15  Data Visualization Basics",
    "section": "\n15.1  Objectives",
    "text": "15.1  Objectives\n\nUse ggplot2/plotnine to create a chart\nBegin to identify issues with data formatting"
  },
  {
    "objectID": "part-wrangling/02-basic-data-vis.html#first-steps",
    "href": "part-wrangling/02-basic-data-vis.html#first-steps",
    "title": "15  Data Visualization Basics",
    "section": "\n15.2 First Steps",
    "text": "15.2 First Steps\nNow that you can read data in to R and python and define new variables, you can create plots! Data visualization is a skill that takes a lifetime to learn, but for now, let’s start out easy: let’s talk about how to make (basic) plots in R (with ggplot2) and in python (with plotnine, which is a ggplot2 clone).\n\n\n\n\n\n\nGraphing HBCU Enrollment\n\n\n\nLet’s work with Historically Black College and University enrollment.\n\n\nLoading Libraries\nLine Chart\nData Formatting\nLong Form Plot\n\n\n\nR:\n\nhbcu_all <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-02/hbcu_all.csv')\n\nlibrary(ggplot2)\n\nPython:\n\nimport pandas as pd\nfrom plotnine import *\n\nhbcu_all = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-02-02/hbcu_all.csv')\n\n\n\nggplot2 and plotnine work with data frames. If you pass a data frame in as the data argument, you can refer to columns in that data with “bare” column names (you don’t have to reference the full data object using df$name or df.name; you can instead use name or \"name\"):\nR:\n\n\nggplot(hbcu_all, aes(x = Year, y = `4-year`)) + geom_line() +\n  ggtitle(\"4-year HBCU College Enrollment\")\n\n\n\n\nPython:\n\n\nggplot(hbcu_all, aes(x = \"Year\", y = \"4-year\")) + geom_line() + \\\n  ggtitle(\"4-year HBCU College Enrollment\")\n## <ggplot: (8786300850080)>\n\n\n\n\n\n\nIf your data is in the right format, ggplot2 is very easy to use; if your data aren’t formatted neatly, it can be a real pain. If you want to plot multiple lines, you need to either list each variable you want to plot, one by one, or (more likely) you want to get your data into “long form”. You don’t need to know exactly how this works, but it is helpful to see the difference in the two datasets:\nR:\n\nlibrary(tidyr)\nhbcu_long <- pivot_longer(hbcu_all, -Year, names_to = \"type\", values_to = \"value\")\n\nhead(hbcu_all)\n## # A tibble: 6 × 12\n##    Year Total e…¹  Males Females 4-yea…² 2-yea…³ Total…⁴ 4-yea…⁵ 2-yea…⁶ Total…⁷\n##   <dbl>     <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n## 1  1976    222613 104669  117944  206676   15937  156836  143528   13308   65777\n## 2  1980    233557 106387  127170  218009   15548  168217  155085   13132   65340\n## 3  1982    228371 104897  123474  212017   16354  165871  151472   14399   62500\n## 4  1984    227519 102823  124696  212844   14675  164116  151289   12827   63403\n## 5  1986    223275  97523  125752  207231   16044  162048  147631   14417   61227\n## 6  1988    239755 100561  139194  223250   16505  173672  158606   15066   66083\n## # … with 2 more variables: `4-year - Private` <dbl>, `2-year - Private` <dbl>,\n## #   and abbreviated variable names ¹​`Total enrollment`, ²​`4-year`, ³​`2-year`,\n## #   ⁴​`Total - Public`, ⁵​`4-year - Public`, ⁶​`2-year - Public`,\n## #   ⁷​`Total - Private`\nhead(hbcu_long)\n## # A tibble: 6 × 3\n##    Year type              value\n##   <dbl> <chr>             <dbl>\n## 1  1976 Total enrollment 222613\n## 2  1976 Males            104669\n## 3  1976 Females          117944\n## 4  1976 4-year           206676\n## 5  1976 2-year            15937\n## 6  1976 Total - Public   156836\n\nPython: (data look the same as in R)\n\nhbcu_long = pd.melt(hbcu_all, id_vars = ['Year'], value_vars = hbcu_all.columns[1:11])\n\nIn the long form of the data, we have a row for each data point (year x measurement type), not for each year.\n\n\nR:\n\n\nggplot(hbcu_long, aes(x = Year, y = value, color = type)) + geom_line() +\n  ggtitle(\"HBCU College Enrollment\")\n\n\n\n\nPython:\n\n\nggplot(hbcu_long, aes(x = \"Year\", y = \"value\", color = \"variable\")) + geom_line() + \\\n  ggtitle(\"HBCU College Enrollment\") + \\\n  theme(subplots_adjust={'right':0.75}) # This moves the key so it takes up 25% of the area\n## <ggplot: (8786300674403)>"
  },
  {
    "objectID": "part-wrangling/10-graphics.html#fa-bullseye-objectives",
    "href": "part-wrangling/10-graphics.html#fa-bullseye-objectives",
    "title": "16  Data Visualization",
    "section": "\n16.1  Objectives",
    "text": "16.1  Objectives\n\nCreate charts designed to communicate specific aspects of the data\nDescribe charts using the grammar of graphics\nCreate layered graphics that highlight multiple aspects of the data\nEvaluate existing charts and develop new versions that improve accessibility and readability\n\nThere are a lot of different types of charts, and equally many ways to categorize and describe the different types of charts. I’m going to be opinionated on this one - while I will provide code for several different plotting programs, this chapter is organized based on the grammar of graphics specifically.\n\n\nVisualization and statistical graphics are also my research area, so I’m more passionate about this material, which means there’s going to be more to read. Sorry about that in advance. I’ll do my best to indicate which content is actually mission-critical and which content you can skip if you’re not that interested.\nThis is going to be a fairly extensive chapter (in terms of content) because I want you to have a resource to access later, if you need it. That’s why I’m showing you code for many different plotting libraries - I want you to be able to make charts in any program you may need to use for your research.\n\n\n\n\n\n\nGuides and Resources\n\n\n\n\n\nGraph galleries contain sample code to create many different types of charts. Similar galaries are available in R and Python.\nCheat Sheets:\n\nPython\nGgplot2\nBase R"
  },
  {
    "objectID": "part-wrangling/10-graphics.html#why-do-we-create-graphics",
    "href": "part-wrangling/10-graphics.html#why-do-we-create-graphics",
    "title": "16  Data Visualization",
    "section": "\n16.2 Why do we create graphics?",
    "text": "16.2 Why do we create graphics?\n\nThe greatest possibilities of visual display lie in vividness and inescapability of the intended message. A visual display can stop your mental flow in its tracks and make you think. A visual display can force you to notice what you never expected to see. (“Why, that scatter diagram has a hole in the middle!”) – John W. Tukey [1]\n\nFundamentally, charts are easier to understand than raw data.\nWhen you think about it, data is a pretty artificial thing. We exist in a world of tangible objects, but data are an abstraction - even when the data record information about the tangible world, the measurements are a way of removing the physical and transforming the “real world” into a virtual thing. As a result, it can be hard to wrap our heads around what our data contain. The solution to this is to transform our data back into something that is “tangible” in some way – if not physical and literally touch-able, at least something we can view and “wrap our heads around”.\nConsider this thought experiment: You have a simple data set - 2 variables, 500 observations. You want to get a sense of how the variables relate to each other. You can do one of the following options:\n\nPrint out the data set\nCreate some summary statistics of each variable and perhaps the covariance between the two variables\nDraw a scatter plot of the two variables\n\nWhich one would you rather use? Why?\nOur brains are very good at processing large amounts of visual information quickly. Evolution is good at optimizing for survival, and it’s important to be able to survey a field and pick out the tiger that might eat you. When we present information visually, in a format that can leverage our visual processing abilities, we offload some of the work of understanding the data to a chart that organizes it for us. You could argue that printing out the data is a visual presentation, but it requires that you read that data in as text, which we’re not nearly as equipped to process quickly (and in parallel).\nIt’s a lot easier to talk to non-experts about complicated statistics using visualizations. Moving the discussion from abstract concepts to concrete shapes and lines keeps people who are potentially already math or stat phobic from completely tuning out."
  },
  {
    "objectID": "part-wrangling/10-graphics.html#general-approaches-to-creating-graphics",
    "href": "part-wrangling/10-graphics.html#general-approaches-to-creating-graphics",
    "title": "16  Data Visualization",
    "section": "\n16.3 General approaches to creating graphics",
    "text": "16.3 General approaches to creating graphics\nThere are two general approaches to generating statistical graphics computationally:\n\nManually specify the plot that you want, possibly doing the preprocessing and summarizing before you create the plot.\nBase R, matplotlib, old-style SAS graphics\nDescribe the relationship between the plot and the data, using sensible defaults that can be customized for common operations.\nggplot2, plotnine, seaborn (sort of)\n\n\n\nThere is a difference between low-level plotting libraries (base R, matplotlib) and high-level plotting libraries (ggplot2, plotnine, seaborn). Grammar of graphics libraries are usually high level, but it is entirely possible to have a high level library that does not follow the grammar of graphics. In general, if you have to manually add a legend, it’s probably a low level library.\nIn the introduction to The Grammar of Graphics [2], Leland Wilkinson suggests that the first approach is what we would call “charts” - pie charts, line charts, bar charts - objects that are “instances of much more general objects”. His argument is that elegant graphical design means we have to think about an underlying theory of graphics, rather than how to create specific charts. The 2nd approach is called the “grammar of graphics”.\n\n\nThere are other graphics systems (namely, lattice in R, seaborn in Python, and some web-based rendering engines like Observable or d3) that you could explore, but it’s far more important that you know how to functionally create plots in R and/or Python. I don’t recommend you try to become proficient in all of them. Pick one (two at most) and get familiar with those libraries, then google for the rest.\nBefore we delve into the grammar of graphics, let’s motivate the philosophy using a simple task. Suppose we want to create a pie chart using some data. Pie charts are terrible, and we’ve known it for 100 years[3], so in the interests of showing that we know that pie charts are awful, we’ll also create a stacked bar chart, which is the most commonly promoted alternative to a pie chart. We’ll talk about what makes pie charts terrible at the end of this module in Creating Good charts.\n\n\n\n\n\n\nExample: Generations of Pokemon\n\n\n\n\nSuppose we want to explore Pokemon. There’s not just the original 150 (gotta catch ’em all!) - now there are over 1000! Let’s start out by looking at the proportion of Pokemon added in each of the 9 generations.\n\n\nR setup\nPython setup\n\n\n\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\n\n# Setup the data\npoke <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\", na = '.') %>%\n  mutate(generation = factor(gen))\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/pokemon_gen_1-9.csv\")\npoke['generation'] = pd.Categorical(poke.gen)\n\n\n\n\nOnce the data is read in, we can start plotting:\n\n\nggplot2\nBase R\nMatplotlib\nPlotnine\n\n\n\nIn ggplot2, we start by specifying which variables we want to be mapped to which features of the data.\nIn a pie or stacked bar chart, we don’t care about the x coordinate - the whole chart is centered at (0,0) or is contained in a single “stack”. So it’s easiest to specify our x variable as a constant, ““. We care about the fill of the slices, though - we want each generation to have a different fill color, so we specify generation as our fill variable.\nThen, we want to summarize our data by the number of objects in each category - this is basically a stacked bar chart. Any variables specified in the plot statement are used to implicitly calculate the statistical summary we want – that is, to count the rows (so if we had multiple x variables, the summary would be computed for both the x and fill variables). ggplot is smart enough to know that when we use geom_bar, we generally want the y variable to be the count, so we can get away with leaving that part out. We just have to specify that we want the bars to be stacked on top of one another (instead of next to each other, “dodge”).\n\nlibrary(ggplot2)\n\nggplot(aes(x = \"\", fill = generation), data = poke) + \n  geom_bar(position = \"stack\") \n\n\n\n\nIf we want a pie chart, we can get one very easily - we transform the coordinate plane from Cartesian coordinates to polar coordinates. We specify that we want angle to correspond to the “y” coordinate, and that we want to start at \\(\\theta = 0\\).\n\nggplot(aes(x = \"\", fill = generation), data = poke) + \n  geom_bar(position = \"stack\") + \n  coord_polar(\"y\", start = 0)\n\n\n\n\nNotice how the syntax and arguments to the functions didn’t change much between the bar chart and the pie chart? That’s because the ggplot package uses what’s called the grammar of graphics, which is a way to describe plots based on the underlying mathematical relationships between data and plotted objects. In base R and in matplotlib in Python, different types of plots will have different syntax, arguments, etc., but in ggplot2, the arguments are consistently named, and for plots which require similar transformations and summary observations, it’s very easy to switch between plot types by changing one word or adding one transformation.\n\n\nLet’s start with what we want: for each generation, we want the total number of pokemon.\nTo get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of pokemon in that generation.\n\n# Create summary of pokemon by type\ntmp <- poke %>%\n  group_by(generation) %>%\n  count() \n\npie(tmp$n, labels = tmp$generation)\n\n\n\n\nWe could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion.\n\n# Create summary of pokemon by type\ntmp <- poke %>%\n  group_by(generation) %>%\n  count() \n\n# Matrix is necessary for a stacked bar chart\nmatrix(tmp$n, nrow = 9, ncol = 1, dimnames = list(tmp$generation)) %>%\nbarplot(beside = F, legend.text = T, main = \"Generations of Pokemon\")\n\n\n\n\nThere’s not a huge amount of similarity between the code for a pie chart and a bar plot, even though the underlying statistics required to create the two charts are very similar. The appearance of the two charts is also very different.\n\n\nLet’s start with what we want: for each generation, we want the total number of pokemon.\nTo get a pie chart, we want that information mapped to a circle, with each generation represented by an angle whose size is proportional to the number of Pokemon in that generation.\n\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts(normalize=True)*100\n\n# Draw the plot\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels = labels, autopct='%1.1f%%', startangle = 90)\n## ([<matplotlib.patches.Wedge object at 0x7ffada29d330>, <matplotlib.patches.Wedge object at 0x7ffada29d090>, <matplotlib.patches.Wedge object at 0x7ffada29de10>, <matplotlib.patches.Wedge object at 0x7ffada29e530>, <matplotlib.patches.Wedge object at 0x7ffada29ec50>, <matplotlib.patches.Wedge object at 0x7ffada29f370>, <matplotlib.patches.Wedge object at 0x7ffada29fa90>, <matplotlib.patches.Wedge object at 0x7ffada0e01f0>, <matplotlib.patches.Wedge object at 0x7ffada0e0910>], [Text(-0.6090072830464104, 0.9160295460280905, '1'), Text(-1.0954901625854225, -0.0995052947262842, '2'), Text(-0.6165300405105602, -0.9109833747923434, '3'), Text(0.18481484283490274, -1.0843631651194678, '4'), Text(0.7975738857504167, -0.7575459700697914, '5'), Text(1.0758366105622164, -0.22929367059298023, '6'), Text(1.044469949497543, 0.3450833589099894, '7'), Text(0.7443080435855409, 0.8099416869465756, '8'), Text(0.2667977873990754, 1.0671546001582704, '9')], [Text(-0.3321857907525874, 0.4996524796516857, '18.7%'), Text(-0.5975400886829577, -0.05427561530524592, '15.5%'), Text(-0.3362891130057601, -0.4969000226140054, '12.6%'), Text(0.10080809609176512, -0.5914708173378915, '11.7%'), Text(0.43504030131840904, -0.4132068927653407, '8.8%'), Text(0.5868199693975725, -0.1250692748688983, '8.7%'), Text(0.5697108815441142, 0.18822728667817604, '8.1%'), Text(0.40598620559211324, 0.44178637469813214, '8.1%'), Text(0.14552606585404113, 0.5820843273590565, '7.8%')])\nax1.axis('equal')\n## (-1.1114088153158663, 1.1045180850389171, -1.106683057272638, 1.1003182408225065)\nplt.show()\n\n\n\n\nWe could alternately make a bar chart and stack the bars on top of each other. This also shows proportion (section vs. total) but does so in a linear fashion.\n\nimport matplotlib.pyplot as plt\nplt.cla() # clear out matplotlib buffer\n\n# Create summary of pokemon by type\nlabels = list(set(poke.generation)) # create labels by getting unique values\nsizes = poke.generation.value_counts()\nsizes = sizes.sort_index()\n\n# Find location of bottom of the bar for each bar\ncumulative_sizes = sizes.cumsum() - sizes\nwidth = 1\n\nfig, ax = plt.subplots()\n\nfor i in sizes.index:\n  ax.bar(\"Generation\", sizes[i-1], width, label=i, bottom = cumulative_sizes[i-1])\n## <BarContainer object of 1 artists>\n## <BarContainer object of 1 artists>\n## <BarContainer object of 1 artists>\n## <BarContainer object of 1 artists>\n## <BarContainer object of 1 artists>\n## <BarContainer object of 1 artists>\n## <BarContainer object of 1 artists>\n## <BarContainer object of 1 artists>\n## <BarContainer object of 1 artists>\nax.set_ylabel('# Pokemon')\nax.set_title('Pokemon Distribution by Generation')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\nAs of January 2023, pie charts are still not supported in plotnine. So this demo will fall a bit flat.\n\nfrom plotnine import *\nplt.cla() # clear out matplotlib buffer\n\nggplot(poke, aes(x = \"1\", fill = \"generation\")) + geom_bar(position = \"stack\")\n## <ggplot: (8794687858442)>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNintendo, Creatures, Game Freak, The Pokémon Company, Public domain, via Wikimedia Commons\n\n\nWe’ll talk first about the general idea behind the grammar of graphics. For each concept, I’ll provide you first with the ggplot grammar of graphics code, and then, where it is possible to replicate the chart easily in base R or Python graphics, I will provide code for that as well - so that you can compare the approaches, but also so that you get a sense for what is easy and what is possible in each plotting system.\n\n\n\n\n\n\nNote\n\n\n\nYou’re going to learn how to make graphics by finding sample code, changing that code to match your data set, and tweaking things as you go. That’s the best way to learn this, and while ggplot and plotnine do have a structure and some syntax to learn, once you’re familiar with the principles, you’ll still want to learn graphics by doing it.\n\n\nIn this chapter, we’re going to use the ggplot2 package to create graphics in R, and the plotnine package to create graphics in python. plotnine is a direct port of ggplot2 to Python using the Python graphics engine. For the most part, the syntax is extremely similar, with only minimal changes to account for the fact that some R syntax doesn’t work in Python, and a few differences with the python rendering engine for graphics."
  },
  {
    "objectID": "part-wrangling/10-graphics.html#the-grammar-of-graphics",
    "href": "part-wrangling/10-graphics.html#the-grammar-of-graphics",
    "title": "16  Data Visualization",
    "section": "\n16.4 The Grammar of Graphics",
    "text": "16.4 The Grammar of Graphics\nThe grammar of graphics is an approach first introduced in Leland Wilkinson’s book [2]. Unlike other graphics classification schemes, the grammar of graphics makes an attempt to describe how the dataset itself relates to the components of the chart.\n\n\nBuilding a masterpiece, by Allison Horst\n\n\nThis has a few advantages:\n\nIt’s relatively easy to represent the same dataset with different types of plots (and to find their strengths and weaknesses)\nGrammar leads to a concise description of the plot and its contents\nWe can add layers to modify the graphics, each with their own basic grammar (just like we combine sentences and clauses to build a rich, descriptive paragraph)\n\n\n\nA pyramid view of the major components of the grammar of graphics, with data as the base, aesthetics building on data, scales building on aesthetics, geometric objects, statistics, facets, and the coordinate system at the top of the pyramid. Source: [4]\n\n\n\n\n\n\n\n\nNote\n\n\n\nI have turned off warnings for all of the code chunks in this chapter. When you run the code you may get warnings about e.g. missing points - this is normal, I just didn’t want to have to see them over and over again - I want you to focus on the changes in the code.\n\n\n\n16.4.1 Exploratory Data Analysis with the grammar of graphics\n\n\nSketch\nR\nPython\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default.\n\n\n\n\n\nlibrary(ggplot2)\ndata(txhousing)\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point()\n\n\n\n\n\n\n\nfrom plotnine import *\nfrom plotnine.data import txhousing\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) + geom_point()\n## <ggplot: (8794687687353)>\n## \n## /home/susan/.local/lib/python3.10/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\n\n\n\nWhen creating a grammar of graphics chart, we start with the data (this is consistent with the data-first tidyverse philosophy).\n\nIdentify the dimensions of your dataset you want to visualize.\n\nDecide what aesthetics you want to map to different variables. For instance, it may be natural to put time on the \\(x\\) axis, or the experimental response variable on the \\(y\\) axis. You may want to think about other aesthetics, such as color, size, shape, etc. at this step as well.\n\nIt may be that your preferred representation requires some summary statistics in order to work. At this stage, you would want to determine what variables you feed in to those statistics, and then how the statistics relate to the geoms that you’re envisioning. You may want to think in terms of layers - showing the raw data AND a summary geom.\n\n\nIn most cases, ggplot will determine the scale for you, but sometimes you want finer control over the scale - for instance, there may be specific, meaningful bounds for a variable that you want to directly set.\nCoordinate system: Are you going to use a polar coordinate system? (Please say no, for reasons we’ll get into later!)\nFacets: Do you want to show subplots based on specific categorical variable values?\n\n(this list modified from [4]).\n\nLet’s explore the txhousing data a bit more thoroughly by adding some complexity to our chart. This example will give me an opportunity to show you how an exploratory data analysis might work in practice, while also demonstrating some of ggplot2’s features.\nBefore we start exploring, let’s add a title and label our axes, so that we’re creating good, informative charts.\n\n\nR\nPython\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) +\\\ngeom_point() +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## <ggplot: (8794711102197)>\n## \n## /home/susan/.local/lib/python3.10/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\n\n\n\nFirst, we may want to show some sort of overall trend line. We can start with a linear regression, but it may be better to use a loess smooth (loess regression is a fancy weighted average and can create curves without too much additional effort on your part).\n\n\nSketch\nR\nPython\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(data = txhousing, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) + geom_point() +\\\ngeom_smooth(method = \"lm\", color = \"blue\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n# By default, geom_smooth in plotnine has a black line you can't see well\n## <ggplot: (8794675637516)>\n## \n## /home/susan/.local/lib/python3.10/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\nWe can also use a loess (locally weighted) smooth:\n\nggplot(txhousing, aes(x = \"date\", y = \"median\")) + geom_point() +\\\ngeom_smooth(method = \"loess\", color = \"blue\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## <ggplot: (8794675646915)>\n## \n## /home/susan/.local/lib/python3.10/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\n\n\n\nLooking at the plots here, it’s clear that there are small sub-groupings (see, for instance, the almost continuous line of points at the very top of the group between 2000 and 2005). Let’s see if we can figure out what those additional variables are…\nAs it happens, the best viable option is City.\n\n\nSketch\nR\nPython\n\n\n\n\n\nA Sketch of the mapping between a data frame and a scatterplot, showing the geoms, aesthetics, transformations, scales, coordinate systems, and statistics that are created by default. In this modification of the original image, we’ve added a summary smooth line which provides a linear trend on top of the original points.\n\n\n\n\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nThat’s a really crowded graph! It’s slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(data = txhousing, aes(x = date, y = median, color = city)) +\n  # geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\nggplot(txhousing, aes(x = \"date\", y = \"median\", color = \"city\")) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## <ggplot: (8794672765793)>\n## \n## /home/susan/.local/lib/python3.10/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 616 rows containing missing values.\n\n\n\n\nThat’s a really crowded graph! It’s slightly easier if we just take the points away and only show the statistics, but there are still way too many cities to be able to tell what shade matches which city.\n\nggplot(txhousing, aes(x = \"date\", y = \"median\", color = \"city\")) +\\\ngeom_smooth(method = \"loess\") +\\\ntheme(subplots_adjust={'right': 0.5}) +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## <ggplot: (8794667610863)>\n\n\n\n\nThis is one of the first places we see differences in Python and R’s graphs - python doesn’t allocate sufficient space for the legend by default. In Python, you have to manually adjust the theme to show the legend (or plot the legend separately).\n\n\n\nIn reality, though, you should not ever map color to something with more than about 7 categories if your goal is to allow people to trace the category back to the label. It just doesn’t work well perceptually.\nSo let’s work with a smaller set of data: Houston, Dallas, Fort worth, Austin, and San Antonio (the major cities).\nAnother way to show this data is to plot each city as its own subplot. In ggplot2 lingo, these subplots are called “facets”. In visualization terms, we call this type of plot “small multiples” - we have many small charts, each showing the trend for a subset of the data.\n\n\nR\nPython\n\n\n\n\ncitylist <- c(\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\")\nhousingsub <- dplyr::filter(txhousing, city %in% citylist)\n\nggplot(data = housingsub, aes(x = date, y = median, color = city)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nHere’s the facetted version of the chart:\n\nggplot(data = housingsub, aes(x = date, y = median)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  facet_wrap(~city) +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\nggplot(housingsub, aes(x = \"date\", y = \"median\", color = \"city\")) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\") +\\\ntheme(subplots_adjust={'right': 0.75}) +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## <ggplot: (8794675644056)>\n\n\n\n\nHere’s the facetted version of the chart:\n\nggplot(housingsub, aes(x = \"date\", y = \"median\")) +\\\ngeom_point() +\\\ngeom_smooth(method = \"loess\", color = \"blue\") +\\\nfacet_wrap(\"city\") +\\\nxlab(\"Date\") + ylab(\"Median Home Price\") +\\\nggtitle(\"Texas Housing Prices\")\n## <ggplot: (8794672708488)>\n\n\n\n\n\n\n\nNow that we’ve simplified our charts a bit, we can explore a couple of the other quantitative variables by mapping them to additional aesthetics:\n\n\nR\nPython\n\n\n\n\nggplot(data = housingsub, aes(x = date, y = median, size = sales)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  facet_wrap(~city) +\n  # Remove extra information from the legend -\n  # line and error bands aren't what we want to show\n  # Also add a title\n  guides(size = guide_legend(title = 'Number of Sales',\n                             override.aes = list(linetype = NA,\n                                                 fill = 'transparent'))) +\n  # Move legend to bottom right of plot\n  theme(legend.position = c(1, 0), legend.justification = c(1, 0)) +\n  xlab(\"Date\") + ylab(\"Median Home Price\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nNotice I’ve removed the aesthetic mapping to color as it’s redundant now that each city is split out in its own plot.\n\n\n\ncitylist = [\"Houston\", \"Austin\", \"Dallas\", \"Fort Worth\", \"San Antonio\"]\nhousingsub = txhousing[txhousing['city'].isin(citylist)]\n\n( # This is used to group lines together in python\nggplot(housingsub, aes(x = \"date\", y = \"median\", size = \"sales\"))\n+ geom_point(alpha = .15) # Make points transparent\n+ geom_smooth(method = \"loess\")\n+ facet_wrap(\"city\")\n+ guides(size = guide_legend(title = 'Number of Sales'))\n+ xlab(\"Date\") + ylab(\"Median Home Price\")\n+ ggtitle(\"Texas Housing Prices\")\n)\n## <ggplot: (8794678684051)>\n\n\n\n\nNot all of the features we used in R are available in plotnine in Python (in part because of limitations of the underlying graphics interface that plotnine uses). This does somewhat limit the customization we can do with python, but for the most part we can still get the same basic information back out.\n\n\n\nUp to this point, we’ve used the same position information - date for the y axis, median sale price for the y axis. Let’s switch that up a bit so that we can play with some transformations on the x and y axis and add variable mappings to a continuous variable.\n\n\nR\nPython\n\n\n\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\nThe points for Fort Worth are compressed pretty tightly relative to the points for Houston and Dallas. When we get this type of difference, it is sometimes common to use a log transformation1. Here, I have transformed both the x and y axis, since the number of sales seems to be proportional to the number of listings.\n\nggplot(data = housingsub, aes(x = listings, y = sales, color = city)) +\n  geom_point(alpha = .15) + # Make points transparent\n  geom_smooth(method = \"loess\") +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Number of Listings\") + ylab(\"Number of Sales\") +\n  ggtitle(\"Texas Housing Prices\")\n\n\n\n\n\n\n\n( # This is used to group lines together in python\nggplot(housingsub, aes(x = \"listings\", y = \"sales\", color = \"city\"))\n+ geom_point(alpha = .15) # Make points transparent\n+ geom_smooth(method = \"loess\")\n+ scale_x_log10()\n+ scale_y_log10()\n+ xlab(\"Date\") + ylab(\"Median Home Price\")\n+ ggtitle(\"Texas Housing Prices\")\n)\n## <ggplot: (8794678443900)>\n## \n## /home/susan/.local/lib/python3.10/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 6 rows containing missing values.\n\n\n\n\nNotice that the gridlines included in python by default are different than those in ggplot2 by default (personally, I vastly prefer the python version - it makes it obvious that we’re using a log scale).\n\n\n\nFor the next demonstration, let’s look at just Houston’s data. We can examine the inventory’s relationship to the number of sales by looking at the inventory-date relationship in x and y, and mapping the size or color of the point to number of sales.\n\n\nR\nPython\n\n\n\n\nhouston <- dplyr::filter(txhousing, city == \"Houston\")\n\nggplot(data = houston, aes(x = date, y = inventory, size = sales)) +\n  geom_point(shape = 1) +\n  xlab(\"Date\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_legend(title = \"Number of Sales\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\n\nggplot(data = houston, aes(x = date, y = inventory, color = sales)) +\n  geom_point() +\n  xlab(\"Date\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_colorbar(title = \"Number of Sales\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\nWhich is easier to read?\nWhat happens if we move the variables around and map date to the point color?\n\nggplot(data = houston, aes(x = sales, y = inventory, color = date)) +\n  geom_point() +\n  xlab(\"Number of Sales\") + ylab(\"Months of Inventory\") +\n  guides(size = guide_colorbar(title = \"Date\")) +\n  ggtitle(\"Houston Housing Data\")\n\n\n\n\nIs that easier or harder to read?\n\n\n\nhouston = txhousing[txhousing.city==\"Houston\"]\n\n(\n  ggplot(houston, aes(x = \"date\", y = \"inventory\", size = \"sales\"))\n  + geom_point(shape = 'o', fill = 'none')\n  + xlab(\"Date\") + ylab(\"Median Home Price\")\n  + guides(size = guide_legend(title = \"Number of Sales\"))\n  + ggtitle(\"Houston Housing Data\")\n)\n## <ggplot: (8794620543514)>\n## \n## /home/susan/.local/lib/python3.10/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\nIn plotnine, we have to use matplotlib marker syntax.\n\n\n(\n  ggplot(houston, aes(x = \"date\", y = \"inventory\", color = \"sales\"))\n  + geom_point()\n  + xlab(\"Date\") + ylab(\"Median Home Price\")\n  + guides(size = guide_legend(title = \"Number of Sales\"))\n  + ggtitle(\"Houston Housing Data\")\n)\n## <ggplot: (8794617704142)>\n## \n## /home/susan/.local/lib/python3.10/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\nPlotnine also defaults to different color schemes than ggplot2 – just something to know if you want the plot to be exactly the same. Personally, I prefer the viridis color scheme (what plotnine uses) to the ggplot2 defaults.\nWhat happens if we move the variables around and map date to the point color?\n\n(\nggplot(houston, aes(x = \"sales\", y = \"inventory\", color = \"date\"))\n  + geom_point()\n  + xlab(\"Number of Sales\") + ylab(\"Months of Inventory\")\n  + guides(size = guide_colorbar(title = \"Date\"))\n  + ggtitle(\"Houston Housing Data\")\n  + theme(subplots_adjust={'right': 0.75})\n)\n## <ggplot: (8794617704277)>\n## \n## /home/susan/.local/lib/python3.10/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\nIs that easier or harder to read?\n\n\n\n\n\n16.4.2 What type of chart to use?\nIt can be hard to know what type of chart to use for a particular type of data. I recommend figuring out what you want to show first, and then thinking about how to show that data with an appropriate plot type. Consider the following factors:\n\nWhat type of variable is x? Categorical? Continuous? Discrete?\nWhat type of variable is y?\nHow many observations do I have for each x/y variable?\nAre there any important moderating variables?\nDo I have data that might be best shown in small multiples? E.g. a categorical moderating variable and a lot of data, where the categorical variable might be important for showing different features of the data?\n\nOnce you’ve thought through this, take a look through catalogues like the R Graph Gallery to see what visualizations match your data and use-case."
  },
  {
    "objectID": "part-wrangling/10-graphics.html#creating-good-charts",
    "href": "part-wrangling/10-graphics.html#creating-good-charts",
    "title": "16  Data Visualization",
    "section": "\n16.5 Creating Good Charts",
    "text": "16.5 Creating Good Charts\nA chart is good if it allows the user to draw useful conclusions that are supported by data. Obviously, this definition depends on the purpose of the chart - a simple EDA chart is going to have a different purpose than a chart showing e.g. the predicted path of a hurricane, which people will use to make decisions about whether or not to evacuate.\nUnfortunately, while our visual system is amazing, it is not always as accurate as the computers we use to render graphics. We have physical limits in the number of colors we can perceive, our short term memory, attention, and our ability to accurately read information off of charts in different forms.\n\n16.5.1 Perceptual and Cognitive Factors\n\n16.5.1.1 Color\nOur eyes are optimized for perceiving the yellow/green region of the color spectrum. Why? Well, our sun produces yellow light, and plants tend to be green. It’s pretty important to be able to distinguish different shades of green (evolutionarily speaking) because it impacts your ability to feed yourself. There aren’t that many purple or blue predators, so there is less selection pressure to improve perception of that part of the visual spectrum.\n\n\nSensitivity of the human eye to different wavelengths of visual light (Image from Wikimedia commons)\n\n\nNot everyone perceives color in the same way. Some individuals are colorblind or color deficient. We have 3 cones used for color detection, as well as cells called rods which detect light intensity (brightness/darkness). In about 5% of the population (10% of XY individuals, <1% of XX individuals), one or more of the cones may be missing or malformed, leading to color blindness - a reduced ability to perceive different shades. The rods, however, function normally in almost all of the population, which means that light/dark contrasts are extremely safe, while contrasts based on the hue of the color are problematic in some instances.\n\nYou can take a test designed to screen for colorblindness here\nYour monitor may affect how you score on these tests - I am colorblind, but on some monitors, I can pass the test, and on some, I perform worse than normal. A different test is available here.\n\n In reality, I know that I have issues with perceiving some shades of red, green, and brown. I have particular trouble with very dark or very light colors, especially when they are close to grey or brown.\nIn addition to colorblindness, there are other factors than the actual color value which are important in how we experience color, such as context.\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\n\n\nThe color constancy illusion. The squares marked A and B are actually the same color\n\n\n\n\nOur brains are extremely dependent on context and make excellent use of the large amounts of experience we have with the real world. As a result, we implicitly “remove” the effect of things like shadows as we make sense of the input to the visual system. This can result in odd things, like the checkerboard and shadow shown above - because we’re correcting for the shadow, B looks lighter than A even though when the context is removed they are clearly the same shade.\nImplications and Guidelines\n\nDo not use rainbow color gradient schemes - because of the unequal perception of different wavelengths, these schemes are misleading - the color distance does not match the perceptual distance.\nAvoid any scheme that uses green-yellow-red signaling if you have a target audience that may include colorblind people.\nTo “colorblind-proof” a graphic, you can use a couple of strategies:\n\ndouble encoding - where you use color, use another aesthetic (line type, shape) as well to help your colorblind readers out\nIf you can print your chart out in black and white and still read it, it will be safe for colorblind users. This is the only foolproof way to do it!\nIf you are using a color gradient, use a monochromatic color scheme where possible. This is perceived as light -> dark by colorblind people, so it will be correctly perceived no matter what color you use.\nIf you have a bidirectional scale (e.g. showing positive and negative values), the safest scheme to use is purple - white - orange. In any color scale that is multi-hue, it is important to transition through white, instead of from one color to another directly.\n\n\nBe conscious of what certain colors “mean”\n\nLeveraging common associations can make it easier to read a color scale and remember what it stands for (e.g. blue for cold, orange/red for hot is a natural scale, red = Republican and blue = Democrat in the US, white -> blue gradients for showing rainfall totals)\nSome colors can can provoke emotional responses that may not be desirable.2\n\nIt is also important to be conscious of the social baggage that certain color schemes may have - the pink/blue color scheme often used to denote gender can be unnecessarily polarizing, and it may be easier to use a colder color (blue or purple) for men and a warmer color (yellow, orange, lighter green) for women3.\n\n\nThere are packages such as RColorBrewer and dichromat that have color palettes which are aesthetically pleasing, and, in many cases, colorblind friendly (dichromat is better for that than RColorBrewer). You can also take a look at other ways to find nice color palettes.\n\n16.5.1.2 Short Term Memory\nWe have a limited amount of memory that we can instantaneously utilize. This mental space, called short-term memory, holds information for active use, but only for a limited amount of time.\nTry it out!\n\nClick here, read the information, and then click to hide it.\n1 4 2 2 3 9 8 0 7 8\n\nWait a few seconds, then expand this section\nWhat was the third number?\nWithout rehearsing the information (repeating it over and over to yourself), the try it out task may have been challenging. Short term memory has a capacity of between 3 and 9 “bits” of information.\nIn charts and graphs, short term memory is important because we need to be able to associate information from e.g. a key, legend, or caption with information plotted on the graph. As a result, if you try to plot more than ~6 categories of information, your reader will have to shift between the legend and the graph repeatedly, increasing the amount of cognitive labor required to digest the information in the chart.\nWhere possible, try to keep your legends to 6 or 7 characteristics.\nImplications and Guidelines\n\n\nLimit the number of categories in your legends to minimize the short term memory demands on your reader.\n\nWhen using continuous color schemes, you may want to use a log scale to better show differences in value across orders of magnitude.\n\n\nUse colors and symbols which have implicit meaning to minimize the need to refer to the legend.\nAdd annotations on the plot, where possible, to reduce the need to re-read captions.\n\n16.5.1.3 Grouping and Sense-making\nImposing order on visual chaos.\n\n\nAmbiguous Images\nIllusory Contours\nFigure/Ground\n\n\n\nWhat does the figure below look like to you?\n\n\nIs it a rabbit, or a duck?\n\n\nWhen faced with ambiguity, our brains use available context and past experience to try to tip the balance between alternate interpretations of an image. When there is still some ambiguity, many times the brain will just decide to interpret an image as one of the possible options.\n\n\n\n\nConsider this image - what do you see?\n\n\nDid you see something like “3 circles, a triangle with a black outline, and a white triangle on top of that”? In reality, there are 3 angles and 3 pac-man shapes. But, it’s much more likely that we’re seeing layers of information, where some of the information is obscured (like the “mouth” of the pac-man circles, or the middle segment of each side of the triangle). This explanation is simpler, and more consistent with our experience.\n\n\nNow, look at the logo for the Pittsburgh Zoo.\n\nDo you see the gorilla and lionness? Or do you see a tree? Here, we’re not entirely sure which part of the image is the figure and which is the background.\n\n\n\nThe ambiguous figures shown above demonstrate that our brains are actively imposing order upon the visual stimuli we encounter. There are some heuristics for how this order is applied which impact our perception of statistical graphs.\nThe catchphrase of Gestalt psychology is\n\nThe whole is greater than the sum of the parts\n\nThat is, what we perceive and the meaning we derive from the visual scene is more than the individual components of that visual scene.\n\n\nThe Gestalt Heuristics help us to impose order on ambiguous visual stimuli\n\n\nYou can read about the gestalt rules here, but they are also demonstrated in the figure above.\nIn graphics, we can leverage the gestalt principles of grouping to create order and meaning. If we color points by another variable, we are creating groups of similar points which assist with the perception of groups instead of individual observations. If we add a trend line, we create the perception that the points are moving “with” the line (in most cases), or occasionally, that the line is dividing up two groups of points. Depending on what features of the data you wish to emphasize, you might choose different aesthetics mappings, facet variables, and factor orders.\n\nSuppose I want to emphasize the change in the murder rate between 1980 and 2010.\nI could use a bar chart (showing only the first 4 states alphabetically for space)\n\n\nR\nPython\n\n\n\n\nfbiwide <- read.csv(\"https://github.com/srvanderplas/Stat151/raw/main/data/fbiwide.csv\")\nlibrary(dplyr)\n\nfbiwide %>%\n  filter(Year %in% c(1980, 2010)) %>%\n  filter(State %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\")) %>%\n  ggplot(aes(x = State, y = Murder/Population*100000, fill = factor(Year))) +\n  geom_col(position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\n\n\n\nimport pandas as pd\nfbiwide = r.fbiwide\nfbiwide = fbiwide.assign(YearFactor = pd.Categorical(fbiwide.Year))\nfbiwide = fbiwide.assign(Murder100k = fbiwide.Murder/fbiwide.Population * 100000)\n\nyr1980_2010 = fbiwide[fbiwide.Year.isin([1980,2010])]\nsubdata = yr1980_2010[yr1980_2010.State.isin([\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\"])]\n\n(\nggplot(subdata, aes(x = \"State\", y = \"Murder100k\", fill = \"YearFactor\")) +\n  geom_col(stat='identity', position = \"dodge\") +\n  coord_flip() +\n  ylab(\"Murders per 100,000 residents\")\n)\n## <ggplot: (8794612095280)>\n\n\n\n\n\n\n\nOr, I could use a line chart\n\n\nR\nPython\n\n\n\n\nfbiwide %>%\n  filter(Year %in% c(1980, 2010)) %>%\n  ggplot(aes(x = Year, y = Murder/Population*100000, group = State)) +\n  geom_line() +\n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\n\n\n\n(\nggplot(yr1980_2010, aes(x = \"Year\", y = \"Murder100k\", group = \"State\")) +\n  geom_line() +\n  ylab(\"Murders per 100,000 residents\")\n)\n## <ggplot: (8794617721484)>\n\n\n\n\n\n\n\nOr, I could use a box plot\n\n\nR\nPython\n\n\n\n\nfbiwide %>%\n  filter(Year %in% c(1980, 2010)) %>%\n  ggplot(aes(x = factor(Year), y = Murder/Population*100000)) +\n  geom_boxplot() +\n  ylab(\"Murders per 100,000 residents\")\n\n\n\n\n\n\n\n\n(\nggplot(yr1980_2010, aes(x = \"YearFactor\", y = \"Murder100k\")) +\n  geom_boxplot() +\n  ylab(\"Murders per 100,000 residents\")\n)\n## <ggplot: (8794678640547)>\n\n\n\n\n\n\n\nWhich one best demonstrates that in every state and region, the murder rate decreased?\nThe line segment plot connects related observations (from the same state) but allows you to assess similarity between the lines (e.g. almost all states have negative slope). The same information goes into the creation of the other two plots, but the bar chart is extremely cluttered, and the boxplot doesn’t allow you to connect single state observations over time. So while you can see an aggregate relationship (overall, the average number of murders in each state per 100k residents decreased) you can’t see the individual relationships.\n\nThe aesthetic mappings and choices you make when creating plots have a huge impact on the conclusions that you (and others) can easily make when examining those plots.4\n\n16.5.2 General guidelines for accuracy\nThere are certain tasks which are easier for us relative to other, similar tasks.\n\n\n\n\nWhich of the lines is the longest? Shortest? It is much easier to determine the relative length of the line when the ends are aligned. In fact, the line lengths are the same in both panels.\n\n\n\n\nWhen making judgments corresponding to numerical quantities, there is an order of tasks from easiest (1) to hardest (6), with equivalent tasks at the same level.5\n\nPosition (common scale)\nPosition (non-aligned scale)\nLength, Direction, Angle, Slope\nArea\nVolume, Density, Curvature\nShading, Color Saturation, Color Hue\n\nIf we compare a pie chart and a stacked bar chart, the bar chart asks readers to make judgements of position on a non-aligned scale, while a pie chart asks readers to assess angle. This is one reason why pie charts are not preferable – they make it harder on the reader, and as a result we are less accurate when reading information from pie charts.\nWhen creating a chart, it is helpful to consider which variables you want to show, and how accurate reader perception needs to be to get useful information from the chart. In many cases, less is more - you can easily overload someone, which may keep them from engaging with your chart at all. Variables which require the reader to notice small changes should be shown on position scales (x, y) rather than using color, alpha blending, etc.\nThere is also a general increase in dimensionality from 1-3 to 4 (2d) to 5 (3d). In general, showing information in 3 dimensions when 2 will suffice is misleading - the addition of that extra dimension causes an increase in chart area allocated to the item that is disproportionate to the actual area.\n.\nTed ED: How to spot a misleading graph - Lea Gaslowitz\nBusiness Insider: The Worst Graphs Ever\nExtra dimensions and other annotations are sometimes called “chartjunk” and should only be used if they contribute to the overall numerical accuracy of the chart (e.g. they should not just be for decoration)."
  },
  {
    "objectID": "part-wrangling/10-graphics.html#references",
    "href": "part-wrangling/10-graphics.html#references",
    "title": "16  Data Visualization",
    "section": "References",
    "text": "References\nR graphics\n\nggplot2 cheat sheet\n\nggplot2 aesthetics cheat sheet - aesthetic mapping one page cheatsheet\nggplot2 reference guide\nR graph cookbook\n\nData Visualization in R (@ramnathv)\nPython graphics\n\nPlotnine documentation\n\nMatplotlib documentation - Matplotlib is the base that plotnine uses to replicate ggplot2 functionality\n\n\nVisualization with Matplotlib chapter of Python Data Science\n\n\nScientific Visualization with Python\n\n\n\n\n\n[1] \nJ. W. Tukey, “Data-Based Graphics: Visual Display in the Decades to Come,” Statistical Science, vol. 5, no. 3, pp. 327–339, Aug. 1990, doi: 10.1214/ss/1177012101. [Online]. Available: https://projecteuclid.org/journals/statistical-science/volume-5/issue-3/Data-Based-Graphics--Visual-Display-in-the-Decades-to/10.1214/ss/1177012101.full. [Accessed: Aug. 22, 2022]\n\n\n[2] \nL. Wilkinson, The grammar of graphics. New York: Springer, 1999 [Online]. Available: http://public.ebookcentral.proquest.com/choice/publicfullrecord.aspx?p=3085765. [Accessed: Jan. 29, 2020]\n\n\n[3] \nF. E. Croxton and R. E. Stryker, “Bar Charts Versus Circle Diagrams,” Journal of the American Statistical Association, vol. 22, no. 160, pp. 473–482, 1927, doi: 10.2307/2276829. [Online]. Available: https://www.jstor.org/stable/2276829. [Accessed: Aug. 22, 2022]\n\n\n[4] \nD. (DJ). Sarkar, “A Comprehensive Guide to the Grammar of Graphics for Effective Visualization of Multi-dimensional…,” Medium. Sep. 2018 [Online]. Available: https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149. [Accessed: Apr. 11, 2022]"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#introduction",
    "href": "part-wrangling/03-data-cleaning.html#introduction",
    "title": "17  Data Cleaning",
    "section": "\n17.1 Introduction",
    "text": "17.1 Introduction\nIn this section, we’re going start learning how to work with data. Generally speaking, data doesn’t come in a form suitable for analysis1 - you have to clean it up, create the variables you care about, get rid of those you don’t care about, and so on.\nSome people call the process of cleaning and organizing your data “data wrangling”, which is a fantastic way to think about chasing down all of the issues in the data.\n\n\nData wrangling (by Allison Horst)\n\n\nIn R, we’ll be using the tidyverse for this. It’s a meta-package (a package that just loads other packages) that collects packages designed with the same philosophy2 and interface (basically, the commands will use predictable argument names and structure). You’ve already been introduced to parts of the tidyverse - specifically, readr and ggplot2.\ndplyr (one of the packages in the tidyverse) creates a “grammar of data manipulation” to make it easier to describe different operations. I find the dplyr grammar to be extremely useful when talking about data operations, so I’m going to attempt to show you how to do the same operations in R with dplyr, and in Python (without the underlying framework).\nEach dplyr verb describes a common task when doing both exploratory data analysis and more formal statistical modeling. In all tidyverse functions, data comes first – literally, as it’s the first argument to any function. In addition, you don’t use df$variable to access a variable - you refer to the variable by its name alone (“bare” names). This makes the syntax much cleaner and easier to read, which is another principle of the tidy philosophy.\nIn Python, most data manipulation tasks are handled using pandas[1]. In the interests of using a single consistent “language” for describing data manipulation tasks, I’ll use the tidyverse “verbs” to describe operations in both languages. The goal of this is to help focus your attention on the essentials of the operations, instead of the specific syntax.\nThere is also the datar python package[2], which attempts to port the dplyr grammar of data wrangling into python. While pandas tends to be fairly similar to base R in basic operation, datar may be more useful if you prefer the dplyr way of handling things using a data-first API.\n\n\nI haven’t had the chance to add the datar package to this book, but it looks promising and may be worth your time to figure out. It’s a bit too new for me to take the time to add it into this book - I want packages that will be maintained long-term if I’m going to teach them to others.\n\n\n\n\n\n\nNote\n\n\n\nThere is an excellent dplyr cheatsheet available from RStudio. You may want to print it out to have a copy to reference as you work through this chapter.\nHere is a data wrangling with pandas cheatsheet that is formatted similarly to the dplyr cheat sheet."
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#tidy-data",
    "href": "part-wrangling/03-data-cleaning.html#tidy-data",
    "title": "17  Data Cleaning",
    "section": "\n17.2 Tidy Data",
    "text": "17.2 Tidy Data\nThere are infinitely many ways to configure “messy” data, but data that is “tidy” has 3 attributes:\n\nEach variable has its own column\nEach observation has its own row\nEach value has its own cell\n\nThese attributes aren’t sufficient to define “clean” data, but they work to define “tidy” data (in the same way that you can have a “tidy” room because all of your clothes are folded, but they aren’t clean just because they’re folded; you could have folded a pile of dirty clothes).\nWe’ll get more into how to work with different “messy” data configurations in Chapter 19 and Chapter 20, but it’s worth keeping rules 1 and 3 in mind while working through this module."
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#filter-subset-rows",
    "href": "part-wrangling/03-data-cleaning.html#filter-subset-rows",
    "title": "17  Data Cleaning",
    "section": "\n17.3 Filter: Subset rows",
    "text": "17.3 Filter: Subset rows\nFilter allows us to work with a subset of a larger data frame, keeping only the rows we’re interested in. We provide one or more logical conditions, and only those rows which meet the logical conditions are returned from filter(). Note that unless we store the result from filter() in the original object, we don’t change the original.\n\n\ndplyr filter() by Allison Horst\n\n\n\n\n\n\n\n\nExample: starwars\n\n\n\nLet’s explore how it works, using the starwars dataset, which contains a comprehensive list of the characters in the Star Wars movies.\nIn the interests of demonstrating the process on the same data, I’ve exported the starwars data to a CSV file using the readr package. I had to remove the list-columns (films, vehicles, starships) because that format isn’t supported by CSV files. You can access the csv data here.\n\n\nR\nPython\n\n\n\nThis data set is included in the dplyr package, so we load that package and then use the data() function to load dataset into memory. The loading isn’t complete until we actually use the dataset though… so let’s print the first few rows.\n\nlibrary(dplyr)\ndata(starwars)\nstarwars\n## # A tibble: 87 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 C-3PO          167    75 <NA>    gold    yellow    112   none  mascu… Tatooi…\n##  3 R2-D2           96    32 <NA>    white,… red        33   none  mascu… Naboo  \n##  4 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  5 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  6 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  7 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  8 R5-D4           97    32 <NA>    white,… red        NA   none  mascu… Tatooi…\n##  9 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 10 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n## # … with 77 more rows, 4 more variables: species <chr>, films <list>,\n## #   vehicles <list>, starships <list>, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\nWe have to use the exported CSV data in python.\n\nimport pandas as pd\nstarwars = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/starwars.csv\")\nstarwars\n##               name  height   mass  ...     gender homeworld species\n## 0   Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 1            C-3PO   167.0   75.0  ...  masculine  Tatooine   Droid\n## 2            R2-D2    96.0   32.0  ...  masculine     Naboo   Droid\n## 3      Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 4      Leia Organa   150.0   49.0  ...   feminine  Alderaan   Human\n## ..             ...     ...    ...  ...        ...       ...     ...\n## 82             Rey     NaN    NaN  ...   feminine       NaN   Human\n## 83     Poe Dameron     NaN    NaN  ...  masculine       NaN   Human\n## 84             BB8     NaN    NaN  ...  masculine       NaN   Droid\n## 85  Captain Phasma     NaN    NaN  ...        NaN       NaN     NaN\n## 86   Padmé Amidala   165.0   45.0  ...   feminine     Naboo   Human\n## \n## [87 rows x 11 columns]\nfrom skimpy import skim\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'skimpy'\nskim(starwars)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'skim' is not defined\n\n\n\n\nOnce the data is set up, filtering the data (selecting certain rows) is actually very simple. Of course, we’ve talked about how to use logical indexing before in Section 9.4.1, but here we’ll focus on using specific functions to perform the same operation.\n\n\nR: dplyr\nPython\nBase R\n\n\n\nThe dplyr verb for selecting rows is filter. filter takes a set of one or more logical conditions, using bare column names and logical operators. Each provided condition is combined using AND.\n\n# Get only the people\nfilter(starwars, species == \"Human\")\n## # A tibble: 35 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  3 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  4 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  5 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  6 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n##  7 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n##  8 Anakin Sky…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n##  9 Wilhuff Ta…    180    NA auburn… fair    blue       64   male  mascu… Eriadu \n## 10 Han Solo       180    80 brown   fair    brown      29   male  mascu… Corell…\n## # … with 25 more rows, 4 more variables: species <chr>, films <list>,\n## #   vehicles <list>, starships <list>, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n# Get only the people who come from Tatooine\nfilter(starwars, species == \"Human\", homeworld == \"Tatooine\")\n## # A tibble: 8 × 14\n##   name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##   <chr>         <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n## 1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n## 2 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n## 3 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n## 4 Beru Whites…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n## 5 Biggs Darkl…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 6 Anakin Skyw…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n## 7 Shmi Skywal…    163    NA black   fair    brown      72   fema… femin… Tatooi…\n## 8 Cliegg Lars     183    NA brown   fair    blue       82   male  mascu… Tatooi…\n## # … with 4 more variables: species <chr>, films <list>, vehicles <list>,\n## #   starships <list>, and abbreviated variable names ¹​hair_color, ²​skin_color,\n## #   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\n\n# Get only the people\nstarwars.query(\"species == 'Human'\")\n\n# Get only the people who come from Tattoine\n##                    name  height   mass  ...     gender     homeworld species\n## 0        Luke Skywalker   172.0   77.0  ...  masculine      Tatooine   Human\n## 3           Darth Vader   202.0  136.0  ...  masculine      Tatooine   Human\n## 4           Leia Organa   150.0   49.0  ...   feminine      Alderaan   Human\n## 5             Owen Lars   178.0  120.0  ...  masculine      Tatooine   Human\n## 6    Beru Whitesun lars   165.0   75.0  ...   feminine      Tatooine   Human\n## 8     Biggs Darklighter   183.0   84.0  ...  masculine      Tatooine   Human\n## 9        Obi-Wan Kenobi   182.0   77.0  ...  masculine       Stewjon   Human\n## 10     Anakin Skywalker   188.0   84.0  ...  masculine      Tatooine   Human\n## 11       Wilhuff Tarkin   180.0    NaN  ...  masculine        Eriadu   Human\n## 13             Han Solo   180.0   80.0  ...  masculine      Corellia   Human\n## 16       Wedge Antilles   170.0   77.0  ...  masculine      Corellia   Human\n## 17     Jek Tono Porkins   180.0  110.0  ...  masculine    Bestine IV   Human\n## 19            Palpatine   170.0   75.0  ...  masculine         Naboo   Human\n## 20            Boba Fett   183.0   78.2  ...  masculine        Kamino   Human\n## 23     Lando Calrissian   177.0   79.0  ...  masculine       Socorro   Human\n## 24                Lobot   175.0   79.0  ...  masculine        Bespin   Human\n## 26           Mon Mothma   150.0    NaN  ...   feminine     Chandrila   Human\n## 27         Arvel Crynyd     NaN    NaN  ...  masculine           NaN   Human\n## 30         Qui-Gon Jinn   193.0   89.0  ...  masculine           NaN   Human\n## 32        Finis Valorum   170.0    NaN  ...  masculine     Coruscant   Human\n## 40       Shmi Skywalker   163.0    NaN  ...   feminine      Tatooine   Human\n## 47           Mace Windu   188.0   84.0  ...  masculine    Haruun Kal   Human\n## 56         Gregar Typho   185.0   85.0  ...  masculine         Naboo   Human\n## 57                Cordé   157.0    NaN  ...   feminine         Naboo   Human\n## 58          Cliegg Lars   183.0    NaN  ...  masculine      Tatooine   Human\n## 62                Dormé   165.0    NaN  ...   feminine         Naboo   Human\n## 63                Dooku   193.0   80.0  ...  masculine       Serenno   Human\n## 64  Bail Prestor Organa   191.0    NaN  ...  masculine      Alderaan   Human\n## 65           Jango Fett   183.0   79.0  ...  masculine  Concord Dawn   Human\n## 70           Jocasta Nu   167.0    NaN  ...   feminine     Coruscant   Human\n## 78      Raymus Antilles   188.0   79.0  ...  masculine      Alderaan   Human\n## 81                 Finn     NaN    NaN  ...  masculine           NaN   Human\n## 82                  Rey     NaN    NaN  ...   feminine           NaN   Human\n## 83          Poe Dameron     NaN    NaN  ...  masculine           NaN   Human\n## 86        Padmé Amidala   165.0   45.0  ...   feminine         Naboo   Human\n## \n## [35 rows x 11 columns]\nstarwars.query(\"species == 'Human' & homeworld == 'Tatooine'\")\n\n# This is another option if you prefer to keep the queries separate\n# starwars.query(\"species == 'Human'\").query(\"homeworld == 'Tatooine'\")\n##                   name  height   mass  ...     gender homeworld species\n## 0       Luke Skywalker   172.0   77.0  ...  masculine  Tatooine   Human\n## 3          Darth Vader   202.0  136.0  ...  masculine  Tatooine   Human\n## 5            Owen Lars   178.0  120.0  ...  masculine  Tatooine   Human\n## 6   Beru Whitesun lars   165.0   75.0  ...   feminine  Tatooine   Human\n## 8    Biggs Darklighter   183.0   84.0  ...  masculine  Tatooine   Human\n## 10    Anakin Skywalker   188.0   84.0  ...  masculine  Tatooine   Human\n## 40      Shmi Skywalker   163.0    NaN  ...   feminine  Tatooine   Human\n## 58         Cliegg Lars   183.0    NaN  ...  masculine  Tatooine   Human\n## \n## [8 rows x 11 columns]\n\n\n\nIn base R, you would perform a filtering operation using subset\n\n# Get only the people\nsubset(starwars, species == \"Human\")\n## # A tibble: 35 × 14\n##    name        height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##    <chr>        <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n##  1 Luke Skywa…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n##  2 Darth Vader    202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n##  3 Leia Organa    150    49 brown   light   brown      19   fema… femin… Aldera…\n##  4 Owen Lars      178   120 brown,… light   blue       52   male  mascu… Tatooi…\n##  5 Beru White…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n##  6 Biggs Dark…    183    84 black   light   brown      24   male  mascu… Tatooi…\n##  7 Obi-Wan Ke…    182    77 auburn… fair    blue-g…    57   male  mascu… Stewjon\n##  8 Anakin Sky…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n##  9 Wilhuff Ta…    180    NA auburn… fair    blue       64   male  mascu… Eriadu \n## 10 Han Solo       180    80 brown   fair    brown      29   male  mascu… Corell…\n## # … with 25 more rows, 4 more variables: species <chr>, films <list>,\n## #   vehicles <list>, starships <list>, and abbreviated variable names\n## #   ¹​hair_color, ²​skin_color, ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n# Get only the people who come from Tatooine\nsubset(starwars, species == \"Human\" & homeworld == \"Tatooine\")\n## # A tibble: 8 × 14\n##   name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n##   <chr>         <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n## 1 Luke Skywal…    172    77 blond   fair    blue       19   male  mascu… Tatooi…\n## 2 Darth Vader     202   136 none    white   yellow     41.9 male  mascu… Tatooi…\n## 3 Owen Lars       178   120 brown,… light   blue       52   male  mascu… Tatooi…\n## 4 Beru Whites…    165    75 brown   light   blue       47   fema… femin… Tatooi…\n## 5 Biggs Darkl…    183    84 black   light   brown      24   male  mascu… Tatooi…\n## 6 Anakin Skyw…    188    84 blond   fair    blue       41.9 male  mascu… Tatooi…\n## 7 Shmi Skywal…    163    NA black   fair    brown      72   fema… femin… Tatooi…\n## 8 Cliegg Lars     183    NA brown   fair    blue       82   male  mascu… Tatooi…\n## # … with 4 more variables: species <chr>, films <list>, vehicles <list>,\n## #   starships <list>, and abbreviated variable names ¹​hair_color, ²​skin_color,\n## #   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\nNotice that with subset, you have to use & to join two logical statements; it does not by default take multiple successive arguments.\n\n\n\n\n\n\n17.3.1 Common Row Selection Tasks\nIn dplyr, there are a few helper functions which may be useful when constructing filter statements. In base R or python, these tasks are still important, and so I’ll do my best to show you easy ways to handle each task in each language.\n\n\n\n\n\n\nFiltering by row number\n\n\n\n\n\n\n\nR: dplyr\nPython\nBase R\n\n\n\nrow_number() is a helper function that is only used inside of another dplyr function (e.g. filter). You might want to keep only even rows, or only the first 10 rows in a table.\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n## Error in read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\"): could not find function \"read_csv\"\nfilter(poke, (row_number() %% 2 == 0)) \n## Error in filter(poke, (row_number()%%2 == 0)): object 'poke' not found\n# There are several pokemon who have multiple entries in the table,\n# so the pokedex_number doesn't line up with the row number.\n\n\n\nIn python, the easiest way to accomplish filtering by row number is by using .iloc. But, up until now, we’ve only talked about how Python creates slices using start:(end+1) notation. There is an additional option with slicing - start:(end+1):by. So if we want to get only even rows, we can use the index [::2], which will give us row 0, 2, 4, 6, … through the end of the dataset, because we didn’t specify the start and end portions of the slice.\nBecause Python is 0-indexed, using ::2 will give us the opposite set of rows from that returned in R, which is 1-indexed.\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\npoke.iloc[0::2]\n##       gen  pokedex_no  ...    type_1  type_2\n## 0       1           1  ...     Grass  Poison\n## 2       1           3  ...     Grass  Poison\n## 4       1           4  ...      Fire     NaN\n## 6       1           6  ...      Fire  Flying\n## 8       1           6  ...      Fire  Flying\n## ...   ...         ...  ...       ...     ...\n## 1516    9         999  ...     Ghost     NaN\n## 1518    9        1001  ...      Dark   Grass\n## 1520    9        1003  ...      Dark  Ground\n## 1522    9        1005  ...    Dragon    Dark\n## 1524    9        1007  ...  Fighting  Dragon\n## \n## [763 rows x 15 columns]\n\nIf we want to get only odd rows, we can use the index [1::2], which will start at row 1 and give us 1, 3, 5, …\n\n\nIn base R, we’d use seq() to create an index vector instead of using the approach in filter and evaluating the whole index for a logical condition. Alternately, we can use subset, which requires a logical condition, and use 1:nrow(poke) to create an index which we then use for deciding whether each row is even or odd.\n\npoke[seq(1, nrow(poke), 2),]\n## Error in eval(expr, envir, enclos): object 'poke' not found\n\nsubset(poke, 1:nrow(poke) %% 2 == 0)\n## Error in subset(poke, 1:nrow(poke)%%2 == 0): object 'poke' not found\n\nThis is less fun than using dplyr because you have to repeat the name of the dataset at least twice using base R, but either option will get you where you’re going. The real power of dplyr is in the collection of the full set of verbs with a consistent user interface; nothing done in dplyr is so special that it can’t be done in base R as well.\n\n\n\n\n\n\n\n\n\n\n\n\nSorting rows by variable values\n\n\n\n\n\nAnother common operation is to sort your data frame by the values of one or more variables.\n\n\nR: dplyr\nPython\nBase R\n\n\n\narrange() is a dplyr verb for sort rows in the table by one or more variables. It is often used with a helper function, desc(), which reverses the order of a variable, sorting it in descending order. Multiple arguments can be passed to arrange to sort the data frame by multiple columns hierarchically; each column can be modified with desc() separately.\n\narrange(poke, desc(total))\n## Error in arrange(poke, desc(total)): object 'poke' not found\n\n\n\nIn pandas, we use the sort_values function, which has an argument ascending. Multiple columns can be passed in to sort by multiple columns in a hierarchical manner.\n\npoke.sort_values(['total'], ascending = False)\n##       gen  pokedex_no  ...   type_1    type_2\n## 1370    8         890  ...   Poison    Dragon\n## 282     1         150  ...  Psychic  Fighting\n## 283     1         150  ...  Psychic       NaN\n## 584     3         384  ...   Dragon    Flying\n## 580     3         382  ...    Water       NaN\n## ...   ...         ...  ...      ...       ...\n## 1336    8         872  ...      Ice       Bug\n## 328     2         191  ...    Grass       NaN\n## 1283    8         824  ...      Bug       NaN\n## 1187    7         746  ...    Water       NaN\n## 1188    7         746  ...    Water       NaN\n## \n## [1526 rows x 15 columns]\n\n\n\nThe sort() function in R can be used to sort a vector, but when sorting a data frame we usually want to use the order() function instead. This is because sort() orders the values of the argument directly, where order() returns a sorted index.\n\nx <- c(32, 25, 98, 45, 31, 19, 5)\nsort(x)\n## [1]  5 19 25 31 32 45 98\norder(x)\n## [1] 7 6 2 5 1 4 3\n\nWhen working with a data frame, we want to sort the entire data frame’s rows by the variables we choose; it is easiest to do this using an index to reorder the rows.\n\npoke[order(poke$total, decreasing = T),]\n## Error in eval(expr, envir, enclos): object 'poke' not found\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeep the top \\(n\\) values of a variable\n\n\n\n\n\n\n\nR: dplyr\nPython\nBase R\n\n\n\nslice_max() will keep the top values of a specified variable. This is like a filter statement, but it’s a shortcut built to handle a common task. You could write a filter statement that would do this, but it would take a few more lines of code.\n\nslice_max(poke, order_by = total, n = 5)\n## Error in slice_max(poke, order_by = total, n = 5): object 'poke' not found\n\nBy default, slice_max() returns values tied with the nth value as well, which is why our result has 6 rows.\n\nslice_max(poke, order_by = total, n = 5, with_ties = F) \n## Error in slice_max(poke, order_by = total, n = 5, with_ties = F): object 'poke' not found\n\nOf course, there is a similar slice_min() function as well:\n\nslice_min(poke, order_by = total, n = 5)\n## Error in slice_min(poke, order_by = total, n = 5): object 'poke' not found\n\nslice_max and slice_min also take a prop argument that gives you a certain proportion of the values:\n\nslice_max(poke, order_by = total, prop = .01)\n## Error in slice_max(poke, order_by = total, prop = 0.01): object 'poke' not found\n\n\n\nIn Python, nlargest and nsmallest work roughly the same as dplyr’s slice_max and slice_min for integer counts.\n\npoke.nlargest(5, 'total')\n##       gen  pokedex_no  ...   type_1    type_2\n## 1370    8         890  ...   Poison    Dragon\n## 282     1         150  ...  Psychic  Fighting\n## 283     1         150  ...  Psychic       NaN\n## 584     3         384  ...   Dragon    Flying\n## 580     3         382  ...    Water       NaN\n## \n## [5 rows x 15 columns]\npoke.nsmallest(5, 'total')\n##       gen  pokedex_no  ... type_1 type_2\n## 1187    7         746  ...  Water    NaN\n## 1188    7         746  ...  Water    NaN\n## 328     2         191  ...  Grass    NaN\n## 1283    8         824  ...    Bug    NaN\n## 1336    8         872  ...    Ice    Bug\n## \n## [5 rows x 15 columns]\n\nTo get proportions, though, we have to do some math:\n\npoke.nlargest(int(len(poke)*0.01), 'total')\n##       gen  pokedex_no  ...   type_1    type_2\n## 1370    8         890  ...   Poison    Dragon\n## 282     1         150  ...  Psychic  Fighting\n## 283     1         150  ...  Psychic       NaN\n## 584     3         384  ...   Dragon    Flying\n## 580     3         382  ...    Water       NaN\n## 582     3         383  ...   Ground      Fire\n## 1257    7         800  ...  Psychic    Dragon\n## 1258    7         800  ...  Psychic    Dragon\n## 1259    7         800  ...  Psychic    Dragon\n## 779     4         493  ...   Normal       NaN\n## 1126    6         718  ...   Dragon    Ground\n## 1127    6         718  ...   Dragon    Ground\n## 1128    6         718  ...   Dragon    Ground\n## 405     2         248  ...     Rock      Dark\n## 567     3         373  ...   Dragon    Flying\n## \n## [15 rows x 15 columns]\npoke.nsmallest(int(len(poke)*0.01), 'total')\n##       gen  pokedex_no  ...   type_1  type_2\n## 1187    7         746  ...    Water     NaN\n## 1188    7         746  ...    Water     NaN\n## 328     2         191  ...    Grass     NaN\n## 1283    8         824  ...      Bug     NaN\n## 1336    8         872  ...      Ice     Bug\n## 465     3         298  ...   Normal   Fairy\n## 616     4         401  ...      Bug     NaN\n## 13      1          10  ...      Bug     NaN\n## 16      1          13  ...      Bug  Poison\n## 431     3         265  ...      Bug     NaN\n## 446     3         280  ...  Psychic   Fairy\n## 248     1         129  ...    Water     NaN\n## 524     3         349  ...    Water     NaN\n## 1032    6         664  ...      Bug     NaN\n## 1237    7         789  ...  Psychic     NaN\n## \n## [15 rows x 15 columns]\n\n\n\nThe simplest way to do this type of task with base R is to combine the order() function and indexing. In the case of selecting the top 1% of rows, we need to use round(nrow(poke)*.01) to get an integer.\n\npoke[order(poke$total, decreasing = T)[1:5],]\n## Error in eval(expr, envir, enclos): object 'poke' not found\npoke[order(poke$total, decreasing = T)[1:round(nrow(poke)*.01)],]\n## Error in eval(expr, envir, enclos): object 'poke' not found\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Filtering\n\n\n\n\n\nProblem\nR: dplyr\nPython\nBase R\n\n\n\nUsing the Pokemon data, can you create a new data frame that has only water type Pokemon? Can you write a filter statement that looks for any Pokemon which has water type for either type1 or type2?\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n## Error in read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\"): could not find function \"read_csv\"\n\nfilter(poke, type_1 == \"Water\")\n## Error in filter(poke, type_1 == \"Water\"): object 'poke' not found\n\nfilter(poke, type_1 == \"Water\" | type_2 == \"Water\")\n## Error in filter(poke, type_1 == \"Water\" | type_2 == \"Water\"): object 'poke' not found\n# The conditions have to be separated by |, which means \"or\"\n\n\n\n\nimport pandas as pd\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke.query(\"type_1=='Water'\")\n##       gen  pokedex_no  ... type_1   type_2\n## 9       1           7  ...  Water      NaN\n## 10      1           8  ...  Water      NaN\n## 11      1           9  ...  Water      NaN\n## 12      1           9  ...  Water      NaN\n## 98      1          54  ...  Water      NaN\n## ...   ...         ...  ...    ...      ...\n## 1467    9         964  ...  Water      NaN\n## 1468    9         964  ...  Water      NaN\n## 1469    9         964  ...  Water      NaN\n## 1481    9         976  ...  Water  Psychic\n## 1482    9         977  ...  Water      NaN\n## \n## [179 rows x 15 columns]\npoke.query(\"type_1=='Water'|type_2=='Water'\")\n# The conditions have to be separated by |, which means \"or\"\n##       gen  pokedex_no  ...  type_1 type_2\n## 9       1           7  ...   Water    NaN\n## 10      1           8  ...   Water    NaN\n## 11      1           9  ...   Water    NaN\n## 12      1           9  ...   Water    NaN\n## 98      1          54  ...   Water    NaN\n## ...   ...         ...  ...     ...    ...\n## 1488    9         978  ...  Dragon  Water\n## 1489    9         978  ...  Dragon  Water\n## 1490    9         978  ...  Dragon  Water\n## 1491    9         978  ...  Dragon  Water\n## 1505    9         991  ...     Ice  Water\n## \n## [221 rows x 15 columns]\n\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n## Error in read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\"): could not find function \"read_csv\"\n\nsubset(poke, type_1 == \"Water\")\n## Error in subset(poke, type_1 == \"Water\"): object 'poke' not found\n\nsubset(poke, type_1 == \"Water\" | type_2 == \"Water\")\n## Error in subset(poke, type_1 == \"Water\" | type_2 == \"Water\"): object 'poke' not found\n# The conditions have to be separated by |, which means \"or\""
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#select-pick-columns",
    "href": "part-wrangling/03-data-cleaning.html#select-pick-columns",
    "title": "17  Data Cleaning",
    "section": "\n17.4 Select: Pick columns",
    "text": "17.4 Select: Pick columns\nSometimes, we don’t want to work with a set of 50 variables when we’re only interested in 5. When that happens, we might be able to pick the variables we want by index (e.g. df[, c(1, 3, 5)]), but this can get tedious.\n\n\nR: dplyr\nPython\nBase R\n\n\n\nIn dplyr, the function to pick a few columns is select(). The syntax from the help file (?select) looks deceptively simple.\n\nselect(.data, …)\n\nSo as with just about every other tidyverse function, the first argument in a select statement is the data. After that, though, you can put just about anything that R can interpret. ... means something along the lines of “put in any additional arguments that make sense in context or might be passed on to other functions”.\nSo what can go in there?\n\n\n\n\n\n\nWays to select variables in dplyr\n\n\n\n\n\nFirst, dplyr aims to work with standard R syntax, making it intuitive (and also, making it work with variable names instead of just variable indices).3\nMost dplyr commands work with “bare” variable names - you don’t need to put the variable name in quotes to reference it. There are a few exceptions to this rule, but they’re very explicitly exceptions.\n\nvar3:var5: select(df, var3:var5) will give you a data frame with columns var3, anything between var3 and var 5, and var5\n\n!(<set of variables>) will give you any columns that aren’t in the set of variables in parentheses\n\n\n(<set of vars 1>) & (<set of vars 2>) will give you any variables that are in both set 1 and set 2. (<set of vars 1>) | (<set of vars 2>) will give you any variables that are in either set 1 or set 2.\n\nc() combines sets of variables.\n\n\n\ndplyr also defines a lot of variable selection “helpers” that can be used inside select() statements. These statements work with bare column names (so you don’t have to put quotes around the column names when you use them).\n\n\neverything() matches all variables\n\nlast_col() matches the last variable. last_col(offset = n) selects the n-th to last variable.\n\nstarts_with(\"xyz\") will match any columns with names that start with xyz. Similarly, ends_with() does exactly what you’d expect as well.\n\ncontains(\"xyz\") will match any columns with names containing the literal string “xyz”. Note, contains does not work with regular expressions (you don’t need to know what that means right now).\n\nmatches(regex) takes a regular expression as an argument and returns all columns matching that expression.\n\nnum_range(prefix, range) selects any columns that start with prefix and have numbers matching the provided numerical range.\n\nThere are also selectors that deal with character vectors. These can be useful if you have a list of important variables and want to just keep those variables.\n\n\nall_of(char) matches all variable names in the character vector char. If one of the variables doesn’t exist, this will return an error.\n\nany_of(char) matches the contents of the character vector char, but does not throw an error if the variable doesn’t exist in the data set.\n\nThere’s one final selector -\n\n\nwhere() applies a function to each variable and selects those for which the function returns TRUE. This provides a lot of flexibility and opportunity to be creative.\n\n\n\n\nLet’s try these selector functions out and see what we can accomplish!\n\nlibrary(nycflights13)\ndata(flights)\nstr(flights)\n## tibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n##  $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n##  $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n##  $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n##  $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n##  $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n##  $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n##  $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n##  $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n##  $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n##  $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n##  $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n##  $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n##  $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n##  $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n##  $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n##  $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n##  $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n##  $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\nWe’ll start out with the nycflights13 package, which contains information on all flights that left a NYC airport to destinations in the US, Puerto Rico, and the US Virgin Islands.\n\n\n\n\n\n\nTip\n\n\n\nYou might want to try out your EDA (Exploratory Data Analysis) skills to see what you can find out about the dataset, before seeing how select() works.\n\n\nWe could get a data frame of departure information for each flight:\n\nselect(flights, flight, year:day, tailnum, origin, matches(\"dep\"))\n## # A tibble: 336,776 × 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     <int> <int> <int> <int> <chr>   <chr>     <int>          <int>     <dbl>\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # … with 336,766 more rows\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\nflights %>%\n  select(carrier:dest, everything())\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    <chr>    <int> <chr>   <chr>  <chr> <int> <int> <int>   <int>   <int>   <dbl>\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time <int>,\n## #   sched_arr_time <int>, arr_delay <dbl>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n\nNote that everything() won’t duplicate columns you’ve already added.\nExploring the difference between bare name selection and all_of()/any_of()\n\nflights %>%\n  select(carrier, flight, tailnum, matches(\"time\"))\n## # A tibble: 336,776 × 9\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    <chr>    <int> <chr>      <int>          <int>    <int>         <int>   <dbl>\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, 1 more variable: time_hour <dttm>, and abbreviated\n## #   variable names ¹​sched_arr_time, ²​air_time\n\nvarlist <- c(\"carrier\", \"flight\", \"tailnum\",\n             \"dep_time\", \"sched_dep_time\", \"arr_time\", \"sched_arr_time\",\n             \"air_time\")\n\nflights %>%\n  select(all_of(varlist))\n## # A tibble: 336,776 × 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    <chr>    <int> <chr>      <int>          <int>    <int>         <int>   <dbl>\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, and abbreviated variable names ¹​sched_arr_time,\n## #   ²​air_time\n\nvarlist <- c(varlist, \"whoops\")\n\nflights %>%\n  select(all_of(varlist)) # this errors out b/c whoops doesn't exist\n## Error in `select()`:\n## ! Problem while evaluating `all_of(varlist)`.\n\nflights %>%\nselect(any_of(varlist)) # this runs just fine\n## # A tibble: 336,776 × 8\n##    carrier flight tailnum dep_time sched_dep_time arr_time sched_arr_t…¹ air_t…²\n##    <chr>    <int> <chr>      <int>          <int>    <int>         <int>   <dbl>\n##  1 UA        1545 N14228       517            515      830           819     227\n##  2 UA        1714 N24211       533            529      850           830     227\n##  3 AA        1141 N619AA       542            540      923           850     160\n##  4 B6         725 N804JB       544            545     1004          1022     183\n##  5 DL         461 N668DN       554            600      812           837     116\n##  6 UA        1696 N39463       554            558      740           728     150\n##  7 B6         507 N516JB       555            600      913           854     158\n##  8 EV        5708 N829AS       557            600      709           723      53\n##  9 B6          79 N593JB       557            600      838           846     140\n## 10 AA         301 N3ALAA       558            600      753           745     138\n## # … with 336,766 more rows, and abbreviated variable names ¹​sched_arr_time,\n## #   ²​air_time\n\nSo for now, at least in R, you know how to cut your data down to size rowwise (with filter) and column-wise (with select).\n\n\nFirst, let’s install the nycflights13 package[3] in python with pip install nycflights13.\n\nfrom nycflights13 import flights\n\nSelect operations are not as easy in python as they are using select() with helpers.\n\ncols = flights.columns\n\n# Rearrange column order by manual indexing\nx = cols[9:13].append(cols[0:9])\nx = x.append(cols[13:19])\n\n# Then use the index to rearrange the columns\nflights.loc[:,x]\n##        carrier  flight tailnum  ... hour  minute             time_hour\n## 0           UA    1545  N14228  ...    5      15  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...    5      29  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...    5      40  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...    5      45  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...    6       0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...  ...     ...                   ...\n## 336771      9E    3393     NaN  ...   14      55  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...   22       0  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...   12      10  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...   11      59  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...    8      40  2013-09-30T12:00:00Z\n## \n## [336776 rows x 19 columns]\n\n\n17.4.0.1 List Comprehensions\nIn Python, there are certain shorthands called “list comprehensions” [4] that can perform similar functions to e.g. the matches() function in dplyr.\nSuppose we want to get all columns containing the word ‘time’. We could iterate through the list of columns (flights.columns) and add the column name any time we detect the word ‘time’ within. That is essentially what the following code does:\n\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\ntimecols\n## ['dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour']\n\nExplaining the code: - for col in flights.columns iterates through the list of columns, storing each column name in the variable col - if 'time' in col detects the presence of the word ‘time’ in the column name stored in col - the col out front adds the column name in the variable col to the array of columns to keep\n\n17.4.0.2 Selecting columns in Python\n\n# This gets all columns that contain time\ntimecols = [col for col in flights.columns if 'time' in col]\n# Other columns\nselcols = [\"carrier\", \"flight\", \"tailnum\"]\n# Combine the two lists\nselcols.extend(timecols)\n\n# Subset the data frame\nflights.loc[:,selcols]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\nselcols.extend([\"whoops\"])\nselcols\n\n# Subset the data frame\n## ['carrier', 'flight', 'tailnum', 'dep_time', 'sched_dep_time', 'arr_time', 'sched_arr_time', 'air_time', 'time_hour', 'whoops']\nflights.loc[:,selcols]\n\n# Error-tolerance - use list comprehension to check if \n# variable names are in the data frame\n## Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: \"['whoops'] not in index\"\nselcols_fixed = [x for x in selcols if x in flights.columns]\nflights.loc[:,selcols_fixed]\n##        carrier  flight tailnum  ...  sched_arr_time  air_time             time_hour\n## 0           UA    1545  N14228  ...             819     227.0  2013-01-01T10:00:00Z\n## 1           UA    1714  N24211  ...             830     227.0  2013-01-01T10:00:00Z\n## 2           AA    1141  N619AA  ...             850     160.0  2013-01-01T10:00:00Z\n## 3           B6     725  N804JB  ...            1022     183.0  2013-01-01T10:00:00Z\n## 4           DL     461  N668DN  ...             837     116.0  2013-01-01T11:00:00Z\n## ...        ...     ...     ...  ...             ...       ...                   ...\n## 336771      9E    3393     NaN  ...            1634       NaN  2013-09-30T18:00:00Z\n## 336772      9E    3525     NaN  ...            2312       NaN  2013-10-01T02:00:00Z\n## 336773      MQ    3461  N535MQ  ...            1330       NaN  2013-09-30T16:00:00Z\n## 336774      MQ    3572  N511MQ  ...            1344       NaN  2013-09-30T15:00:00Z\n## 336775      MQ    3531  N839MQ  ...            1020       NaN  2013-09-30T12:00:00Z\n## \n## [336776 rows x 9 columns]\n\n\n\n\nIn base R, we typically select columns by name or index directly. This is nowhere near as convenient, of course, but there are little shorthand ways to replicate the functionality of e.g. matches in dplyr.\ngrepl is a shorthand function for grep, which searches for a pattern in a vector of strings. grepl returns a logical vector indicating whether the pattern (\"dep\", in this case) was found in the vector (names(flights), in this case).\n\n\ndepcols <- names(flights)[grepl(\"dep\", names(flights))]\ncollist <- c(\"flight\", \"year\", \"month\", \"day\", \"tailnum\", \"origin\", depcols)\n\nflights[,collist]\n## # A tibble: 336,776 × 9\n##    flight  year month   day tailnum origin dep_time sched_dep_time dep_delay\n##     <int> <int> <int> <int> <chr>   <chr>     <int>          <int>     <dbl>\n##  1   1545  2013     1     1 N14228  EWR         517            515         2\n##  2   1714  2013     1     1 N24211  LGA         533            529         4\n##  3   1141  2013     1     1 N619AA  JFK         542            540         2\n##  4    725  2013     1     1 N804JB  JFK         544            545        -1\n##  5    461  2013     1     1 N668DN  LGA         554            600        -6\n##  6   1696  2013     1     1 N39463  EWR         554            558        -4\n##  7    507  2013     1     1 N516JB  EWR         555            600        -5\n##  8   5708  2013     1     1 N829AS  LGA         557            600        -3\n##  9     79  2013     1     1 N593JB  JFK         557            600        -3\n## 10    301  2013     1     1 N3ALAA  LGA         558            600        -2\n## # … with 336,766 more rows\n\nPerhaps we want the plane and flight ID information to be the first columns:\n\nnew_order <- names(flights)\nnew_order <- new_order[c(10:14, 1:9, 15:19)]\n\nflights[,new_order]\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    <chr>    <int> <chr>   <chr>  <chr> <int> <int> <int>   <int>   <int>   <dbl>\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time <int>,\n## #   sched_arr_time <int>, arr_delay <dbl>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n\nThis is less convenient than dplyr::everything in part because it depends on us to get the column indexes right.\n\n\n\n\n\n\n\n\n\ndplyr::relocate\n\n\n\nAnother handy dplyr function is relocate; while you definitely can do this operation in many, many different ways, it may be simpler to do it using relocate. But, I’m covering relocate here mostly because it also comes with this amazing cartoon illustration.\n\n\nrelocate lets you rearrange columns (by Allison Horst)\n\n\n\n# Move flight specific info to the front\ndata(flights, package = \"nycflights13\")\nrelocate(flights, carrier:dest, everything())\n## # A tibble: 336,776 × 19\n##    carrier flight tailnum origin dest   year month   day dep_t…¹ sched…² dep_d…³\n##    <chr>    <int> <chr>   <chr>  <chr> <int> <int> <int>   <int>   <int>   <dbl>\n##  1 UA        1545 N14228  EWR    IAH    2013     1     1     517     515       2\n##  2 UA        1714 N24211  LGA    IAH    2013     1     1     533     529       4\n##  3 AA        1141 N619AA  JFK    MIA    2013     1     1     542     540       2\n##  4 B6         725 N804JB  JFK    BQN    2013     1     1     544     545      -1\n##  5 DL         461 N668DN  LGA    ATL    2013     1     1     554     600      -6\n##  6 UA        1696 N39463  EWR    ORD    2013     1     1     554     558      -4\n##  7 B6         507 N516JB  EWR    FLL    2013     1     1     555     600      -5\n##  8 EV        5708 N829AS  LGA    IAD    2013     1     1     557     600      -3\n##  9 B6          79 N593JB  JFK    MCO    2013     1     1     557     600      -3\n## 10 AA         301 N3ALAA  LGA    ORD    2013     1     1     558     600      -2\n## # … with 336,766 more rows, 8 more variables: arr_time <int>,\n## #   sched_arr_time <int>, arr_delay <dbl>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​dep_time, ²​sched_dep_time, ³​dep_delay\n\n# move numeric variables to the front\nflights %>% relocate(where(is.numeric))\n## # A tibble: 336,776 × 19\n##     year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ flight\n##    <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl>  <int>\n##  1  2013     1     1      517         515       2     830     819      11   1545\n##  2  2013     1     1      533         529       4     850     830      20   1714\n##  3  2013     1     1      542         540       2     923     850      33   1141\n##  4  2013     1     1      544         545      -1    1004    1022     -18    725\n##  5  2013     1     1      554         600      -6     812     837     -25    461\n##  6  2013     1     1      554         558      -4     740     728      12   1696\n##  7  2013     1     1      555         600      -5     913     854      19    507\n##  8  2013     1     1      557         600      -3     709     723     -14   5708\n##  9  2013     1     1      557         600      -3     838     846      -8     79\n## 10  2013     1     1      558         600      -2     753     745       8    301\n## # … with 336,766 more rows, 9 more variables: air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, carrier <chr>, tailnum <chr>, origin <chr>,\n## #   dest <chr>, time_hour <dttm>, and abbreviated variable names\n## #   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#mutate-add-and-transform-variables",
    "href": "part-wrangling/03-data-cleaning.html#mutate-add-and-transform-variables",
    "title": "17  Data Cleaning",
    "section": "\n17.5 Mutate: Add and transform variables",
    "text": "17.5 Mutate: Add and transform variables\nUp to this point, we’ve been primarily focusing on how to decrease the dimensionality of our dataset in various ways. But frequently, we also need to add columns for derived measures (e.g. BMI from weight and height information), change units, and replace missing or erroneous observations. The tidyverse verb for this is mutate, but in base R and python, we’ll simply use assignment to add columns to our data frames.\n\n\nMutate (by Allison Horst)\n\n\nWe’ll use the Pokemon data to demonstrate. Some Pokemon have a single “type”, which is usually elemental, such as Water, Ice, Fire, etc., but others have two. Let’s add a column that indicates how many types a pokemon has.\n\n\nBase R\nR: dplyr\nPython\n\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n## Error in read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\"): could not find function \"read_csv\"\n\npoke$no_types <- 1 # set a default value\n## Error in poke$no_types <- 1: object 'poke' not found\npoke$no_types[!is.na(poke$type_2)] <- 2 # set the value if type_2 is not NA\n## Error in poke$no_types[!is.na(poke$type_2)] <- 2: object 'poke' not found\n\n# This is a bit faster\npoke$no_types <- ifelse(is.na(poke$type_2), 1, 2)\n## Error in ifelse(is.na(poke$type_2), 1, 2): object 'poke' not found\n\n# This checks number of types vs. value of type_2 (sanity)\nt(table(poke$type_2, poke$no_types, useNA = 'ifany'))\n## Error in table(poke$type_2, poke$no_types, useNA = \"ifany\"): object 'poke' not found\n\nNotice that we had to type the name of the dataset at least 3 times to perform the operation we were looking for. I could reduce that to 2x with the ifelse function, but it’s still a lot of typing.\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n## Error in read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\"): could not find function \"read_csv\"\n\npoke <- poke %>%\n  mutate(no_types = if_else(is.na(type_2), 1, 2))\n## Error in mutate(., no_types = if_else(is.na(type_2), 1, 2)): object 'poke' not found\n\nselect(poke, type_2, no_types) %>% table(useNA = 'ifany') %>% t()\n## Error in select(poke, type_2, no_types): object 'poke' not found\n\nThe last 2 rows are just to organize the output - we keep only the two variables we’re working with, and get a crosstab.\n\n\nIn python, this type of variable operation (replacing one value with another) can be most easily done with the replace function, which takes arguments (thing_to_replace, value_to_replace_with).\n\nimport pandas as pd\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke[\"no_types\"] = 1 # default value\npoke.loc[~poke.type_2.isna(), \"no_types\"] = 2 # change those with a defined type 2\n\nAnother function that may be useful is the assign function, which can be used to create new variables if you don’t want to use the [\"new_col\"] notation. In some circumstances, .assign(var = ...) is a bit easier to work with because Python distinguishes between modifications to data and making a copy of the entire data frame (which is something I’d like to not get into right now for simplicity’s sake).\n\n\n\nThe learning curve here isn’t actually knowing how to assign new variables (though that’s important). The challenge comes when you want to do something new and have to figure out how to e.g. use find and replace in a string, or work with dates and times, or recode variables.\n\n\n\n\n\n\nMutate and new challenges\n\n\n\n\n\nI’m not going to be able to teach you how to handle every mutate statement task you’ll come across (people invent new ways to screw up data all the time!) but my goal is instead to teach you how to read documentation, google things intelligently, and to understand what you’re reading enough to actually implement it. This is something that comes with practice (and lots of googling, stack overflow searches, etc.).\nGoogle and StackOverflow are very common and important programming skills!\n\n\nSource\n\n\n\n\nSource\n\n\nIn this textbook, the examples will expose you to solutions to common problems (or require that you do some basic reading yourself); unfortunately, there are too many common problems for us to work through line-by-line.\nPart of the goal of this textbook is to help you learn how to read through a package description and evaluate whether the package will do what you want. We’re going to try to build some of those skills starting now. It would be relatively easy to teach you how to do a set list of tasks, but you’ll be better statisticians and programmers if you learn the skills to solve niche problems on your own.\n\n\nApologies for the noninclusive language, but the sentiment is real. Source\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere is a quick table of places to look in R and python to solve some of the more common problems.\n\n\nProblem\nR\nPython\n\n\n\nDates and Times\n\nlubridate package (esp. ymd_hms() and variants, decimal_date(), and other convenience functions)\n\npandas has some date time support by default; see the datetime module for more functionality.\n\n\nString manipulation\n\nstringr package\nQuick Tips [5], Whirlwind Tour of Python chapter [6]"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#summarize",
    "href": "part-wrangling/03-data-cleaning.html#summarize",
    "title": "17  Data Cleaning",
    "section": "\n17.6 Summarize",
    "text": "17.6 Summarize\nThe next verb is one that we’ve already implicitly seen in action: summarize takes a data frame with potentially many rows of data and reduces it down to one row of data using some function. You have used it to get single-row summaries of vectorized data in R, and we’ve used e.g. group_by + count in Python to perform certain tasks as well.\nHere (in a trivial example), I compute the overall average HP of a Pokemon in each generation, as well as the average number of characters in their name. Admittedly, that last computation is a bit silly, but it’s mostly for demonstration purposes.\n\n\nR: dplyr\nPython\n\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n## Error in read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\"): could not find function \"read_csv\"\npoke %>%\n  mutate(name_chr = nchar(name)) %>%\n  summarize(n = max(pokedex_no), hp = mean(hp), name_chr = mean(name_chr))\n## Error in mutate(., name_chr = nchar(name)): object 'poke' not found\n\n\n\nIn python, instead of a summarize function, there are a number of shorthand functions that we often use to summarize things, such as mean. You can also build custom summary functions [7], or use the agg() function to define multiple summary variables. agg() will even let you use different summary functions for each variable, just like summarize.\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke = poke.assign(name_length = poke.name.str.len())\npoke[['hp', 'name_length']].mean()\n## hp             71.178244\n## name_length     7.545216\n## dtype: float64\npoke[['hp', 'name_length']].agg(['mean', 'min'])\n##              hp  name_length\n## mean  71.178244     7.545216\n## min    1.000000     3.000000\npoke[['pokedex_no', 'hp', 'name_length']].agg({'pokedex_no':'nunique', 'hp':'mean', 'name_length':'mean'})\n## pokedex_no     1008.000000\n## hp               71.178244\n## name_length       7.545216\n## dtype: float64\n\n\n\n\nThe real power of summarize, though, is in combination with Group By. We’ll see more summarize examples, but it’s easier to make good examples when you have all the tools - it’s hard to demonstrate how to use a hammer if you don’t also have a nail."
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#group-by-power",
    "href": "part-wrangling/03-data-cleaning.html#group-by-power",
    "title": "17  Data Cleaning",
    "section": "\n17.7 Group By + (?) = Power!",
    "text": "17.7 Group By + (?) = Power!\nFrequently, we have data that is more specific than the data we need - for instance, I may have observations of the temperature at 15-minute intervals, but I might want to record the daily high and low value. To do this, I need to\n\nsplit my dataset into smaller datasets - one for each day\ncompute summary values for each smaller dataset\nput my summarized data back together into a single dataset\n\nThis is known as the split-apply-combine [9] or sometimes, map-reduce [10] strategy (though map-reduce is usually on specifically large datasets and performed in parallel).\nIn tidy parlance, group_by is the verb that accomplishes the first task. summarize accomplishes the second task and implicitly accomplishes the third as well.\n\n\nR: dplyr\nPython\n\n\n\n\npoke <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n## Error in read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\"): could not find function \"read_csv\"\npoke %>%\n  mutate(name_chr = nchar(name)) %>%\n  group_by(gen) %>%\n  summarize(n = length(unique(pokedex_no)), hp = mean(hp), name_chr = mean(name_chr))\n## Error in mutate(., name_chr = nchar(name)): object 'poke' not found\n\n\n\n\nimport pandas as pd\n\npoke = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/clean/pokemon_gen_1-9.csv\")\n\npoke = poke.assign(name_length = poke.name.str.len())\npoke.groupby('gen')[['hp', 'name_length']].mean()\n##             hp  name_length\n## gen                        \n## 1    65.333333     7.231579\n## 2    71.024194     7.362903\n## 3    65.777202     7.160622\n## 4    69.382022     6.853933\n## 5    75.827004     7.767932\n## 6    72.882353     7.470588\n## 7    73.233083     8.037594\n## 8    77.940299     8.014925\n## 9    75.756098     8.658537\npoke.groupby('gen')[['hp', 'name_length']].agg(['mean', 'min'])\n##             hp     name_length    \n##           mean min        mean min\n## gen                               \n## 1    65.333333  10    7.231579   3\n## 2    71.024194  20    7.362903   4\n## 3    65.777202   1    7.160622   4\n## 4    69.382022  20    6.853933   4\n## 5    75.827004  30    7.767932   4\n## 6    72.882353  38    7.470588   5\n## 7    73.233083  25    8.037594   6\n## 8    77.940299  25    8.014925   4\n## 9    75.756098  10    8.658537   5\npoke.groupby('gen')[['pokedex_no', 'hp', 'name_length']].agg({'pokedex_no':'nunique', 'hp':'mean', 'name_length':'mean'})\n##      pokedex_no         hp  name_length\n## gen                                    \n## 1           151  65.333333     7.231579\n## 2           100  71.024194     7.362903\n## 3           135  65.777202     7.160622\n## 4           107  69.382022     6.853933\n## 5           156  75.827004     7.767932\n## 6            72  72.882353     7.470588\n## 7            88  73.233083     8.037594\n## 8            96  77.940299     8.014925\n## 9           103  75.756098     8.658537\n\n\n\n\nWhen you group_by a variable, your result carries this grouping with it. In R, summarize will remove one layer of grouping (by default), but if you ever want to return to a completely ungrouped data set, you should use the ungroup() command. In Python, you should consider using reset_index or grouped_thing.obj() to access the original information[11].\n\n\nThe ungroup() command is just as important as the group_by() command! (by Allison Horst)\n\n\n\n\n\n\n\n\nStorms Example\n\n\n\nLet’s try a non-trivial example, using the storms dataset that is part of the dplyr package.\n\n\nR\nPython\n\n\n\n\nlibrary(dplyr)\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\nstorms\n## # A tibble: 11,859 × 13\n##    name   year month   day  hour   lat  long status        categ…¹  wind press…²\n##    <chr> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>         <ord>   <int>   <int>\n##  1 Amy    1975     6    27     0  27.5 -79   tropical dep… -1         25    1013\n##  2 Amy    1975     6    27     6  28.5 -79   tropical dep… -1         25    1013\n##  3 Amy    1975     6    27    12  29.5 -79   tropical dep… -1         25    1013\n##  4 Amy    1975     6    27    18  30.5 -79   tropical dep… -1         25    1013\n##  5 Amy    1975     6    28     0  31.5 -78.8 tropical dep… -1         25    1012\n##  6 Amy    1975     6    28     6  32.4 -78.7 tropical dep… -1         25    1012\n##  7 Amy    1975     6    28    12  33.3 -78   tropical dep… -1         25    1011\n##  8 Amy    1975     6    28    18  34   -77   tropical dep… -1         30    1006\n##  9 Amy    1975     6    29     0  34.4 -75.8 tropical sto… 0          35    1004\n## 10 Amy    1975     6    29     6  34   -74.8 tropical sto… 0          40    1002\n## # … with 11,849 more rows, 2 more variables:\n## #   tropicalstorm_force_diameter <int>, hurricane_force_diameter <int>, and\n## #   abbreviated variable names ¹​category, ²​pressure\n\nstorms <- storms %>%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n\n\n\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://github.com/srvanderplas/datasets/blob/main/clean/storms.csv\")\n\n# Construct a time variable that behaves like a number but is formatted as a date\n## Error in py_call_impl(callable, dots$args, dots$keywords): pandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 28, saw 367\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove month/day/hour \n# (keep year for ID purposes, names are reused)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'storms' is not defined\nstorms = storms.drop([\"month\", \"day\", \"hour\"], axis = 1)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'storms' is not defined\n\n\n\n\nWe have named storms, observation time, storm location, status, wind, pressure, and diameter (for tropical storms and hurricanes).\nOne thing we might want to know is at what point each storm was the strongest. Let’s define strongest in the following way:\n\nThe points where the storm is at its lowest atmospheric pressure (generally, the lower the atmospheric pressure, the more trouble a tropical disturbance will cause).\nIf there’s a tie, we might want to know when the maximum wind speed occurred.\nIf that still doesn’t get us a single row for each observation, lets just pick out the status and category (these are determined by wind speed, so they should be the same if maximum wind speed is the same) and compute the average time where this occurred.\n\nLet’s start by translating these criteria into basic operations. I’ll use dplyr function names here, but I’ll also specify what I mean when there’s a conflict (e.g. filter in dplyr means something different than filter in python).\nInitial attempt:\n\n\nFor each storm (group_by),\nwe need the point where the storm has lowest atmospheric pressure. (filter - pick the row with the lowest pressure).\n\nThen we read the next part: “If there is a tie, pick the maximum wind speed.”\n\ngroup_by\n\narrange by ascending pressure and descending wind speed\n\nfilter - pick the row(s) which have the lowest pressure and highest wind speed\n\nThen, we read the final condition: if there is still a tie, pick the status and category and compute the average time.\n\ngroup_by\n\narrange by ascending pressure and descending wind speed (this is optional if we write our filter in a particular way)\n\nfilter - pick the row(s) which have the lowest pressure and highest wind speed\n\nsummarize - compute the average time and category (if there are multiple rows)\n\nLet’s write the code, now that we have the order of operations straight!\n\n\nR\nPython\n\n\n\n\nmax_power_storm <- storms %>%\n  # Storm names can be reused, so we need to have year to be sure it's the same instance\n  group_by(name, year) %>%\n  filter(pressure == min(pressure, na.rm = T)) %>%\n  filter(wind == max(wind, na.rm = T)) %>%\n  summarize(pressure = mean(pressure), \n            wind = mean(wind), \n            category = unique(category), \n            status = unique(status), \n            time = mean(time)) %>%\n  arrange(time) %>%\n  ungroup()\nmax_power_storm\n## # A tibble: 512 × 7\n##    name      year pressure  wind category status         time               \n##    <chr>    <dbl>    <dbl> <dbl> <ord>    <chr>          <dttm>             \n##  1 Amy       1975      981    60 0        tropical storm 1975-07-02 12:00:00\n##  2 Caroline  1975      963   100 3        hurricane      1975-08-31 06:00:00\n##  3 Doris     1975      965    95 2        hurricane      1975-09-02 21:00:00\n##  4 Belle     1976      957   105 3        hurricane      1976-08-09 00:00:00\n##  5 Gloria    1976      970    80 1        hurricane      1976-09-30 00:00:00\n##  6 Anita     1977      926   150 5        hurricane      1977-09-02 06:00:00\n##  7 Clara     1977      993    65 1        hurricane      1977-09-08 12:00:00\n##  8 Evelyn    1977      994    65 1        hurricane      1977-10-15 00:00:00\n##  9 Amelia    1978     1005    45 0        tropical storm 1978-07-31 00:00:00\n## 10 Bess      1978     1005    40 0        tropical storm 1978-08-07 12:00:00\n## # … with 502 more rows\n\n\n\n\ngrouped_storms = storms.groupby([\"name\", \"year\"])\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'storms' is not defined\ngrouped_storm_sum = grouped_storms.agg({\n  \"pressure\": lambda x: x.min()\n}).reindex()\n\n# This gets all the information from storms\n# corresponding to name/year/max pressure\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'grouped_storms' is not defined\nmax_power_storm = grouped_storm_sum.merge(storms, on = [\"name\", \"year\", \"pressure\"])\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'grouped_storm_sum' is not defined\nmax_power_storm = max_power_storm.groupby([\"name\", \"year\"]).agg({\n  \"pressure\": \"min\",\n  \"wind\": \"max\",\n  \"category\": \"mean\",\n  \"status\": \"unique\",\n  \"time\": \"mean\"\n})\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'max_power_storm' is not defined\n\n\n\n\nIf we want to see a visual summary, we could plot a histogram of the minimum pressure of each storm.\n\n\nR\nPython\n\n\n\n\nlibrary(ggplot2)\nggplot(max_power_storm, aes(x = pressure)) + geom_histogram()\n\n\n\n\n\n\n\nfrom plotnine import *\n\nggplot(max_power_storm, aes(x = \"pressure\")) + geom_histogram(bins=30)\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'max_power_storm' is not defined\n\n\n\n\nWe could also look to see whether there has been any change over time in pressure.\n\n\nR\nPython\n\n\n\n\nggplot(max_power_storm, aes(x = time, y = pressure)) + geom_point()\n\n\n\n\n\n\n\nggplot(max_power_storm, aes(x = \"time\", y = \"pressure\")) + geom_point()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'max_power_storm' is not defined\n\n\n\n\nIt seems to me that there are fewer high-pressure storms before 1990 or so, which may be due to the fact that some weak storms may not have been observed or recorded prior to widespread radar coverage in the Atlantic.\n\nAnother interesting way to look at this data would be to examine the duration of time a storm existed, as a function of its maximum category. Do stronger storms exist for a longer period of time?\n\n\nR\nPython\n\n\n\n\nstorm_strength_duration <- storms %>%\n  group_by(name, year) %>%\n  summarize(duration = difftime(max(time), min(time), units = \"days\"), \n            max_strength = max(category)) %>%\n  ungroup() %>%\n  arrange(desc(max_strength))\n\nstorm_strength_duration %>%\n  ggplot(aes(x = max_strength, y = duration)) + geom_boxplot()\n\n\n\n\n\n\n\nstorm_strength_duration = storms.groupby([\"name\", \"year\"]).agg(duration = (\"time\", lambda x: max(x) - min(x)),max_strength = (\"category\", \"max\"))\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'storms' is not defined\nggplot(aes(x = \"factor(max_strength)\", y = \"duration\"), data = storm_strength_duration) + geom_boxplot()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'storm_strength_duration' is not defined\n\n\n\n\nYou don’t need to know how to create these plots yet, but I find it much easier to look at the chart and answer the question I started out with.\nWe could also look to see how a storm’s diameter evolves over time, from when the storm is first identified (group_by + mutate)\nDiameter measurements don’t exist for all storms, and they appear to measure the diameter of the wind field - that is, the region where the winds are hurricane or tropical storm force. (?storms documents the dataset and its variables).\n\n\nR\nPython\n\n\n\nNote the use of as.numeric(as.character(max(category))) to get the maximum (ordinal categorical) strength and convert that into something numeric that can be plotted.\n\nstorm_evolution <- storms %>%\n  filter(!is.na(hurricane_force_diameter)) %>%\n  group_by(name, year) %>%\n  mutate(time_since_start = difftime(time, min(time), units = \"days\")) %>%\n  ungroup()\n\nggplot(storm_evolution, \n       aes(x = time_since_start, y = hurricane_force_diameter, \n           group = name)) + geom_line(alpha = .2) + \n  facet_wrap(~year, scales = \"free_y\")\n\n\n\n\n\n\n\nstorm_evolution = storms.loc[storms.hurricane_force_diameter.notnull(),:]\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'storms' is not defined\nstorm_evolution = storm_evolution.assign(age = storm_evolution.groupby([\"name\", \"year\"], group_keys = False).apply(lambda x: x.time - x.time.min()))\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'storm_evolution' is not defined\n(ggplot(storm_evolution, \n       aes(x = \"age\", y = \"hurricane_force_diameter\", \n           group = \"name\")) + geom_line(alpha = .2) + \n  facet_wrap(\"year\", scales = \"free_y\"))\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'storm_evolution' is not defined\n\n\n\n\nFor this plot, I’ve added facet_wrap(~year) to produce sub-plots for each year. This helps us to be able to see some individuality, because otherwise there are far too many storms.\nIt seems that the vast majority of storms have a single bout of hurricane force winds (which either decreases or just terminates near the peak, presumably when the storm hits land and rapidly disintegrates). However, there are a few interesting exceptions - my favorite is in 2008 - the longest-lasting storm seems to have several local peaks in wind field diameter. If we want, we can examine that further by plotting it separately.\n\n\nR\nPython\n\n\n\n\nstorm_evolution %>%\n  filter(year == 2008) %>%\n  arrange(desc(time_since_start))\n## # A tibble: 327 × 15\n##    name   year month   day  hour   lat  long status        categ…¹  wind press…²\n##    <chr> <dbl> <dbl> <int> <dbl> <dbl> <dbl> <chr>         <ord>   <int>   <int>\n##  1 Ike    2008     9    14     6  35.5 -93.7 tropical sto… 0          35     985\n##  2 Ike    2008     9    14     0  33.5 -94.9 tropical sto… 0          35     980\n##  3 Ike    2008     9    13    18  31.7 -95.3 tropical sto… 0          50     974\n##  4 Ike    2008     9    13    12  30.3 -95.2 hurricane     2          85     959\n##  5 Ike    2008     9    13     7  29.3 -94.7 hurricane     2          95     950\n##  6 Ike    2008     9    13     6  29.1 -94.6 hurricane     2          95     951\n##  7 Ike    2008     9    13     0  28.3 -94   hurricane     2          95     952\n##  8 Fay    2008     8    27     0  35   -85.8 tropical dep… -1         15    1005\n##  9 Ike    2008     9    12    18  27.5 -93.2 hurricane     2          95     954\n## 10 Fay    2008     8    26    18  34.6 -86.5 tropical dep… -1         20    1004\n## # … with 317 more rows, 4 more variables: tropicalstorm_force_diameter <int>,\n## #   hurricane_force_diameter <int>, time <dttm>, time_since_start <drtn>, and\n## #   abbreviated variable names ¹​category, ²​pressure\n\nstorm_evolution %>% filter(name == \"Ike\") %>%\n  ggplot(aes(x = time, y = hurricane_force_diameter, color = category)) + geom_point()\n\n\n\n\n\n\n\nstorm_evolution.query(\"year==2008\").sort_values(['age'], ascending = False).head()\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'storm_evolution' is not defined\n(ggplot(\n  storm_evolution.query(\"year==2008 & name=='Ike'\"),\n  aes(x = \"time\", y = \"hurricane_force_diameter\", color = \"category\")) +\n  geom_point())\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'storm_evolution' is not defined\n\n\n\n\n\n\n\n\n\nRadar coverage map from 1995, from [12]\n\n\n\n17.7.1 Summarizing Across Multiple Variables\nThe dplyr package is filled with other handy functions for accomplishing common data-wrangling tasks. across() is particularly useful - it allows you to make a modification to several columns at the same time.\n\n\ndplyr’s across() function lets you apply a mutate or summarize statement to many columns (by Allison Horst)\n\n\nSuppose we want to summarize the numerical columns of any storm which was a hurricane (over the entire period it was a hurricane). We don’t want to write out all of the summarize statements individually, so we use across() instead.\n\n\nR\nPython\n\n\n\n\nlibrary(lubridate) # for the make_datetime() function\ndata(storms)\n\nstorms <- storms %>%\n  # Construct a time variable that behaves like a number but is formatted as a date\n  mutate(time = make_datetime(year, month, day, hour))\n\n# Use across to get average of all numeric variables\navg_hurricane_intensity <- storms %>%\n  filter(status == \"hurricane\") %>%\n  group_by(name) %>%\n  summarize(across(where(is.numeric), mean, na.rm = T), .groups = \"drop\") \n\navg_hurricane_intensity %>%\n  select(name, year, month, wind, pressure, tropicalstorm_force_diameter, hurricane_force_diameter) %>%\n  arrange(desc(wind)) %>% \n  # get top 10\n  filter(row_number() <= 10) %>%\n  knitr::kable() # Make into a pretty table\n\n\n\n\n\n\n\n\n\n\n\n\nname\nyear\nmonth\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\nAndrew\n1992\n8.000000\n118.2609\n946.6522\nNaN\nNaN\n\n\nMitch\n1998\n10.000000\n115.9091\n945.3182\nNaN\nNaN\n\n\nRita\n2005\n9.000000\n114.7368\n931.6316\n265.2941\n97.05882\n\n\nIsabel\n2003\n9.000000\n112.1875\n946.5417\nNaN\nNaN\n\n\nGilbert\n1988\n9.000000\n110.8929\n945.4286\nNaN\nNaN\n\n\nLuis\n1995\n8.928571\n110.5952\n948.6190\nNaN\nNaN\n\n\nWilma\n2005\n10.000000\n110.3030\n939.4242\n349.8333\n118.33333\n\n\nMatthew\n2016\n9.880952\n109.5238\n952.1190\n263.5714\n62.02381\n\n\nHugo\n1989\n9.000000\n106.5789\n950.9211\nNaN\nNaN\n\n\nDavid\n1979\n8.457143\n105.1429\n956.1429\nNaN\nNaN\n\n\n\n\n\n\n\nStackoverflow reference\nIn the interests of using the same example, I’ve exported dplyr’s storms data to CSV.\n\nimport pandas as pd\nimport numpy as np\nstorms = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/clean/storms.csv\")\n\n# Construct a time variable that behaves like a number but is formatted as a date\nstorms = storms.assign(time = pd.to_datetime(storms[[\"year\", \"month\", \"day\", \"hour\"]]))\n\n# Remove year/month/day/hour\nstorms = storms.drop([\"year\", \"month\", \"day\", \"hour\"], axis = 1)\n\n# Remove non-hurricane points\nstorms = storms.query(\"status == 'hurricane'\")\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: 'mean' for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]\n\nBy default, pandas skips NaN values. If we want to be more clear, or want to pass another argument into the function, we can use what is called a lambda function - basically, a “dummy” function that has some arguments but not all of the arguments. Here, our lambda function is a function of x, and we calculate x.mean(skipna=True) for each x passed in (so, for each column).\n\n# Get list of all remaining numeric variables\ncols = storms.select_dtypes(include =[np.number]).columns.values\n(storms.\nset_index(\"name\").\nfilter(cols).\ngroupby('name').\nagg({col: lambda x: x.mean(skipna=True) for col in cols}))\n##                 lat  ...  hurricane_force_diameter\n## name                 ...                          \n## AL121991  38.850000  ...                       NaN\n## Alberto   30.836735  ...                       NaN\n## Alex      32.880769  ...                 48.461538\n## Alicia    28.400000  ...                       NaN\n## Allison   26.166667  ...                       NaN\n## ...             ...  ...                       ...\n## Teddy     25.793103  ...                103.448276\n## Tomas     17.346154  ...                 24.230769\n## Vince     34.100000  ...                 30.000000\n## Wilma     22.327273  ...                118.333333\n## Zeta      23.227273  ...                 29.545455\n## \n## [137 rows x 7 columns]"
  },
  {
    "objectID": "part-wrangling/03-data-cleaning.html#try-it-out",
    "href": "part-wrangling/03-data-cleaning.html#try-it-out",
    "title": "17  Data Cleaning",
    "section": "\n17.8 Try it out",
    "text": "17.8 Try it out\nYou can read about the gapminder project here.\nThe gapminder data used for this set of problems contains data from 142 countries on 5 continents. The filtered data in gapminder (in R) contain data about every 5 year period between 1952 and 2007, the country’s life expectancy at birth, population, and per capita GDP (in US $, inflation adjusted). In the gapminder_unfiltered table, however, things are a bit different. Some countries have yearly data, observations are missing, and some countries don’t have complete data. The gapminder package in python (install with pip install gapminder) is a port of the R package, but doesn’t contain the unfiltered data, so we’ll instead use a CSV export.\n\n\n\n\n\n\nRead in the Data\n\n\n\n\n\nR\nPython\n\n\n\n\nif (!\"gapminder\" %in% installed.packages()) install.packages(\"gapminder\")\nlibrary(gapminder)\ngapminder_unfiltered\n## # A tibble: 3,313 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 3,303 more rows\n\n\n\n\nimport pandas as pd\n\ngapminder_unfiltered = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/gapminder_unfiltered.csv\")\n\n\n\n\n\n17.8.1 Task 1: How Bad is It?\n\n\nProblem\nR\nPython\n\n\n\nUsing your EDA skills, determine how bad the unfiltered data are. You may want to look for missing values, number of records, etc. Use query or filter to show any countries which have incomplete data. Describe, in words, what operations were necessary to get this information.\n\n\n\ngapminder_unfiltered %>% \n  group_by(country) %>% \n  summarize(n = n(), missinglifeExp = sum(is.na(lifeExp)), \n            missingpop = sum(is.na(pop)),\n            missingGDP = sum(is.na(gdpPercap))) %>%\n  filter(n != length(seq(1952, 2007, by = 5)))\n## # A tibble: 83 × 5\n##    country        n missinglifeExp missingpop missingGDP\n##    <fct>      <int>          <int>      <int>      <int>\n##  1 Armenia        4              0          0          0\n##  2 Aruba          8              0          0          0\n##  3 Australia     56              0          0          0\n##  4 Austria       57              0          0          0\n##  5 Azerbaijan     4              0          0          0\n##  6 Bahamas       10              0          0          0\n##  7 Barbados      10              0          0          0\n##  8 Belarus       18              0          0          0\n##  9 Belgium       57              0          0          0\n## 10 Belize        11              0          0          0\n## # … with 73 more rows\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data.\n\n\n\n(\n  gapminder_unfiltered.\n  set_index(\"country\").\n  filter([\"lifeExp\", \"pop\", \"gdpPercap\"]).\n  groupby(\"country\").\n  agg(lambda x: x.notnull().sum()).\n  query(\"lifeExp != 12 | pop != 12 | gdpPercap != 12\")\n  )\n##                       lifeExp  pop  gdpPercap\n## country                                      \n## Armenia                     4    4          4\n## Aruba                       8    8          8\n## Australia                  56   56         56\n## Austria                    57   57         57\n## Azerbaijan                  4    4          4\n## ...                       ...  ...        ...\n## United Arab Emirates        8    8          8\n## United Kingdom             13   13         13\n## United States              57   57         57\n## Uzbekistan                  4    4          4\n## Vanuatu                     7    7          7\n## \n## [83 rows x 3 columns]\n\nIn order to determine what gaps were present in the gapminder dataset, I determined how many years of data were available for each country by grouping the dataset and counting the rows. There should be 12 years worth of data between 1952 and 2007; as a result, I displayed the countries which did not have exactly 12 years of data.\n\n\n\n\n17.8.2 Task 2: Exclude any data which isn’t at 5-year increments\nStart in 1952 (so 1952, 1957, 1962, …, 2007).\n\n\nR\nPython\n\n\n\n\ngapminder_unfiltered %>%\n  filter(year %in% seq(1952, 2007, by = 5))\n## # A tibble: 2,013 × 6\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 2,003 more rows\n\n\n\nReminder about python list comprehensions\nExplanation of the query @ statement\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\ngapminder_unfiltered.query(\"year in @years_to_keep\")\n##           country continent  year  lifeExp       pop   gdpPercap\n## 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n## 1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n## 2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n## 3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n## 4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...           ...       ...   ...      ...       ...         ...\n## 3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n## 3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n## 3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n## 3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n## 3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [2013 rows x 6 columns]\n\n\n\n\n\n17.8.3 Task 3: Exclude any countries that don’t have a full set of observations\n\n\nR\nPython\n\n\n\n\ngapminder_unfiltered %>%\n  filter(year %in% seq(1952, 2007, by = 5)) %>%\n  group_by(country) %>%\n  mutate(nobs = n()) %>% # Use mutate instead of summarize so that all rows stay\n  filter(nobs == 12) %>%\n  select(-nobs)\n## # A tibble: 1,704 × 6\n## # Groups:   country [142]\n##    country     continent  year lifeExp      pop gdpPercap\n##    <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n##  1 Afghanistan Asia       1952    28.8  8425333      779.\n##  2 Afghanistan Asia       1957    30.3  9240934      821.\n##  3 Afghanistan Asia       1962    32.0 10267083      853.\n##  4 Afghanistan Asia       1967    34.0 11537966      836.\n##  5 Afghanistan Asia       1972    36.1 13079460      740.\n##  6 Afghanistan Asia       1977    38.4 14880372      786.\n##  7 Afghanistan Asia       1982    39.9 12881816      978.\n##  8 Afghanistan Asia       1987    40.8 13867957      852.\n##  9 Afghanistan Asia       1992    41.7 16317921      649.\n## 10 Afghanistan Asia       1997    41.8 22227415      635.\n## # … with 1,694 more rows\n\n\n\n\n\nyears_to_keep = [i for i in range(1952, 2008, 5)]\n\n(\n  gapminder_unfiltered.\n  # Remove extra years\n  query(\"year in @years_to_keep\").\n  groupby(\"country\").\n  # Calculate number of observations (should be exactly 12)\n  # This is the equivalent of mutate on a grouped data set\n  apply(lambda grp: grp.assign(nobs = grp['lifeExp'].notnull().sum())).\n  # Keep rows with 12 observations\n  query(\"nobs == 12\").\n  # remove nobs column\n  drop(\"nobs\", axis = 1)\n  )\n##                       country continent  year  lifeExp       pop   gdpPercap\n## country                                                                     \n## Afghanistan 0     Afghanistan      Asia  1952   28.801   8425333  779.445314\n##             1     Afghanistan      Asia  1957   30.332   9240934  820.853030\n##             2     Afghanistan      Asia  1962   31.997  10267083  853.100710\n##             3     Afghanistan      Asia  1967   34.020  11537966  836.197138\n##             4     Afghanistan      Asia  1972   36.088  13079460  739.981106\n## ...                       ...       ...   ...      ...       ...         ...\n## Zimbabwe    3308     Zimbabwe    Africa  1987   62.351   9216418  706.157306\n##             3309     Zimbabwe    Africa  1992   60.377  10704340  693.420786\n##             3310     Zimbabwe    Africa  1997   46.809  11404948  792.449960\n##             3311     Zimbabwe    Africa  2002   39.989  11926563  672.038623\n##             3312     Zimbabwe    Africa  2007   43.487  12311143  469.709298\n## \n## [1704 rows x 6 columns]\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\nIntroduction to dplyr and Single Table dplyr functions\nR for Data Science: Data Transformations\nAdditional practice exercises: Intro to the tidyverse, group_by + summarize examples, group_by + mutate examples (from a similar class at Iowa State)\nBase R data manipulation\n\nVideos of analysis of new data from Tidy Tuesday - may include use of other packages, but almost definitely includes use of dplyr as well.\n\n\nTidyTuesday Python github repo - replicating Tidy Tuesday analyses in Python with Pandas\n\n\n\n\n\n\n\n\n\n[1] \nPandas, “Indexing and selecting data,” Pandas 1.4.3 Documentation. 2022 [Online]. Available: https://pandas.pydata.org/docs/user_guide/indexing.html#indexing. [Accessed: Jun. 30, 2022]\n\n\n[2] \npwwang, “Datar: A Grammar of Data Manipulation in python.” May 2022 [Online]. Available: https://pwwang.github.io/datar/. [Accessed: Jun. 30, 2022]\n\n\n[3] \nM. Chow, “nycflights13: A data package for nyc flights (the nycflights13 R package).” 2020 [Online]. Available: https://github.com/machow/nycflights13. [Accessed: Jun. 30, 2022]\n\n\n[4] \nPython Foundation, “Data Structures,” Python 3.10.5 documentation. Jun. 2022 [Online]. Available: https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions. [Accessed: Jun. 30, 2022]\n\n\n[5] \nC. Nguyen, “Tips for String Manipulation in Python,” Towards Data Science. Sep. 2021 [Online]. Available: https://towardsdatascience.com/tips-for-string-manipulation-in-python-92b1fc3f4d9f. [Accessed: Jul. 01, 2022]\n\n\n[6] \nJ. VanderPlas, “String Manipulation and Regular Expressions,” in A Whirlwind Tour of Python, O’Reilly Media, 2016 [Online]. Available: https://jakevdp.github.io/WhirlwindTourOfPython/14-strings-and-regular-expressions.html. [Accessed: Jul. 01, 2022]\n\n\n[7] \nC. Whorton, “Applying Custom Functions to Groups of Data in Pandas,” Medium. Jul. 2021 [Online]. Available: https://towardsdatascience.com/applying-custom-functions-to-groups-of-data-in-pandas-928d7eece0aa. [Accessed: Jul. 01, 2022]\n\n\n[8] \nH. Wickham, “The split-apply-combine strategy for data analysis,” Journal of statistical software, vol. 40, pp. 1–29, 2011. \n\n\n[9] \n\n“Group by: Split-apply-combine,” in Pandas 1.4.3 documentation, Python, 2022 [Online]. Available: https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html. [Accessed: Jul. 01, 2022]\n\n\n[10] \nJ. Dean and S. Ghemawat, “MapReduce: Simplified data processing on large clusters,” Communications of the ACM, vol. 51, no. 1, pp. 107–113, Jan. 2008, doi: 10.1145/1327452.1327492. [Online]. Available: https://doi.org/10.1145/1327452.1327492. [Accessed: Jul. 01, 2022]\n\n\n[11] \nM. Dancho, “Answer to \"Is there an \"ungroup by\" operation opposite to .groupby in pandas?\",” Stack Overflow. Mar. 2021 [Online]. Available: https://stackoverflow.com/a/66879388/2859168. [Accessed: Jul. 01, 2022]\n\n\n[12] \nC. Mass, “The pacific northwest has the worst coastal weather radar coverage in the continental u.s.: Documentation of the problem and a call for action. Department of atmospheric sciences,” Jan. 12, 2006. [Online]. Available: https://www.atmos.washington.edu/~cliff/coastalradarold.html. [Accessed: Jan. 14, 2023]"
  },
  {
    "objectID": "part-wrangling/04-strings.html#fa-bullseye-objectives",
    "href": "part-wrangling/04-strings.html#fa-bullseye-objectives",
    "title": "18  Working with Strings",
    "section": "\n18.1  Objectives",
    "text": "18.1  Objectives\n\nUse functions to perform find-and-replace operations\nUse functions to split string data into multiple columns/variables\nUse functions to join string data from multiple columns/variables into a single column/variable"
  },
  {
    "objectID": "part-wrangling/04-strings.html#basic-operations",
    "href": "part-wrangling/04-strings.html#basic-operations",
    "title": "18  Working with Strings",
    "section": "\n18.2 Basic Operations",
    "text": "18.2 Basic Operations\nNearly always, when multiple variables are stored in a single column, they are stored as character variables. There are many different “levels” of working with strings in programming, from simple find-and-replaced of fixed (constant) strings to regular expressions, which are extremely powerful (and extremely complicated).\n\nSome people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. - Jamie Zawinski\n\n\n\nAlternately, the xkcd version of the above quote\n\n\nThe stringr cheatsheet by RStudio may be helpful as you complete tasks related to this section - it may even be useful in Python as the 2nd page has a nice summary of regular expressions.\n\n\nTable 18.1: Table of string functions in R and python. x is the string or vector of strings, pattern is a pattern to be found within the string, a and b are indexes, and encoding is a string encoding, such as UTF8 or ASCII.\n\n\n\n\n\n\nTask\nR\nPython\n\n\n\nReplace pattern with replacement\n\n\nbase: gsub(pattern, replacement, x)\nstringr: str_replace(x, pattern, replacement) and str_replace_all(x, pattern, replacement)\n\npandas: x.str.replace(pattern, replacement) (not vectorized over pattern or replacement)\n\n\nConvert case\n\nbase: tolower(x), toupper(x)\nstringr: str_to_lower(x), str_to_upper(x) , str_to_title(x)\n\npandas: x.str.lower(), x.str.upper()\n\n\n\nStrip whitespace from start/end\n\nbase: trimws(x)\nstringr: str_trim(x) , str_squish(x)\n\npandas: x.str.strip()\n\n\n\nPad strings to a specific length\n\nbase: sprintf(format, x)\nstringr: str_pad(x, …)\n\npandas: x.str.pad()\n\n\n\nTest if the string contains a pattern\n\nbase: grep(pattern, x) or grepl(pattern, x)\nstringr: str_detect(x, pattern)\n\npandas: x.str.contains(pattern)\n\n\n\nCount how many times a pattern appears in the string\n\nbase: gregexpr(pattern, x) + sapply to count length of the returned list\nstringi: stri_count(x, pattern)\nstringr: str_count(x, pattern)\n\npandas: x.str.count(pattern)\n\n\n\nFind the first appearance of the pattern within the string\n\nbase: regexpr(pattern, x)\nstringr: str_locate(x, pattern)\n\npandas: x.str.find(pattern)\n\n\n\nFind all appearances of the pattern within the string\n\nbase: gregexpr\nstringr: str_locate_all(x, pattern)\n\npandas: x.str.findall(pattern)\n\n\n\nDetect a match at the start/end of the string\n\nbase: use regular expr.\nstringr: str_starts(x, pattern) ,str_ends(x, pattern)\n\npandas: x.str.startswith(pattern) , x.str.endswith(pattern)\n\n\n\nSubset a string from index a to b\n\nbase: substr(x, a, b)\nstringr: str_sub(x, a, b)\n\npandas: x.str.slice(a, b, step)\n\n\n\nConvert string encoding\n\nbase: iconv(x, encoding)\nstringr: str_conv(x, encoding)\n\npandas: x.str.encode(encoding)\n\n\n\n\n\nIn Table 18.1, multiple functions are provided for e.g. common packages and situations. Pandas methods are specifically those which work in some sort of vectorized manner. Base methods (in R) do not require additional packages, where stringr methods require the stringr package, which is included in the tidyverse1."
  },
  {
    "objectID": "part-wrangling/04-strings.html#converting-strings-to-numbers",
    "href": "part-wrangling/04-strings.html#converting-strings-to-numbers",
    "title": "18  Working with Strings",
    "section": "Converting strings to numbers",
    "text": "Converting strings to numbers\nOne of the most common tasks when reading in and tidying messy data is that numeric-ish data can come in many forms that are read (by default) as strings. The data frame below provides an example of a few types of data which may be read in in unexpected ways. How do we tell R or Python that we want all of these columns to be treated as numbers?\n\n\n\nTable 18.2: Different “messy” number formats\n\n\nint_col\nfloat_col\nmix_col\nmissing_col\nmoney_col\neu_numbers\nboolean_col\ncustom\n\n\n\n0\n1\n1.1\na\n1\n£1,000.00\n1.000.000,00\nTrue\nY\n\n\n1\n2\n1.2\n2\n2\n£2,400.00\n2.000.342,00\nFalse\nY\n\n\n2\n3\n1.3\n3\n3\n£2,400.00\n3.141,59\nTrue\nN\n\n\n3\n4\n4.7\n4\nnan\n£2,400.00\n34,25\nTrue\nN\n\n\n\n\n\nNote that numbers, currencies, dates, and times are written differently based on what country you’re in [1]. In computer terms, this is the “locale”, and it affects everything from how your computer formats the date/time to what character set it will try to use to display things [2].\nIf you’ve never had to deal with the complexities of working on a laptop designed for one country using another country’s conventions, know that it isn’t necessarily the easiest thing to do.\n\n\n\n\n\n\nAdvanced: Locales\n\n\n\n\n\nFind your locale\n\n\n Type Get-WinSystemLocale into your CMD or powershell terminal.\n\n (10.4 and later) and  Type locale into your terminal\nGet set up to work with locales\nWhile this isn’t required, it may be useful and is definitely good practice if you’re planning to work with data generated internationally.\nThis article tells you how to set things up in linux . The biggest difference in other OS is going to be how to install new locales, so here are some instructions on that for other OS.\n\n\n Installing languages\n\n\n Change locales. Installing or creating new locales seems to be more complicated, and since I do not have a mac, I can’t test this out easily myself.\n\n\n\n\nWe’ll use Table 18.2 to explore different string operations focused specifically on converting strings to numbers.\n\n\nGet the data: Python\nR\n\n\n\n\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/number-formats.csv\")\n\n\n\n\ndf <- read.csv(\"https://raw.githubusercontent.com/srvanderplas/unl-stat850/main/data/number-formats.csv\", colClasses = \"character\")\n\nBy default, R tries to outsmart us and read the data in as numbers. I’ve disabled this behavior by setting colClasses='character' so that you can see how these functions work… but in general, R seems to be a bit more willing to try to guess what you want. This can be useful, but can also be frustrating when you don’t know how to disable it.\n\n\n\n\n\n\n\n\n\nConverting Columns Using Your Best Guess\n\n\n\nBoth R and Python have ways to “guess” what type a column is and read the data in as that type. When we initially read in the data above, I had to explicitly disable this behavior in R. If you’re working with data that is already read in, how do you get R and Python to guess what type something is?\n\n\nR\nPython\n\n\n\nHere, R gets everything “right” except the eu_numbers, money_col, and custom cols, which makes sense - these contain information that isn’t clearly numeric or doesn’t match the default numeric formatting on my machine (which is using en_US.UTF-8 for almost everything). If we additionally want R to handle mix_col, we would have to explicitly convert to numeric, causing the a to be converted to NA\n\nlibrary(dplyr)\nlibrary(readr)\ndf_guess <- type_convert(df)\nstr(df_guess)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    :List of 4\n##   ..$ : chr \"a\"\n##   ..$ : int 2\n##   ..$ : int 3\n##   ..$ : int 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n\nThe type_convert function has a locale argument; readr includes a locale() function that you can pass to type_convert that allows you to define your own locale. Because we have numeric types structured from at least two locales in this data frame, we would have to specifically read the data in specifying which columns we wanted read with each locale.\n\nlibrary(dplyr)\nlibrary(readr)\nfixed_df <- type_convert(df) \nfixed_df2 <- type_convert(df, locale = locale(decimal_mark = ',', grouping_mark = '.'))\n# Replace EU numbers col with the type_convert results specifying that locale\nfixed_df$eu_numbers = fixed_df$eu_numbers\nstr(fixed_df)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    :List of 4\n##   ..$ : chr \"a\"\n##   ..$ : int 2\n##   ..$ : int 3\n##   ..$ : int 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n\n\n\nSimilarly, Python does basically the same thing as R: mix_col, money_col, and custom are all left as strings, while floats, integers, and logical values are handled correctly.\n\nfixed_df = df.infer_objects()\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\n\nAs in R, we can set the locale in Python to change how things are read in.\n\nfrom babel.numbers import parse_decimal\n\n# Convert eu_numbers column specifically\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'babel'\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'parse_decimal' is not defined\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: Unable to parse string \"1.000.000,00\" at position 0\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nConverting Columns Directly\n\n\n\nObviously, we can also convert some strings to numbers using type conversion functions that we discussed in Section 7.5. This is fairly easy in R, but a bit more complex in Python, because Python has several different types of ‘missing’ or NA variables that are not necessarily compatible.\n\n\nR\nPython\n\n\n\nHere, we use the across helper function from dplyr to convert all of the columns to numeric. Note that the last 3 columns don’t work here, because they contain characters R doesn’t recognize as numeric characters.\n\nlibrary(dplyr)\n\ndf_numeric <- mutate(df, across(everything(), as.numeric))\nstr(df_numeric)\n## 'data.frame':    4 obs. of  8 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    : num  NA 2 3 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : num  NA NA NA NA\n##  $ eu_numbers : num  NA NA NA NA\n##  $ boolean_col: num  1 0 1 1\n##  $ custom     : num  NA NA NA NA\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n\n\n\n\ndf_numeric = df.apply(pd.to_numeric, errors='coerce')\ndf_numeric.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col        float64\n## missing_col    float64\n## money_col      float64\n## eu_numbers     float64\n## boolean_col       bool\n## custom         float64\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Converting Y/N data\n\n\n\nThe next thing we might want to do is convert our custom column so that it has 1 instead of Y and 0 instead of N. There are several ways we can handle this process:\n\nWe could use factors/categorical variables, which have numeric values “under the hood”, but show up as labeled.\nWe could (in this particular case) test for equality with “Y”, but this approach would not generalize well if we had more than 2 categories.\nWe could take a less nuanced approach and just find-replace and then convert to a number.\n\nSome of these solutions are more kludgy than others, but I’ve used all 3 approaches when dealing with categorical data in the past, depending on what I wanted to do with it afterwards.\n\n\nR\nPython\n\n\n\n\nlibrary(stringr) # work with strings easily\nfixed_df = fixed_df %>%\n  mutate(\n    # factor approach\n    custom1 = factor(custom, levels = c(\"N\", \"Y\"), labels = c(\"Y\", \"N\")),\n    # test for equality\n    custom2 = (custom == \"Y\"),\n    # string replacement\n    custom3 = str_replace_all(custom, c(\"Y\" = \"1\", \"N\" = \"0\")) %>%\n      as.numeric()\n  )\n\nstr(fixed_df)\n## 'data.frame':    4 obs. of  11 variables:\n##  $ int_col    : num  1 2 3 4\n##  $ float_col  : num  1.1 1.2 1.3 4.7\n##  $ mix_col    :List of 4\n##   ..$ : chr \"a\"\n##   ..$ : int 2\n##   ..$ : int 3\n##   ..$ : int 4\n##  $ missing_col: num  1 2 3 NaN\n##  $ money_col  : chr  \"£1,000.00\" \"£2,400.00\" \"£2,400.00\" \"£2,400.00\"\n##  $ eu_numbers : chr  \"1.000.000,00\" \"2.000.342,00\" \"3.141,59\" \"34,25\"\n##  $ boolean_col: logi  TRUE FALSE TRUE TRUE\n##  $ custom     : chr  \"Y\" \"Y\" \"N\" \"N\"\n##  $ custom1    : Factor w/ 2 levels \"Y\",\"N\": 2 2 1 1\n##  $ custom2    : logi  TRUE TRUE FALSE FALSE\n##  $ custom3    : num  1 1 0 0\n##  - attr(*, \"pandas.index\")=RangeIndex(start=0, stop=4, step=1)\n\n\n\nWe’ve already done a brief demonstration of string methods in Python when we trimmed off the £ character. In this situation, it’s better to use the pandas replace method, which allows you to pass in a list of values and a list of replacements.\n\n# Categorical (factor) approach\nfixed_df['custom1'] = fixed_df['custom'].astype(\"category\") # convert to categorical variable\n# Equality/boolean approach\nfixed_df['custom2'] = fixed_df['custom'] == \"Y\"\n# string replacement\nfixed_df['custom3'] = fixed_df['custom'].replace([\"Y\", \"N\"], [\"1\", \"0\"]).astype(\"int\")\n\nfixed_df.dtypes\n## int_col           int64\n## float_col       float64\n## mix_col          object\n## missing_col     float64\n## money_col        object\n## eu_numbers       object\n## boolean_col        bool\n## custom           object\n## custom1        category\n## custom2            bool\n## custom3           int64\n## dtype: object"
  },
  {
    "objectID": "part-wrangling/04-strings.html#find-and-replace",
    "href": "part-wrangling/04-strings.html#find-and-replace",
    "title": "18  Working with Strings",
    "section": "\n18.3 Find and replace",
    "text": "18.3 Find and replace\nAnother way to fix some issues is to just find-and-replace the problematic characters. This is not always the best solution2, and may introduce bugs if you use the same code to analyze new data with characters you haven’t anticipated, but in so many cases it’s also the absolute easiest, fastest, simplest way forward and easily solves many different problems.\nI’ll show you how to correct all of the issues reading in the data using solutions shown above, but please do consider reading [3] so that you know why find-and-replace isn’t (necessarily) the best option for locale-specific formatting.\n\n\n\n\n\n\nExample: find and replace\n\n\n\nLet’s start with the money column.\n\n\nR\nPython\n\n\n\nIn R, parse_number() handles the money column just fine - the pound sign goes away and we get a numeric value. This didn’t work by default with type_convert, but as long as we mutate and tell R we expect a number, things work well. Then, as we did above, we can specify the locale settings so that decimal and grouping marks are handled correctly even for countries which use ‘,’ for decimal and ‘.’ for thousands separators.\n\nfixed_df = df %>%\n  type_convert() %>% # guess everything\n  mutate(money_col = parse_number(money_col),\n         eu_numbers = parse_number(eu_numbers, \n                                   locale = locale(decimal_mark = ',', \n                                                   grouping_mark = '.')))\n\n\n\nIn python, a similar approach doesn’t work out, because the pound sign is not handled correctly.\n\nfrom babel.numbers import parse_decimal\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'babel'\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].apply(lambda x: parse_decimal(x, locale = 'it'))\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'parse_decimal' is not defined\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n\n# Convert money_col\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: Unable to parse string \"1.000.000,00\" at position 0\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x, locale = 'en_GB'))\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'parse_decimal' is not defined\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n# Remove £ from string\nfixed_df['money_col'] = fixed_df['money_col'].str.removeprefix(\"£\")\n# Then parse the number\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'StringMethods' object has no attribute 'removeprefix'\nfixed_df['money_col'] = fixed_df['money_col'].apply(lambda x: parse_decimal(x))\n# Then convert to numeric\n## Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name 'parse_decimal' is not defined\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: Unable to parse string \"£1,000.00\" at position 0\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers      object\n## boolean_col       bool\n## custom          object\n## dtype: object\n\n\n\n\n\n\n\n\n\n\n\n\nExample: Locale find-and-replace\n\n\n\nWe could also handle the locale issues using find-and-replace, if we wanted to…\n\n\nR\nPython\n\n\n\nNote that str_remove is shorthand for str_replace(x, pattern, \"\"). There is a little bit of additional complexity in switching “,” for “.” and vice versa - we have to change “,” to something else first, so that we can replace “.” with “,”. This is not elegant but it does work. It also doesn’t generalize - it will mess up numbers formatted using the US/UK convention, and it won’t handle numbers formatted using other conventions from other locales.\n\nfixed_df = df %>%\n  type_convert() %>% # guess everything\n  mutate(money_col = str_remove(money_col, \"£\") %>% parse_number(),\n         eu_numbers = str_replace_all(eu_numbers, \n                                      c(\",\" = \"_\", \n                                        \"\\\\.\" = \",\", \n                                        \"_\" = \".\")) %>%\n           parse_number())\n\n\n\n\nfrom babel.numbers import parse_decimal\n## Error in py_call_impl(callable, dots$args, dots$keywords): ModuleNotFoundError: No module named 'babel'\nfixed_df = df.infer_objects()\n\n# Convert eu_numbers column: \n# Replace . with nothing (remove .), then\n# Replace , with .\nfixed_df['eu_numbers'] = fixed_df['eu_numbers'].\\\nstr.replace('\\.', '').\\\nstr.replace(',', '.')\n## <string>:2: FutureWarning: The default value of regex will change from True to False in a future version.\nfixed_df['eu_numbers'] = pd.to_numeric(fixed_df['eu_numbers'])\n\n# Convert money_col\nfixed_df['money_col'] = fixed_df['money_col'].\\\nstr.removeprefix(\"£\").\\\nstr.replace(',', '')\n## Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: 'StringMethods' object has no attribute 'removeprefix'\nfixed_df['money_col'] = pd.to_numeric(fixed_df['money_col'])\n## Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: Unable to parse string \"£1,000.00\" at position 0\nfixed_df.dtypes\n## int_col          int64\n## float_col      float64\n## mix_col         object\n## missing_col    float64\n## money_col       object\n## eu_numbers     float64\n## boolean_col       bool\n## custom          object\n## dtype: object\nfixed_df\n##    int_col  float_col mix_col  ...  eu_numbers boolean_col  custom\n## 0        1        1.1       a  ...  1000000.00        True       Y\n## 1        2        1.2       2  ...  2000342.00       False       Y\n## 2        3        1.3       3  ...     3141.59        True       N\n## 3        4        4.7       4  ...       34.25        True       N\n## \n## [4 rows x 8 columns]"
  },
  {
    "objectID": "part-wrangling/04-strings.html#separating-multi-variable-columns",
    "href": "part-wrangling/04-strings.html#separating-multi-variable-columns",
    "title": "18  Working with Strings",
    "section": "\n18.4 Separating multi-variable columns",
    "text": "18.4 Separating multi-variable columns\nAnother common situation is to have multiple variables in one column. This can happen, for instance, when conducting a factorial experiment: Instead of having separate columns for each factor, researchers sometimes combine several different factors into a single label for a condition to simplify data entry.\nIn pandas, we use x.str.split() to split columns in a DataFrame, in R we use the tidyr package’s separate function.\n\n\n\n\n\n\nExample: Separating columns\n\n\n\nWe’ll use the table3 object included in dplyr for this example. You can load it in R and then load the reticuate package to be able to access the object in python as r.table3.\n\n\nPicture the operation\nR\nPython\n\n\n\n\n\nWe want to separate the rate column into two new columns, cases and population.\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table3)\nseparate(table3, rate, into = c('cases', 'pop'), sep = \"/\", remove = F)\n## # A tibble: 6 × 5\n##   country      year rate              cases  pop       \n##   <chr>       <int> <chr>             <chr>  <chr>     \n## 1 Afghanistan  1999 745/19987071      745    19987071  \n## 2 Afghanistan  2000 2666/20595360     2666   20595360  \n## 3 Brazil       1999 37737/172006362   37737  172006362 \n## 4 Brazil       2000 80488/174504898   80488  174504898 \n## 5 China        1999 212258/1272915272 212258 1272915272\n## 6 China        2000 213766/1280428583 213766 1280428583\n\n\n\n\ntable3 = r.table3\ntable3[['cases', 'pop']] = table3.rate.str.split(\"/\", expand = True)\ntable3\n##        country  year               rate   cases         pop\n## 0  Afghanistan  1999       745/19987071     745    19987071\n## 1  Afghanistan  2000      2666/20595360    2666    20595360\n## 2       Brazil  1999    37737/172006362   37737   172006362\n## 3       Brazil  2000    80488/174504898   80488   174504898\n## 4        China  1999  212258/1272915272  212258  1272915272\n## 5        China  2000  213766/1280428583  213766  1280428583\n\nThis uses python’s multiassign capability. Python can assign multiple things at once if those things are specified as a sequence (e.g. cases, pop). In this case, we split the rate column and assign two new columns, essentially adding two columns to our data frame and labeling them at the same time."
  },
  {
    "objectID": "part-wrangling/04-strings.html#joining-columns",
    "href": "part-wrangling/04-strings.html#joining-columns",
    "title": "18  Working with Strings",
    "section": "\n18.5 Joining columns",
    "text": "18.5 Joining columns\nIt’s also not uncommon to need to join information stored in two columns into one column. A good example of a situation in which you might need to do this is when we store first and last name separately and then need to have a ‘name’ column that has both pieces of information together.\n\n\n\n\n\n\nExample: Joining columns\n\n\n\nWe’ll use the table5 object included in dplyr for this example. You can load it in R and then load the reticuate package to be able to access the object in python as r.table5.\n\n\nPicture the operation\nR\nPython\n\n\n\n\n\nWe want to join the century and year columns into a new column, yyyy.\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(reticulate) # so we can access table3 in python\ndata(table5)\nunite(table5, col = yyyy, c(century, year), sep = \"\", remove = F) %>%\n  # convert all columns to sensible types\n  readr::type_convert()\n## # A tibble: 6 × 5\n##   country      yyyy century year  rate             \n##   <chr>       <dbl>   <dbl> <chr> <chr>            \n## 1 Afghanistan  1999      19 99    745/19987071     \n## 2 Afghanistan  2000      20 00    2666/20595360    \n## 3 Brazil       1999      19 99    37737/172006362  \n## 4 Brazil       2000      20 00    80488/174504898  \n## 5 China        1999      19 99    212258/1272915272\n## 6 China        2000      20 00    213766/1280428583\n\n\n\n\nimport pandas as pd\n\ntable5 = r.table5\n# Concatenate the two columns with string addition\ntable5['yyyy'] = table5.century + table5.year\n# convert to number\ntable5['yyyy'] = pd.to_numeric(table5.yyyy)\ntable5\n##        country century year               rate  yyyy\n## 0  Afghanistan      19   99       745/19987071  1999\n## 1  Afghanistan      20   00      2666/20595360  2000\n## 2       Brazil      19   99    37737/172006362  1999\n## 3       Brazil      20   00    80488/174504898  2000\n## 4        China      19   99  212258/1272915272  1999\n## 5        China      20   00  213766/1280428583  2000"
  },
  {
    "objectID": "part-wrangling/04-strings.html#regular-expressions",
    "href": "part-wrangling/04-strings.html#regular-expressions",
    "title": "18  Working with Strings",
    "section": "\n18.6 Regular Expressions",
    "text": "18.6 Regular Expressions\nMatching exact strings is easy - it’s just like using find and replace.\n\nhuman_talk <- \"blah, blah, blah. Do you want to go for a walk?\"\ndog_hears <- str_extract(human_talk, \"walk\")\ndog_hears\n## [1] \"walk\"\n\nBut, if you can master even a small amount of regular expression notation, you’ll have exponentially more power to do good (or evil) when working with strings. You can get by without regular expressions if you’re creative, but often they’re much simpler.\n\n\n\n\n\n\nAdvanced: Short Regular Expressions Primer\n\n\n\n\n\nYou may find it helpful to follow along with this section using this web app built to test R regular expressions for R. A similar application for Perl compatible regular expressions (used by SAS and Python) can be found here. The subset of regular expression syntax we’re going to cover here is fairly limited (and common to SAS, Python, and R, with a few adjustments), but you can find regular expressions to do just about anything string-related. As with any tool, there are situations where it’s useful, and situations where you should not use a regular expression, no matter how much you want to.\nHere are the basics of regular expressions:\n\n\n[] enclose sets of characters\nEx: [abc] will match any single character a, b, c\n\n\n- specifies a range of characters (A-z matches all upper and lower case letters)\nto match - exactly, precede with a backslash (outside of []) or put the - last (inside [])\n\n\n\n. matches any character (except a newline)\nTo match special characters, escape them using \\ (in most languages) or \\\\ (in R). So \\. or \\\\. will match a literal ., \\$ or \\\\$ will match a literal $.\n\n\n\nR\nPython\n\n\n\n\nnum_string <- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn <- str_extract(num_string, \"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\")\nssn\n## [1] \"123-45-6789\"\n\n\n\nIn python, a regular expression is indicated by putting the character ‘r’ right before the quoted expression. This tells python that any backslashes in the string should be left alone – if R had that feature, we wouldn’t have to escape all the backslashes!\n\nimport re\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789\"\n\nssn = re.search(r\"[0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9][0-9][0-9]\", num_string)\nssn\n## <re.Match object; span=(42, 53), match='123-45-6789'>\n\n\n\n\nListing out all of those numbers can get repetitive, though. How do we specify repetition?\n\n\n* means repeat between 0 and inf times\n\n+ means 1 or more times\n\n? means 0 or 1 times – most useful when you’re looking for something optional\n\n{a, b} means repeat between a and b times, where a and b are integers. b can be blank. So [abc]{3,} will match abc, aaaa, cbbaa, but not ab, bb, or a. For a single number of repeated characters, you can use {a}. So {3, } means “3 or more times” and {3} means “exactly 3 times”\n\n\n\nR\nPython\n\n\n\n\nlibrary(stringr)\nstr_extract(\"banana\", \"[a-z]{1,}\") # match any sequence of lowercase characters\n## [1] \"banana\"\nstr_extract(\"banana\", \"[ab]{1,}\") # Match any sequence of a and b characters\n## [1] \"ba\"\nstr_extract_all(\"banana\", \"(..)\") # Match any two characters\n## [[1]]\n## [1] \"ba\" \"na\" \"na\"\nstr_extract(\"banana\", \"(..)\\\\1\") # Match a repeated thing\n## [1] \"anan\"\n\n\nnum_string <- \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn <- str_extract(num_string, \"[0-9]{3}-[0-9]{2}-[0-9]{4}\")\nssn\n## [1] \"123-45-6789\"\nphone <- str_extract(num_string, \"[0-9]{3}.[0-9]{3}.[0-9]{4}\")\nphone\n## [1] \"123-456-7890\"\nnuid <- str_extract(num_string, \"[0-9]{8}\")\nnuid\n## [1] \"12345678\"\nbank_balance <- str_extract(num_string, \"\\\\$[0-9,]+\\\\.[0-9]{2}\")\nbank_balance\n## [1] \"$50,000,000.23\"\n\n\n\n\nimport re\nre.search(r\"[a-z]{1,}\", \"banana\") # match any sequence of lowercase characters\n## <re.Match object; span=(0, 6), match='banana'>\nre.search(r\"[ab]{1,}\", \"banana\") # Match any sequence of a and b characters\n## <re.Match object; span=(0, 2), match='ba'>\nre.findall(r\"(..)\", \"banana\") # Match any two characters\n## ['ba', 'na', 'na']\nre.search(r\"(..)\\1\", \"banana\") # Match a repeated thing\n## <re.Match object; span=(1, 5), match='anan'>\n\n\nimport re\n\nnum_string = \"phone: 123-456-7890, nuid: 12345678, ssn: 123-45-6789, bank account balance: $50,000,000.23\"\n\nssn = re.search(r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\", num_string)\nssn\n## <re.Match object; span=(42, 53), match='123-45-6789'>\nphone = re.search(r\"[0-9]{3}.[0-9]{3}.[0-9]{4}\", num_string)\nphone\n## <re.Match object; span=(7, 19), match='123-456-7890'>\nnuid = re.search(r\"[0-9]{8}\", num_string)\nnuid\n## <re.Match object; span=(27, 35), match='12345678'>\nbank_balance = re.search(r\"\\$[0-9,]+\\.[0-9]{2}\", num_string)\nbank_balance\n## <re.Match object; span=(77, 91), match='$50,000,000.23'>\n\n\n\n\nThere are also ways to “anchor” a pattern to a part of the string (e.g. the beginning or the end)\n\n\n^ has multiple meanings:\n\nif it’s the first character in a pattern, ^ matches the beginning of a string\nif it follows [, e.g. [^abc], ^ means “not” - for instance, “the collection of all characters that aren’t a, b, or c”.\n\n\n\n$ means the end of a string\n\nCombined with pre and post-processing, these let you make sense out of semi-structured string data, such as addresses.\n\n\nR\nPython\n\n\n\n\naddress <- \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num <- str_extract(address, \"^[0-9]{1,}\")\n\n # Match everything alphanumeric up to the comma\nstreet <- str_extract(address, \"[A-z0-9 ]{1,}\")\nstreet <- str_remove(street, house_num) %>% str_trim() # remove house number\n\ncity <- str_extract(address, \",.*,\") %>% str_remove_all(\",\") %>% str_trim()\n\nzip <- str_extract(address, \"[0-9-]{5,10}$\") # match 5 and 9 digit zip codes\n\n\n\nPython match objects contain 3 things: .span(), which has the start and end positions of the match, .string, which contains the original string passed into the function, and .group(), which contains the actual matching portion of the string.\n\nimport re\n\naddress = \"1600 Pennsylvania Ave NW, Washington D.C., 20500\"\n\nhouse_num = re.search(r\"^[0-9]{1,}\", address).group()\n\n# Match everything alphanumeric up to the comma\nstreet = re.search(r\"[A-z0-9 ]{1,}\", address).group()\nstreet = street.replace(house_num, \"\").strip() # remove house number\n\ncity = re.search(\",.*,\", address).group().replace(\",\", \"\").strip()\n\nzip = re.search(r\"[0-9-]{5,10}$\", address).group() # match 5 and 9 digit zip codes\n\n\n\n\n\n\n() are used to capture information. So ([0-9]{4}) captures any 4-digit number\n\na|b will select a or b.\n\nIf you’ve captured information using (), you can reference that information using backreferences. In most languages, those look like this: \\1 for the first reference, \\9 for the ninth. In R, backreferences are \\\\1 through \\\\9.\n\n\nR\nPython\n\n\n\nIn R, the \\ character is special, so you have to escape it. So in R, \\\\1 is the first reference, and \\\\2 is the second, and so on.\n\nphone_num_variants <- c(\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\")\nphone_regex <- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).?([0-9]{4})\"\n# \\\\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\\\( and \\\\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nstr_extract(phone_num_variants, phone_regex)\n## [1] \"(123) 456-7980\"  \"123.456.7890\"    \"+1 123-456-7890\"\nstr_replace(phone_num_variants, phone_regex, \"\\\\1\\\\2\\\\3\")\n## [1] \"1234567980\" \"1234567890\" \"1234567890\"\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk <- \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears <- str_extract_all(human_talk, \"walk|treat\")\ndog_hears\n## [[1]]\n## [1] \"walk\"  \"treat\"\n\n\n\n\nimport pandas as pd\nimport re\n\nphone_num_variants = pd.Series([\"(123) 456-7980\", \"123.456.7890\", \"+1 123-456-7890\"])\nphone_regex = re.compile(\"\\+?[0-9]{0,3}? ?\\(?([0-9]{3})?\\)?.?([0-9]{3}).?([0-9]{4})\")\n# \\+?[0-9]{0,3} matches the country code, if specified, \n#    but won't take the first 3 digits from the area code \n#    unless a country code is also specified\n# \\( and \\) match literal parentheses if they exist\n# ([0-9]{3})? captures the area code, if it exists\n# .? matches any character\n# ([0-9]{3}) captures the exchange code\n# ([0-9]{4}) captures the 4-digit individual code\n\nres = phone_num_variants.str.findall(phone_regex)\nres2 = phone_num_variants.str.replace(phone_regex, \"\\\\1\\\\2\\\\3\")\n# We didn't capture the country code, so it remained in the string\n\nhuman_talk = \"blah, blah, blah. Do you want to go for a walk? I think I'm going to treat myself to some ice cream for working so hard. \"\ndog_hears = re.findall(r\"walk|treat\", human_talk)\ndog_hears\n## ['walk', 'treat']\n\n\n\n\nPutting it all together, we can test our regular expressions to ensure that they are specific enough to pull out what we want, while not pulling out other similar information:\n\n\nR\nPython\n\n\n\n\nstrings <- c(\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\")\n\nphone_regex <- \"\\\\+?[0-9]{0,3}? ?\\\\(?([0-9]{3})?\\\\)?.?([0-9]{3}).([0-9]{4})\"\ndog_regex <- \"(walk|treat)\"\naddr_regex <- \"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\"\nabab_regex <- \"(..)\\\\1\"\n\ntibble(\n  text = strings,\n  phone = str_detect(strings, phone_regex),\n  dog = str_detect(strings, dog_regex),\n  addr = str_detect(strings, addr_regex),\n  abab = str_detect(strings, abab_regex))\n## # A tibble: 6 × 5\n##   text                                                   phone dog   addr  abab \n##   <chr>                                                  <lgl> <lgl> <lgl> <lgl>\n## 1 abcdefghijklmnopqrstuvwxyzABAB                         FALSE FALSE FALSE TRUE \n## 2 banana orange strawberry apple                         FALSE FALSE FALSE TRUE \n## 3 ana went to montana to eat a banana                    FALSE FALSE FALSE TRUE \n## 4 call me at 432-394-2873. Do you want to go for a walk… TRUE  TRUE  FALSE FALSE\n## 5 phone: (123) 456-7890, nuid: 12345678, bank account b… TRUE  FALSE FALSE FALSE\n## 6 1600 Pennsylvania Ave NW, Washington D.C., 20500       FALSE FALSE TRUE  FALSE\n\n\n\n\nimport pandas as pd\nimport re\n\nstrings = pd.Series([\"abcdefghijklmnopqrstuvwxyzABAB\",\n\"banana orange strawberry apple\",\n\"ana went to montana to eat a banana\",\n\"call me at 432-394-2873. Do you want to go for a walk? I'm going to treat myself to some ice cream for working so hard.\",\n\"phone: (123) 456-7890, nuid: 12345678, bank account balance: $50,000,000.23\",\n\"1600 Pennsylvania Ave NW, Washington D.C., 20500\"])\n\nphone_regex = re.compile(r\"\\(?([0-9]{3})?\\)?.?([0-9]{3}).([0-9]{4})\")\ndog_regex = re.compile(r\"(walk|treat)\")\naddr_regex = re.compile(r\"([0-9]*) ([A-z0-9 ]{3,}), ([A-z\\\\. ]{3,}), ([0-9]{5})\")\nabab_regex = re.compile(r\"(..)\\1\")\n\npd.DataFrame({\n  \"text\": strings,\n  \"phone\": strings.str.contains(phone_regex),\n  \"dog\": strings.str.contains(dog_regex),\n  \"addr\": strings.str.contains(addr_regex),\n  \"abab\": strings.str.contains(abab_regex)})\n##                                                 text  phone  ...   addr   abab\n## 0                     abcdefghijklmnopqrstuvwxyzABAB  False  ...  False   True\n## 1                     banana orange strawberry apple  False  ...  False   True\n## 2                ana went to montana to eat a banana  False  ...  False   True\n## 3  call me at 432-394-2873. Do you want to go for...   True  ...  False  False\n## 4  phone: (123) 456-7890, nuid: 12345678, bank ac...   True  ...  False  False\n## 5   1600 Pennsylvania Ave NW, Washington D.C., 20500  False  ...   True  False\n## \n## [6 rows x 5 columns]\n## \n## <string>:3: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n## <string>:4: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n## <string>:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n## <string>:6: UserWarning: This pattern has match groups. To actually get the groups, use str.extract."
  },
  {
    "objectID": "part-wrangling/04-strings.html#references",
    "href": "part-wrangling/04-strings.html#references",
    "title": "18  Working with Strings",
    "section": "\n18.7 References",
    "text": "18.7 References\n\n\n\n\n[1] \nM. Ashour, “A Concise Guide to Number Localization,” Phrase. Feb. 2022 [Online]. Available: https://phrase.com/blog/posts/number-localization/. [Accessed: Jul. 25, 2022]\n\n\n[2] \nWikipedia Contributors, “Locale (computer software),” Wikipedia. Apr. 2022 [Online]. Available: https://en.wikipedia.org/w/index.php?title=Locale_(computer_software)&oldid=1082900932. [Accessed: Jul. 25, 2022]\n\n\n[3] \nA. Herrmann, “How to deal with international data formats in Python,” herrmann.tech. Feb. 2021 [Online]. Available: https://herrmann.tech/en/blog/2021/02/05/how-to-deal-with-international-data-formats-in-python.html. [Accessed: Jul. 25, 2022]"
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#fa-bullseye-objectives",
    "href": "part-wrangling/05-data-reshape.html#fa-bullseye-objectives",
    "title": "19  Reshaping Data",
    "section": "\n19.1  Objectives",
    "text": "19.1  Objectives\nBroadly, your objective while reading this chapter is to be able to identify datasets which have “messy” formats and determine a sequence of operations to transition the data into “tidy” format. To do this, you should be master the following concepts:\n\nDetermine what data format is necessary to generate a desired plot or statistical model\nUnderstand the differences between “wide” and “long” format data and how to transition between the two structures"
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#tidy-and-messy-data",
    "href": "part-wrangling/05-data-reshape.html#tidy-and-messy-data",
    "title": "19  Reshaping Data",
    "section": "\n19.2 Tidy and Messy Data",
    "text": "19.2 Tidy and Messy Data\n\n19.2.1 Motivating Example\nConsider the spreadsheet screenshot in Figure 19.1.\n\n\nFigure 19.1: Spreadsheet intended for human consumption, from [1] (Chapter 3)\n\n\nThis spreadsheet shows New Zealand High School certificate achievement levels for a boys-only school. Typically, students would get level 1 in year 11, level 2 in year 12, and level 3 in year 13, but it is possible for students to gain multiple levels in a single year. This data is organized to show the number of students gaining each type of certification (broken out by gender) across each of the 3 years. There are many blank cells that provide ample space to see the data, and all of the necessary variables are represented: there are essentially three 2x3 tables showing the number of students attaining each NCEA level in each year of school. If all of the information is present in this table, is there really a problem? Perhaps not if the goal is just to display the data, but analyzing this data effectively, or plotting it in a way that is useful, requires some restructuring. Figure 19.2 shows a restructured version of this data in a more compact rectangular format.\n\n\nFigure 19.2: Spreadsheet reorganized for data analysis\n\n\nIn Figure 19.2, each column contains one variable: Year, gender, level, and total number of students. Each row contains one observation. We still have 18 data points, but this format is optimized for statistical analysis, rather than to display for (human) visual consumption. We will refer to this restructured data as “tidy” data: it has a single column for each variable and a single row for each observation.\n\n19.2.2 Defining Tidy data\nThe illustrations below are lifted from an excellent blog post [2] about tidy data; they’re reproduced here because\n\nthey’re beautiful and licensed as CCA-4.0-by, and\nthey might be more memorable than the equivalent paragraphs of text without illustration.\n\nMost of the time, data does not come in a format suitable for analysis. Spreadsheets are generally optimized for data entry or viewing, rather than for statistical analysis:\n\nTables may be laid out for easy data entry, so that there are multiple observations in a single row\nIt may be visually preferable to arrange columns of data to show multiple times or categories on the same row for easy comparison\n\nWhen we analyze data, however, we care much more about the fundamental structure of observations: discrete units of data collection. Each observation may have several corresponding variables that may be measured simultaneously, but fundamentally each discrete data point is what we are interested in analyzing.\nThe structure of tidy data reflects this preference for keeping the data in a fundamental form: each observation is in its own row, any observed variables are in single columns. This format is inherently rectangular, which is also important for statistical analysis - our methods are typically designed to work with matrices of data.\n\n\nFigure 19.3: Tidy data format, illustrated.\n\n\n\n\nAn illustration of the principle that every messy dataset is messy in its own way.\n\n\nThe preference for tidy data has several practical implications: it is easier to reuse code on tidy data, allowing for analysis using a standardized set of tools (rather than having to build a custom tool for each data analysis job).\n\n\nTidy data is easier to manage because the same tools and approaches apply to multiple datasets.\n\n\nIn addition, standardized tools for data analysis means that it is easier to collaborate with others: if everyone starts with the same set of assumptions about the data set, you can borrow methods and tools from a collaborator’s analysis and easily apply them to your own data set.\n\n\n\n\n\nCollaboration with tidy data.\n\n\n\n\n\n\nTidy data enables standardized workflows.\n\n\n\n\nFigure 19.4: Tidy data makes it easier to collaborate with others and analyze new data using standardized workflows.\n\n\n\n\n\n\n\n\nExamples: Messy Data\n\n\n\nThese datasets all display the same data: TB cases documented by the WHO in Afghanistan, Brazil, and China, between 1999 and 2000. There are 4 variables: country, year, cases, and population, but each table has a different layout.\n\n\nTable 1\n2\n3\n4\n5\n\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nHere, each observation is a single row, each variable is a column, and everything is nicely arranged for e.g. regression or statistical analysis. We can easily compute another measure, such as cases per 100,000 population, by taking cases/population * 100000 (this would define a new column).\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nHere, we have 4 columns again, but we now have 12 rows: one of the columns is an indicator of which of two numerical observations is recorded in that row; a second column stores the value. This form of the data is more easily plotted in e.g. ggplot2, if we want to show lines for both cases and population, but computing per capita cases would be much more difficult in this form than in the arrangement in table 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nThis form has only 3 columns, because the rate variable (which is a character) stores both the case count and the population. We can’t do anything with this format as it stands, because we can’t do math on data stored as characters. However, this form might be easier to read and record for a human being.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nIn this form, we have two tables - one for population, and one for cases. Each year’s observations are in a separate column. This format is often found in separate sheets of an excel workbook. To work with this data, we’ll need to transform each table so that there is a column indicating which year an observation is from, and then merge the two tables together by country and year.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nTable 5 is very similar to table 3, but the year has been separated into two columns - century, and year. This is more common with year, month, and day in separate columns (or date and time in separate columns), often to deal with the fact that spreadsheets don’t always handle dates the way you’d hope they would.\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Classifying Messy Data\n\n\n\n\n\nProblem\nTable 1\n2\n3\n4\n5\n\n\n\nFor each of the datasets in the previous example, determine whether each table is tidy. If it is not, identify which rule or rules it violates.\nWhat would you have to do in order to compute a standardized TB infection rate per 100,000 people?\n\n\n\n\n\nTable 1\n\ncountry\nyear\ncases\npopulation\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\nThis is tidy data. Computing a standardized infection rate is as simple as creating the variable rate = cases/population*100,000.\n\n\n\n\n\nTable 2\n\ncountry\nyear\ntype\ncount\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\nBrazil\n2000\ncases\n80488\n\n\nBrazil\n2000\npopulation\n174504898\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n1999\npopulation\n1272915272\n\n\nChina\n2000\ncases\n213766\n\n\nChina\n2000\npopulation\n1280428583\n\n\n\n\n\nEach variable does not have its own column (so a single year’s observation of one country actually has 2 rows). Computing a standardized infection rate requires moving cases and population so that each variable has its own column, and then you can proceed using the process in 1.\n\n\n\n\n\nTable 3\n\ncountry\nyear\nrate\n\n\n\nAfghanistan\n1999\n745/19987071\n\n\nAfghanistan\n2000\n2666/20595360\n\n\nBrazil\n1999\n37737/172006362\n\n\nBrazil\n2000\n80488/174504898\n\n\nChina\n1999\n212258/1272915272\n\n\nChina\n2000\n213766/1280428583\n\n\n\n\n\nEach value does not have its own cell (and each variable does not have its own column). In Table 3, you’d have to separate the numerator and denominator of each cell, convert each to a numeric variable, and then you could proceed as in 1.\n\n\n\n\n\nTable 4a\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n745\n2666\n\n\nBrazil\n37737\n80488\n\n\nChina\n212258\n213766\n\n\n\n\n\n\nTable 4b\n\ncountry\n1999\n2000\n\n\n\nAfghanistan\n19987071\n20595360\n\n\nBrazil\n172006362\n174504898\n\n\nChina\n1272915272\n1280428583\n\n\n\n\n\nThere are multiple observations in each row because there is not a column for year. To compute the rate, you’d need to “stack” the two columns in each table into a single column, add a year column that is 1999, 1999, 1999, 2000, 2000, 2000, and then merge the two tables. Then you could proceed as in 1.\n\n\n\n\n\nTable 5\n\ncountry\ncentury\nyear\nrate\n\n\n\nAfghanistan\n19\n99\n745/19987071\n\n\nAfghanistan\n20\n00\n2666/20595360\n\n\nBrazil\n19\n99\n37737/172006362\n\n\nBrazil\n20\n00\n80488/174504898\n\n\nChina\n19\n99\n212258/1272915272\n\n\nChina\n20\n00\n213766/1280428583\n\n\n\n\n\nEach variable does not have its own column (there are two columns for year, in addition to the issues noted in table3). Computing the rate would be similar to table 3; the year issues aren’t actually a huge deal unless you plot them, at which point 99 will seem to be bigger than 00 (so you’d need to combine the two year columns together first).\n\n\n\n\n\nIt is actually impossible to have a table that violates only one of the rules of tidy data - you have to violate at least two. So a simpler way to state the rules might be:\n\nEach data set goes into its own table (or tibble, if you are using R)\nEach variable gets its own column\n\n\n\n\n\n\n\nAdditional reading\n\n\n\n[3] - IBM SPSS ad that talks about the perils of spreadsheets\n[4] - assembled news stories involving spreadsheet mishaps\n\n\nYou have learned some of the skills to tidy data in Chapter 18, and you’ll learn more in Chapter 20, but by the end of this chapter you will have many of the skills needed to wrangle the most common “messy” data sets into “tidy” form."
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#pivot-operations",
    "href": "part-wrangling/05-data-reshape.html#pivot-operations",
    "title": "19  Reshaping Data",
    "section": "\n19.3 Pivot operations",
    "text": "19.3 Pivot operations\nIt’s fairly common for data to come in forms which are convenient for either human viewing or data entry. Unfortunately, these forms aren’t necessarily the most friendly for analysis.\n\n\nWide and Long format data. Source\n\n\nThe two operations we’ll learn here are wide -> long and long -> wide.\n\n\nPivoting from wide to long (and back) Source\n\n\nThis animation uses the R functions pivot_wider() and pivot_longer() Animation source, but the concept is the same in both R and python.\n\n19.3.1 Longer\nIn many cases, the data come in what we might call “wide” form - some of the column names are not names of variables, but instead, are themselves values of another variable.\n\n\nPicture the Operation\nR\nPython\n\n\n\nTables 4a and 4b are good examples of data which is in “wide” form and should be in long(er) form: the years, which are variables, are column names, and the values are cases and population respectively.\n\ntable4a\n## # A tibble: 3 × 3\n##   country     `1999` `2000`\n## * <chr>        <int>  <int>\n## 1 Afghanistan    745   2666\n## 2 Brazil       37737  80488\n## 3 China       212258 213766\ntable4b\n## # A tibble: 3 × 3\n##   country         `1999`     `2000`\n## * <chr>            <int>      <int>\n## 1 Afghanistan   19987071   20595360\n## 2 Brazil       172006362  174504898\n## 3 China       1272915272 1280428583\n\nThe solution to this is to rearrange the data into “long form”: to take the columns which contain values and “stack” them, adding a variable to indicate which column each value came from. To do this, we have to duplicate the values in any column which isn’t being stacked (e.g. country, in both the example above and the image below).\n\n\nA visual representation of what the pivot_longer operation looks like in practice.\n\n\nOnce our data are in long form, we can (if necessary) separate values that once served as column labels into actual variables, and we’ll have tidy(er) data.\n\n\n\ntba <- table4a %>% \n  pivot_longer(-country, names_to = \"year\", values_to = \"cases\")\ntbb <- table4b %>% \n  pivot_longer(-country, names_to = \"year\", values_to = \"population\")\n\n# To get the tidy data, we join the two together (see Table joins below)\nleft_join(tba, tbb, by = c(\"country\", \"year\")) %>%\n  # make year numeric b/c it's dumb not to\n  mutate(year = as.numeric(year))\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <dbl>  <int>      <int>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\nThe columns are moved to a variable with the name passed to the argument “names_to” (hopefully, that is easy to remember), and the values are moved to a variable with the name passed to the argument “values_to” (again, hopefully easy to remember).\nWe identify ID variables (variables which we don’t want to pivot) by not including them in the pivot statement. We can do this in one of two ways:\n\nselect only variables we want to pivot: pivot_longer(table4a, cols =1999:2000, names_to = \"year\", values_to = \"cases\")\n\nselect variables we don’t want to pivot, using - to remove them. (see above, where -country excludes country from the pivot operation)\n\nWhich option is easier depends how many things you’re pivoting (and how the columns are structured).\nIf we wanted to avoid the table join, we could do this process another way: first, we would add a column to each tibble called id with values “cases” and “population” respectively. Then, we could bind the two tables together by row (so stack them on top of each other). We could then do a wide-to-long pivot, followed by a long-to-wide pivot to get our data into tidy form.\n\n# Create ID columns\ntable4a.x <- table4a %>% mutate(id = \"cases\")\ntable4b.x <- table4b %>% mutate(id = \"population\")\n# Create one table\ntable4 <- bind_rows(table4a.x, table4b.x)\n\ntable4_long <- table4 %>%\n  # rearrange columns\n  select(country, id, `1999`, `2000`) %>%\n  # Don't pivot country or id\n  pivot_longer(-c(country:id), names_to = \"year\", values_to = \"count\")\n\n# Intermediate fully-long form\ntable4_long\n## # A tibble: 12 × 4\n##    country     id         year       count\n##    <chr>       <chr>      <chr>      <int>\n##  1 Afghanistan cases      1999         745\n##  2 Afghanistan cases      2000        2666\n##  3 Brazil      cases      1999       37737\n##  4 Brazil      cases      2000       80488\n##  5 China       cases      1999      212258\n##  6 China       cases      2000      213766\n##  7 Afghanistan population 1999    19987071\n##  8 Afghanistan population 2000    20595360\n##  9 Brazil      population 1999   172006362\n## 10 Brazil      population 2000   174504898\n## 11 China       population 1999  1272915272\n## 12 China       population 2000  1280428583\n\n# make wider, with case and population columns\ntable4_tidy <- table4_long %>%\n  pivot_wider(names_from = id, values_from = count)\n\ntable4_tidy\n## # A tibble: 6 × 4\n##   country     year   cases population\n##   <chr>       <chr>  <int>      <int>\n## 1 Afghanistan 1999     745   19987071\n## 2 Afghanistan 2000    2666   20595360\n## 3 Brazil      1999   37737  172006362\n## 4 Brazil      2000   80488  174504898\n## 5 China       1999  212258 1272915272\n## 6 China       2000  213766 1280428583\n\n\n\nIn Pandas, pandas.melt(...) takes id_vars, value_vars, var_name, and value_name. Otherwise, it functions nearly exactly the same as pivot_longer; the biggest difference is that column selection works differently in python than it does in the tidyverse.\nAs in R, we can choose to either do a melt/pivot_longer operation on each table and then join the tables together, or we can concatenate the rows and do a melt/pivot_longer operation followed by a pivot/pivot_wider operation.\n\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntba = pd.melt(table4a, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'cases')\ntbb = pd.melt(table4b, id_vars = ['country'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'population')\n\n# To get the tidy data, we join the two together (see Table joins below)\ntable4_tidy = pd.merge(tba, tbb, on = [\"country\", \"year\"], how = 'left')\n\nHere’s the melt/pivot_longer + pivot/pivot_wider version:\n\nimport pandas as pd\n\n# Get tables from R\ntable4a = r.table4a\ntable4b = r.table4b\n\ntable4a['id'] = \"cases\"\ntable4b['id'] = \"population\"\n\ntable4 = pd.concat([table4a, table4b])\n\n# Fully long form\ntable4_long = pd.melt(table4, id_vars = ['country', 'id'], value_vars = ['1999', '2000'], var_name = 'year', value_name = 'count')\n\n# Tidy form - case and population columns\ntable4_tidy2 = pd.pivot(table4_long, index = ['country', 'year'], columns = ['id'], values = 'count')\n# reset_index() gets rid of the grouped index\ntable4_tidy2.reset_index()\n## id      country  year   cases  population\n## 0   Afghanistan  1999     745    19987071\n## 1   Afghanistan  2000    2666    20595360\n## 2        Brazil  1999   37737   172006362\n## 3        Brazil  2000   80488   174504898\n## 4         China  1999  212258  1272915272\n## 5         China  2000  213766  1280428583\n\n\n\n\n\n19.3.2 Wider\nWhile it’s very common to need to transform data into a longer format, it’s not that uncommon to need to do the reverse operation. When an observation is scattered across multiple rows, your data is too long and needs to be made wider again.\n\n\nPicture the Operation\nR\nPython\n\n\n\nTable 2 is an example of a table that is in long format but needs to be converted to a wider layout to be “tidy” - there are separate rows for cases and population, which means that a single observation (one year, one country) has two rows.\n\n\nA visual representation of what the pivot_wider operation looks like in practice.\n\n\n\n\n\ntable2 %>%\n  pivot_wider(names_from = type, values_from = count)\n## # A tibble: 6 × 4\n##   country      year  cases population\n##   <chr>       <int>  <int>      <int>\n## 1 Afghanistan  1999    745   19987071\n## 2 Afghanistan  2000   2666   20595360\n## 3 Brazil       1999  37737  172006362\n## 4 Brazil       2000  80488  174504898\n## 5 China        1999 212258 1272915272\n## 6 China        2000 213766 1280428583\n\n\n\n\ntable2 = r.table2\n\npd.pivot(table2, index = ['country', 'year'], columns = ['type'], values = 'count').reset_index()\n## type      country  year   cases  population\n## 0     Afghanistan  1999     745    19987071\n## 1     Afghanistan  2000    2666    20595360\n## 2          Brazil  1999   37737   172006362\n## 3          Brazil  2000   80488   174504898\n## 4           China  1999  212258  1272915272\n## 5           China  2000  213766  1280428583\n\n\n\n\n\n\n\n\n\n\nTry it Out!\n\n\n\nIn the next section, we’ll be using the WHO surveillance of disease incidence data (link). I originally wrote this using data from 2020, but the WHO has since migrated to a new system and now provides their data in a much tidier long form (Excel link). For demonstration purposes, I’ll continue using the messier 2020 data, but the link is no longer available on the WHO’s site.\nIt will require some preprocessing before it’s suitable for a demonstration. I’ll do some of it, but in this section, you’re going to do the rest.\n\n\nPreprocessing\nProblem\nR solution\nPython solution\n\n\n\nYou don’t have to understand what this code is doing just yet.\n\nlibrary(readxl)\nlibrary(purrr) # This uses the map() function as a replacement for for loops. \n# It's pretty sweet\nlibrary(tibble)\nlibrary(dplyr)\n\ndownload.file(\"https://github.com/srvanderplas/datasets/raw/main/raw/2020_WHO_incidence_series.xls\", \"../data/2020_WHO_incidence_series.xls\")\nsheets <- excel_sheets(\"../data/2020_WHO_incidence_series.xls\")\nsheets <- sheets[-c(1, length(sheets))] # get rid of 1st and last sheet name\n\n# This command says \"for each sheet, read in the excel file with that sheet name\"\n# map_df means paste them all together into a single data frame\ndisease_incidence <- map_df(sheets, ~read_xls(path =\"../data/2020_WHO_incidence_series.xls\", sheet = .))\n\n# Alternately, we could write a loop:\ndisease_incidence2 <- tibble() # Blank data frame\nfor (i in 1:length(sheets)) {\n  disease_incidence2 <- bind_rows(\n    disease_incidence2, \n    read_xls(path = \"../data/2020_WHO_incidence_series.xls\", sheet = sheets[i])\n  )\n}\n\n# export for Python (and R, if you want)\nreadr::write_csv(disease_incidence, file = \"../data/2020_who_disease_incidence.csv\")\n\n\n\nDownload the exported data here and import it into Python and R. Transform it into long format, so that there is a year column. You should end up with a table that has dimensions of approximately 6 columns and 83,000 rows (or something close to that).\nCan you make a line plot of cases of measles in Bangladesh over time?\n\nhead(disease_incidence)\n## # A tibble: 6 × 43\n##   WHO_R…¹ ISO_c…² Cname Disease `2018` `2017` `2016` `2015` `2014` `2013` `2012`\n##   <chr>   <chr>   <chr> <chr>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n## 1 EMR     AFG     Afgh… CRS         NA     NA     NA      0      0      0     NA\n## 2 EUR     ALB     Alba… CRS          0      0     NA     NA     NA      0      0\n## 3 AFR     DZA     Alge… CRS         NA     NA      0      0     NA     NA      0\n## 4 EUR     AND     Ando… CRS          0      0      0     NA     NA      0      0\n## 5 AFR     AGO     Ango… CRS         NA     NA     NA     NA     NA     NA     NA\n## 6 AMR     ATG     Anti… CRS          0      0      0      0      0      0      0\n## # … with 32 more variables: `2011` <dbl>, `2010` <dbl>, `2009` <dbl>,\n## #   `2008` <dbl>, `2007` <dbl>, `2006` <dbl>, `2005` <dbl>, `2004` <dbl>,\n## #   `2003` <dbl>, `2002` <dbl>, `2001` <dbl>, `2000` <dbl>, `1999` <dbl>,\n## #   `1998` <dbl>, `1997` <dbl>, `1996` <dbl>, `1995` <dbl>, `1994` <dbl>,\n## #   `1993` <dbl>, `1992` <dbl>, `1991` <dbl>, `1990` <dbl>, `1989` <dbl>,\n## #   `1988` <dbl>, `1987` <dbl>, `1986` <dbl>, `1985` <dbl>, `1984` <dbl>,\n## #   `1983` <dbl>, `1982` <dbl>, `1981` <dbl>, `1980` <dbl>, and abbreviated …\n\n\n\n\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(stringr)\nwho_disease <- read_csv(\"../data/2020_who_disease_incidence.csv\", na = \".\")\n\nwho_disease_long <- who_disease %>%\n  pivot_longer(matches(\"\\\\d{4}\"), names_to = \"year\", values_to = \"cases\") %>%\n  rename(Country = Cname) %>%\n  mutate(Disease = str_replace(Disease, \"CRS\", \"Congenital Rubella\"),\n         year = as.numeric(year),\n         cases = as.numeric(cases))\n\nfilter(who_disease_long, Country == \"Bangladesh\", Disease == \"measles\") %>%\n  ggplot(aes(x = year, y = cases)) + geom_line()\n\n\n\n\n\n\n\nimport pandas as pd\nfrom plotnine import *\n\nwho_disease = pd.read_csv(\"../data/2020_who_disease_incidence.csv\", na_values = ['NA', 'NaN'])\nwho_disease_long = pd.melt(who_disease, id_vars = ['WHO_REGION', 'ISO_code', 'Cname', 'Disease'], var_name = 'year', value_name = 'cases')\n# Rename cname to country\nwho_disease_long = who_disease_long.rename(columns={\"Cname\": \"Country\"})\nwho_disease_long.replace(\"CRS\", \"Congenital Rubella\")\nwho_disease_long['year'] = pd.to_numeric(who_disease_long['year'])\n\ntmp = who_disease_long.query(\"Country=='Bangladesh' & Disease == 'measles'\")\nggplot(tmp, aes(x = \"year\", y = \"cases\")) + geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\nOther resources\n\n\n\n[5] - very nice task-oriented chapter that’s below the level addressed in this course but still useful"
  },
  {
    "objectID": "part-wrangling/05-data-reshape.html#references",
    "href": "part-wrangling/05-data-reshape.html#references",
    "title": "19  Reshaping Data",
    "section": "\n19.4 References",
    "text": "19.4 References\n\n\n\n\n[1] \nQ. E. McCallum, Bad data handbook: Mapping the world of data problems, 1. ed. Beijing, Köln: O’Reilly, 2013. \n\n\n[2] \nJ. Lowndes and A. Horst, “Tidy data for efficiency, reproducibility, and collaboration,” Openscapes. Oct. 2020 [Online]. Available: https://www.openscapes.org/blog/2020/10/12/tidy-data//. [Accessed: Jul. 21, 2022]\n\n\n[3] \nInternational Business Machines, “The risks of using spreadsheets for statistical analysis,” The risks of using spreadsheets for statistical analysis. Nov. 2018 [Online]. Available: https://www.ibm.com/downloads/cas/7YEX9BKK. [Accessed: Jul. 21, 2022]\n\n\n[4] \nP. O’Beirne, F. Hermans, T. Cheng, and M. P. Campbell, “Horror Stories,” European Spreadsheet Risks Interest Group Horror Stories. Oct. 2020 [Online]. Available: http://www.eusprig.org/horror-stories.htm. [Accessed: Jul. 21, 2022]\n\n\n[5] \nJ. Dougherty and I. Ilyankou, “Clean Up Messy Data,” in Hands-On Data Visualization, 1st ed., O’Reilly Media, 2021, p. 480 [Online]. Available: http://handsondataviz.github.io/. [Accessed: Jul. 21, 2022]"
  },
  {
    "objectID": "part-wrangling/06-data-join.html#example-gas-prices-data",
    "href": "part-wrangling/06-data-join.html#example-gas-prices-data",
    "title": "20  Joining Data",
    "section": "\n20.1 Example: Gas Prices Data",
    "text": "20.1 Example: Gas Prices Data\nThe US Energy Information Administration tracks gasoline prices, with data available on a weekly level since late 1994. You can go to this site to see a nice graph of gas prices, along with a corresponding table. \n\n\nGas prices at US EIA site\n\n\nThe data in the table is structured in a fairly easy to read form: each row is a month; each week in the month is a set of two columns: one for the date, one for the average gas price. While this data is definitely not tidy, it is readable.\nBut looking at the chart at the top of the page, it’s not clear how we might get that chart from the data in the format it’s presented here: to get a chart like that, we would need a table where each row was a single date, and there were columns for date and price. That would be tidy form data, and so we have to get from the wide, human-readable form into the long, tidier form that we can graph.\n\n\n\n\n\n\nTry it out: Manual Formatting in Excel\n\n\n\n\n\nProblem\nSolution\nVideo\n\n\n\nAn excel spreadsheet of the data as downloaded in January 2023 is available here. Can you manually format the data (or even just the first year or two of data) into a long, skinny format?\nWhat steps are involved?\n\n\n\nCopy the year-month column, creating one vertical copy for every set of columns\nMove each block of two columns down to the corresponding vertical copy\nDelete empty rows\nFormat dates\nDelete empty columns\n\n\n\n\n\n\nFigure 20.1: Here is a video of me doing most of the cleaning steps - I skipped out on cleaning up the dates because Excel is miserable for working with dates.\n\n\n\n\n\n\n\n\n20.1.1 Setup: Gas Price Data Cleaning\nFor the next two examples, we’ll read the data in from the HTML table online and work to make it something we could e.g. plot. Before we can start cleaning, we have to read in the data:\n\nlibrary(rvest) # scrape data from the web\nlibrary(xml2) # parse xml data\nurl <- \"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\"\n\nhtmldoc <- read_html(url)\ngas_prices_html <- html_table(htmldoc, fill = T, trim = T)[[5]][,1:11]\n\n\n\n\nFirst 6 rows of gas prices data as read into R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear-Month\nWeek 1\nWeek 1\nWeek 2\nWeek 2\nWeek 3\nWeek 3\nWeek 4\nWeek 4\nWeek 5\nWeek 5\n\n\n\nYear-Month\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\nEnd Date\nValue\n\n\n1994-Nov\n\n\n\n\n\n\n11/28\n1.175\n\n\n\n\n1994-Dec\n12/05\n1.143\n12/12\n1.118\n12/19\n1.099\n12/26\n1.088\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1995-Jan\n01/02\n1.104\n01/09\n1.111\n01/16\n1.102\n01/23\n1.110\n01/30\n1.109\n\n\n1995-Feb\n02/06\n1.103\n02/13\n1.099\n02/20\n1.093\n02/27\n1.101\n\n\n\n\n\n\n\n\nimport pandas as pd\ngas_prices_html = pd.read_html(\"https://www.eia.gov/dnav/pet/hist/LeafHandler.ashx?n=pet&s=emm_epm0u_pte_nus_dpg&f=w\")[4]\n\n\n\n\n(‘Year-Month’, ‘Year-Month’)\n(‘Week 1’, ‘End Date’)\n(‘Week 1’, ‘Value’)\n(‘Week 2’, ‘End Date’)\n(‘Week 2’, ‘Value’)\n(‘Week 3’, ‘End Date’)\n(‘Week 3’, ‘Value’)\n(‘Week 4’, ‘End Date’)\n(‘Week 4’, ‘Value’)\n(‘Week 5’, ‘End Date’)\n(‘Week 5’, ‘Value’)\n(‘Unnamed: 11_level_0’, ‘Unnamed: 11_level_1’)\n(‘Unnamed: 12_level_0’, ‘Unnamed: 12_level_1’)\n\n\n\n0\n1994-Nov\nnan\nnan\nnan\nnan\nnan\nnan\n11/28\n1.175\nnan\nnan\nnan\nnan\n\n\n1\n1994-Dec\n12/05\n1.143\n12/12\n1.118\n12/19\n1.099\n12/26\n1.088\nnan\nnan\nnan\nnan\n\n\n2\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\n3\n1995-Jan\n01/02\n1.104\n01/09\n1.111\n01/16\n1.102\n01/23\n1.11\n01/30\n1.109\nnan\nnan\n\n\n4\n1995-Feb\n02/06\n1.103\n02/13\n1.099\n02/20\n1.093\n02/27\n1.101\nnan\nnan\nnan\nnan\n\n\n\n\n\n\n\n\n\nTry it out: Formatting with Pivot Operations\n\n\n\n\n\nProblem\nSketch\nR solution\nPython solution\n\n\n\nCan you format the data in a long-skinny format for plotting using pivot operations without any database merges?\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\nHow do your steps compare to the steps you used for the manual approach?\n\n\n\n\nSteps to work through the gas prices data cleaning process\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(magrittr) # pipe friendly operations\n\n# Function to clean up column names\n# Written as an extra function because it makes the code a lot cleaner\nfix_gas_names <- function(x) {\n  # Add extra header row information\n  paste(x, c(\"\", rep(c(\"Date\", \"Value\"), times = 5))) %>%\n    # trim leading/trailing spaces\n    str_trim() %>%\n    # replace characters in names that aren't ok for variables in R\n    make.names()\n}\n\n# Clean up the table a bit\ngas_prices_raw <- gas_prices_html %>%\n  set_names(fix_gas_names(names(.))) %>%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %>%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n\nhead(gas_prices_raw)\n## # A tibble: 6 × 11\n##   Year.Month Week.1.Date Week.…¹ Week.…² Week.…³ Week.…⁴ Week.…⁵ Week.…⁶ Week.…⁷\n##   <chr>      <chr>       <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n## 1 1994-Nov   \"\"          \"\"      \"\"      \"\"      \"\"      \"\"      11/28   1.175  \n## 2 1994-Dec   \"12/05\"     \"1.143\" \"12/12\" \"1.118\" \"12/19\" \"1.099\" 12/26   1.088  \n## 3 1995-Jan   \"01/02\"     \"1.104\" \"01/09\" \"1.111\" \"01/16\" \"1.102\" 01/23   1.110  \n## 4 1995-Feb   \"02/06\"     \"1.103\" \"02/13\" \"1.099\" \"02/20\" \"1.093\" 02/27   1.101  \n## 5 1995-Mar   \"03/06\"     \"1.103\" \"03/13\" \"1.096\" \"03/20\" \"1.095\" 03/27   1.102  \n## 6 1995-Apr   \"04/03\"     \"1.116\" \"04/10\" \"1.134\" \"04/17\" \"1.149\" 04/24   1.173  \n## # … with 2 more variables: Week.5.Date <chr>, Week.5.Value <chr>, and\n## #   abbreviated variable names ¹​Week.1.Value, ²​Week.2.Date, ³​Week.2.Value,\n## #   ⁴​Week.3.Date, ⁵​Week.3.Value, ⁶​Week.4.Date, ⁷​Week.4.Value\n\n\n# gas_prices_raw <- select(gas_prices_raw, -c(X, Date))\ngas_prices_long <- pivot_longer(gas_prices_raw, -Year.Month,\n                                names_to = \"variable\", values_to = \"value\")\n\nhead(gas_prices_long)\n## # A tibble: 6 × 3\n##   Year.Month variable     value\n##   <chr>      <chr>        <chr>\n## 1 1994-Nov   Week.1.Date  \"\"   \n## 2 1994-Nov   Week.1.Value \"\"   \n## 3 1994-Nov   Week.2.Date  \"\"   \n## 4 1994-Nov   Week.2.Value \"\"   \n## 5 1994-Nov   Week.3.Date  \"\"   \n## 6 1994-Nov   Week.3.Value \"\"\n\n\ngas_prices_sep <- separate(gas_prices_long, variable, into = c(\"extra\", \"week\", \"variable\"), sep = \"\\\\.\") %>%\n  select(-extra)\nhead(gas_prices_sep)\n## # A tibble: 6 × 4\n##   Year.Month week  variable value\n##   <chr>      <chr> <chr>    <chr>\n## 1 1994-Nov   1     Date     \"\"   \n## 2 1994-Nov   1     Value    \"\"   \n## 3 1994-Nov   2     Date     \"\"   \n## 4 1994-Nov   2     Value    \"\"   \n## 5 1994-Nov   3     Date     \"\"   \n## 6 1994-Nov   3     Value    \"\"\n\n\ngas_prices_wide <- pivot_wider(gas_prices_sep, id_cols = c(\"Year.Month\", \"week\"), names_from = variable, values_from = value)\nhead(gas_prices_wide)\n## # A tibble: 6 × 4\n##   Year.Month week  Date    Value  \n##   <chr>      <chr> <chr>   <chr>  \n## 1 1994-Nov   1     \"\"      \"\"     \n## 2 1994-Nov   2     \"\"      \"\"     \n## 3 1994-Nov   3     \"\"      \"\"     \n## 4 1994-Nov   4     \"11/28\" \"1.175\"\n## 5 1994-Nov   5     \"\"      \"\"     \n## 6 1994-Dec   1     \"12/05\" \"1.143\"\n\n\ngas_prices_date <- gas_prices_wide %>%\n  filter(nchar(Value) > 0) %>%\n  separate(Year.Month, into = c(\"Year\", \"Month\"), sep = \"-\") %>%\n  mutate(Date = paste(Year, Date, sep = \"/\")) %>%\n  select(-c(1:3))\n  \nhead(gas_prices_date)\n## # A tibble: 6 × 2\n##   Date       Value\n##   <chr>      <chr>\n## 1 1994/11/28 1.175\n## 2 1994/12/05 1.143\n## 3 1994/12/12 1.118\n## 4 1994/12/19 1.099\n## 5 1994/12/26 1.088\n## 6 1995/01/02 1.104\n\n\nlibrary(lubridate)\ngas_prices <- gas_prices_date %>%\n  mutate(Date = ymd(Date),\n         Price.per.gallon = as.numeric(Value)) %>%\n  select(-Value)\n  \nhead(gas_prices)\n## # A tibble: 6 × 2\n##   Date       Price.per.gallon\n##   <date>                <dbl>\n## 1 1994-11-28             1.18\n## 2 1994-12-05             1.14\n## 3 1994-12-12             1.12\n## 4 1994-12-19             1.10\n## 5 1994-12-26             1.09\n## 6 1995-01-02             1.10\n\n\n\n\nimport numpy as np\n\ndef fix_gas_names(x):\n  xx = pd.Series(x)\n  # add extra stuff to x\n  y = [\"Date\", \"Value\"]*5\n  y = [\"\", *y, \"\", \"\"]\n  names = xx + ' ' + y\n  names = names.str.strip()\n  names = names.str.replace(\" \", \".\")\n  return list(names)\n\n\ngas_prices_raw = gas_prices_html.copy()\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n## MultiIndex([(         'Year-Month',          'Year-Month'),\n##             (             'Week 1',            'End Date'),\n##             (             'Week 1',               'Value'),\n##             (             'Week 2',            'End Date'),\n##             (             'Week 2',               'Value'),\n##             (             'Week 3',            'End Date'),\n##             (             'Week 3',               'Value'),\n##             (             'Week 4',            'End Date'),\n##             (             'Week 4',               'Value'),\n##             (             'Week 5',            'End Date'),\n##             (             'Week 5',               'Value'),\n##             ('Unnamed: 11_level_0', 'Unnamed: 11_level_1'),\n##             ('Unnamed: 12_level_0', 'Unnamed: 12_level_1')],\n##            )\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\ncolnames\n\n# Set new column names\n## ['Year-Month', 'Week.1.Date', 'Week.1.Value', 'Week.2.Date', 'Week.2.Value', 'Week.3.Date', 'Week.3.Value', 'Week.4.Date', 'Week.4.Value', 'Week.5.Date', 'Week.5.Value', 'Unnamed:.11_level_0', 'Unnamed:.12_level_0']\ngas_prices_raw.columns = colnames\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n\n# Drop extra columns on the end\ngas_prices_raw = gas_prices_raw.iloc[:,0:11]\ngas_prices_raw.head()\n##   Year-Month Week.1.Date  Week.1.Value  ... Week.4.Value  Week.5.Date Week.5.Value\n## 0   1994-Nov         NaN           NaN  ...        1.175          NaN          NaN\n## 1   1994-Dec       12/05         1.143  ...        1.088          NaN          NaN\n## 3   1995-Jan       01/02         1.104  ...        1.110        01/30        1.109\n## 4   1995-Feb       02/06         1.103  ...        1.101          NaN          NaN\n## 5   1995-Mar       03/06         1.103  ...        1.102          NaN          NaN\n## \n## [5 rows x 11 columns]\n\n\ngas_prices_long = pd.melt(gas_prices_raw, id_vars = 'Year-Month', var_name = 'variable')\ngas_prices_long.head()\n##   Year-Month     variable  value\n## 0   1994-Nov  Week.1.Date    NaN\n## 1   1994-Dec  Week.1.Date  12/05\n## 2   1995-Jan  Week.1.Date  01/02\n## 3   1995-Feb  Week.1.Date  02/06\n## 4   1995-Mar  Week.1.Date  03/06\n\n\ngas_prices_sep = gas_prices_long\ngas_prices_sep[[\"extra\", \"week\", \"variable\"]] = gas_prices_sep.variable.str.split(r'\\.', expand = True)\ngas_prices_sep = gas_prices_sep.drop('extra', axis = 1)\ngas_prices_sep.head()\n##   Year-Month variable  value week\n## 0   1994-Nov     Date    NaN    1\n## 1   1994-Dec     Date  12/05    1\n## 2   1995-Jan     Date  01/02    1\n## 3   1995-Feb     Date  02/06    1\n## 4   1995-Mar     Date  03/06    1\n\n\ngas_prices_wide = pd.pivot(gas_prices_sep, index=['Year-Month', 'week'], columns = 'variable', values = 'value')\ngas_prices_wide.head()\n## variable          Date  Value\n## Year-Month week              \n## 1994-Dec   1     12/05  1.143\n##            2     12/12  1.118\n##            3     12/19  1.099\n##            4     12/26  1.088\n##            5       NaN    NaN\n\n\ngas_prices_date = gas_prices_wide.dropna(axis = 0, subset = ['Date', 'Value']).reset_index()\ngas_prices_date[['Year', 'Month']] = gas_prices_date['Year-Month'].str.split(r'-', expand = True)\ngas_prices_date['Date'] = gas_prices_date.Year + '/' + gas_prices_date.Date\ngas_prices_date['Date'] = pd.to_datetime(gas_prices_date.Date)\n\ngas_prices_date.head()\n## variable Year-Month week       Date  Value  Year Month\n## 0          1994-Dec    1 1994-12-05  1.143  1994   Dec\n## 1          1994-Dec    2 1994-12-12  1.118  1994   Dec\n## 2          1994-Dec    3 1994-12-19  1.099  1994   Dec\n## 3          1994-Dec    4 1994-12-26  1.088  1994   Dec\n## 4          1994-Nov    4 1994-11-28  1.175  1994   Nov\n\n\n\ngas_prices = gas_prices_date.drop([\"Year-Month\", \"Year\", \"Month\", \"week\"], axis = 1)\ngas_prices['Price_per_gallon'] = gas_prices.Value\ngas_prices = gas_prices.drop(\"Value\", axis = 1)\ngas_prices.head()\n## variable       Date Price_per_gallon\n## 0        1994-12-05            1.143\n## 1        1994-12-12            1.118\n## 2        1994-12-19            1.099\n## 3        1994-12-26            1.088\n## 4        1994-11-28            1.175\n\n\n\n\n\n\n\n\n\n\n\n\nTry it out: Formatting using merge + pivot\n\n\n\n\n\nProblem\nSketch\nR solution\nPython solution\n\n\n\nCan you format the data in a long-skinny format for plotting using pivot operations using wide-to-long pivot operation(s) and a database merge?\nYou can start with the gas_prices_raw\nWrite out a list of steps, and for each step, sketch out what the data frame should look like.\nHow do your steps compare to the steps you used for the manual approach?\n\n\n\n\n\nWe’ll use the same data cleaning function as before:\n\n# Clean up the table a bit\ngas_prices_raw <- gas_prices_html %>%\n  set_names(fix_gas_names(names(.))) %>%\n  # remove first row that is really an extra header row\n  filter(Year.Month != \"Year-Month\") %>%\n  # get rid of empty rows\n  filter(Year.Month != \"\")\n\nhead(gas_prices_raw)\n## # A tibble: 6 × 11\n##   Year.Month Week.1.Date Week.…¹ Week.…² Week.…³ Week.…⁴ Week.…⁵ Week.…⁶ Week.…⁷\n##   <chr>      <chr>       <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n## 1 1994-Nov   \"\"          \"\"      \"\"      \"\"      \"\"      \"\"      11/28   1.175  \n## 2 1994-Dec   \"12/05\"     \"1.143\" \"12/12\" \"1.118\" \"12/19\" \"1.099\" 12/26   1.088  \n## 3 1995-Jan   \"01/02\"     \"1.104\" \"01/09\" \"1.111\" \"01/16\" \"1.102\" 01/23   1.110  \n## 4 1995-Feb   \"02/06\"     \"1.103\" \"02/13\" \"1.099\" \"02/20\" \"1.093\" 02/27   1.101  \n## 5 1995-Mar   \"03/06\"     \"1.103\" \"03/13\" \"1.096\" \"03/20\" \"1.095\" 03/27   1.102  \n## 6 1995-Apr   \"04/03\"     \"1.116\" \"04/10\" \"1.134\" \"04/17\" \"1.149\" 04/24   1.173  \n## # … with 2 more variables: Week.5.Date <chr>, Week.5.Value <chr>, and\n## #   abbreviated variable names ¹​Week.1.Value, ²​Week.2.Date, ³​Week.2.Value,\n## #   ⁴​Week.3.Date, ⁵​Week.3.Value, ⁶​Week.4.Date, ⁷​Week.4.Value\n\n\ngas_prices_dates <- select(gas_prices_raw, 1, matches(\"Week.[1-5].Date\"))\ngas_prices_values <- select(gas_prices_raw, 1, matches(\"Week.[1-5].Value\"))\n\nhead(gas_prices_dates)\n## # A tibble: 6 × 6\n##   Year.Month Week.1.Date Week.2.Date Week.3.Date Week.4.Date Week.5.Date\n##   <chr>      <chr>       <chr>       <chr>       <chr>       <chr>      \n## 1 1994-Nov   \"\"          \"\"          \"\"          11/28       \"\"         \n## 2 1994-Dec   \"12/05\"     \"12/12\"     \"12/19\"     12/26       \"\"         \n## 3 1995-Jan   \"01/02\"     \"01/09\"     \"01/16\"     01/23       \"01/30\"    \n## 4 1995-Feb   \"02/06\"     \"02/13\"     \"02/20\"     02/27       \"\"         \n## 5 1995-Mar   \"03/06\"     \"03/13\"     \"03/20\"     03/27       \"\"         \n## 6 1995-Apr   \"04/03\"     \"04/10\"     \"04/17\"     04/24       \"\"\nhead(gas_prices_values)\n## # A tibble: 6 × 6\n##   Year.Month Week.1.Value Week.2.Value Week.3.Value Week.4.Value Week.5.Value\n##   <chr>      <chr>        <chr>        <chr>        <chr>        <chr>       \n## 1 1994-Nov   \"\"           \"\"           \"\"           1.175        \"\"          \n## 2 1994-Dec   \"1.143\"      \"1.118\"      \"1.099\"      1.088        \"\"          \n## 3 1995-Jan   \"1.104\"      \"1.111\"      \"1.102\"      1.110        \"1.109\"     \n## 4 1995-Feb   \"1.103\"      \"1.099\"      \"1.093\"      1.101        \"\"          \n## 5 1995-Mar   \"1.103\"      \"1.096\"      \"1.095\"      1.102        \"\"          \n## 6 1995-Apr   \"1.116\"      \"1.134\"      \"1.149\"      1.173        \"\"\n\n\ngas_prices_dates_long <- pivot_longer(gas_prices_dates, -Year.Month, names_to = \"week\", values_to = \"month_day\")\ngas_prices_values_long <- pivot_longer(gas_prices_values, -Year.Month, names_to = \"week\", values_to = \"price_per_gallon\")\n\nhead(gas_prices_dates_long)\n## # A tibble: 6 × 3\n##   Year.Month week        month_day\n##   <chr>      <chr>       <chr>    \n## 1 1994-Nov   Week.1.Date \"\"       \n## 2 1994-Nov   Week.2.Date \"\"       \n## 3 1994-Nov   Week.3.Date \"\"       \n## 4 1994-Nov   Week.4.Date \"11/28\"  \n## 5 1994-Nov   Week.5.Date \"\"       \n## 6 1994-Dec   Week.1.Date \"12/05\"\nhead(gas_prices_values_long)\n## # A tibble: 6 × 3\n##   Year.Month week         price_per_gallon\n##   <chr>      <chr>        <chr>           \n## 1 1994-Nov   Week.1.Value \"\"              \n## 2 1994-Nov   Week.2.Value \"\"              \n## 3 1994-Nov   Week.3.Value \"\"              \n## 4 1994-Nov   Week.4.Value \"1.175\"         \n## 5 1994-Nov   Week.5.Value \"\"              \n## 6 1994-Dec   Week.1.Value \"1.143\"\n\n\nlibrary(lubridate) # ymd function\ngas_prices_dates_long_clean <- gas_prices_dates_long %>%\n  filter(month_day != \"\") %>%\n  mutate(week = str_extract(week, \"\\\\d\") %>% as.numeric()) %>%\n  mutate(year = str_extract(Year.Month, \"\\\\d{4}\"), \n         Date = paste(year, month_day, sep = \"/\") %>% \n           ymd())\n\ngas_prices_values_long_clean <- gas_prices_values_long %>%\n  filter(price_per_gallon != \"\") %>%\n  mutate(week = str_extract(week, \"\\\\d\") %>% as.numeric()) %>%\n  mutate(price_per_gallon = as.numeric(price_per_gallon))\n\nhead(gas_prices_dates_long_clean)\n## # A tibble: 6 × 5\n##   Year.Month  week month_day year  Date      \n##   <chr>      <dbl> <chr>     <chr> <date>    \n## 1 1994-Nov       4 11/28     1994  1994-11-28\n## 2 1994-Dec       1 12/05     1994  1994-12-05\n## 3 1994-Dec       2 12/12     1994  1994-12-12\n## 4 1994-Dec       3 12/19     1994  1994-12-19\n## 5 1994-Dec       4 12/26     1994  1994-12-26\n## 6 1995-Jan       1 01/02     1995  1995-01-02\nhead(gas_prices_values_long_clean)\n## # A tibble: 6 × 3\n##   Year.Month  week price_per_gallon\n##   <chr>      <dbl>            <dbl>\n## 1 1994-Nov       4             1.18\n## 2 1994-Dec       1             1.14\n## 3 1994-Dec       2             1.12\n## 4 1994-Dec       3             1.10\n## 5 1994-Dec       4             1.09\n## 6 1995-Jan       1             1.10\n\n\ngas_prices <- left_join(gas_prices_dates_long_clean, gas_prices_values_long_clean, by = c(\"Year.Month\", \"week\")) %>%\n  select(Date, price_per_gallon)\nhead(gas_prices)\n## # A tibble: 6 × 2\n##   Date       price_per_gallon\n##   <date>                <dbl>\n## 1 1994-11-28             1.18\n## 2 1994-12-05             1.14\n## 3 1994-12-12             1.12\n## 4 1994-12-19             1.10\n## 5 1994-12-26             1.09\n## 6 1995-01-02             1.10\n\n\n\n\ngas_prices_raw = gas_prices_html.copy()\n\n# What do column names look like?\ngas_prices_raw.columns # Multi-Index \n# (https://stackoverflow.com/questions/25189575/pandas-dataframe-select-columns-in-multiindex)\n## MultiIndex([(         'Year-Month',          'Year-Month'),\n##             (             'Week 1',            'End Date'),\n##             (             'Week 1',               'Value'),\n##             (             'Week 2',            'End Date'),\n##             (             'Week 2',               'Value'),\n##             (             'Week 3',            'End Date'),\n##             (             'Week 3',               'Value'),\n##             (             'Week 4',            'End Date'),\n##             (             'Week 4',               'Value'),\n##             (             'Week 5',            'End Date'),\n##             (             'Week 5',               'Value'),\n##             ('Unnamed: 11_level_0', 'Unnamed: 11_level_1'),\n##             ('Unnamed: 12_level_0', 'Unnamed: 12_level_1')],\n##            )\ncolnames = fix_gas_names(gas_prices_raw.columns.get_level_values(0))\ncolnames\n\n# Set new column names\n## ['Year-Month', 'Week.1.Date', 'Week.1.Value', 'Week.2.Date', 'Week.2.Value', 'Week.3.Date', 'Week.3.Value', 'Week.4.Date', 'Week.4.Value', 'Week.5.Date', 'Week.5.Value', 'Unnamed:.11_level_0', 'Unnamed:.12_level_0']\ngas_prices_raw.columns = colnames\n\n# Drop any rows with NaN in Year-Month\ngas_prices_raw = gas_prices_raw.dropna(axis = 0, subset = ['Year-Month'])\n\ngas_prices_raw.head()\n##   Year-Month Week.1.Date  ...  Unnamed:.11_level_0 Unnamed:.12_level_0\n## 0   1994-Nov         NaN  ...                  NaN                 NaN\n## 1   1994-Dec       12/05  ...                  NaN                 NaN\n## 3   1995-Jan       01/02  ...                  NaN                 NaN\n## 4   1995-Feb       02/06  ...                  NaN                 NaN\n## 5   1995-Mar       03/06  ...                  NaN                 NaN\n## \n## [5 rows x 13 columns]\n\n\ngas_prices_dates = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Date', axis = 1)\ngas_prices_values = gas_prices_raw.filter(regex = 'Year-Month|Week.\\d.Value', axis = 1)\n\ngas_prices_dates.head()\n##   Year-Month Week.1.Date Week.2.Date Week.3.Date Week.4.Date Week.5.Date\n## 0   1994-Nov         NaN         NaN         NaN       11/28         NaN\n## 1   1994-Dec       12/05       12/12       12/19       12/26         NaN\n## 3   1995-Jan       01/02       01/09       01/16       01/23       01/30\n## 4   1995-Feb       02/06       02/13       02/20       02/27         NaN\n## 5   1995-Mar       03/06       03/13       03/20       03/27         NaN\ngas_prices_values.head()\n##   Year-Month  Week.1.Value  ...  Week.4.Value  Week.5.Value\n## 0   1994-Nov           NaN  ...         1.175           NaN\n## 1   1994-Dec         1.143  ...         1.088           NaN\n## 3   1995-Jan         1.104  ...         1.110         1.109\n## 4   1995-Feb         1.103  ...         1.101           NaN\n## 5   1995-Mar         1.103  ...         1.102           NaN\n## \n## [5 rows x 6 columns]\n\n\ngas_prices_dates_long = pd.melt(gas_prices_dates, id_vars = 'Year-Month', var_name = \"week\", value_name = \"month_day\")\ngas_prices_values_long = pd.melt(gas_prices_values, id_vars = 'Year-Month', var_name = \"week\", value_name = \"price_per_gallon\")\n\ngas_prices_dates_long.head()\n##   Year-Month         week month_day\n## 0   1994-Nov  Week.1.Date       NaN\n## 1   1994-Dec  Week.1.Date     12/05\n## 2   1995-Jan  Week.1.Date     01/02\n## 3   1995-Feb  Week.1.Date     02/06\n## 4   1995-Mar  Week.1.Date     03/06\ngas_prices_values_long.head()\n##   Year-Month          week  price_per_gallon\n## 0   1994-Nov  Week.1.Value               NaN\n## 1   1994-Dec  Week.1.Value             1.143\n## 2   1995-Jan  Week.1.Value             1.104\n## 3   1995-Feb  Week.1.Value             1.103\n## 4   1995-Mar  Week.1.Value             1.103\n\n\ngas_prices_dates_long_clean = gas_prices_dates_long.dropna().copy()\ngas_prices_dates_long_clean[\"week\"] = gas_prices_dates_long_clean.week.str.extract(r\"Week.(\\d).Date\")\ngas_prices_dates_long_clean[\"year\"] = gas_prices_dates_long_clean[\"Year-Month\"].str.extract(r\"(\\d{4})-[A-z]{3}\")\ngas_prices_dates_long_clean[\"Date\"] = gas_prices_dates_long_clean.year + \"/\" + gas_prices_dates_long_clean.month_day\ngas_prices_dates_long_clean[\"Date\"] = pd.to_datetime(gas_prices_dates_long_clean.Date)\n\n\ngas_prices_values_long_clean = gas_prices_values_long.dropna().copy()\ngas_prices_values_long_clean[\"week\"] = gas_prices_values_long_clean.week.str.extract(r\"Week.(\\d).Value\")\ngas_prices_values_long_clean[\"price_per_gallon\"] = pd.to_numeric(gas_prices_values_long_clean[\"price_per_gallon\"])\n\ngas_prices_dates_long_clean.head()\n##   Year-Month week month_day  year       Date\n## 1   1994-Dec    1     12/05  1994 1994-12-05\n## 2   1995-Jan    1     01/02  1995 1995-01-02\n## 3   1995-Feb    1     02/06  1995 1995-02-06\n## 4   1995-Mar    1     03/06  1995 1995-03-06\n## 5   1995-Apr    1     04/03  1995 1995-04-03\ngas_prices_values_long_clean.head()\n##   Year-Month week  price_per_gallon\n## 1   1994-Dec    1             1.143\n## 2   1995-Jan    1             1.104\n## 3   1995-Feb    1             1.103\n## 4   1995-Mar    1             1.103\n## 5   1995-Apr    1             1.116\n\n\ngas_prices = pd.merge(gas_prices_dates_long_clean, gas_prices_values_long_clean, on = (\"Year-Month\", \"week\")).loc[:,[\"Date\", \"price_per_gallon\"]]\ngas_prices.head()\n##         Date  price_per_gallon\n## 0 1994-12-05             1.143\n## 1 1995-01-02             1.104\n## 2 1995-02-06             1.103\n## 3 1995-03-06             1.103\n## 4 1995-04-03             1.116\n\n\n\n\n\n\n\n\n\n\n[1] \nG. Grolemund and H. Wickham, R for Data Science, 1st ed. O’Reilly Media, 2017 [Online]. Available: https://r4ds.had.co.nz/. [Accessed: May 09, 2022]\n\n\n[2] \nThe Carpentries, “Accessing SQLite Databases Using Python and Pandas,” Data Analysis and Visualization in Python for Ecologists. 2022 [Online]. Available: https://datacarpentry.org/python-ecology-lesson/09-working-with-sql/index.html. [Accessed: Jul. 26, 2022]"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#fa-bullseye-objectives",
    "href": "part-wrangling/07-datetime.html#fa-bullseye-objectives",
    "title": "21  Dates and Times",
    "section": "\n21.1  Objectives",
    "text": "21.1  Objectives\n\nUnderstand the complexities of working with datetime data\nCreate datetime formatted data from character and numeric encodings\nFormat/print datetime data in the desired format"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#why-dates-and-times-are-hard",
    "href": "part-wrangling/07-datetime.html#why-dates-and-times-are-hard",
    "title": "21  Dates and Times",
    "section": "\n21.2 Why Dates and Times are hard",
    "text": "21.2 Why Dates and Times are hard\nI’m going to let Tom Scott deliver this portion of the material for me, as his times and timezones video is excellent and entertaining.\n\n\n\n\nThere is also an excellent StackOverflow question [1] and answers [3] demonstrating exactly how times and time zones can get very confusing even in relatively simple circumstances.\nLong story short, we will be using libraries in R and python which handle some of these complexities for us, because dates, times, and timezones are hard and we really don’t want to know exactly how hard they are. The libraries I’ve chosen for this are datetime in Python (used by Pandas), and lubridate in R.\n\n\n\n\n\n\nTry It Out - Getting Set up\n\n\n\n\n\nR\nPython\n\n\n\n\n## install.packages(\"lubridate\")\nlibrary(lubridate)\n\n# get current date/time\ntoday()\n## [1] \"2023-01-23\"\nnow()\n## [1] \"2023-01-23 19:20:47 CST\"\n\nLubridate cheat sheet Lubridate documentation\n\n\n\npip install datetime\n\n\nimport datetime\n\ntoday = datetime.date.today()\ntoday\n## datetime.date(2023, 1, 23)\nprint(today)\n## 2023-01-23\nnow = datetime.datetime.now()\nnow\n## datetime.datetime(2023, 1, 23, 19, 20, 49, 39205)\nprint(now)\n## 2023-01-23 19:20:49.039205\n\npandas datetime documentation"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#getting-started",
    "href": "part-wrangling/07-datetime.html#getting-started",
    "title": "21  Dates and Times",
    "section": "\n21.3 Getting Started",
    "text": "21.3 Getting Started\nLet’s differentiate between three types of data which refer to a point in time:\n\na date\n\na time within a day\na date-time - a specific time on a specific date\n\nNow, let’s think about all of the different ways we can specify dates. The table below has examples along with strptime formats that are used in both R and python for telling the computer which date format is used.\n\n\nTable 21.1: Different ways to specify dates and times.\n\n\n\n\n\n\n\n\n\nExample\nType\nNotes\n\nstrptime format\n\n\n\n1\nJanuary 12, 2023\ndate\nCommon in US/N. America\n%B %d, %Y\n\n\n2\n12 January 2023\ndate\nCommon in Europe\n%d %B %Y\n\n\n3\n01/12/2023\ndate\nCommon in US\n%m/%d/%Y\n\n\n4\n1/12/23\ndate\nCommon in US\n%m/%d/%y\n\n\n5\n12/01/2023\ndate\nCommon in Europe/Asia\n%d/%m/%Y\n\n\n6\n2023-01-12\ndate\nISO 8601 standard\n(automatically sorts chronologically)\n%Y-%m-%d\nor %F\n\n\n7\n12 2023\ndate\nday of year + year\n%j %Y\n\n\n8\n9:23 PM\ntime\n12h time\n%I:%M %p\n\n\n9\n21:23\ntime\n24h time (military time)\n%H:%M\nor %R\n\n\n10\n21:23:05\ntime\n24h time (with seconds)\n%H:%M:%S\nor %T\n\n\n11\n2023-01-12T21:23:05\ndatetime\nISO 8601 international standard\n%FT%T\n\n\n\n\nNote that rows 4 and 5 of Table 21.1 are ambiguous if you don’t know what location your data comes from - the dates could refer to December 1, 2023 or January 12, 2023. This only gets worse if you use 2-digit years.\nThere are three main ways that you might want to create a date/time [4]:\n\nFrom a string\nFrom individual date/time components\nFrom an existing date/time object"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#creation-from-strings",
    "href": "part-wrangling/07-datetime.html#creation-from-strings",
    "title": "21  Dates and Times",
    "section": "\n21.4 Creation from Strings",
    "text": "21.4 Creation from Strings\nDates and times are often stored in tabular formats as strings. In some cases, these are read in and automatically formatted as date-times, but in other situations, you have to specify the format yourself.\n\n\n\n\n\n\nDemo: Datetimes from Strings\n\n\n\nLet’s use some data from the US Geological Service with records of earthquakes with magnitude greater than 6 on the Richter scale that occurred between January 1, 2000 and January 1, 2023. You can pull this data yourself using https://earthquake.usgs.gov/earthquakes/map/, but you can also access a CSV of the data here.\n\n\nR + lubridate\nBase R\nPandas\n\n\n\n\nlibrary(lubridate)\nquake <- read.csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake)\n## 'data.frame':    3484 obs. of  13 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\n\nBy default, read.csv reads the time information in as a character variable.\n\nlibrary(readr)\nquake2 <- read_csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake2)\n## spc_tbl_ [3,484 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ #EventID         : chr [1:3484] \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : POSIXct[1:3484], format: \"2022-12-28 16:34:20\" \"2022-12-20 10:34:24\" ...\n##  $ Latitude         : num [1:3484] -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num [1:3484] 171 -124 179 -101 -173 ...\n##  $ Depth/km         : num [1:3484] 10 17.9 73 18 38 ...\n##  $ Author           : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr [1:3484] \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr [1:3484] \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num [1:3484] 6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr [1:3484] \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr [1:3484] \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   `#EventID` = col_character(),\n##   ..   Time = col_datetime(format = \"\"),\n##   ..   Latitude = col_double(),\n##   ..   Longitude = col_double(),\n##   ..   `Depth/km` = col_double(),\n##   ..   Author = col_character(),\n##   ..   Catalog = col_character(),\n##   ..   Contributor = col_character(),\n##   ..   ContributorID = col_character(),\n##   ..   MagType = col_character(),\n##   ..   Magnitude = col_double(),\n##   ..   MagAuthor = col_character(),\n##   ..   EventLocationName = col_character()\n##   .. )\n##  - attr(*, \"problems\")=<externalptr>\n\nHowever, if we use readr::read_csv, the data is correctly read in as a POSIXct format, which is how R indicates that something is a datetime object.\nIf we want to directly convert the Time column in quake to a datetime, we can use the lubridate package, which has helper functions ymd_hms, ymd, and more. Our data is formatted in ISO 8601 standard format, which means we can easily read it in with ymd_hms() .\n\nlibrary(lubridate)\nlibrary(dplyr)\nquake <- quake %>% \n  mutate(dateTime = ymd_hms(Time))\nstr(quake)\n## 'data.frame':    3484 obs. of  14 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\n##  $ dateTime         : POSIXct, format: \"2022-12-28 16:34:20\" \"2022-12-20 10:34:24\" ...\n\nWe can then test whether quake$dateTime is the same as quake2$Time :\n\nall.equal(quake2$Time, quake$dateTime)\n## [1] TRUE\n\nSo in the case that your data is not automatically read in as a date-time, you can use the helper functions from lubridate (ymd_hms, ymd, mdy, …) to convert strings to date-time data.\n\n\nAs lovely as the lubridate package is, there are some situations where using the tidyverse may not be desirable or even allowed. It is helpful to know how to solve this problem in base R, even if 99% of the time we can use the much easier-to-remember lubridate package.\nIn this case, we would use the as.POSIXct function, and we probably want to have the reference page up (run ?strptime in the R console to pull up the help page).\nWe’ll need to get the codes that tell R what format our datetimes use - you can use Table 21.1, if you like, or read the as.POSIXct help page to see all possible format codes.\n\nquake <- read.csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nstr(quake)\n## 'data.frame':    3484 obs. of  13 variables:\n##  $ X.EventID        : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ Time             : chr  \"2022-12-28T16:34:20Z\" \"2022-12-20T10:34:24Z\" \"2022-12-14T18:40:26Z\" \"2022-12-11T14:31:29Z\" ...\n##  $ Latitude         : num  -21.3 40.5 51.6 17.2 -15.3 ...\n##  $ Longitude        : num  171 -124 179 -101 -173 ...\n##  $ Depth.km         : num  10 17.9 73 18 38 ...\n##  $ Author           : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Catalog          : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ Contributor      : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ ContributorID    : chr  \"us7000j0n4\" \"nc73821036\" \"us6000j985\" \"us6000j8lp\" ...\n##  $ MagType          : chr  \"mww\" \"mw\" \"mww\" \"mww\" ...\n##  $ Magnitude        : num  6.1 6.4 6.3 6 6.8 6.1 6.2 6 7 6.9 ...\n##  $ MagAuthor        : chr  \"us\" \"nc\" \"us\" \"us\" ...\n##  $ EventLocationName: chr  \"southeast of the Loyalty Islands\" \"15km WSW of Ferndale, CA\" \"Rat Islands, Aleutian Islands, Alaska\" \"8 km E of Técpan de Galeana, Mexico\" ...\nquake$dateTime2 <- as.POSIXct(quake$Time, \"%Y-%m-%dT%H:%M:%S\")\nall.equal(quake$dateTime, quake$dateTime2)\n## [1] TRUE\n\nSo using as.POSIXct we do not get the convenient handling of time zones that we got using ymd_hms, but we can set the time zone explicitly if we want to do so.\n\nquake$dateTime2 <- as.POSIXct(quake$Time, tz = \"UTC\", \"%Y-%m-%dT%H:%M:%S\")\nall.equal(quake$dateTime, quake$dateTime2)\n## [1] TRUE\n\n\n\nIn pandas, we can use the to_datetime method. If the format is not specified, pandas will try to guess the date-time format; in this case, the guess works, but if not, you can provide a format = … argument that works the same way as R.\n\nimport pandas as pd\nquake = pd.read_csv(\"https://github.com/srvanderplas/datasets/raw/main/raw/earthquakes2000.csv\")\nquake.dtypes\n## #EventID              object\n## Time                  object\n## Latitude             float64\n## Longitude            float64\n## Depth/km             float64\n## Author                object\n## Catalog               object\n## Contributor           object\n## ContributorID         object\n## MagType               object\n## Magnitude            float64\n## MagAuthor             object\n## EventLocationName     object\n## dtype: object\nquake.Time[0:10]\n\n# Convert to datetime\n## 0    2022-12-28T16:34:20Z\n## 1    2022-12-20T10:34:24Z\n## 2    2022-12-14T18:40:26Z\n## 3    2022-12-11T14:31:29Z\n## 4    2022-12-04T19:24:15Z\n## 5    2022-11-23T01:08:15Z\n## 6    2022-11-22T16:39:05Z\n## 7    2022-11-22T02:37:57Z\n## 8    2022-11-22T02:03:06Z\n## 9    2022-11-18T13:37:08Z\n## Name: Time, dtype: object\nquake['dateTime'] = pd.to_datetime(quake.Time)\nquake.dtypes\n## #EventID                          object\n## Time                              object\n## Latitude                         float64\n## Longitude                        float64\n## Depth/km                         float64\n## Author                            object\n## Catalog                           object\n## Contributor                       object\n## ContributorID                     object\n## MagType                           object\n## Magnitude                        float64\n## MagAuthor                         object\n## EventLocationName                 object\n## dateTime             datetime64[ns, UTC]\n## dtype: object\nquake.dateTime[0:10]\n\n# Convert to datetime\n## 0   2022-12-28 16:34:20+00:00\n## 1   2022-12-20 10:34:24+00:00\n## 2   2022-12-14 18:40:26+00:00\n## 3   2022-12-11 14:31:29+00:00\n## 4   2022-12-04 19:24:15+00:00\n## 5   2022-11-23 01:08:15+00:00\n## 6   2022-11-22 16:39:05+00:00\n## 7   2022-11-22 02:37:57+00:00\n## 8   2022-11-22 02:03:06+00:00\n## 9   2022-11-18 13:37:08+00:00\n## Name: dateTime, dtype: datetime64[ns, UTC]\nquake['dateTime2'] = pd.to_datetime(quake.Time, format = \"%Y-%m-%dT%H:%M:%S\")\nquake.dtypes\n## #EventID                          object\n## Time                              object\n## Latitude                         float64\n## Longitude                        float64\n## Depth/km                         float64\n## Author                            object\n## Catalog                           object\n## Contributor                       object\n## ContributorID                     object\n## MagType                           object\n## Magnitude                        float64\n## MagAuthor                         object\n## EventLocationName                 object\n## dateTime             datetime64[ns, UTC]\n## dateTime2            datetime64[ns, UTC]\n## dtype: object\nquake.dateTime2[0:10]\n## 0   2022-12-28 16:34:20+00:00\n## 1   2022-12-20 10:34:24+00:00\n## 2   2022-12-14 18:40:26+00:00\n## 3   2022-12-11 14:31:29+00:00\n## 4   2022-12-04 19:24:15+00:00\n## 5   2022-11-23 01:08:15+00:00\n## 6   2022-11-22 16:39:05+00:00\n## 7   2022-11-22 02:37:57+00:00\n## 8   2022-11-22 02:03:06+00:00\n## 9   2022-11-18 13:37:08+00:00\n## Name: dateTime2, dtype: datetime64[ns, UTC]\n\n\n\n\n\n\n\n\n\n\n\n\nTry it Out - Datetimes from Strings\n\n\n\nIt’s usually important for new parents to keep a log of the new baby’s feeds, to ensure that the baby is getting enough liquids and isn’t getting dehydrated. I used an app to keep track of my daughter’s feeds from birth (though here, we’ll only work with the first month of data), and it used a reasonable, if not standard way to store dates and times.\n\n\nProblem\nR solution\nPython solution\n\n\n\nTake a look at the first month of feeds. Note that these data are from August 7, 2021 to November 4, 2021 – roughly baby’s first 90 days.\n\nConvert Start and End to datetime variables\nCan you plot the feeds somehow?\nCan you do arithmetic with datetimes to see if there are any user entry errors?\nThis data was created by a highly unreliable and error prone couple of individuals – specifically, sleep-deprived new parents.\n\nTo do this, you may need to figure out how to specify a non-standard date format in R and/or python. The parse_date_time function is useful in R, and pd.to_datetime() takes a format argument in python.\n\n\nFirst, let’s read the data in and explore a bit.\n\nlibrary(lubridate)\nlibrary(readr)\nfeeds <- read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/feeds_initial.csv\")\nhead(feeds)\n## # A tibble: 6 × 6\n##      id Start               End                 Type   `Quantity (oz)` Quantit…¹\n##   <dbl> <chr>               <chr>               <chr>            <dbl>     <dbl>\n## 1  1368 20:03:30 11-04-2021 20:45:21 11-04-2021 Breast              NA        NA\n## 2  1366 18:00:29 11-04-2021 18:18:29 11-04-2021 Breast              NA        NA\n## 3  1365 16:27:29 11-04-2021 17:03:26 11-04-2021 Breast              NA        NA\n## 4  1364 14:30:01 11-04-2021 14:42:05 11-04-2021 Breast              NA        NA\n## 5  1367 12:48:29 11-04-2021 13:50:29 11-04-2021 Bottle               3        88\n## 6  1363 10:59:18 11-04-2021 11:15:18 11-04-2021 Bottle               3        88\n## # … with abbreviated variable name ¹​`Quantity (ml or g)`\n\n# Looks like %H:%M:%S %m-%d-%Y format.\n\nIt looks like the data is stored in a format where the time (%H:%M:%S) is first and the date (%m-%d-%Y) is second. We can use the parse_date_time function in lubridate\n\nfeeds <- feeds %>%\n  mutate(Start = parse_date_time(Start, orders = c(\"%H:%M:%S %m-%d-%Y\")),\n         End = parse_date_time(End, orders = c(\"%H:%M:%S %m-%d-%Y\")))\n\nLet’s then explore how we might plot this data:\n\nlibrary(ggplot2)\nggplot(feeds, aes(xmin = Start, xmax = End, fill = Type)) + \n  geom_rect(aes(ymin = 1, ymax = 2)) + # Specify default aes\n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  theme_bw() + theme(legend.position = \"bottom\") + \n  scale_y_continuous(breaks = NULL)\n\n\n\n\n\nlibrary(ggplot2)\nfeeds %>%\n  mutate(day = floor_date(Start, \"day\"),\n         hour_start = Start - day,\n         hour_end = End - day) %>%\n  mutate(across(starts_with(\"hour\"), ~as.numeric(., units = \"hours\"))) %>%\n  mutate(doy = yday(day)) %>%\nggplot(aes(ymin = day, ymax = day+days(1), xmin = hour_start, xmax = hour_end, fill = Type)) + \n  geom_rect() + # Specify default aes\n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  scale_x_continuous(\"Hour of the day\") + \n  theme_bw() + theme(legend.position = \"bottom\")\n\n\n\n\nWe can also calculate the duration of each feed and look at the distributions for each type of feed.\n\nfeeds <- feeds %>%\n  mutate(duration = End - Start)\n\nggplot(feeds, aes(x = duration, fill = Type)) + geom_histogram(color = \"black\") + \n  scale_fill_manual(values = c(\"Bottle\" = \"cornflowerblue\", \"Breast\" = \"pink\")) + \n  theme_bw() + theme(legend.position = \"none\") + \n  xlab(\"Feed duration, in seconds\") + facet_wrap(~Type)\n\n\n\n\nWe can see a few suspiciously long feeds - 9000 seconds is 2.5 hours, which is not unheard of for a baby to breastfeed, but would be an exceptionally long bottle feed (unless a parent fell asleep before hitting “stop” on the feed, which is much more likely).\n\n\nFirst, let’s read the data in and explore a bit.\n\nimport pandas as pd\n\nfeeds = pd.read_csv(\"https://raw.githubusercontent.com/srvanderplas/datasets/main/raw/feeds_initial.csv\")\nfeeds.head()\n\n# Looks like %H:%M:%S %m-%d-%Y format.\n##      id                Start  ... Quantity (oz) Quantity (ml or g)\n## 0  1368  20:03:30 11-04-2021  ...           NaN                NaN\n## 1  1366  18:00:29 11-04-2021  ...           NaN                NaN\n## 2  1365  16:27:29 11-04-2021  ...           NaN                NaN\n## 3  1364  14:30:01 11-04-2021  ...           NaN                NaN\n## 4  1367  12:48:29 11-04-2021  ...           3.0               88.0\n## \n## [5 rows x 6 columns]\n\nIt looks like the data is stored in a format where the time (%H:%M:%S) is first and the date (%m-%d-%Y) is second. We can use the format argument to pd.to_datetime to specify this:\n\nfeeds[\"Start\"] = pd.to_datetime(feeds.Start, format = \"%H:%M:%S %m-%d-%Y\")\nfeeds[\"End\"] = pd.to_datetime(feeds.End, format = \"%H:%M:%S %m-%d-%Y\")\nfeeds.head()\n##      id               Start  ... Quantity (oz) Quantity (ml or g)\n## 0  1368 2021-11-04 20:03:30  ...           NaN                NaN\n## 1  1366 2021-11-04 18:00:29  ...           NaN                NaN\n## 2  1365 2021-11-04 16:27:29  ...           NaN                NaN\n## 3  1364 2021-11-04 14:30:01  ...           NaN                NaN\n## 4  1367 2021-11-04 12:48:29  ...           3.0               88.0\n## \n## [5 rows x 6 columns]\n\nIn Python, it is helpful to do a bit of transformation first - this is partly because I’m not as good with Python plotting systems.\n\nimport datetime as dt\nfeeds[\"day\"] = feeds.Start.dt.strftime(\"%Y-%m-%d\")\nfeeds[\"day\"] = pd.to_datetime(feeds.day, format = \"%Y-%m-%d\")\nfeeds[\"day_end\"] = feeds.day + dt.timedelta(days = 1)\n\nfeeds[\"time_start\"] = feeds.Start - feeds.day\nfeeds[\"time_end\"] = feeds.End - feeds.day\nfeeds[\"duration\"] = feeds.time_end - feeds.time_start\n\nNote that as of January 2023, RStudio does not correctly display timedelta data types in python. They show up as NAs in the table, but are printed fine in the console. Don’t spend hours trying to figure out why it isn’t working – it’s bad enough that I did.\n\nfrom plotnine import *\n\n(\n  ggplot(feeds, aes(xmin = \"Start\", xmax = \"End\", fill = \"Type\")) + \n  geom_rect(aes(ymin = 1, ymax = 2)) + \n  scale_fill_manual(values = [\"cornflowerblue\", \"pink\"]) + \n  theme_bw() + scale_y_continuous(breaks = [])\n)\n## <ggplot: (8742786489678)>\n\n\n\n\n\nfrom plotnine import *\n\n(\n  ggplot(feeds, aes(xmin = \"time_start\", xmax = \"time_end\", ymin = \"day\", ymax = \"day_end\", fill = \"Type\")) + \n  geom_rect() + \n  scale_fill_manual(values = [\"cornflowerblue\", \"pink\"]) + \n  theme_bw()\n)\n## <ggplot: (8742809566169)>"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#creation-from-components",
    "href": "part-wrangling/07-datetime.html#creation-from-components",
    "title": "21  Dates and Times",
    "section": "\n21.5 Creation from Components",
    "text": "21.5 Creation from Components\nSometimes, instead of a single string, you’ll have the individual components of the date-time spread across columns. The nycflights13 data is a good example of this.\n\n\n\n\n\n\nDemo: Datetimes from Components\n\n\n\n\n\nR + lubridate\nBase R\nPython\n\n\n\nIn lubridate, the make_date() and make_datetime() functions can be used to create date-times from component pieces.\n\nlibrary(nycflights13)\n\nflights %>%\n  select(year, month, day, hour, minute) %>% \n  head()\n## # A tibble: 6 × 5\n##    year month   day  hour minute\n##   <int> <int> <int> <dbl>  <dbl>\n## 1  2013     1     1     5     15\n## 2  2013     1     1     5     29\n## 3  2013     1     1     5     40\n## 4  2013     1     1     5     45\n## 5  2013     1     1     6      0\n## 6  2013     1     1     5     58\n\nflights <- flights %>%\n  mutate(date = make_date(year, month, day),\n         datetime = make_datetime(year, month, day, hour, minute))\n\nflights %>% select(date, datetime, year, month, day, hour, minute)\n## # A tibble: 336,776 × 7\n##    date       datetime             year month   day  hour minute\n##    <date>     <dttm>              <int> <int> <int> <dbl>  <dbl>\n##  1 2013-01-01 2013-01-01 05:15:00  2013     1     1     5     15\n##  2 2013-01-01 2013-01-01 05:29:00  2013     1     1     5     29\n##  3 2013-01-01 2013-01-01 05:40:00  2013     1     1     5     40\n##  4 2013-01-01 2013-01-01 05:45:00  2013     1     1     5     45\n##  5 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  6 2013-01-01 2013-01-01 05:58:00  2013     1     1     5     58\n##  7 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  8 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n##  9 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n## 10 2013-01-01 2013-01-01 06:00:00  2013     1     1     6      0\n## # … with 336,766 more rows\n\n\n\nIn base R, we can use the ISOdate function to create date times.\n\nflights$datetime_base = with(flights, ISOdatetime(year, month, day, hour, minute, sec= 0, tz=\"UTC\"))\nall.equal(flights$datetime, flights$datetime_base)\n## [1] TRUE\n\n\n\nIn pandas, we can pass multiple columns to pd.to_datetime() and as long as they are named reasonably, pandas will handle the conversion. If we want to have the date but not the time for some reason, we just pass fewer columns to pandas.\n\nfrom nycflights13 import flights\n\nflights[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n##         year  month  day  hour  minute\n## 0       2013      1    1     5      15\n## 1       2013      1    1     5      29\n## 2       2013      1    1     5      40\n## 3       2013      1    1     5      45\n## 4       2013      1    1     6       0\n## ...      ...    ...  ...   ...     ...\n## 336771  2013      9   30    14      55\n## 336772  2013      9   30    22       0\n## 336773  2013      9   30    12      10\n## 336774  2013      9   30    11      59\n## 336775  2013      9   30     8      40\n## \n## [336776 rows x 5 columns]\nflights[\"date\"] = pd.to_datetime(flights[[\"year\", \"month\", \"day\"]])\nflights[\"datetime\"] = pd.to_datetime(flights[[\"year\", \"month\", \"day\", \"hour\", \"minute\"]])\n\n\nflights[[\"date\", \"datetime\", \"year\", \"month\", \"day\", \"hour\", \"minute\"]]\n##              date            datetime  year  month  day  hour  minute\n## 0      2013-01-01 2013-01-01 05:15:00  2013      1    1     5      15\n## 1      2013-01-01 2013-01-01 05:29:00  2013      1    1     5      29\n## 2      2013-01-01 2013-01-01 05:40:00  2013      1    1     5      40\n## 3      2013-01-01 2013-01-01 05:45:00  2013      1    1     5      45\n## 4      2013-01-01 2013-01-01 06:00:00  2013      1    1     6       0\n## ...           ...                 ...   ...    ...  ...   ...     ...\n## 336771 2013-09-30 2013-09-30 14:55:00  2013      9   30    14      55\n## 336772 2013-09-30 2013-09-30 22:00:00  2013      9   30    22       0\n## 336773 2013-09-30 2013-09-30 12:10:00  2013      9   30    12      10\n## 336774 2013-09-30 2013-09-30 11:59:00  2013      9   30    11      59\n## 336775 2013-09-30 2013-09-30 08:40:00  2013      9   30     8      40\n## \n## [336776 rows x 7 columns]"
  },
  {
    "objectID": "part-wrangling/07-datetime.html#creation-from-other-objects",
    "href": "part-wrangling/07-datetime.html#creation-from-other-objects",
    "title": "21  Dates and Times",
    "section": "\n21.6 Creation from other objects",
    "text": "21.6 Creation from other objects\nAnother way to create date-time objects is to modify other objects.\n\n\n\n\n[1] \nFreewind, “Why is subtracting these two times (in 1927) giving a strange result? Stack overflow,” May 22, 2021. [Online]. Available: https://stackoverflow.com/q/6841333/2859168. [Accessed: Jan. 21, 2023]\n\n\n[2] \nJ. Skeet, “Answer to \"why is subtracting these two times (in 1927) giving a strange result?\". Stack overflow,” Jul. 27, 2011. [Online]. Available: https://stackoverflow.com/a/6841479/2859168. [Accessed: Jan. 21, 2023]\n\n\n[3] \nM. Borgwardt, “Answer to \"why is subtracting these two times (in 1927) giving a strange result?\". Stack overflow,” Jul. 27, 2011. [Online]. Available: https://stackoverflow.com/a/6841572/2859168. [Accessed: Jan. 21, 2023]\n\n\n[4] \nH. W. {and}. G. Grolemund, “Dates and times,” in R for data science, 1st ed., O’Reilly Media, p. 518 [Online]. Available: https://r4ds.had.co.nz/dates-and-times.html. [Accessed: Jan. 23, 2023]"
  },
  {
    "objectID": "graveyard.html#sec-math-logic",
    "href": "graveyard.html#sec-math-logic",
    "title": "24  Other Topics",
    "section": "\n24.1 Mathematical Logic",
    "text": "24.1 Mathematical Logic\nIn Chapter 9 and Chapter 11 we talk about more complicated data structures and control structures (for loops, if statements). I’ve included this section because it may be useful to review some concepts from mathematical logic.\nUnfortunately, to best demonstrate mathematical logic, I’m going to need you to know that a vector is like a list of the same type of thing. In R, vectors are defined using c(), so c(1, 2, 3) produces a vector with entries 1, 2, 3. In Python, we’ll primarily use numpy arrays, which we create using np.array([1, 2, 3]). Technically, this is creating a list, and then converting that list to a numpy array.\n\n24.1.1 And, Or, and Not\nWe can combine logical statements using and, or, and not.\n\n(X AND Y) requires that both X and Y are true.\n(X OR Y) requires that one of X or Y is true.\n(NOT X) is true if X is false, and false if X is true. Sometimes called negation.\n\nIn R, we use ! to symbolize NOT, in Python, we use ~ for vector-wise negation (NOT).\nOrder of operations dictates that NOT is applied before other operations. So NOT X AND Y is read as (NOT X) AND (Y). You must use parentheses to change the way this is interpreted.\n\n\nR\nPython\n\n\n\n\nx <- c(TRUE, FALSE, TRUE, FALSE)\ny <- c(TRUE, TRUE, FALSE, FALSE)\n\nx & y # AND\n## [1]  TRUE FALSE FALSE FALSE\nx | y # OR\n## [1]  TRUE  TRUE  TRUE FALSE\n!x & y # NOT X AND Y\n## [1] FALSE  TRUE FALSE FALSE\nx & !y # X AND NOT Y\n## [1] FALSE FALSE  TRUE FALSE\n\n\n\n\nimport numpy as np\nx = np.array([True, False, True, False])\ny = np.array([True, True, False, False])\n\nx & y\n## array([ True, False, False, False])\nx | y\n## array([ True,  True,  True, False])\n~x & y\n## array([False,  True, False, False])\nx & ~y\n## array([False, False,  True, False])\n\n\n\n\n\n24.1.2 De Morgan’s Laws\nDe Morgan’s Laws are a set of rules for how to combine logical statements. You can represent them in a number of ways:\n\nNOT(A or B) is equivalent to NOT(A) and NOT(B)\nNOT(A and B) is equivalent to NOT(A) or NOT(B)\n\n\n\nDefinitions\nDeMorgan’s First Law\nDeMorgan’s Second Law\n\n\n\n\n\nVenn Diagram of Set A and Set B\n\n\nSuppose that we set the convention that .\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A OR B (aka NOT (A OR B)) is the same as the region that is outside of (NOT A) and (NOT B)\n\n\n\n\n\n\nA venn diagram illustration of De Morgan’s laws showing that the region that is outside of the union of A AND B (aka NOT (A AND B)) is the same as the region that is outside of (NOT A) OR (NOT B)"
  },
  {
    "objectID": "graveyard.html#sec-controlling-loops",
    "href": "graveyard.html#sec-controlling-loops",
    "title": "24  Other Topics",
    "section": "\n24.2 Controlling Loops with Break, Next, Continue",
    "text": "24.2 Controlling Loops with Break, Next, Continue\n\n\nSometimes it is useful to control the statements in a loop with a bit more precision. You may want to skip over code and proceed directly to the next iteration, or, as demonstrated in the previous section with the break statement, it may be useful to exit the loop prematurely.\n\n24.2.1 Break Statement\n\n\nA break statement is used to exit a loop prematurely\n\n\n\n24.2.2 Next/Continue Statement\n\n\nA next (or continue) statement is used to skip the body of the loop and continue to the next iteration\n\n\n\n\n\n\n\n\nExample: Next/continue and Break statements\n\n\n\nLet’s demonstrate the details of next/continue and break statements.\nWe can do different things based on whether i is evenly divisible by 3, 5, or both 3 and 5 (thus divisible by 15)\n\n\nR\nPython\n\n\n\n\nfor (i in 1:20) {\n  if (i %% 15 == 0) {\n    print(\"Exiting now\")\n    break\n  } else if (i %% 3 == 0) {    \n    print(\"Divisible by 3\")\n    next\n    print(\"After the next statement\") # this should never execute\n  } else if (i %% 5 == 0) {\n    print(\"Divisible by 5\")\n  } else {\n    print(i)\n  }\n}\n## [1] 1\n## [1] 2\n## [1] \"Divisible by 3\"\n## [1] 4\n## [1] \"Divisible by 5\"\n## [1] \"Divisible by 3\"\n## [1] 7\n## [1] 8\n## [1] \"Divisible by 3\"\n## [1] \"Divisible by 5\"\n## [1] 11\n## [1] \"Divisible by 3\"\n## [1] 13\n## [1] 14\n## [1] \"Exiting now\"\n\n\n\n\nfor i in range(1, 20):\n  if i%15 == 0:\n    print(\"Exiting now\")\n    break\n  elif i%3 == 0:\n    print(\"Divisible by 3\")\n    continue\n    print(\"After the next statement\") # this should never execute\n  elif i%5 == 0:\n    print(\"Divisible by 5\")\n  else: \n    print(i)\n## 1\n## 2\n## Divisible by 3\n## 4\n## Divisible by 5\n## Divisible by 3\n## 7\n## 8\n## Divisible by 3\n## Divisible by 5\n## 11\n## Divisible by 3\n## 13\n## 14\n## Exiting now\n\n\n\n\n\n\nTo be quite honest, I haven’t really ever needed to use next/continue statements when I’m programming, and I rarely use break statements. However, it’s useful to know they exist just in case you come across a problem where you could put either one to use."
  },
  {
    "objectID": "graveyard.html#sec-recursion",
    "href": "graveyard.html#sec-recursion",
    "title": "24  Other Topics",
    "section": "\n24.3 Recursion",
    "text": "24.3 Recursion\nUnder construction.\nIn the meantime, check out [1] (R) and [2] (Python) for decent coverage of the basic idea of recursive functions."
  },
  {
    "objectID": "graveyard.html#sec-text-encoding",
    "href": "graveyard.html#sec-text-encoding",
    "title": "24  Other Topics",
    "section": "\n24.4 Text Encoding",
    "text": "24.4 Text Encoding\nI’ve left this section in because it’s a useful set of tricks, even though it does primarily deal with SAS.\nDon’t know what UTF-8 is? Watch this excellent YouTube video explaining the history of file encoding!\nSAS also has procs to accommodate CSV and other delimited files. PROC IMPORT may be the simplest way to do this, but of course a DATA step will work as well. We do have to tell SAS to treat the data file as a UTF-8 file (because of the japanese characters).\nWhile writing this code, I got an error of “Invalid logical name” because originally the filename was pokemonloc. Let this be a friendly reminder that your dataset names in SAS are limited to 8 characters in SAS.\n/* x \"curl https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv > ../data/pokemon_gen_1-8.csv\";\nonly run this once to download the file... */\nfilename pokeloc '../data/pokemon_gen_1-8.csv' encoding=\"utf-8\";\n\n\nproc import datafile = pokeloc out=poke\n  DBMS = csv; /* comma delimited file */\n  GETNAMES = YES\n  ;\nproc print data=poke (obs=10); /* print the first 10 observations */\n  run;\nAlternately (because UTF-8 is finicky depending on your OS and the OS the data file was created under), you can convert the UTF-8 file to ASCII or some other safer encoding before trying to read it in.\nIf I fix the file in R (because I know how to fix it there… another option is to fix it manually),\n\nlibrary(readr)\nlibrary(dplyr)\ntmp <- read_csv(\"https://raw.githubusercontent.com/shahinrostami/pokemon_dataset/master/pokemon_gen_1_to_8.csv\")[,-1]\nwrite_csv(tmp, \"../data/pokemon_gen_1-8.csv\")\n\ntmp <- select(tmp, -japanese_name) %>%\n  # iconv converts strings from UTF8 to ASCII by transliteration - \n  # changing the characters to their closest A-Z equivalents.\n  # mutate_all applies the function to every column\n  mutate_all(iconv, from=\"UTF-8\", to = \"ASCII//TRANSLIT\")\n\nwrite_csv(tmp, \"../data/pokemon_gen_1-8_ascii.csv\", na='.')\n\nThen, reading in the new file allows us to actually see the output.\nlibname classdat \"sas/\";\n/* Create a library of class data */\n\nfilename pokeloc  \"../data/pokemon_gen_1-8_ascii.csv\";\n\nproc import datafile = pokeloc out=classdat.poke\n  DBMS = csv /* comma delimited file */\n  replace;\n  GETNAMES = YES;\n  GUESSINGROWS = 1028 /* use all data for guessing the variable type */\n  ;\nproc print data=classdat.poke (obs=10); /* print the first 10 observations */\n  run; \nThis trick works in so many different situations. It’s very common to read and do initial processing in one language, then do the modeling in another language, and even move to a different language for visualization. Each programming language has its strengths and weaknesses; if you know enough of each of them, you can use each tool where it is most appropriate."
  },
  {
    "objectID": "graveyard.html#sec-other-topics-refs",
    "href": "graveyard.html#sec-other-topics-refs",
    "title": "24  Other Topics",
    "section": "\n24.5 References",
    "text": "24.5 References\n\n\n\n\n[1] \nDataMentor, “R recursion. DataMentor,” Nov. 24, 2017. [Online]. Available: https://www.datamentor.io/r-programming/recursion/. [Accessed: Jan. 10, 2023]\n\n\n[2] \nParewa Labs Pvt. Ltd., “Python recursion. Learn python interactively,” 2020. [Online]. Available: https://www.programiz.com/python-programming/recursion. [Accessed: Jan. 10, 2023]"
  }
]