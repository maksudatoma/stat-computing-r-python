# Web Scraping {#sec-data-web}

```{r, include=F}
knitr::knit_engines$set(html = function(options) {
  code <- paste(options$code, collapse = '\n')
  out  <- ""
  knitr::engine_output(options, code, out)
})
knitr::knit_engines$set(xml = function(options) {
  code <- paste(options$code, collapse = '\n')
  out  <- ""
  knitr::engine_output(options, code, out)
})
```


## Objectives {.nonumber}

-   Understand the structure of XML and HTML files
-   Use developer tools to locate nodes of interest and CSS or XPATH to precisely identify those nodes
-   Identify web pages that cannot be scraped using simple methods because of content injection
-   Scrape data from web pages
    -   by reading in HTML tables directly using R or Python
    -   by writing custom functions to pull data from individual fields

## HTML and XML Language Specification

Let's start at the beginning.
**eXtensible Markup Language (XML)** was developed in the late 1990s to provide a flexible specification for documents, data files, and various other documents.
You've (likely) been using XML-based documents for most of your life, either in Microsoft Office (.docx, .xlsx, .pptx - the x is for XML) or in web pages.
**HyperText Markup Language (HTML)** is a specific flavor of XML [@wikimediacontributorsXML2025] that uses a defined set of fields to specify the composition of a web page[^03-web-scraping-1].

[^03-web-scraping-1]: Technically, HTML was developed first, but it does have many of the same components as XML, and we use similar terminology to describe elements, content, attributes, and values, so it's easiest (imo) to think of HTML as a subset of XML with additional rules and with certain rules relaxed.

If you right click in your browser on this page, and "Inspect Source", you can see the HTML code that generates this book.

::: {#fig-browser-inspect layout-ncol="2"}
![Chromium right click menu](../images/advanced/chromium-inspect-source.png){fig-alt="A screenshot of https://duckduckgo.com in chromium, where the user has right-clicked on the page. The options 'View Page Source' and 'Inspect' are located at the bottom of the pop-up menu and are highlighted."}

![Firefox right click menu](../images/advanced/firefox-inspect-source.png){fig-alt="A screenshot of https://duckduckgo.com in Firefox, where the user has right-clicked on the page. The options 'View Page Source' and 'Inspect' are located towards the bottom of the pop-up menu and are highlighted."}

Right click menu options that allow the user to see the HTML code for a webpage.
:::

### XML

XML documents are composed of a series of tags that look like this:

```{html}
<tag-type attribute="value">Tag contents</tag-type>
```

Each tag name, `tag`, is enclosed in angle brackets, `<tag>`, and has a closing tag that starts with a slash but has the same name, `</tag>`.
Tags can include additional information that is typically encoded in `key="value"` pairs - above, there is an **attribute** field (the part before the `=`) that has **value** `"value"` (the part after the `=`).
A set of matching tags is called an **element**, and the information within the start and end tags is called the **content**.

The XML 1.0 (5th edition) specification requires that

-   The document contains only one element, called the **root** or document element, at the top level.
    No part of the root element appears in the content of any other element.

-   Any other elements
    -   are defined by start and end tags, and
    -   nest properly within each other (are well formed)

-   An element contains **text**, a sequence of characters.
    Legal characters are tab, carriage return, line feed, and the legal characters of [Unicode](https://home.unicode.org/) and [ISO/IEC 10646 (PDF warning)](https://unicode.org/L2/L2010/10038-fcd10646-main.pdf)[^03-web-scraping-2].

-   Comments: 
```{html}
<!-- This is a valid comment -->
<!-- This is not a valid comment ---> (The end must have exactly 2 dashes)
<!-- This is not a valid comment -- 
     two dashes cannot be included in the string -->
```

[^03-web-scraping-2]: This is a real rabbit hole if you are curious about the very technical details of what makes up a character in text.

::: advanced
#### Advanced: Additional XML metadata {.unnumbered}

A valid XML document also contains some additional metadata at the start of the file.
First, there should be an **XML declaration** that specifies the version of XML being used: `<?xml version="1.0"?>` (Note the `?` immediately before and after the angle brackets).

In addition, valid XML documents contain a **Document Type Definition (DTD)** that provides some information about valid tags used within the document.

```{xml}
<!DOCTYPE Name (ExternalID)? '[' intSubset ']' >
```

Document Type Definition requirements:

-   `Name` in the DTD has the same value as the root node tag name
-   An Internal or External Subset of valid entity names (tag names) is provided. It is also possible to specify valid tag attributes, types, and other information within these listings.

```{xml}
<?xml version="1.0"?> # <1> 
<!DOCTYPE greeting SYSTEM "hello.dtd"> # <2>
<greeting>Hello, world!</greeting>  # <3>
```

1.  XML version declaration
2.  Document Type Definition. `greeting` is the name, and the root node matches that name. `hello.dtd` is the system identifier and provides an address leading to an external document type declaration file
3.  Root node. `greeting` is the tag name. `Hello, world!` is the tag value. The tag is closed with a valid, matching end tag.

```{xml}
<?xml version="1.0" encoding="UTF-8" ?> # <1> 
<!DOCTYPE greeting [ # <2> 
  <!ELEMENT greeting (#PCDATA)>
]>
<greeting>Hello, world!</greeting>  # <3> 
```

1.  XML version declaration, specifying the document's text encoding
2.  An internally defined DTD that specifies valid elements (tags) (the only valid element is `greeting`). Additional tags could be specified within the `[` `]` space, if desired.
3.  The root node with value `Hello, world!`
:::

Elements are typically nested in an XML document.


Consider the following set of elements that describe the main characters in the children's TV show [Bluey](https://www.youtube.com/channel/UCVzLLZkDuFGAE2BGdBuBNBg). 
Note that the same information about the family could be described in a number of different hierarchical ways.

::: panel-tabset

##### Bluey {-}

```{xml}
<family>
  <adults>
    <person sex="M" coat="blue">Bandit Heeler</person>
    <person sex="F" coat="orange">Chili Heeler</person>
  </adults>
  <kids>
    <person sex="F" coat="blue">Bluey Heeler</person>
    <person sex="F" coat="orange">Bingo Heeler</person>
  </kids>
</family>
```

-   family contains two **child nodes** - 'adults', and 'kids'. Collectively, family has two direct **children**.
-   'adults' contains two **child nodes** - persons - that describe the adults in the family, Chili and Bandit Heeler
-   'kids' contains two **child nodes** - persons - that describe the kids in the family, Bluey and Bingo Heeler.
-   The **parent** of the element describing Bingo Heeler is 'kids'
-   The **parent** of the 'kids' element is 'family'
-   'adults' and 'kids' are **sibling** elements

##### Bluey Alt 1 {-}

```{xml}
<family>
    <person status="adult" sex="M" coat="blue"   > Bandit Heeler  </person>
    <person status="adult" sex="F" coat="orange" > Chili  Heeler  </person>
    <person status="kid"   sex="F" coat="blue"   > Bluey  Heeler  </person>
    <person status="kid"   sex="F" coat="orange" > Bingo  Heeler  </person>
</family>
```

##### Bluey Alt 2 {-}

```{xml}
<family>
    <person status="adult" sex="M" coat="blue"   > 
      <name><first>Bandit</first> <last>Heeler</last></name> 
    </person>
    <person status="adult" sex="F" coat="orange" > 
      <name><first>Chili </first> <last>Heeler</last></name> 
    </person>
    <person status="kid"   sex="F" coat="blue"   > 
      <name><first>Bluey </first> <last>Heeler</last></name> 
    </person>
    <person status="kid"   sex="F" coat="orange" > 
      <name><first>Bingo </first> <last>Heeler</last></name> 
    </person>
</family>
```


##### Bluey Alt 3 {-}

```{xml}
<family>
    <person>
      <name><first>Bandit</first> <last>Heeler</last></name> 
      <status>adult</status> 
      <sex>male</sex>   
      <coat>blue</coat>
    </person>
    <person>
      <name><first>Chili </first> <last>Heeler</last></name> 
      <status>adult</status> 
      <sex>female</sex> 
      <coat>orange</coat>
    </person>
    <person>
      <name><first>Bluey </first> <last>Heeler</last></name> 
      <status>child</status> 
      <sex>female</sex> 
      <coat>blue</coat>
    </person>
    <person>
      <name><first>Bingo </first> <last>Heeler</last></name> 
      <status>child</status> 
      <sex>female</sex> 
      <coat>orange</coat>
    </person>
</family>
```

:::

When working with XML (and HTML), it is important to understand how the data are represented structurally, so that you can get the components of the data back out of this hierarchical format.

::: demo
#### Demo: Exploring an XML file {.unnumbered}

The District of Columbia (Washington, DC) local government provides a [dataset of grocery store locations throughout the district](https://catalog.data.gov/dataset/grocery-store-locations) using a definition of full service grocery store that requires:

-   Sales of six of the following categories of food:
    -   Fresh fruits and vegetables
    -   Fresh and uncooked meats, poultry and seafood
    -   Dairy products
    -   Canned foods
    -   Frozen foods
    -   Dry groceries and baked goods
    -   Non-alcoholic beverages
-   Either 50% of the store's total square footage, or 6000 square feet must be dedicated to selling the food products above.
-   At least 5% of the selling area must be dedicated to each food category.

The city then included some small grocery stores that are very close to meeting the full-service grocery store definition based on [Appendix D of a city-wide food system assessment](https://dcfoodpolicy.org/wp-content/uploads/2019/06/2018-food-system-assessment-final-6.13.pdf) (PDF warning).

We can [download the KML file](https://opendata.dc.gov/api/download/v1/items/1d7c9d0e3aac49c1aa88d377a3bae430/kml?layers=4) and change the extension to XML so that the file opens in a standard web browser, or we can directly download the KML file as an XML file as in the code chunk below.

```{r, include = F}
options(width = 120)
```

```{r get-dc-grocery-data, error = T}
url <- "https://opendata.dc.gov/api/download/v1/items/1d7c9d0e3aac49c1aa88d377a3bae430/kml?layers=4"
filename <- "../data/DC_Grocery_Stores_2025.xml"

if(!file.exists(filename)) {
  download.file(url, destfile = filename, mode = "wb")
}

xml_text <- readLines(filename)
xml_tbl <- data.frame(line = 1:length(xml_text), 
                      xml = xml_text)
```

```{r echo=F}
#| paged-print: true

DT::datatable(xml_tbl[c(1:10, 35:64, 3145:3151),], rownames = F, escape = 2, editable = F, options = list(searching = F))
```


```{r, include = F}
options(width = 80)
```

-   The first line contains the document type definition and encoding.
-   The second line contains a link to the [KML specification](https://schemas.opengis.net/kml/2.2.0/ogckml22.xsd), which is itself another XML document. Note that it is also a valid tag, so at the end of the file, we should find </kml>.
-   The third line is another tag that indicates that this is the start of the document.
-   Lines 4-40 contain the dataset schema -- a list of all of the fields present for each store, along with their data type.
-   Line 41 defines a folder, which contains Placemarks (Line 42), which contain ExtendedData (Line 43). The `\t` characters are a text representation of tabs and indicate some indentation.
-   Each grocery store appears to be described by a Placemark node that contains ExtendedData nodes containing variables, along with a Point node that contains two coordinates (latitude and longitude).
:::

::: column-margin
![(Click to Enlarge) Screenshot of DC_Grocery_Stores_2025.xml showing the nodes corresponding to the Schema definition and to one individual grocery store.](../images/advanced/DC_Grocery_Stores_2025_xml_screenshot_annotated.png){.lightbox fig-alt="XML document with schema definition and store data for grocery store locations. The top section, labeled \"Schema Definition,\" is enclosed in a blue border and contains XML schema elements describing various data fields related to grocery store locations. The fields include store name, address, zip code, phone number, and other identifiers. Each field is defined by a SimpleField tag with a name and type attribute. The bottom section, labeled \"Individual Grocery Store Data,\" is enclosed in an orange border and provides detailed data about a specific grocery store location using XML tags. This includes the store’s name, address, phone number, GIS coordinates, and several other identifiers. The text \"Schema Definition\" and \"Individual Grocery Store Data\" are prominently displayed in larger font sizes within their respective sections."}
:::

### HTML

HTML is a markup language that appears very similar to XML.
Technically, HTML predates XML by a few years (1993 vs 1996) [@HTMLVsXML2024], but they've been developed in parallel and there are obvious influences in both directions.

Important differences between HTML and XML [@HTMLVsXML2024]:

-   HTML tags **display** information. XML tags **describe** information.
-   HTML uses **pre-defined** tags instead of XML's user-defined tags.
-   HTML doesn't always require closing tags, while XML does.
-   HTML is more robust than XML, in that it will ignore small errors.
-   HTML is not case sensitive, while XML is.
-   HTML ignores white space, but XML doesn't *necessarily* ignore white space.

::: demo

#### Demo: HTML Document Structure {.unnumbered}

Like XML, HTML documents have a basic structure:

```{html}
<!DOCTYPE html> <!-- <1> -->
<html> <!--  <2> -->
<head> <!--  <3> -->
<title>Page Title</title> <!--  <4> -->
</head>

<body> <!--  <5> -->
<h1>Level 1 Heading</h1>
<p>This paragraph provides information about the topic in the L1 heading.</p>
<h2>Level 2 Heading</h2>
<p>This paragraph provides even more information relating the L2 heading to L1.</p>
<br> <!--  <6> -->
</body>

</html> 
```

1.  This indicates that the document is an HTML5 document
2.  The root element of an HTML page
3.  The head element contains meta information about the page. Most scripts and formatting information (CSS) are also loaded in this element.
4.  The page title is what will show up in the browser tab.
5.  The body element contains the actual information rendered on the page.
6.  This element creates a line break, but has no content. Empty elements (elements with no content) do not have to have a closing tag (but you can add one if it makes you happy).

:::

You can easily see the HTML code that creates any website by right clicking on the web page in your browser and selecting some variant of "Inspect" or "Inspect Source" or "View Source", as shown in @fig-browser-inspect.
This is an incredibly helpful tool when you want to figure out how to pull data out of a webpage, getting only the parts you want without the rest of the clutter.

The best way to think about a HTML page is that it consists of a series of elements that are best thought of as boxes.
Elements like `<h1> </h1>` define a box with specific contents.
These boxes are then styled and arranged via the "magic" of *Cascading Style Sheets (CSS)*.
When I first learned HTML (circa 2001?), there were a reasonable number of tags and CSS was not really a thing, so it was easier to understand how web pages were laid out, and we could spend our time decking out webpages [with neon colors, animated images, and annoying music](https://www.cameronsworld.net/).

::: column-margin

![CSS is "magic" ... finicky and annoying magic.](../images/advanced/css-mug.jpg){#fig-css-awesome fig-alt="A mug that says 'CSS is Awesome' with a box around it, but the box cuts off the 'me' in awesome."}

:::

Now, there are too many valid HTML tags for to cover in an introduction, but first let's review the anatomy of an XML or HTML element, and then we can look at some of the most important HTML tags.

#### HTML Element Anatomy {.unnumbered}

$$\underbrace{\ \ \overbrace{< \text{p}\ \  \underbrace{\text{class}}_{\text{Attribute}}=\underbrace{\text{'important'}}_{\text{Value}} >}^{\text{Start tag}} \ \ \ \underbrace{\text{This is a paragraph}}_{\text{Content}} \ \ \ \overbrace{</ \text{p} >}^{\text{End tag}}\ \ \ }_{\text{Element}} $$

#### Important HTML Tags {.unnumbered}

-   `<h1>`, `<h2>`, `<h3>`, `<h4>`, `<h5>`, `<h6>`: Headings.\
    Headings should be nested, so you should never have content in `<h2>This is wrong</h2>` unless there is an `<h1> </h1>` element above it in the same block of content.\
    Corresponds to `#`, ..., `######` in markdown.

-   `<p>`: Paragraphs.
    All text that isn't part of some other tag should be contained in paragraph tags.

-   `<a>`: links.
    `<a href="https://google.com">This links to google</a>`\
    `href` is a tag attribute, and `https://google.com` is the attribute's value.

-   `<img>`: Images.
    `<img src="path/to/picture.jpg" alt="Alt-text description of the picture">`.
    As with line breaks, you do not have to have a `</img>` tag unless having unclosed tags bothers you.

-   `<table>`: Tables.
    In HTML, tables are constructed by row.
    Here is a minimal table that has a header:

:::: columns

::: {.column width="45%"}

HTML:

```{html}        
<table>   <!--  <1> -->
  <tr>   <!--  <2> -->
    <th>Heading Col 1</th>   <!--  <3> -->
    <th>Heading Col 2</th>
  </tr>
  <tr>
    <td>Value 1</td>   <!--  <4> -->
    <td>Value 2</td>
  </tr>
</table>
```

1. The table element contains all table contents
2. The tr element defines a new row of the table
3. The th element defines a header cell
4. The td element defines a normal (body) cell

:::

:::  {.column width="10%"}
:::

::: {.column width="45%"}

Rendered:
```{r, echo=F}
htmltools::knit_print.html("<table><tr><th>Heading Col 1</th><th>Heading Col 2</th></tr><tr><td>Value 1</td><td>Value 2</td></tr></table>")
```


:::

::::

-   `<ul>` and `<ol>`: Unordered (bulleted) and ordered lists.
    List elements are included as child elements and start/end with `<li></li>` (for both types of lists).

-   `<span>`: a container for inline content -- that is, content that is part of another piece of text.
    Spans are often used to highlight specific text using color or other formatting attributes (font, boldness, size).

-   `<div>`: a container for other content.
    These containers are often used to help style and lay out content using CSS.

#### Common HTML Attributes {.unnumbered}

Some attributes in HTML are special and are used across a number of different tag types.

-   ID: A shorthand name for the element. For instance, the ID for this level 4 section heading is `common-html-attributes`
-   Class: The category the element belongs to. Often, this is used to help style the element with appropriate CSS - for instance, to ensure that all level 4 headings have the same font size and color. There can be multiple values for the class attribute, separated by spaces. The class value for this level 4 section heading is `level4 unnumbered`, indicating both the heading level and that I've switched off numbering for this sub-sub-sub-sub-section.
-   Style: Any CSS items which apply only to the specific element. For this section, only the link to this section added by quarto/pandoc has a style attribute specified (because it is applied automatically when the quarto document is compiled)

::: {.demo collapse="true"}
##### Demo: HTML for the Common HTML Attributes subsection {.unnumbered}

I have added line-breaks and indentation for readability, but the rest of the code is copied from my browser.

```         
<section id="common-html-attributes" class="level4 unnumbered">
  <h4 class="unnumbered anchored" data-anchor-id="common-html-attributes">
    Common HTML Attributes
    <a class="anchorjs-link " aria-label="Anchor" data-anchorjs-icon="" 
       href="#common-html-attributes" 
       style="font: 1em / 1 anchorjs-icons; margin-left: 0.1875em; 
              padding-right: 0.1875em; padding-left: 0.1875em;"></a>
  </h4>
  <p>Some attributes in HTML are special and are used across a number of 
     different tag types.</p>
  <ul>
    <li>ID: A shorthand name for the element. 
        For instance, the ID for this level 4 section heading is 
        <code>common-html-attributes</code></li>
    <li>Class: The category the element belongs to. 
        Often, this is used to help style the element with appropriate CSS - 
        for instance, to ensure that all level 4 headings have the same font 
        size and color. 
        There can be multiple values for the class attribute, separated by spaces. 
        The class value for this level 4 section heading is 
        <code>level4 unnumbered</code>, 
        indicating both the heading level and that I’ve switched off numbering 
        for this sub-sub-sub-sub-section.</li>
    <li>Style: Any CSS items which apply only to the specific element. 
        For this section, only the link to this section added by quarto/pandoc 
        has a style attribute specified (because it is applied automatically 
        when the quarto document is compiled)</li>
  </ul>
</section>
```
:::

## Filtering Data from HTML/XML: CSS and XPath Selectors

Often, when pulling data from HTML or XML documents, it is useful to be able to navigate to one or more specific elements of the document and extract only those elements.
There are two different "languages" for doing this - CSS selectors, which primarily are used with HTML documents, and XPath selectors, which can be used with both HTML and XML.

::: learnmore
You may find it helpful to reference these guides to XPath and CSS selectors directly:

-   [XPath syntax](https://www.w3schools.com/xml/xpath_syntax.asp)
-   [CSS selectors](https://www.w3schools.com/cssref/css_selectors.php)
:::

### Selecting by element type

+--------------------------------------------------------------------------------------------------------+-----------+---------+
| Task                                                                                                   | CSS       | XPath   |
+========================================================================================================+===========+=========+
| Select all `p` elements from the document                                                              | `p`       | `//p`   |
+--------------------------------------------------------------------------------------------------------+-----------+---------+
| Select all `p` elements that are direct descendants of an `h1` element                                 | `h1 \> p` | `h1/p`  |
+--------------------------------------------------------------------------------------------------------+-----------+---------+
| Select all `p` elements that are descendants of an `h1` element, no matter where they are under `h1`   | `h1 p`    | `h1//p` |
+--------------------------------------------------------------------------------------------------------+-----------+---------+

CSS elements can be combined using **combinators** - [specific combination operators](https://www.w3schools.com/cssref/css_ref_combinators.php). 
There are also [XPath axes](https://www.w3schools.com/xml/xpath_axes.asp) that provide some useful ways to combine selectors to get specific results. 

### Selecting by element ID/Class

| Task | CSS | XPath | 
|----|----|----|
| Select the element with ID "objectives" | `#objectives` | `//[@id='objectives']` |
| Select the `<p>` element with ID "objectives" | `p#objectives` | `//p[@id='objectives']` |
| Select the element with class 'level2' | `.level2` | `//[@class='level2']` |
| Select the `<p>` element with class 'level2' | `p.level2` | `//p[@class='level2']` |

Of course, you can chain these selectors together just as in the previous section.

### Selecting by element attribute/value

| Task | CSS | XPath |
|----|----|----|
| Select all elements with a "dataID" attribute | `[dataID]` | `//@dataID` |
| Select all `<p>` elements with any attribute | not supported | `//p[@*]` | 
| Select all `<p>` elements with a "dataID" attribute | `p[dataID]` | `//p[@dataID]` |
| Select all elements with a "dataID" attribute equal to "mydata" | `[dataID='mydata']` | `//*[@dataID='mydata']`  |
| Select all elements with a "dataID" attribute containing "my" | `[dataID~='my']` or `[dataID*='my']` | `//*[contains(@dataID, 'mydata')]` |




:::: demo

### Demo: Reading in Olympics Medals from HTML {- #sec-demo-olympics-medals}

The IOC (International Olympic Committee) publishes medal counts for each Olympics, including the [2024 Paris games](https://www.olympics.com/en/olympic-games/paris-2024/medals). 
If the code below doesn't work, try using the [archived page](../data/Paris 2024 Olympic Medal Table - Gold, Silver & Bronze.html) instead. 
A screenshot of the rendered page is available in @fig-olympic-medal-page. 
Though the page *appears* to have a table, in fact, the table does not make use of an HTML table element; instead, each row is a series of `div` elements

::: panel-tabset
#### R 

```{r olympic-data-setup-r}
#| message: false
#| warning: false
library(rvest)
library(xml2)
library(tibble)
library(tidyr)
library(dplyr)
library(purrr)
library(stringr)

doc <- read_html("../data/Paris 2024 Olympic Medal Table - Gold, Silver & Bronze.html")

html_element(doc, "table") # <1>
```
1. Search the HTML for `<table>` -- no results



#### Python

```{python}
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup, SoupStrainer # <1>
import urllib.request # <2>
import html2text # <3>

req = urllib.request.Request( # <4>
  "https://www.olympics.com/en/olympic-games/paris-2024/medals",  # <4>
  data = None,  # <4>
  headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0'} # <3>
) # <4>

page_bytearray = urllib.request.urlopen(req) # <5>
page = page_bytearray.read()  # <5>
page_bytearray.close() # <5>
```
1. `bs4` is a library for web scraping. `BeautifulSoup` is its primary function.
2. `urllib` helps with web calls, and `request` allows us to set browser headers (so it looks like we're using e.g. Firefox instead of a programming language to access the web). 
3. `html2text` converts HTML to text, which is useful for debugging.
3. This requests the URL using headers that suggest we're using an older version of Firefox on Windows. 
4. These lines open the page, read it, and close the open page in memory (which is important because open files in memory can cause memory leaks and other badness)



Pandas has a function that will read in an HTML table to a data frame. Let's try that first. 

```{python}
#| error: true
pd.read_html(page)
```

Well, that didn't work so well, so let's look for an actual table element:

```{python}
soup = BeautifulSoup(page)
tables = soup.find_all("table")
tables
```

:::


Next, we look at the HTML and see that the table itself is wrapped in an element: 

```{html}
<div class="Table-styles__Wrapper-sc-7c5c517a-0 cqlSCS"></div>
```

That's too much to type, so search for elements with a class that contains "Table". 

::: panel-tabset

#### R 

```{r olympic-data-table-container-r}
table_nodes <- html_element(doc, css = "[class*='Table']")
table_nodes
```

#### Python

```{python}
table_nodes = soup.select("*[class*=Table]") # <1>
len(table_nodes) # <2>

[i.__str__()[0:100] for i in table_nodes] # <3>
```
1. Get all nodes that have class that starts with Table
2. How many nodes did we find?
3. Print out the first 100 characters of each element to see what we found without drowning in text.

:::

It can be hard to get the hang of "reading" HTML to make sense of it -- what I usually do is try to find the narrowest set of nodes I can that have the content I want. 
To identify this, I'll use a process like that shown in @fig-olympic-html-gif, where I'll right click on the web page in the browser, select "Inspect element", and then interactively hover over various HTML elements until I get a sense of what each component of the rendered page looks like in HTML. 

In this case, the 'minimal node' is: 

```{html}
<div data-cy="table-content" class="Table-styles__CommonGrid-sc-7c5c517a-1 Table-styles__Content-sc-7c5c517a-3 dTTMvn jfNiTo"></div>
```

It has just the content, without the sortable headers and other stuff that we don't particularly care about.
That is, the headers and such are important for context, but can be added in to the table relatively quickly (and thus aren't worth the time to scrape). 

I also noticed that in each "column" of a single row, there are div elements with attributes that contain the row number (`data-row-id` or `data-medal-id`), and that each div element has one or the other of these attributes, but not both. 
That means that if we can get the row numbers from the attributes and the column numbers based on the relative position of the elements, we can reconstruct the table structure from just a list of disjoint `div` elements. 

::: {.learnmore collapse=true}

#### Another Approach: Text Editor HTML Magic {-}

Sometimes, if I'm not feeling the interactive inspection in the browser, or if things are complicated and/or the inspection isn't going well, I'll take a more direct approach and copy a subset of the HTML out into a text editor. Then, I do a little bit of find-and-replace magic: 

1. Search for `<` and replace it with `\n<`, to get each tag on its own line in the text document. HTML may not care much about whitespace between text nodes, but it makes it a lot more human-readable. 
2. Search for `\n</` and replace it with `</` to get the end tags back on the same line as the corresponding start tag (at least for elements with no children). 

You can see the result of this find-and-replace magic in [this file](../data/Paris 2024 Olympic Medal Table - Gold, Silver & Bronze - table only.txt). 

In my simplified view, I start to notice a pattern of div-span-span. 
That is, each div element contains a couple of span elements, and there are several div- elements that show up for each row in the table.
Some of these elements have attribute `data-row-id` and some have attribute `data-medal-id`, but the values of each of these attributes have `row-##` where `##` is the row number in the table. 
As a result, it seems like we might want to focus primarily on the div elements and only work with the span elements if we absolutely have to. 

:::

There is always a "prettier" way to do HTML parsing, but in most cases it's faster and more straightforward to take a quick-and-dirty approach and only make things more complicated if necessary. 

Based on the basic structure I've described above, let's try to write code that 

1. Looks for the div elements that have attributes `data-row-id` and `data-medal-id`
2. Gets the text from these elements
3. Rearranges the text from (2) into rows and columns using a pivot operation
4. Converts medal counts to numeric variables, if they are characters, replacing '-' with `NA`

Each step in this outline may take more than one line of code.

::: panel-tabset

#### R 

The CSS selector we use here is a compound selector - it looks for div elements that have `data-row-id` or `data-medal-id` as an attribute. 

```{r olympic-data-scrape-r}
table_data <- html_elements(doc, css = "div[data-row-id],div[data-medal-id]")   # <1>

header_names <- c("row", "flag", "country_abb", # <2>
                  "gold", "silver", "bronze", "total", "country") # <2>

medal_data <- tibble(
  idx = 1:length(table_data), # <3>
  text = map_chr(table_data, html_text), # <4>
  medal_attributes = map_chr(table_data, ~html_attr(., "data-medal-id")), # <5>
  row_attributes = map_chr(table_data, ~html_attr(., "data-row-id")) # <6>
) |>
  mutate(row = if_else(is.na(medal_attributes), # <7>
                       str_extract(row_attributes, "\\d{1,}"), # <7>
                       str_extract(medal_attributes, "\\d{1,}"))) |> # <7>
  group_by(row) |> # <8>
  mutate(column = 1:n()) # <8>

```
1. Search for `div` nodes that have either `data-row-id` or `data-medal-id` attributes. 
2. Define a header row that corresponds to the 8 `div` elements in each row.
3. Create a table with the index of the nodes in the list created in step 1.
4. Convert each HTML node to text (quick and dirty option)
5. Get the `data-row-id` if it exists
6. Get the `data-medal-id` if it exists
7. Get the row number from `data-row-id` if `medal_attributes` is `NA`, and otherwise, get it from `data-medal-id.`
8. Group by row number and create column number, assuming the nodes are in row, column order. 


These steps account for the first two steps listed in the outline above. 
Next, we need to pivot the data to get the table as it is shown visually on the webpage, and then we need to convert the medal totals from character to numeric variables. 

```{r}
medal_data_wide <- medal_data |>
  select(-idx, -medal_attributes, -row_attributes) |> # <1>
  pivot_wider(names_from = column, values_from = text) |> # <2>
  set_names(header_names) |> # <3>
  mutate(across(gold:total, as.numeric)) # <4>
```
1. Remove columns that are no longer necessary so they don't interfere with pivot operation
2. Pivot to replicate structure of original table, using the `column` values as column names for now.
3. Set column names to descriptive labels
4. Convert gold, silver, bronze, and total columns to numeric variables, which causes `NA` to replace '-' in the original table. 


```{r}
#| echo: false
knitr::kable(head(medal_data), row.names = F)
```

#### Python

```{python}
table_nodes = soup.select("div[data-row-id],div[data-medal-id]") # <1>

header_names = ["flag", "country_abb", "gold", "silver", "bronze", "total", "country"] # <2>


medal_data = pd.DataFrame({
  "i": range(0, len(table_nodes)), # <3>
  "chr": [i.get_text() for i in table_nodes], # <4>
  "attr": [dict(i.attrs) for i in table_nodes] # <5>
  })
  
medal_data["row"] = [medal_data.attr[i]['data-row-id'] # <6>
                      if 'data-row-id' in medal_data.attr[i]  # <6>
                      else medal_data.attr[i]['data-medal-id']  # <6>
                      for i in medal_data.i] # <6>

```
1. This gets all of the `div` elements with a `data-row-id` or `data-medal-id` attribute
2. Create a list of column names (for later)
3. Create an index along `table_nodes` to match the row number
4. Get the text from `table_nodes` as it would be rendered in HTML
5. Create a dictionary of attributes for each of the entries in `table_nodes`. This will allow us to pull values out by attribute name.
6. This is a fairly long list comprehension (sorry). For each of the rows, we're going to check and see if `attr` has an entry `data-row-id`, and if so, we're going to pull the value out. If not, we'll pull out the value from `data-medal-id`. 


Next, we need to pivot the data to get the table as it is shown visually on the webpage, and then we need to convert the medal totals from character to numeric variables. 

```{python}
import re # <1>

medal_data['rownum'] = [re.search(r'\d+', i).group() for i in medal_data.row] # <2>
medal_data['colnum'] = medal_data.groupby('rownum').cumcount() + 1 # <3>
medal_data_wide = medal_data.loc[:,['chr', 'rownum', 'colnum']].pivot(columns='colnum', index = 'rownum') # <4>
medal_data_wide.columns = header_names # <5>

tmp = [pd.to_numeric(medal_data_wide[i], errors='coerce') for i in ['gold', 'silver', 'bronze', 'total']] # <6>

tmpfix = pd.DataFrame(tmp).transpose() # <7>
medal_data_wide[['gold', 'silver', 'bronze', 'total']] = tmpfix # <8>
```
1. Load regular expression library
2. Search for one or more consecutive digits in the row name
3. group by the row number and count the rows in each group to get columns
4. Select only the columns we need - row, col, and the text shown on the HTML page, and then pivot so that rows are rows and cols are cols
5. Set names to be sensible/meaningful.
6. For each of the four columns with medal totals, convert to numeric (coercing '-' to NaN). The result is a list of length $4$, and each element in the list is a list with $92$ entries. 
7. Convert the list of lists to a pandas DataFrame with dimensions $4\times 92$ and then transpose it so that it will fit back in our data frame. 
8. Store the numeric variables back into the wide data frame in the correct columns. 



```{r}
#| echo: false
library(reticulate)
knitr::kable(head(py$medal_data_wide), row.names = F)
```



:::
::: column-margin

![(Click to enlarge) Screenshot of the 2024 Paris Olympics Medal "Table"](../images/advanced/paris-2024-medal-table.png){.lightbox #fig-olympic-medal-page fig-alt="Screenshot of https://www.olympics.com/en/olympic-games/paris-2024/medals, showing gold, silver, and bronze medal counts for the first 7 countries listed alphabetically."}


![(Click to enlarge) Screen recording: Using the Developer Console to map web page elements to HTML code in order to identify attributes and document structure that can be parsed.](../images/advanced/Paris-2024-HTML-explorer.gif){.lightbox #fig-olympic-html-gif fig-alt="Screen recording of https://www.olympics.com/en/olympic-games/paris-2024/medals, showing the HTML corresponding to the first row of the table. Each div element has an attribute that contains information on the row of the table, and that information can be used along with the rendered text on the HTML page to reconstruct the table quickly."}

:::


::::

## Scraping Static Webpages

**Static** web pages have HTML that stays the same as the web page loads -- all of the content is specified in the HTML, and it doesn't change (much). 
Any scripting changes are cosmetic, in that they might change the appearance of the page somewhat but they don't e.g. insert a data table during the page loading process. 
These pages can be scraped using `rvest` and `xml2` in R or `bs4` (BeautifulSoup) in Python. 

### Reading in HTML Tables {#sec-html-table-direct}

In the simplest possible scenario, the data is already well-organized in a tabular structure (with actual `<table>` tags, unlike the example in @#sec-demo-olympics-medals). 

:::: demo

#### Demo: Reading HTML Tables Directly {-}

Consider the Wikipedia page containing a [List of American Revolutionary War Battles](https://en.wikipedia.org/wiki/List_of_American_Revolutionary_War_battles). 

::: panel-tabset

##### R {-}
We would use the `html_table()` function from `rvest` to read each of the tables on this webpage into a `tibble` object. 

```{r}
library(rvest)
rev_war_tables <- read_html("https://en.wikipedia.org/wiki/List_of_American_Revolutionary_War_battles") |>
  html_table()

length(rev_war_tables)
```

Ok, I wasn't expecting 33 tables! 
Looking at the page source, we can see that actual data tables (as opposed to tables used for formatting and arranging content^[you aren't supposed to do for accessibility purposes because it's hard to make sense of the information with a screen reader.]) are sortable and have class `class="sortable wikitable jquery-tablesorter"`. 
This is a good example of a static page that uses JavaScript (JQuery, to be specific) to enhance the page (e.g. making the table sortable) without actually injecting large amounts of content into the page. 

```{r}
library(rvest)
rev_war_battles <- read_html("https://en.wikipedia.org/wiki/List_of_American_Revolutionary_War_battles") |>
  html_element(".wikitable") |>
  html_table()

head(rev_war_battles)
```

##### Python {-}

:::


If all we wanted was the date of the battle and the state, we could stop here. 
However, if we want any supplemental information, we will need to use a more customized approach... continue on to @sec-html-custom to augment this data some more!

::::

### Scraping HTML with Custom Functions {#sec-html-custom}

Much of the time, the information we want from a website is not directly in tabular form, even if it may have a common structure. 
Consider, for instance, a school directory listing the names, email addresses, office numbers, and phone numbers of the faculty and staff. 
This could be formatted as a table (and we probably want the end result to be tabular), but more commonly it's structured on the web as a set of "Cards" formatted like the one shown in @fig-cartoon-photo-directory. 
Typically, these cards are generated automatically from a structured database, and if we are strategic, we can get the information back into a structured form by using custom functions to abstract out specific tasks. 
Often, in addition to the text, you will want to acquire image data, links, or data from linked pages along with the information presented on the landing page. 
Or, you may want to acquire information from several pages with similar structure -- for instance, you may want to get all departments at an institution, or information from all of the animal shelters in your state. 
Writing flexible, modular code is the best way to handle this challenge. 


:::: demo
#### Demo: Revolutionary War Battles {-}


If we want to get information from the table of revolutionary war battles that isn't just text (for instance, the link locations to the pages with more detailed information), then we need to write a bit more custom code -- first, to get the links from the table, and second to get additional information from the specific pages for each battle. 

In this case, it's easiest to think of the table as a set of rows with columns that have a specific order. 
We can use CSS pseudo-classes (things you can append to elements): `:first-child`, `:nth-child(2)`,  `:nth-child(3)` to get the information from the first 3 columns -- the fourth is text that we don't need right now, and the fifth is a citation. 

::: callout

In order to reduce the load on wikipedia and the compile time for the textbook, I have saved the artifacts from this example after running the code once, and am invisibly loading those saved objects rather than re-scraping everything each time this book is re-compiled. 

:::

::: panel-tabset

##### R {-}

```{r}
library(rvest)
library(dplyr)
doc <- read_html("https://en.wikipedia.org/wiki/List_of_American_Revolutionary_War_battles") 

rev_war_rows <- html_elements(doc, ".wikitable tr")
head(rev_war_rows)
```

Our operational node here will be each row of the table. To keep the data together (in case, for instance, some battle doesn't have its own page, and thus doesn't have a link), we need to get a node list of the rows first, and then use selectors on that list to get our data in a consistent order. 

```{r}
#| error: true
rev_war_battles <- tibble(
  name = html_elements(rev_war_rows, "td:first-child") |> html_text(),
  link = html_elements(rev_war_rows, "td:first-child > a") |> html_attr("href"),
  date = html_elements(rev_war_rows, "td:nth-child(2)") |> html_text(),
  location = html_elements(rev_war_rows, "td:nth-child(3)") |> html_text()
)

```

Unfortunately, that doesn't work either! One of the battles doesn't have a unique page, so there's a row mismatch. 
We need to process each row separately and then rbind them together, which is best done by mapping a processing function onto the list. 

```{r, eval=F}
library(purrr)

process_battle_row <- function(node){
  link = html_element(node, "td:first-child a") |> html_attr("href")
  tibble(
    name = html_element(node, "td:first-child") |> html_text(),
    link = if_else(length(link) == 0, NA, link),
    date = html_element(node, "td:nth-child(2)") |> html_text(),
    location = html_element(node, "td:nth-child(3)") |> html_text()
  )
}

rev_war_battles2 <- map(rev_war_rows, process_battle_row) |> bind_rows()
```
```{r, eval=F, include=F}
saveRDS(rev_war_battles2, "../data/rev-war-battles2.RDS")
```
```{r, echo=-1}
rev_war_battles2 <- readRDS("../data/rev-war-battles2.RDS")
head(rev_war_battles2)
```




##### Python {-}


:::
<!-- End panel tabset -->

Now that we have links to the individual pages (where they exist), we can supplement the data in the original table with data from the more detailed battle-level pages. 
One way to display this data might be to show an animated map that shows simultaneous actions in different locations. 
To do this, we need latitude and longitude that are at least approximate (and ideally, at a higher resolution than state-level). 
If we click on the [Powder Alarm](https://en.wikipedia.org/wiki/Powder_Alarm) battle page, we can see a structured table on the right that contains the location of the battle and links to [Suffolk County, Massachusetts](https://en.wikipedia.org/wiki/Suffolk_County,_Massachusetts). 
The page for Suffolk County, MA has a latitude and longitude. 

::: {#fig-rev-war-page-chain layout="[[45, -10, 45]]"}

![(Click to enlarge) Each battle has a formatted table on the right with a sub-table that has a location field.](../images/advanced/rev-war-battle-page.png){fig-alt="The image is a composite screenshot showing a Wikipedia article titled \"Powder Alarm\" on the left and a web inspector view on the right. The Wikipedia article discusses the historical event known as the Powder Alarm, including its background and significance. It features an infobox with a picture of the Powder House in Somerville, Massachusetts, and details such as the event's date, location, and key figures like Thomas Gage and Paul Revere prominently mentioned throughout the text. The web inspector panel on the right displays HTML code related to the page, with a section highlighted in blue. The page elements are structured into tables, revealing details like styles, links, and locations mentioned in the article." .lightbox}

![(Click to enlarge) Each location also has a formatted table on the right. GPS location is located within a span element with class `geo-dms`.](../images/advanced/rev-war-location-page.png){fig-alt='The image is a screenshot of a webpage and includes a section of HTML source code. The left side of the image displays a Wikipedia page about Suffolk County, Massachusetts. It features a historical photograph of the Old Suffolk County Courthouse and a map showing its location within the U.S. state of Massachusetts. The page provides details about the county, including its history, government, and politics. Visible text sections include a county overview table with information such as location, founded date, population, and time zone. A voter registration and party enrollment table is also shown. On the right side, the image shows the browser\'s Developer Tools interface, specifically the \"Inspector\" tab, which highlights a segment of the HTML code corresponding to the webpage\'s content. The code focuses on a table section displaying geographic coordinates with HTML elements and classes revealed. The cursor is highlighting the span containing the latitude and longitude.' .lightbox}

Two steps are necessary in order to get geographic information from the information provided in the table of revolutionary war battles. 
:::

So, our function needs to:

1. Identify the structured table on the battle page
2. Find the location row in that table
3. Look for a link in the location row and follow it, if it exists, to the linked location page
4. Look for a latitude/longitude entry in the location link page

If the structured table doesn't exist, the location row doesn't exist, or there is no location link, the function should return a data frame row with `lat=NA, long=NA`. 

::: panel-tabset

##### R {-}

```{r, eval=F}
library(tidyr)
library(tibble)

get_battle_location <- function(battle_link) {
  if(is.na(battle_link)) return(tibble(latitude = NA, longitude = NA, battle_link = NA))
  battle_url <- paste0("https://en.wikipedia.org/", battle_link)
  battle_doc <- try(read_html(battle_url))
  if("try-error" %in% class(battle_doc)) {
    return(tibble(latitude = NA, longitude = NA, battle_link = battle_link, location_link = NA))
  }
  
  # Get first (hopefully most specific) location link
  location_url <- html_node(battle_doc, ".infobox .location a") |> html_attr("href")
  if(is.na(location_url)) {
    return(tibble(latitude = NA, longitude = NA, battle_link = battle_link, location_link = NA))
  }
  location_doc <- read_html(paste0("https://en.wikipedia.org/", location_url))
  if("try-error" %in% class(location_doc)) {
    return(tibble(latitude = NA, longitude = NA, battle_link = battle_link, location_link = location_url))
  }
  
  # Get location coords
  location_node <- html_node(location_doc, ".geo-dms")
  tibble(
    type = location_node |> html_children() |> html_attr("class"),
    value = location_node |> html_children() |> html_text()
  ) |> pivot_wider(names_from = "type") |>
    mutate(battle_link = battle_link, location_link = location_url) # makes merging easier
}

battle_locations <- purrr::map(rev_war_battles2$link, get_battle_location)
battle_locations <- bind_rows(battle_locations)
```
```{r, include = F, eval = F}
saveRDS(battle_locations, file = "../data/rev_war_battle_locations.RDS")
```
```{r,echo=-1}
battle_locations <- readRDS("../data/rev_war_battle_locations.RDS")
battle_locations <- battle_locations |> unique() # filter out NA values that don't have any links at all

rev_war_battles3 <- left_join(rev_war_battles2, battle_locations, by = c("link" = "battle_link"))
head(rev_war_battles3)
```


##### Python {-}

:::
<!-- End panel tabset -->

A spatial package could be used to transform the latitude and longitude into numerical values for plotting, but we will stop here, as this example has demonstrated the utility of using custom functions to extract components of a webpage without cataloging all of the information. 

::::
<!-- End Demo -->

## Scraping Dynamic Webpages


::: demo

#### Demo: Roller Derby Skater Names


[Roller Derby](https://en.wikipedia.org/wiki/Roller_derby) is a team sport played on a roller skating rink, and most leagues seem to be female-oriented.
The Junior Roller Derby Association (JRDA) web page contains a [list of US leagues](
https://www.juniorrollerderby.org/us-leagues), and each league contains a list of registered organizations. Organizations can have multiple teams that are either open (anyone can join) or female-only, and each team has a [roster](https://www.juniorrollerderby.org/renegades) on the organization page. 
Suppose that our goal is to examine the names that players register with (examples: "Hitty Stardust", "Collide-a-scope", "Malice in Wonderland", "The Curly Dervish", "Rainbow Bite") that are often puns with slight hints of violence, gendered humor, or personal characteristics. 
In addition to players, we want to keep the league, organization, team, and team type (open/female-only). 

In order to scrape this data, we have to consider multiple levels because of the hierarchical nature of both the website and the organization. 
In these cases, it is often useful to write separate functions which scrape each level of the hierarchy. 

I'll show the screenshots first, since they're relevant to both R and python. 

![Roller Derby leagues - 5 tabs, and each tab has a list of organizations. Organization "blocks" of information are contained in divs that have class `textBlockElement` -- hopefully, that's enough of a selector to get the information.](../images/advanced/roller-derby-league-html-screenshot.png){fig-alt='The left side of the image shows a map of the United States divided by regions, each in distinct colors. The Northwest is in red, the Southwest in green, the Midwest in blue, the Northeast in yellow, and the Southeast in purple. Each state is labeled with its postal abbreviation. A legend in the bottom right corner matches the colors to the regions they represent. Below the map is the text "US Leagues Registered for the 2024 - 2025 Season." A menu appears with options like "NORTHWEST," "SOUTHWEST," "MIDWEST," "NORTHEAST," and "SOUTHEAST." The section labeled "RECREATIONAL" contains a logo for Pikes Peak Junior Roller Derby, located in Colorado Springs, CO. A dropdown menu labeled "NORTHWEST" is open, showing options such as "Open Link in New Tab" and "Inspect." The right side of the image displays a web browser's developer tools window. The HTML code is visible, highlighting a section related to tab content with a class of "selected"'. .lightbox}

![Roller Derby organizations are specified within the tab content div. There are several divs, only one of which is displayed (the others correspond to tabs which aren't selected). Organization "blocks" of information are contained in divs that have class `textBlockElement` -- hopefully, that's enough of a selector to get the information.](../images/advanced/roller-derby-team-html-screenshot.png){fig-alt='The image is a screenshot of a webpage and its corresponding HTML/CSS code displayed in a developer tools panel. The left side of the image shows a webpage listing US roller derby leagues registered for the 2024-2025 season. The page has a white background with blue and purple accents. Section headers divide leagues into "National" and "Recreational" categories. Logos of different roller derby teams are displayed along with their names and locations. Notable logos include the 509 Junior Roller Derby and Pikes Peak Junior Roller Derby. The right side is the developer tools interface, displaying HTML code and CSS rules corresponding to the webpage. Selected elements are highlighted, showing details such as class names and style properties.' .lightbox}

What's interesting is that the first time you click on a new tab, you see it "fill" with data. 
That is, the data isn't present when the page is loaded. 
This page is a dynamic webpage -- it is loading the data with scripts, and thus has to be treated more carefully. 

We can do this in one of two ways

1. Mirror the entire site using a command-line tool like `wget`, and then trawl through the downloaded information to see if there's a spreadsheet, table, CSV, or a bunch of HTML files that have the data present. 
2. Launch a remote-controlled browser, click on things using commands, and then read the HTML after the script has run. 

The second approach tends to be more successful (though the first is usually faster, if it works), and has the bonus effect of making you feel like a very powerful programmer (or mad scientist) as you sit back and watch your computer navigate the web according to your instructions. 
Unfortunately, I can't easily demonstrate the output of this approach

::: panel-tabset

##### R

```{r}
#| message: false
#| warning: false
library(rvest)
library(xml2)
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)
url <- "https://www.juniorrollerderby.org/us-leagues"

library(chromote)
doc <- read_html_live(url)
doc |>
  xml_nodes(".tabbedElement .tabContainer") |>
  xml_children()
```


:::

:::


## References {#sec-webscraping-refs}
